[
  ["test_jit", ["diff --git a/aten/src/ATen/native/BlasKernel.cpp b/aten/src/ATen/native/BlasKernel.cpp", "index 6a4f3d8168f..87182b3514d 100644", "--- a/aten/src/ATen/native/BlasKernel.cpp", "+++ b/aten/src/ATen/native/BlasKernel.cpp", "@@ -1,14 +1,13 @@", " #define TORCH_ASSERT_ONLY_METHOD_OPERATORS", "+#include <limits>", "+#include <algorithm>", "+#include <climits>", " #include <ATen/Config.h>", "-#include <ATen/OpMathType.h>", " #include <c10/core/ScalarType.h>", "+#include <c10/util/irange.h>", " #include <c10/util/Exception.h>", " #include <c10/util/complex.h>", "-#include <c10/util/irange.h>", "-#include <algorithm>", "-#include <climits>", "-#include <iostream>", "-#include <limits>", "+", " #if AT_BUILD_WITH_BLAS()", " extern \"C\" double ddot_(int *n, double *x, int *incx, double *y, int *incy);", " extern \"C\" void dscal_(int *n, double *a, double *x, int *incx);", "@@ -181,10 +180,9 @@ void gemv(char trans, int64_t m, int64_t n, scalar_t alpha, scalar_t *a, int64_t", "     return;", "   }", " ", "-  using opmath_t = at::opmath_type<scalar_t>;", "   if ((trans == 'T') || (trans == 't')) {", "     for (const auto i : c10::irange(n)) {", "-      opmath_t sum = 0;", "+      scalar_t sum = 0;", "       scalar_t *row_ = a + lda * i;", "       for (const auto j : c10::irange(m)) {", "         sum += x[j * incx] * row_[j];", "@@ -198,37 +196,15 @@ void gemv(char trans, int64_t m, int64_t n, scalar_t alpha, scalar_t *a, int64_t", "   } else {", "     if (beta != scalar_t(1) && beta != scalar_t(0)) scal<scalar_t>(m, beta, y, incy);", " ", "-    bool is_low_precision = !std::is_same<opmath_t, scalar_t>::value;", "-    std::vector<opmath_t> sum;", "-    if (is_low_precision) {", "-      sum.resize(m);", "-    }", "     for (const auto j : c10::irange(n)) {", "       scalar_t *column_ = a + lda * j;", "-      opmath_t z = alpha * static_cast<opmath_t>(x[j * incx]);", "+      scalar_t z = alpha * x[j * incx];", "       for (const auto i : c10::irange(m)) {", "         //output values are ignored if beta is 0, and set to 0, nans and infs are not propagated", "         if (j==0 && beta==scalar_t(0)) {", "-          if (!is_low_precision) {", "-            y[i * incy] = 0;", "-          }", "-        }", "-        if (is_low_precision) {", "-          sum[i] += z * column_[i];", "-        } else {", "-          y[i * incy] += z * column_[i];", "-        }", "-      }", "-    }", "-    if (is_low_precision) {", "-      if (beta == scalar_t(0)) {", "-        for (const auto i : c10::irange(m)) {", "-          y[i * incy] = sum[i];", "-        }", "-      } else {", "-        for (const auto i : c10::irange(m)) {", "-          y[i * incy] += sum[i];", "+         y[i * incy] = scalar_t(0);", "         }", "+        y[i * incy] += z * column_[i];", "       }", "     }", "   }", "@@ -287,12 +263,11 @@ scalar_t dot_naive(", "     Functor op) {", "   // NOLINTNEXTLINE(cppcoreguidelines-init-variables)", "   int64_t i;", "-  using opmath_t = at::opmath_type<scalar_t>;", "-  opmath_t sum = 0;", "+  scalar_t sum = 0;", "   for (i = 0; i < n; i++) {", "-    sum += op(static_cast<opmath_t>(x[i * incx]), static_cast<opmath_t>(y[i * incy]));", "+    sum += op(x[i * incx], y[i * incy]);", "   }", "-  return static_cast<scalar_t>(sum);", "+  return sum;", " }", " ", " } // namespace blas_impl", "diff --git a/aten/src/ATen/native/LinearAlgebra.cpp b/aten/src/ATen/native/LinearAlgebra.cpp", "index 5a4a0389a84..39998c35731 100644", "--- a/aten/src/ATen/native/LinearAlgebra.cpp", "+++ b/aten/src/ATen/native/LinearAlgebra.cpp", "@@ -1,23 +1,23 @@", " #define TORCH_ASSERT_ONLY_METHOD_OPERATORS", "+#include <ATen/core/Tensor.h>", " #include <ATen/Context.h>", " #include <ATen/Dispatch.h>", " #include <ATen/ExpandUtils.h>", " #include <ATen/NamedTensorUtils.h>", " #include <ATen/OpMathType.h>", "-#include <ATen/Parallel.h>", "-#include <ATen/TensorIndexing.h>", "-#include <ATen/TensorIterator.h>", "-#include <ATen/TensorOperators.h>", "-#include <ATen/TensorSubclassLikeUtils.h>", "-#include <ATen/TensorUtils.h>", "-#include <ATen/core/Tensor.h>", "+#include <ATen/native/mkldnn/Matmul.h>", " #include <ATen/native/CPUBlas.h>", " #include <ATen/native/LinearAlgebra.h>", " #include <ATen/native/LinearAlgebraUtils.h>", " #include <ATen/native/ReduceOps.h>", " #include <ATen/native/ReduceOpsUtils.h>", " #include <ATen/native/Resize.h>", "-#include <ATen/native/mkldnn/Matmul.h>", "+#include <ATen/Parallel.h>", "+#include <ATen/TensorIndexing.h>", "+#include <ATen/TensorIterator.h>", "+#include <ATen/TensorOperators.h>", "+#include <ATen/TensorUtils.h>", "+#include <ATen/TensorSubclassLikeUtils.h>", " #include <c10/util/accumulate.h>", " #include <c10/util/irange.h>", " #include <c10/util/variant.h>", "@@ -1533,16 +1533,14 @@ inline void baddbmm_cpu_kernel(const Tensor& result, const Tensor& self, const T", "   int64_t js = result.size(2);", "   int64_t ks = self.size(2);", " ", "-  using opmath_t = at::opmath_type<scalar_t>;", "-  opmath_t alpha = alpha_.to<opmath_t>();", "-  opmath_t beta = beta_.to<opmath_t>();", "+  scalar_t alpha = alpha_.to<scalar_t>();", "+  scalar_t beta = beta_.to<scalar_t>();", " ", "   auto r0 = result.accessor<scalar_t, 3>();", "   auto s0 = self.accessor<scalar_t, 3>();", "   auto m0 = mat2.accessor<scalar_t, 3>();", " ", "   int64_t grain_size = std::min(internal::GRAIN_SIZE / (is * js * ks), (int64_t)1);", "-  using opmath_t = at::opmath_type<scalar_t>;", "   parallel_for(0, bs, grain_size, [&](int64_t b_begin, int64_t b_end) {", "       for (const auto b : c10::irange(b_begin, b_end)) {", "         auto r1 = r0[b];", "@@ -1552,19 +1550,17 @@ inline void baddbmm_cpu_kernel(const Tensor& result, const Tensor& self, const T", "           auto r2 = r1[i];", "           auto s2 = s1[i];", "           for (const auto j : c10::irange(js)) {", "-            opmath_t acc_value = 0;//is_bmm ? opmath_t(0) : opmath_t(r2[j]);", "-            for (const auto k : c10::irange(ks)) {", "-              acc_value += static_cast<opmath_t>(s2[k]) *", "-                  static_cast<opmath_t>(m1[k][j]);", "-            }", "+            scalar_t &r = r2[j];", "             if (is_bmm) {", "-              r2[j] = acc_value;", "+              r = 0;", "+              for (const auto k : c10::irange(ks)) {", "+                r += s2[k] * m1[k][j];", "+              }", "             } else {", "               // For beta == 0, the r's value will be ignored, especially for nan value.", "-              if (beta == opmath_t{0}) {", "-                r2[j] = alpha * acc_value;", "-              } else {", "-                r2[j] = static_cast<opmath_t>(r2[j]) * beta + alpha * acc_value;", "+              r = beta == scalar_t(0) ? scalar_t(0) : beta * r;", "+              for (const auto k : c10::irange(ks)) {", "+                r += alpha * s2[k] * m1[k][j];", "               }", "             }", "           }", "diff --git a/aten/src/ATen/native/cpu/BlasKernel.cpp b/aten/src/ATen/native/cpu/BlasKernel.cpp", "index 3114e0b3918..e143d8a6546 100644", "--- a/aten/src/ATen/native/cpu/BlasKernel.cpp", "+++ b/aten/src/ATen/native/cpu/BlasKernel.cpp", "@@ -53,20 +53,15 @@ auto sum(int64_t N, Func f) {", "   return partial_sums[0];", " }", " ", "+", " template <typename scalar_t, typename opmath_t>", "-typename std::enable_if<std::is_same<scalar_t, opmath_t>::value, void>::type", "-gemm_notrans_(", "-    int64_t m,", "-    int64_t n,", "-    int64_t k,", "+void gemm_notrans_(", "+    int64_t m, int64_t n, int64_t k,", "     opmath_t alpha,", "-    const scalar_t* a,", "-    int64_t lda,", "-    const scalar_t* b,", "-    int64_t ldb,", "+    const scalar_t *a, int64_t lda,", "+    const scalar_t *b, int64_t ldb,", "     opmath_t beta,", "-    scalar_t* c,", "-    int64_t ldc) {", "+    scalar_t *c, int64_t ldc) {", "   // c *= beta", "   scale_(m, n, beta, c, ldc);", " ", "@@ -88,37 +83,6 @@ gemm_notrans_(", "   }", " }", " ", "-// std::is_same<scalar_t, at::BFloat16> || std::is_same<scalar_t, at::Half>", "-template <typename scalar_t, typename opmath_t>", "-typename std::enable_if<!std::is_same<scalar_t, opmath_t>::value, void>::type", "-gemm_notrans_(", "-    int64_t m,", "-    int64_t n,", "-    int64_t k,", "-    opmath_t alpha,", "-    const scalar_t* a,", "-    int64_t lda,", "-    const scalar_t* b,", "-    int64_t ldb,", "-    opmath_t beta,", "-    scalar_t* c,", "-    int64_t ldc) {", "-  // c += alpha * (a @ b)", "-  for (const auto i : c10::irange(m)) {", "-    for (const auto j : c10::irange(n)) {", "-      const auto dot = sum(k, [&](int64_t l) -> opmath_t {", "-        return static_cast<opmath_t>(a[l * lda + i]) *", "-            static_cast<opmath_t>(b[j * ldb + l]);", "-      });", "-      if (beta == opmath_t(0)) {", "-        c[j * ldc + i] = alpha * dot;", "-      } else {", "-        c[j * ldc + i] = beta * c[j * ldc + i] + alpha * dot;", "-      }", "-    }", "-  }", "-}", "-", " template <typename scalar_t, typename opmath_t>", " void gemm_transa_(", "     int64_t m, int64_t n, int64_t k,", "@@ -147,19 +111,13 @@ void gemm_transa_(", " }", " ", " template <typename scalar_t, typename opmath_t>", "-typename std::enable_if<std::is_same<scalar_t, opmath_t>::value, void>::type", "-gemm_transb_(", "-    int64_t m,", "-    int64_t n,", "-    int64_t k,", "+void gemm_transb_(", "+    int64_t m, int64_t n, int64_t k,", "     opmath_t alpha,", "-    const scalar_t* a,", "-    int64_t lda,", "-    const scalar_t* b,", "-    int64_t ldb,", "+    const scalar_t *a, int64_t lda,", "+    const scalar_t *b, int64_t ldb,", "     opmath_t beta,", "-    scalar_t* c,", "-    int64_t ldc) {", "+    scalar_t *c, int64_t ldc) {", "   // c *= beta", "   scale_(m, n, beta, c, ldc);", " ", "@@ -181,37 +139,6 @@ gemm_transb_(", "   }", " }", " ", "-// std::is_same<scalar_t, at::BFloat16> || std::is_same<scalar_t, at::Half>", "-template <typename scalar_t, typename opmath_t>", "-typename std::enable_if<!std::is_same<scalar_t, opmath_t>::value, void>::type", "-gemm_transb_(", "-    int64_t m,", "-    int64_t n,", "-    int64_t k,", "-    opmath_t alpha,", "-    const scalar_t* a,", "-    int64_t lda,", "-    const scalar_t* b,", "-    int64_t ldb,", "-    opmath_t beta,", "-    scalar_t* c,", "-    int64_t ldc) {", "-  // c += alpha * (a @ b.T)", "-  for (const auto i : c10::irange(m)) {", "-    for (const auto j : c10::irange(n)) {", "-      const auto dot = sum(k, [&](int64_t l) -> opmath_t {", "-        return static_cast<opmath_t>(a[l * lda + i]) *", "-            static_cast<opmath_t>(b[l * ldb + j]);", "-      });", "-      if (beta == opmath_t(0)) {", "-        c[j * ldc + i] = alpha * dot;", "-      } else {", "-        c[j * ldc + i] = beta * c[j * ldc + i] + alpha * dot;", "-      }", "-    }", "-  }", "-}", "-", " template <typename scalar_t, typename opmath_t>", " void gemm_transab_(", "     int64_t m, int64_t n, int64_t k,", "@@ -246,19 +173,13 @@ void gemm_core_(", "     const scalar_t *b, int64_t ldb,", "     opmath_t beta,", "     scalar_t *c, int64_t ldc) {", "-  if (transa == TransposeType::NoTranspose &&", "-      transb == TransposeType::NoTranspose) {", "+  if(transa == TransposeType::NoTranspose && transb == TransposeType::NoTranspose) {", "     return gemm_notrans_(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);", "-  } else if (", "-      transa == TransposeType::Transpose &&", "-      transb != TransposeType::Transpose) {", "+  } else if(transa == TransposeType::Transpose && transb != TransposeType::Transpose) {", "     gemm_transa_(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);", "-  } else if (", "-      transa == TransposeType::NoTranspose &&", "-      transb == TransposeType::Transpose) {", "+  } else if(transa == TransposeType::NoTranspose && transb == TransposeType::Transpose) {", "     gemm_transb_(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);", "-  } else { // transa == TransposeType::Transpose && transb ==", "-           // TransposeType::Transpose", "+  } else {  // transa == TransposeType::Transpose && transb == TransposeType::Transpose", "     gemm_transab_(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);", "   }", " }", "diff --git a/test/test_linalg.py b/test/test_linalg.py", "index b097437e06b..1bfe5677d47 100644", "--- a/test/test_linalg.py", "+++ b/test/test_linalg.py", "@@ -7347,53 +7347,6 @@ scipy_lobpcg  | {:10.2e}  | {:10.2e}  | {:6} | N/A", "         c = a.permute(0, 1, 3, 2).matmul(b)", "         self.assertEqual([c.min(), c.max(), c.sum()], [24, 24, 414720])", " ", "-    def test_bfloat16_accumulation_with_ref_path(self):", "-        # fix https://github.com/pytorch/pytorch/issues/95125", "-        # and https://github.com/pytorch/pytorch/issues/83863", "-        # for bf16 accumulation in gemm ref path", "-        def check_correctness(fn, *args):", "-            expected = fn(*args).bfloat16()", "-            with torch.backends.mkldnn.flags(enabled=False):", "-                def test():", "-                    bf16_args = (arg.bfloat16() for arg in args)", "-                    tmp_result = fn(*bf16_args)", "-                    return tmp_result", "-                c = test()", "-                assert (torch.all(c == expected)), \"Incorrect result with\\n\" \\", "-                                                   f\"expected: {expected}\\n\" \\", "-                                                   f\"got: {c}\\n\"", "-        # test matmul", "-        for transa in [True, False]:", "-            for transb in [True, False]:", "-                a = torch.ones(300, 300)", "-                b = torch.ones(300, 300)", "-                if transa:", "-                    a = a.transpose(0, 1).contiguous().transpose(0, 1)", "-                if transb:", "-                    b = b.transpose(0, 1).contiguous().transpose(0, 1)", "-                check_correctness(torch.matmul, a, b)", "-        # test bmm", "-        a = torch.ones(1, 1, 300)", "-        b = torch.ones(1, 300, 1)", "-        check_correctness(torch.bmm, a, b)", "-        # test baddbmm", "-        a = torch.ones(1, 1, 300)", "-        b = torch.ones(1, 300, 1)", "-        c = torch.ones(1, 1, 1)", "-        check_correctness(torch.baddbmm, c, a, b)", "-        # test mv/addmv", "-        for trans in [True, False]:", "-            c = torch.ones(300) * -300", "-            a = torch.ones(300, 300)", "-            if trans:", "-                a = a.transpose(0, 1).contiguous().transpose(0, 1)", "-            b = torch.ones(300)", "-            check_correctness(torch.mv, a, b)", "-            check_correctness(torch.addmv, c, a, b)", "-        # test dot", "-        a = torch.ones(300)", "-        b = torch.ones(300)", "-        check_correctness(torch.dot, a, b)", " ", " instantiate_device_type_tests(TestLinalg, globals())", " ", "diff --git a/test/test_sparse_csr.py b/test/test_sparse_csr.py", "index 2e5e93f031e..54c460e27b7 100644", "--- a/test/test_sparse_csr.py", "+++ b/test/test_sparse_csr.py", "@@ -2466,7 +2466,6 @@ class TestSparseCSR(TestCase):", " ", "     @onlyCPU", "     @dtypes(torch.float32, torch.float64, torch.bfloat16)", "-    @precisionOverride({torch.bfloat16: 0.01})", "     def test_sparse_mm_reduce_sum(self, device, dtype):", "         def run_test(m, n, k, nnz, train):", "             sparse = self.genSparseCSRTensor((m, k), nnz, dtype=dtype, device=device, index_dtype=torch.int64)"]],
  ["distributed.test_dynamo_distributed", ["diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake", "index 015038e2fe4..f62f3986283 100644", "--- a/cmake/Dependencies.cmake", "+++ b/cmake/Dependencies.cmake", "@@ -1288,7 +1288,7 @@ if(USE_ROCM)", "     # host linker to link.", "     list(APPEND HIP_CLANG_FLAGS -fno-gpu-rdc)", "     foreach(pytorch_rocm_arch ${PYTORCH_ROCM_ARCH})", "-      list(APPEND HIP_CLANG_FLAGS --offload-arch=${pytorch_rocm_arch})", "+      list(APPEND HIP_CLANG_FLAGS --amdgpu-target=${pytorch_rocm_arch})", "     endforeach()", " ", "     set(Caffe2_HIP_INCLUDE", "diff --git a/cmake/public/LoadHIP.cmake b/cmake/public/LoadHIP.cmake", "index ba3959297c9..b51284115f1 100644", "--- a/cmake/public/LoadHIP.cmake", "+++ b/cmake/public/LoadHIP.cmake", "@@ -313,4 +313,5 @@ if(HIP_FOUND)", "   find_library(ROCM_HIPRTC_LIB ${hip_library_name} HINTS ${HIP_PATH}/lib)", "   # roctx is part of roctracer", "   find_library(ROCM_ROCTX_LIB roctx64 HINTS ${ROCTRACER_PATH}/lib)", "+  set(roctracer_INCLUDE_DIRS ${ROCTRACER_PATH}/include)", " endif()", "diff --git a/torch/CMakeLists.txt b/torch/CMakeLists.txt", "index f1a1fec8154..280d7a14c9d 100644", "--- a/torch/CMakeLists.txt", "+++ b/torch/CMakeLists.txt", "@@ -139,6 +139,7 @@ if(USE_ROCM)", "       __HIP_PLATFORM_HCC__", "       )", "     list(APPEND TORCH_PYTHON_LINK_LIBRARIES ${ROCM_ROCTX_LIB})", "+    list(APPEND TORCH_PYTHON_INCLUDE_DIRECTORIES ${roctracer_INCLUDE_DIRS})", " endif()", " ", " if(USE_EXPERIMENTAL_CUDNN_V8_API)", "diff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py", "index 91b27b9018a..b236ef606e6 100644", "--- a/torch/utils/cpp_extension.py", "+++ b/torch/utils/cpp_extension.py", "@@ -200,6 +200,7 @@ CUDA was not found on the system, please set the CUDA_HOME or the CUDA_PATH", " environment variable or add NVCC to your system PATH. The extension compilation will fail.", " '''", " ROCM_HOME = _find_rocm_home()", "+MIOPEN_HOME = _join_rocm_home('miopen') if ROCM_HOME else None", " HIP_HOME = _join_rocm_home('hip') if ROCM_HOME else None", " IS_HIP_EXTENSION = True if ((ROCM_HOME is not None) and (torch.version.hip is not None)) else False", " ROCM_VERSION = None", "@@ -1141,6 +1142,10 @@ def include_paths(cuda: bool = False) -> List[str]:", "     if cuda and IS_HIP_EXTENSION:", "         paths.append(os.path.join(lib_include, 'THH'))", "         paths.append(_join_rocm_home('include'))", "+        if MIOPEN_HOME is not None:", "+            paths.append(os.path.join(MIOPEN_HOME, 'include'))", "+        if HIP_HOME is not None:", "+            paths.append(os.path.join(HIP_HOME, 'include'))", "     elif cuda:", "         cuda_home_include = _join_cuda_home('include')", "         # if we have the Debian/Ubuntu packages for cuda, we get /usr as cuda home.", "@@ -1799,7 +1804,7 @@ def _get_rocm_arch_flags(cflags: Optional[List[str]] = None) -> List[str]:", "     # (from `extra_compile_args`)", "     if cflags is not None:", "         for flag in cflags:", "-            if 'amdgpu-target' in flag or 'offload-arch' in flag:", "+            if 'amdgpu-target' in flag:", "                 return ['-fno-gpu-rdc']", "     # Use same defaults as used for building PyTorch", "     # Allow env var to override, just like during initial cmake build.", "@@ -1812,7 +1817,7 @@ def _get_rocm_arch_flags(cflags: Optional[List[str]] = None) -> List[str]:", "             archs = []", "     else:", "         archs = _archs.replace(' ', ';').split(';')", "-    flags = ['--offload-arch=%s' % arch for arch in archs]", "+    flags = ['--amdgpu-target=%s' % arch for arch in archs]", "     flags += ['-fno-gpu-rdc']", "     return flags"]],
  ["functorch.test_aotdispatch", ["diff --git a/aten/src/ATen/native/UpSample.cpp b/aten/src/ATen/native/UpSample.cpp", "index 0c13791bb3b..02cf9a6864c 100644", "--- a/aten/src/ATen/native/UpSample.cpp", "+++ b/aten/src/ATen/native/UpSample.cpp", "@@ -12,8 +12,7 @@ namespace upsample {", " TORCH_API c10::SmallVector<int64_t, 3> compute_output_size(", "     c10::IntArrayRef input_size,  // Full input tensor size.", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<c10::ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "+    c10::optional<c10::ArrayRef<double>> scale_factors) {", "   const auto spatial_dimensions = static_cast<int64_t>(input_size.size()) - 2;", "   if (output_size) {", "     TORCH_CHECK(!scale_factors, \"Must specify exactly one of output_size and scale_factors\");", "@@ -25,10 +24,7 @@ TORCH_API c10::SmallVector<int64_t, 3> compute_output_size(", "     TORCH_CHECK(static_cast<int64_t>(scale_factors->size()) == spatial_dimensions);", "     c10::SmallVector<int64_t, 3> ret;", "     for (const auto i : c10::irange(spatial_dimensions)) {", "-      const double d = round_with_scale_factor ? 0.5 : 0.0;", "-      // if round_with_scale_factor=true we perform round (i.e. int(0.5 + x)) to match opencv, scipy,", "-      // scikit-image output size", "-      const double odim = d + static_cast<double>(input_size[i+2]) * scale_factors.value()[i];", "+      const double odim = static_cast<double>(input_size[i+2]) * scale_factors.value()[i];", "       ret.push_back(c10::checked_convert<int64_t>(odim, \"int64_t\"));", "     }", "     return ret;", "diff --git a/aten/src/ATen/native/UpSample.h b/aten/src/ATen/native/UpSample.h", "index d75aff46195..92ee7252d1b 100644", "--- a/aten/src/ATen/native/UpSample.h", "+++ b/aten/src/ATen/native/UpSample.h", "@@ -52,8 +52,7 @@ namespace upsample {", " TORCH_API c10::SmallVector<int64_t, 3> compute_output_size(", "     c10::IntArrayRef input_size,  // Full input tensor size.", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<c10::ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor=false);", "+    c10::optional<c10::ArrayRef<double>> scale_factors);", " ", " inline c10::optional<double> get_scale_value(c10::optional<c10::ArrayRef<double>> scales, int idx) {", "   if (!scales) {", "diff --git a/aten/src/ATen/native/UpSampleBicubic2d.cpp b/aten/src/ATen/native/UpSampleBicubic2d.cpp", "index 8df7f1eabd5..99970e2f36f 100644", "--- a/aten/src/ATen/native/UpSampleBicubic2d.cpp", "+++ b/aten/src/ATen/native/UpSampleBicubic2d.cpp", "@@ -280,9 +280,8 @@ Tensor upsample_bicubic2d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "     bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return at::upsample_bicubic2d(input, osize, align_corners, scale_h, scale_w);", "@@ -292,9 +291,8 @@ Tensor _upsample_bicubic2d_aa(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "     bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return at::_upsample_bicubic2d_aa(input, osize, align_corners, scale_h, scale_w);", "diff --git a/aten/src/ATen/native/UpSampleBilinear2d.cpp b/aten/src/ATen/native/UpSampleBilinear2d.cpp", "index efaea938db6..5d91e93e016 100644", "--- a/aten/src/ATen/native/UpSampleBilinear2d.cpp", "+++ b/aten/src/ATen/native/UpSampleBilinear2d.cpp", "@@ -162,9 +162,8 @@ Tensor upsample_bilinear2d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "     bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return at::upsample_bilinear2d(input, osize, align_corners, scale_h, scale_w);", "@@ -174,9 +173,8 @@ Tensor _upsample_bilinear2d_aa(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "     bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return at::_upsample_bilinear2d_aa(input, osize, align_corners, scale_h, scale_w);", "diff --git a/aten/src/ATen/native/UpSampleLinear1d.cpp b/aten/src/ATen/native/UpSampleLinear1d.cpp", "index 7a4fca3bebe..aed082b6856 100644", "--- a/aten/src/ATen/native/UpSampleLinear1d.cpp", "+++ b/aten/src/ATen/native/UpSampleLinear1d.cpp", "@@ -93,9 +93,8 @@ Tensor upsample_linear1d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "     bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_w = get_scale_value(scale_factors, 0);", "   return at::upsample_linear1d(input, osize, align_corners, scale_w);", " }", "diff --git a/aten/src/ATen/native/UpSampleNearest1d.cpp b/aten/src/ATen/native/UpSampleNearest1d.cpp", "index 29da56b725e..1bdbda8f66c 100644", "--- a/aten/src/ATen/native/UpSampleNearest1d.cpp", "+++ b/aten/src/ATen/native/UpSampleNearest1d.cpp", "@@ -126,9 +126,8 @@ using at::native::upsample::get_scale_value;", " Tensor upsample_nearest1d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_w = get_scale_value(scale_factors, 0);", "   return at::upsample_nearest1d(input, osize, scale_w);", " }", "@@ -136,9 +135,8 @@ Tensor upsample_nearest1d(", " Tensor _upsample_nearest_exact1d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_w = get_scale_value(scale_factors, 0);", "   return at::_upsample_nearest_exact1d(input, osize, scale_w);", " }", "diff --git a/aten/src/ATen/native/UpSampleNearest2d.cpp b/aten/src/ATen/native/UpSampleNearest2d.cpp", "index 5c2439995f4..65e20b78f86 100644", "--- a/aten/src/ATen/native/UpSampleNearest2d.cpp", "+++ b/aten/src/ATen/native/UpSampleNearest2d.cpp", "@@ -150,9 +150,8 @@ using at::native::upsample::get_scale_value;", " Tensor upsample_nearest2d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return at::upsample_nearest2d(input, osize, scale_h, scale_w);", "@@ -161,9 +160,8 @@ Tensor upsample_nearest2d(", " Tensor _upsample_nearest_exact2d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return at::_upsample_nearest_exact2d(input, osize, scale_h, scale_w);", "diff --git a/aten/src/ATen/native/UpSampleNearest3d.cpp b/aten/src/ATen/native/UpSampleNearest3d.cpp", "index 900c9630102..27ca6745655 100644", "--- a/aten/src/ATen/native/UpSampleNearest3d.cpp", "+++ b/aten/src/ATen/native/UpSampleNearest3d.cpp", "@@ -165,9 +165,8 @@ using at::native::upsample::get_scale_value;", " Tensor upsample_nearest3d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_d = get_scale_value(scale_factors, 0);", "   auto scale_h = get_scale_value(scale_factors, 1);", "   auto scale_w = get_scale_value(scale_factors, 2);", "@@ -177,9 +176,8 @@ Tensor upsample_nearest3d(", " Tensor _upsample_nearest_exact3d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_d = get_scale_value(scale_factors, 0);", "   auto scale_h = get_scale_value(scale_factors, 1);", "   auto scale_w = get_scale_value(scale_factors, 2);", "diff --git a/aten/src/ATen/native/UpSampleTrilinear3d.cpp b/aten/src/ATen/native/UpSampleTrilinear3d.cpp", "index 282a7d79480..1bf9c8f6cb4 100644", "--- a/aten/src/ATen/native/UpSampleTrilinear3d.cpp", "+++ b/aten/src/ATen/native/UpSampleTrilinear3d.cpp", "@@ -103,9 +103,8 @@ Tensor upsample_trilinear3d(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "     bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_d = get_scale_value(scale_factors, 0);", "   auto scale_h = get_scale_value(scale_factors, 1);", "   auto scale_w = get_scale_value(scale_factors, 2);", "diff --git a/aten/src/ATen/native/cuda/UpSample.cuh b/aten/src/ATen/native/cuda/UpSample.cuh", "index d950fdcff72..09e460640df 100644", "--- a/aten/src/ATen/native/cuda/UpSample.cuh", "+++ b/aten/src/ATen/native/cuda/UpSample.cuh", "@@ -17,8 +17,7 @@ namespace upsample {", " TORCH_API c10::SmallVector<int64_t, 3> compute_output_size(", "     c10::IntArrayRef input_size,  // Full input tensor size.", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<c10::ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor=false);", "+    c10::optional<c10::ArrayRef<double>> scale_factors);", " } // namespace upsample", " ", " namespace upsample_cuda {", "diff --git a/aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm b/aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm", "index 6cfc96e3732..39524569bae 100644", "--- a/aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm", "+++ b/aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm", "@@ -18,11 +18,10 @@ namespace metal {", " Tensor upsample_nearest2d_vec(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "+    c10::optional<ArrayRef<double>> scale_factors) {", "   TORCH_CHECK(input.is_metal());", "   auto osize =", "-      upsample::compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+      upsample::compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = upsample::get_scale_value(scale_factors, 0);", "   auto scale_w = upsample::get_scale_value(scale_factors, 1);", "   int64_t output_height = osize[0];", "diff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml", "index 4938ccf791c..354b9aaac67 100644", "--- a/aten/src/ATen/native/native_functions.yaml", "+++ b/aten/src/ATen/native/native_functions.yaml", "@@ -11822,53 +11822,53 @@", "   dispatch:", "     CompositeImplicitAutograd: pad_symint", " ", "-- func: upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_linear1d.vec_out", " ", "-- func: upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_bilinear2d.vec_out", "   tags: core", " ", "-- func: _upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: _upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: _upsample_bilinear2d_aa.vec_out", " ", "-- func: upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_trilinear3d.vec_out", " ", "-- func: upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_bicubic2d.vec_out", " ", "-- func: _upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: _upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: _upsample_bicubic2d_aa.vec_out", " ", "-- func: upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_nearest1d.vec_out", " ", "-- func: _upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: _upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: _upsample_nearest_exact1d.vec_out", " ", "-- func: upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_nearest2d.vec_out", "   tags: core", " ", "-- func: _upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: _upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: _upsample_nearest_exact2d.vec_out", " ", "-- func: upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: upsample_nearest3d.vec_out", " ", "-- func: _upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors, bool round_with_scale_factor=False) -> Tensor", "+- func: _upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "   python_module: nn", "   autogen: _upsample_nearest_exact3d.vec_out", " ", "diff --git a/aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp b/aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp", "index 53ab306d251..ac0fb23eb4c 100644", "--- a/aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp", "+++ b/aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp", "@@ -222,10 +222,9 @@ using at::native::upsample::get_scale_value;", " Tensor upsample_bilinear2d_quantized_cpu(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    bool align_corners,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+      bool align_corners,", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return upsample_bilinear2d_quantized_cpu(input, osize, align_corners, scale_h, scale_w);", "diff --git a/aten/src/ATen/native/quantized/cpu/UpSampleNearest2d.cpp b/aten/src/ATen/native/quantized/cpu/UpSampleNearest2d.cpp", "index 6d3cca35bf1..abe6dfd2258 100644", "--- a/aten/src/ATen/native/quantized/cpu/UpSampleNearest2d.cpp", "+++ b/aten/src/ATen/native/quantized/cpu/UpSampleNearest2d.cpp", "@@ -221,9 +221,8 @@ Tensor _upsample_nearest_exact2d_quantized_cpu(", " Tensor upsample_nearest2d_quantized_cpu(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return upsample_nearest2d_quantized_cpu(input, osize, scale_h, scale_w);", "@@ -232,9 +231,8 @@ Tensor upsample_nearest2d_quantized_cpu(", " Tensor _upsample_nearest_exact2d_quantized_cpu(", "     const Tensor& input,", "     at::OptionalIntArrayRef output_size,", "-    c10::optional<ArrayRef<double>> scale_factors,", "-    bool round_with_scale_factor) {", "-  auto osize = compute_output_size(input.sizes(), output_size, scale_factors, round_with_scale_factor);", "+    c10::optional<ArrayRef<double>> scale_factors) {", "+  auto osize = compute_output_size(input.sizes(), output_size, scale_factors);", "   auto scale_h = get_scale_value(scale_factors, 0);", "   auto scale_w = get_scale_value(scale_factors, 1);", "   return _upsample_nearest_exact2d_quantized_cpu(input, osize, scale_h, scale_w);", "diff --git a/test/test_nn.py b/test/test_nn.py", "index 805fea48905..401969c410d 100644", "--- a/test/test_nn.py", "+++ b/test/test_nn.py", "@@ -6253,7 +6253,7 @@ tensor(..., device='meta', size=(1,), requires_grad=True)\"\"\")", " ", " ", "     def test_channel_shuffle_return_self(self):", "-        # gh-76616: nn.ChannelShuffle will return self with an empty input tensor", "+        # gh-76616: nn.ChannelShuffle will return self with an  empty input tensor", "         groups = 3", "         input_tensor = torch.rand([0, 9, 4, 4])", "         output = torch.nn.ChannelShuffle(groups)(input_tensor)", "@@ -6280,18 +6280,6 @@ tensor(..., device='meta', size=(1,), requires_grad=True)\"\"\")", "                     else:", "                         gradcheck(lambda x: F.interpolate(x, scale_factor=scale_factor, **kwargs), (input,))", " ", "-        # Check https://github.com/pytorch/pytorch/issues/62396", "-        test_scales = [0.1234, 0.9999, 1.8]", "-        isize = 32", "-        expected_out_sizes = [int(0.5 + s * isize) for s in test_scales]", "-        t_in = torch.randint(0, 256, size=(1, 1, isize), dtype=torch.float)", "-        for r in [True, False]:", "-            for s, expected_osize in zip(test_scales, expected_out_sizes):", "-                t_out = F.interpolate(", "-                    t_in, scale_factor=s, recompute_scale_factor=r, mode=\"nearest\"", "-                )", "-                self.assertEqual(t_out.shape[-1], expected_osize)", "-", "     def test_upsamplingLinear1d_spatial_invariance(self):", "         m = nn.Upsample(scale_factor=3, mode='linear', align_corners=False)", "         in_t_9 = torch.zeros(1, 1, 9)", "@@ -6303,20 +6291,17 @@ tensor(..., device='meta', size=(1,), requires_grad=True)\"\"\")", " ", "     def test_upsampling_not_recompute_scale_factor(self):", "         # test output against known input: result must match opencv", "-        # opencv gives output of shape (5, 5, 2)", "         in_t = torch.arange(8.).view(1, 2, 2, 2)", "         expected_out_t = torch.tensor(", "-            [[[[-0.32725, -0.08843, 0.37933, 0.79744, 0.88296],", "-              [0.15039, 0.38921, 0.85697, 1.27508, 1.3606],", "-              [1.08591, 1.32473, 1.79249, 2.21060, 2.29613],", "-              [1.92213, 2.16095, 2.62871, 3.04682, 3.13234],", "-              [2.09318, 2.33200, 2.79976, 3.21787, 3.30340]],", "-", "-             [[3.67275, 3.91157, 4.37933, 4.79744, 4.88296],", "-              [4.15039, 4.38921, 4.85697, 5.27508, 5.36060],", "-              [5.08591, 5.32473, 5.79249, 6.21060, 6.29613],", "-              [5.92213, 6.16095, 6.62871, 7.04682, 7.13234],", "-              [6.09318, 6.33200, 6.79976, 7.21787, 7.30340]]]])", "+            [[[[-0.32725, -0.08843, 0.37933, 0.79744],", "+              [0.15039, 0.38921, 0.85697, 1.27508],", "+              [1.08591, 1.32473, 1.79249, 2.21060],", "+              [1.92213, 2.16095, 2.62871, 3.04682]],", "+", "+             [[3.67275, 3.91157, 4.37933, 4.79744],", "+              [4.15039, 4.38921, 4.85697, 5.27508],", "+              [5.08591, 5.32473, 5.79249, 6.21060],", "+              [5.92213, 6.16095, 6.62871, 7.04682]]]])", "         if IS_PPC:", "             # Both OpenCV and PyTorch give a slightly different result on PPC", "             expected_out_t = torch.tensor(", "@@ -6331,10 +6316,7 @@ tensor(..., device='meta', size=(1,), requires_grad=True)\"\"\")", "                   [5.92212, 6.16094, 6.62870, 7.04680]]]])", "         out_t = F.interpolate(in_t, scale_factor=2.3, mode='bicubic', align_corners=False, recompute_scale_factor=False)", "         torch.set_printoptions(precision=5)", "-        if IS_PPC:", "-            self.assertEqual(out_t[..., :3, :3], expected_out_t[..., :3, :3], atol=1e-4, rtol=0)", "-        else:", "-            self.assertEqual(out_t, expected_out_t, atol=1e-4, rtol=0)", "+        self.assertEqual(out_t, expected_out_t, atol=1e-4, rtol=0)", " ", "         device_list = ['cpu']", "         if TEST_CUDA:", "@@ -6347,7 +6329,7 @@ tensor(..., device='meta', size=(1,), requires_grad=True)\"\"\")", "                 for scale_factor in [0.6, 1.6, 2.3]:", "                     in_t = torch.ones(2, 2, 2, 2).to(device)", "                     out_t = F.interpolate(in_t, scale_factor=scale_factor, **kwargs)", "-                    out_size = int(math.floor(0.5 + in_t.shape[-1] * scale_factor))", "+                    out_size = int(math.floor(in_t.shape[-1] * scale_factor))", "                     self.assertEqual(torch.ones(2, 2, out_size, out_size), out_t.data, atol=1e-5, rtol=0)", " ", "                     input = torch.randn(2, 2, 2, 2, requires_grad=True)", "@@ -9106,40 +9088,6 @@ class TestNNDeviceType(NNTestCase):", "         test('threshold', 3, 2)", "         test('threshold', 3, 2, inplace=True)", " ", "-    @parametrize_test(\"round_with_scale_factor\", [True, False, None])", "-    @parametrize_test(\"mode\", [\"nearest\", \"linear\", \"bilinear\", \"trilinear\", \"bicubic\"])", "-    def test_upsampling_output_size(self, device, round_with_scale_factor, mode):", "-        s = 1.15", "-        isize = 32", "-", "-        input_shape = [1, 1, isize]", "-        if \"bi\" in mode:", "-            input_shape.append(isize)", "-        if \"tri\" in mode:", "-            input_shape += (isize, isize)", "-", "-        d = 0.5 if round_with_scale_factor in (True, None) else 0.0", "-        expected_out_size = int(d + s * isize)", "-", "-        t_in = torch.randint(0, 256, size=input_shape, dtype=torch.float, device=device)", "-", "-        with warnings.catch_warnings(record=True) as ws:", "-            warnings.simplefilter(\"always\")", "-            t_out = F.interpolate(", "-                t_in, scale_factor=s, mode=mode, round_with_scale_factor=round_with_scale_factor", "-            )", "-            if round_with_scale_factor is None:", "-                self.assertEqual(len(ws), 1)", "-            else:", "-                self.assertEqual(len(ws), 0)", "-", "-        self.assertEqual(t_out.shape[-1], expected_out_size)", "-        if \"bi\" in mode:", "-            self.assertEqual(t_out.shape[-2], expected_out_size)", "-        if \"tri\" in mode:", "-            self.assertEqual(t_out.shape[-2], expected_out_size)", "-            self.assertEqual(t_out.shape[-3], expected_out_size)", "-", "     def test_upsamplingNearest1d(self, device):", "         # Forward AD does not support XLA because XLA tensors don't have storage", "         check_forward_ad = torch.device(device).type != 'xla'", "diff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py", "index cbe2fa19364..157a9a245ae 100644", "--- a/torch/_decomp/decompositions.py", "+++ b/torch/_decomp/decompositions.py", "@@ -2000,7 +2000,7 @@ def uniform_(self, low=0, high=1, generator=None):", " ", " ", " # aten/src/ATen/native/UpSample.cpp compute_output_size", "-def upsample_compute_output_size(input_size, output_size, scale_factors, round_with_scale_factor=False):", "+def upsample_compute_output_size(input_size, output_size, scale_factors):", "     spatial_dimensions = len(input_size) - 2", "     if output_size is not None:", "         utils.check(", "@@ -2017,12 +2017,11 @@ def upsample_compute_output_size(input_size, output_size, scale_factors, round_w", "         )", "         utils.check(len(scale_factors) == spatial_dimensions, lambda: \"\")", "         output_size = []", "-        d = 0.5 if round_with_scale_factor else 0.0", "         for i, s in enumerate(scale_factors):", "             if int(s) == s:", "                 output_size.append(input_size[i + 2] * int(s))", "             else:", "-                output_size.append(sym_int(d + input_size[i + 2] * s))", "+                output_size.append(sym_int(input_size[i + 2] * s))", "         return output_size", "     utils.check(", "         False, lambda: \"Must specify exactly one of output_size and scale_factors\"", "@@ -2038,10 +2037,8 @@ def get_scale_value(scales, idx):", " @register_decomposition(aten.upsample_nearest1d.vec)", " @aten.upsample_nearest1d.vec.py_impl(DispatchKey.CompositeImplicitAutograd)", " @aten.upsample_nearest1d.vec.py_impl(DispatchKey.Autograd)", "-def upsample_nearest1d_vec(input, output_size, scale_factors, round_with_scale_factor=False):", "-    osize = upsample_compute_output_size(", "-        input.size(), output_size, scale_factors, round_with_scale_factor=round_with_scale_factor", "-    )", "+def upsample_nearest1d_vec(input, output_size, scale_factors):", "+    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)", "     scale = get_scale_value(scale_factors, 0)", " ", "     return upsample_nearest1d(input, osize, scale)", "@@ -2050,10 +2047,8 @@ def upsample_nearest1d_vec(input, output_size, scale_factors, round_with_scale_f", " @register_decomposition(aten.upsample_nearest2d.vec)", " @aten.upsample_nearest2d.vec.py_impl(DispatchKey.CompositeImplicitAutograd)", " @aten.upsample_nearest2d.vec.py_impl(DispatchKey.Autograd)", "-def upsample_nearest2d_vec(input, output_size, scale_factors, round_with_scale_factor=False):", "-    osize = upsample_compute_output_size(", "-        input.size(), output_size, scale_factors, round_with_scale_factor=round_with_scale_factor", "-    )", "+def upsample_nearest2d_vec(input, output_size, scale_factors):", "+    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)", "     scale_h = get_scale_value(scale_factors, 0)", "     scale_w = get_scale_value(scale_factors, 1)", " ", "@@ -2063,10 +2058,8 @@ def upsample_nearest2d_vec(input, output_size, scale_factors, round_with_scale_f", " @register_decomposition(aten.upsample_nearest3d.vec)", " @aten.upsample_nearest3d.vec.py_impl(DispatchKey.CompositeImplicitAutograd)", " @aten.upsample_nearest3d.vec.py_impl(DispatchKey.Autograd)", "-def upsample_nearest3d_vec(input, output_size, scale_factors, round_with_scale_factor=False):", "-    osize = upsample_compute_output_size(", "-        input.size(), output_size, scale_factors, round_with_scale_factor=round_with_scale_factor", "-    )", "+def upsample_nearest3d_vec(input, output_size, scale_factors):", "+    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)", "     scale_d = get_scale_value(scale_factors, 0)", "     scale_h = get_scale_value(scale_factors, 1)", "     scale_w = get_scale_value(scale_factors, 2)", "@@ -2708,10 +2701,8 @@ def gru_impl(", " @register_decomposition(aten._upsample_bilinear2d_aa.vec)", " @aten._upsample_bilinear2d_aa.vec.py_impl(DispatchKey.CompositeImplicitAutograd)", " @aten._upsample_bilinear2d_aa.vec.py_impl(DispatchKey.Autograd)", "-def upsample_bilinear2d_aa_vec(input, output_size, align_corners, scale_factors, round_with_scale_factor=False):", "-    osize = upsample_compute_output_size(", "-        input.size(), output_size, scale_factors, round_with_scale_factor=round_with_scale_factor", "-    )", "+def upsample_bilinear2d_aa_vec(input, output_size, align_corners, scale_factors):", "+    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)", "     scale_h = get_scale_value(scale_factors, 0)", "     scale_w = get_scale_value(scale_factors, 1)", "     return torch.ops.aten._upsample_bilinear2d_aa(", "@@ -2722,10 +2713,8 @@ def upsample_bilinear2d_aa_vec(input, output_size, align_corners, scale_factors,", " @register_decomposition(aten.upsample_bilinear2d.vec)", " @aten.upsample_bilinear2d.vec.py_impl(DispatchKey.CompositeImplicitAutograd)", " @aten.upsample_bilinear2d.vec.py_impl(DispatchKey.Autograd)", "-def upsample_bilinear2d_vec(input, output_size, align_corners, scale_factors, round_with_scale_factor=False):", "-    osize = upsample_compute_output_size(", "-        input.size(), output_size, scale_factors, round_with_scale_factor=round_with_scale_factor", "-    )", "+def upsample_bilinear2d_vec(input, output_size, align_corners, scale_factors):", "+    osize = upsample_compute_output_size(input.size(), output_size, scale_factors)", "     scale_h = get_scale_value(scale_factors, 0)", "     scale_w = get_scale_value(scale_factors, 1)", "     return upsample_bilinear2d(input, osize, align_corners, scale_h, scale_w)", "diff --git a/torch/nn/functional.py b/torch/nn/functional.py", "index f54caa1541a..0a4cffd9881 100644", "--- a/torch/nn/functional.py", "+++ b/torch/nn/functional.py", "@@ -3752,17 +3752,17 @@ if upsample.__doc__:", " ", " ", " @_overload  # noqa: F811", "-def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False, round_with_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811", "+def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811", "     pass", " ", " ", " @_overload  # noqa: F811", "-def interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False, round_with_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811", "+def interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811", "     pass", " ", " ", " @_overload  # noqa: F811", "-def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False, round_with_scale_factor: Optional[bool] = None) -> Tensor:  # noqa: F811", "+def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811", "     pass", " ", " ", "@@ -3775,20 +3775,10 @@ def interpolate(  # noqa: F811", "     align_corners: Optional[bool] = None,", "     recompute_scale_factor: Optional[bool] = None,", "     antialias: bool = False,", "-    round_with_scale_factor: Optional[bool] = None,", " ) -> Tensor:  # noqa: F811", "     pass", " ", "-def interpolate(", "-    input: Tensor,", "-    size: Optional[int] = None,", "-    scale_factor: Optional[List[float]] = None,", "-    mode: str = 'nearest',", "-    align_corners: Optional[bool] = None,", "-    recompute_scale_factor: Optional[bool] = None,", "-    antialias: bool = False,", "-    round_with_scale_factor: Optional[bool] = None,", "-) -> Tensor:  # noqa: F811", "+def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811", "     r\"\"\"Down/up samples the input to either the given :attr:`size` or the given", "     :attr:`scale_factor`", " ", "@@ -3833,13 +3823,6 @@ def interpolate(", "         antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias", "             option together with ``align_corners=False``, interpolation result would match Pillow", "             result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``.", "-        round_with_scale_factor (bool, optional): if ``True`` output size is computed as", "-            ``round(input_size * scale_factor)`` (correct way) instead of ``int(input_size * scale_factor)``", "-            (old incorrect way).", "-            See the note below for further details. If `round_with_scale_factor` is ``None``,", "-            a warning is raised and computation is done as ``round_with_scale_factor=True``.", "-            If ``False``, old incorrect way to compute the output size is used for backward compatibility.", "-            By default, ``None``.", " ", "     .. note::", "         With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce", "@@ -3853,14 +3836,6 @@ def interpolate(", "         backward compatibility.", "         Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm.", " ", "-    .. note::", "-        ``round_with_scale_factor`` argument fixes the output size issue when ``scale_factor`` is", "-        provided. Historically, the output size is computed as ``output_size = int(input_size * scale_factor)`", "-        which does not match with OpenCV, Scikit-Image, Scipy. Correct way to compute the output size is to", "-        ``output_size = round(input_size * scale_factor)``. The difference of +1 in the output size is", "-        visible for floating ``scale_factor`` values.", "-        Related issue: https://github.com/pytorch/pytorch/issues/62396", "-", "     Note:", "         {backward_reproducibility_note}", "     \"\"\"", "@@ -3874,8 +3849,7 @@ def interpolate(", "             mode=mode,", "             align_corners=align_corners,", "             recompute_scale_factor=recompute_scale_factor,", "-            antialias=antialias,", "-            round_with_scale_factor=round_with_scale_factor,", "+            antialias=antialias", "         )", " ", "     if mode in (\"nearest\", \"area\", \"nearest-exact\"):", "@@ -3926,17 +3900,6 @@ def interpolate(", "             scale_factors = scale_factor", "         else:", "             scale_factors = [scale_factor for _ in range(dim)]", "-", "-        if round_with_scale_factor is None:", "-            correct_output_size = [", "-                int(0.5 + input.size(i + 2) * scale_factors[i]) for i in range(dim)", "-            ]", "-            incorrect_output_size = [", "-                int(input.size(i + 2) * scale_factors[i]) for i in range(dim)", "-            ]", "-            if correct_output_size != incorrect_output_size:", "-                # TODO: Write a deprecation message", "-                warnings.warn(\"...\")", "     else:", "         raise ValueError(\"either size or scale_factor should be defined\")", " ", "@@ -3952,24 +3915,18 @@ def interpolate(", "         # We compute output_size here, then un-set scale_factors.", "         # The C++ code will recompute it based on the (integer) output size.", "         assert scale_factors is not None", "-        d = 0.5 if round_with_scale_factor in (None, True) else 0.0", "         if not torch.jit.is_scripting() and torch._C._get_tracing_state():", "             # make scale_factor a tensor in tracing so constant doesn't get baked in", "-            # we perform round (i.e. floor(0.5 + x)) to match opencv, scipy, scikit-image output size", "             output_size = [", "-                (torch.floor(d + (input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))", "+                (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))", "                 for i in range(dim)", "             ]", "         elif torch.jit.is_scripting():", "-            # we perform round (i.e. floor(0.5 + x)) to match opencv, scipy, scikit-image output size", "-            output_size = [", "-                int(math.floor(d + float(input.size(i + 2)) * scale_factors[i]))", "-                for i in range(dim)", "-            ]", "+            output_size = [int(math.floor(float(input.size(i + 2)) * scale_factors[i]))", "+                           for i in range(dim)]", "         else:", "-            # we perform round (i.e. floor(0.5 + x)) to match opencv, scipy, scikit-image output size", "             output_size = [", "-                _sym_int(d + input.size(i + 2) * scale_factors[i])", "+                _sym_int(input.size(i + 2) * scale_factors[i])", "                 for i in range(dim)", "             ]", "         scale_factors = None", "@@ -3977,22 +3934,19 @@ def interpolate(", "     if antialias and not (mode in (\"bilinear\", \"bicubic\") and input.ndim == 4):", "         raise ValueError(\"Anti-alias option is only supported for bilinear and bicubic modes\")", " ", "-    if round_with_scale_factor is None:", "-        round_with_scale_factor = True", "-", "     if input.dim() == 3 and mode == \"nearest\":", "-        return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors, round_with_scale_factor)", "+        return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)", "     if input.dim() == 4 and mode == \"nearest\":", "-        return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors, round_with_scale_factor)", "+        return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)", "     if input.dim() == 5 and mode == \"nearest\":", "-        return torch._C._nn.upsample_nearest3d(input, output_size, scale_factors, round_with_scale_factor)", "+        return torch._C._nn.upsample_nearest3d(input, output_size, scale_factors)", " ", "     if input.dim() == 3 and mode == \"nearest-exact\":", "-        return torch._C._nn._upsample_nearest_exact1d(input, output_size, scale_factors, round_with_scale_factor)", "+        return torch._C._nn._upsample_nearest_exact1d(input, output_size, scale_factors)", "     if input.dim() == 4 and mode == \"nearest-exact\":", "-        return torch._C._nn._upsample_nearest_exact2d(input, output_size, scale_factors, round_with_scale_factor)", "+        return torch._C._nn._upsample_nearest_exact2d(input, output_size, scale_factors)", "     if input.dim() == 5 and mode == \"nearest-exact\":", "-        return torch._C._nn._upsample_nearest_exact3d(input, output_size, scale_factors, round_with_scale_factor)", "+        return torch._C._nn._upsample_nearest_exact3d(input, output_size, scale_factors)", " ", "     if input.dim() == 3 and mode == \"area\":", "         assert output_size is not None", "@@ -4006,20 +3960,20 @@ def interpolate(", " ", "     if input.dim() == 3 and mode == \"linear\":", "         assert align_corners is not None", "-        return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors, round_with_scale_factor)", "+        return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors)", "     if input.dim() == 4 and mode == \"bilinear\":", "         assert align_corners is not None", "         if antialias:", "-            return torch._C._nn._upsample_bilinear2d_aa(input, output_size, align_corners, scale_factors, round_with_scale_factor)", "-        return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors, round_with_scale_factor)", "+            return torch._C._nn._upsample_bilinear2d_aa(input, output_size, align_corners, scale_factors)", "+        return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)", "     if input.dim() == 5 and mode == \"trilinear\":", "         assert align_corners is not None", "-        return torch._C._nn.upsample_trilinear3d(input, output_size, align_corners, scale_factors, round_with_scale_factor)", "+        return torch._C._nn.upsample_trilinear3d(input, output_size, align_corners, scale_factors)", "     if input.dim() == 4 and mode == \"bicubic\":", "         assert align_corners is not None", "         if antialias:", "-            return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors, round_with_scale_factor)", "-        return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors, round_with_scale_factor)", "+            return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)", "+        return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors)", " ", "     if input.dim() == 3 and mode == \"bilinear\":", "         raise NotImplementedError(\"Got 3D input, but bilinear mode needs 4D input\")"]],
  ["test_cuda_trace", ["diff --git a/torch/_inductor/config.py b/torch/_inductor/config.py", "index fcb2646da0e..8d4e1d05da6 100644", "--- a/torch/_inductor/config.py", "+++ b/torch/_inductor/config.py", "@@ -102,8 +102,7 @@ def is_fbcode():", " ", " ", " # warnings intended for PyTorch developers, disable for point releases", "-is_nightly_or_source = \"dev\" in torch.__version__ or \"git\" in torch.__version__", "-developer_warnings = is_fbcode() or is_nightly_or_source", "+developer_warnings = is_fbcode() or \"+\" in torch.__version__", " ", " ", " def decide_compile_threads():"]],
  ["test_quantization", ["diff --git a/torch/_dynamo/guards.py b/torch/_dynamo/guards.py", "index d54c889e0e8..72c9467b2af 100644", "--- a/torch/_dynamo/guards.py", "+++ b/torch/_dynamo/guards.py", "@@ -35,6 +35,7 @@ from .utils import (", "     istype,", "     np,", "     orig_code_map,", "+    rename_implicit,", "     tensor_always_has_static_shape,", "     tensor_static_reason_to_message,", "     tuple_iterator_getitem,", "@@ -86,15 +87,17 @@ class GuardBuilder(GuardBuilderBase):", "         self,", "         id_ref: Callable[[Type[object]], str],", "         source_ref: Callable[[Source], str],", "-        user_scope: Optional[Dict[str, object]],", "+        scope: Optional[Dict[str, object]],", "         check_fn_manager: \"CheckFunctionManager\",", "+        renames=True,", "     ):", "         self.id_ref = id_ref", "         self.source_ref = source_ref", "-        if user_scope:", "-            scope = {'E': user_scope}", "+        if scope:", "+            if renames:", "+                scope = {rename_implicit(k): v for k, v in scope.items()}", "         else:", "-            scope = {'E': dict()}", "+            scope = dict()", "         self.scope: Dict[str, object] = scope", "         self.scope[\"__builtins__\"] = builtins.__dict__.copy()", "         for (", "@@ -620,9 +623,10 @@ class CheckFunctionManager:", "             source_ref,", "             combine_scopes(f_globals, f_locals),", "             self,", "+            renames=True,", "         )", "         global_builder = GuardBuilder(", "-            self.id_ref, source_ref, f_globals, self", "+            self.id_ref, source_ref, f_globals, self, renames=False", "         )", "         # source_ref can cause a cycle, make sure we break it with weakref", "         w_local = weakref.ref(local_builder)", "@@ -719,7 +723,7 @@ class CheckFunctionManager:", "         closure_vars.update(CLOSURE_VARS)", "         py_code = f\"\"\"\\", " def ___make_guard_fn({','.join(closure_vars.keys())}):", "-    return lambda E: {code}", "+    return lambda {args}: {code}", " \"\"\"", "         if os.environ.get(\"TORCHDYNAMO_PRINT_GUARDS\", None) == \"1\":", "             print(\"GUARDS\", code)", "@@ -770,7 +774,7 @@ def guard_fail_hook(", "     # Don't waste time computing the fail reason for guards we aren't going to report out.", "     if not guard_fn.guard_fail_fn and not (first or last):", "         return", "-    scope = {'E': f_locals}", "+    scope = {rename_implicit(k): v for k, v in f_locals.items()}", "     scope.update(guard_fn.closure_vars)", "     reason = None", "     for part in guard_fn.verbose_code_parts:", "diff --git a/torch/_dynamo/source.py b/torch/_dynamo/source.py", "index e01622cb254..4b89aa4ee9f 100644", "--- a/torch/_dynamo/source.py", "+++ b/torch/_dynamo/source.py", "@@ -7,7 +7,7 @@ from torch._guards import GuardSource, Source", " ", " from . import utils", " from .bytecode_transformation import create_call_function, create_instruction", "-from .utils import enum_repr", "+from .utils import enum_repr, rename_implicit", " ", " _GUARD_SOURCE_NN_MODULE = {", "     GuardSource.LOCAL: GuardSource.LOCAL_NN_MODULE,", "@@ -56,7 +56,7 @@ class LocalSource(Source):", "         return GuardSource.LOCAL", " ", "     def name(self):", "-        return f\"E[{repr(self.local_name)}]\"", "+        return rename_implicit(self.local_name)", " ", " ", " @dataclasses.dataclass", "@@ -79,7 +79,7 @@ class RandomValueSource(Source):", "         ]", " ", "     def name(self):", "-        return f\"random_value_{self.random_call_index}\"", "+        return rename_implicit(f\"random_value_{self.random_call_index}\")", " ", " ", " @dataclasses.dataclass", "@@ -93,7 +93,7 @@ class GlobalSource(Source):", "         return GuardSource.GLOBAL", " ", "     def name(self):", "-        return f\"E[{repr(self.global_name)}]\"", "+        return self.global_name", " ", " ", " @dataclasses.dataclass", "@@ -109,7 +109,7 @@ class GlobalWeakRefSource(Source):", "         return GuardSource.GLOBAL", " ", "     def name(self):", "-        return f\"E[{repr(self.global_name)}]()\"", "+        return f\"{self.global_name}()\"", " ", " ", " @dataclasses.dataclass", "diff --git a/torch/_dynamo/utils.py b/torch/_dynamo/utils.py", "index 2d60976ae4d..0c721cad254 100644", "--- a/torch/_dynamo/utils.py", "+++ b/torch/_dynamo/utils.py", "@@ -803,6 +803,19 @@ def global_key_name(key):", "     return f\"__dict_key_{id(key)}\"", " ", " ", "+def rename_implicit(v):", "+    \"\"\"", "+    Usage of inline comprehensions generates a implicit \".0\" variable that", "+    trips up guard generation.  This renames these variables in guards.", "+    \"\"\"", "+    m = re.match(r\"^[.](\\d+)$\", v)", "+    if m:", "+        assert v == \".0\", f\"currently only .0 supported: {v}\"", "+        # to support .1 etc see guards.py and _eval_frame.c", "+        return f\"___implicit{m.group(1)}\"", "+    return v", "+", "+", " from torch._subclasses import (  # noqa: F401", "     FakeTensorMode,", "     UnsupportedFakeTensorException,", "diff --git a/torch/csrc/dynamo/eval_frame.c b/torch/csrc/dynamo/eval_frame.c", "index e315bc51232..23013141f0a 100644", "--- a/torch/csrc/dynamo/eval_frame.c", "+++ b/torch/csrc/dynamo/eval_frame.c", "@@ -300,6 +300,8 @@ THPPyInterpreterFrame* THPPyInterpreterFrame_New(_PyInterpreterFrame* frame) {", " // Flag to just run a frame normally", " #define SKIP_CODE ((void*)0x1)", " ", "+static PyObject* noargs = NULL; /* cached empty tuple */", "+static PyObject* dotzerokey = NULL; /* \".0\" */", " static PyObject* guard_fail_hook = NULL;", " static PyObject* guard_error_hook = NULL;", " static PyObject* profiler_start_hook = NULL;", "@@ -520,10 +522,17 @@ static PyObject* lookup(CacheEntry* e, THP_EVAL_API_FRAME_OBJECT *frame, CacheEn", "     return Py_None;", "   }", "   PyObject *f_locals = frame->f_locals;", "-  // TODO: In Python 3.9 can optimize this to PyObject_CallOneArg", "-  PyObject* args = PyTuple_Pack(1, f_locals);", "-  if (args == NULL) return NULL;", "-  PyObject* valid = PyObject_Call(e->check_fn, args, NULL);", "+  PyObject* dotzero = PyDict_GetItem(f_locals, dotzerokey);", "+  PyObject* valid = NULL;", "+  if (unlikely(dotzero != NULL)) {", "+    // .0 is a special variable name used for implicit args", "+    PyObject* args = PyTuple_Pack(1, dotzero);", "+    if (args == NULL) return NULL;", "+    valid = PyObject_Call(e->check_fn, args, f_locals);", "+    Py_DECREF(args);", "+  } else {", "+    valid = PyObject_Call(e->check_fn, noargs, f_locals);", "+  }", "   if (unlikely(valid == NULL)) {", "     if (guard_error_hook != NULL) {", "       PyObject *type, *value, *traceback;", "@@ -964,6 +973,8 @@ PyObject* torch_c_dynamo_eval_frame_init(void) {", "   Py_INCREF(Py_None);", "   eval_frame_callback_set(Py_None);", " ", "+  noargs = PyTuple_New(0);", "+  dotzerokey = PyUnicode_InternFromString(\".0\");", "   PyObject* module = PyModule_Create(&_module);", " ", " #if IS_PYTHON_3_11_PLUS"]]
]
