import json
import os
import time
from argparse import ArgumentParser
from glob import glob

import torch
import torch.nn.functional as F
from config import TDArgs

from llama import Llama
from tokenizer import Tokenizer

from preproc import get_functions
from transformers import AutoModelForCausalLM


class Retriever:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.config = TDArgs()
        assets_path = os.path.join("assets", self.experiment_name)

        # Get the list of artifacts:
        # 1. Indexes of unittests generated by indexer (*.pt)
        # 2. Mapping from indices to unittest names (*.json)
        embeddings_files = glob(f"{assets_path}/unittest_index_*.pt")
        mapping_files = glob(f"{assets_path}/unittest_index_mapping_*.json")

        # Sort the above lists
        embeddings_files = sorted(embeddings_files)
        mapping_files = sorted(mapping_files)

        # Read the artifact files and concatenate them into:
        # 1. self.embeddings - with the entire index as a single pytorch tensor
        # 2. self.unittest_names - a single Dict of the form {idx: test_name}
        embeddings = []
        self.unittest_names = []
        for i in range(len(embeddings_files)):
            embeddings.append(torch.load(embeddings_files[i]))

            with open(mapping_files[i]) as f:
                test_map = json.load(f)
            self.unittest_names.extend(test_map["mapping"])

        self.embeddings = torch.cat(embeddings).to("cuda:0")
        print(self.embeddings.shape)

        # self.tokenizer = Tokenizer("bert-base-uncased")
        # self.model = AutoModelForCausalLM.from_pretrained(
        #     "bert-base-uncased"
        # ).to("cuda:0")
        generator = Llama.build(
            ckpt_dir=os.path.expanduser(self.config.model_ckpt_dir),
            tokenizer_path=os.path.expanduser(self.config.tokenizer_path),
            max_seq_len=self.config.max_context_len,
            max_batch_size=self.config.max_batch_size,
            use_kv_cache=False,
            model_parallel_size=1,
        )
        self.model = generator.model
        self.tokenizer = Tokenizer(self.config)

    def retrieve(self):
        # parse and tokenize input (function from a file)
        # run model forward on each chunk of the embeddings
        # cosine similarity per chunk
        # rank and print most/least match
        tensor = torch.full(
            (1, self.config.max_context_len),
            self.tokenizer.pad_id,
            dtype=torch.long,
        )

        functions = get_functions(
            "/home/osalpekar/pytorch/torch/distributed/fsdp/fully_sharded_data_parallel.py",
            indexing=False,
            # "/home/osalpekar/pytorch/torch/fx/node.py"
        )
        for signature in functions:
            # Just doing a sample function with the __init__ here
            if "__init__" in signature:
                function_body = functions[signature]
                print(signature)
                tokens = self.tokenizer.encode(function_body)  # .to("cuda:0")
                tokens = tokens[: self.config.max_context_len]
                tensor[0, : len(tokens)] = torch.tensor(
                    tokens, dtype=torch.long
                )

        tensor = tensor.to("cuda:0")

        self.model.eval()
        with torch.no_grad():
            # full_model_states = self.model(
            #     tokens, output_hidden_states=True
            # )
            # test_embedding = full_model_states.hidden_states[-1].detach()

            _, func_embedding = self.model.forward(
                tensor, 0, output_last_hidden_state=True
            )
            pooled_embedding = torch.sum(func_embedding, dim=1)

            similarity_matrix = F.cosine_similarity(
                self.embeddings, pooled_embedding
            )
            # Largest scores are last!
            sorted_indices = torch.argsort(similarity_matrix, descending=False)

            print("Top 10 Most Relevant Tests")
            top_indices = sorted_indices[-10:]
            for ind in top_indices:
                test = self.unittest_names[int(ind.item())]
                score = similarity_matrix[ind]
                print(f"Score: {score}; Test: {test}")

            print("Filtered Least Relevant Tests")
            bot_indices = sorted_indices[:1000]
            for ind in bot_indices:
                test = self.unittest_names[int(ind.item())]
                score = similarity_matrix[ind]
                if "test" in test.split(":")[-1].lower():
                    print(f"Score: {score}; Test: {test}")


def main():
    parser = ArgumentParser("Retriever")
    parser.add_argument(
        "--experiment-name",
        type=str,
        required=True,
        help="Uses artifacts from the specified Indexer Experiment",
    )
    args = parser.parse_args()

    start = time.time()
    retriever = Retriever(args.experiment_name)
    retriever.retrieve()
    end = time.time()

    print(f"Total time to retreieve: {end-start} seconds")


if __name__ == "__main__":
    main()
