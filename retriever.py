import time
import json
import os

import torch
import torch.nn.functional as F
from glob import glob
from argparse import ArgumentParser

from preproc import get_functions
from pr_tokenization import PTTokenizer, CONTEXT_LENGTH
from transformers import AutoModelForCausalLM

from llama import Llama

class Retriever:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        assets_path = os.path.join("assets", self.experiment_name) 

        # Get the list of artifacts:
        # 1. Indexes of unittests generated by indexer (*.pt)
        # 2. Mapping from indices to unittest names (*.json)
        embeddings_files = glob(f"{assets_path}/unittest_index_*.pt")
        mapping_files = glob(f"{assets_path}/unittest_index_mapping_*.json")

        # Sort the above lists
        embeddings_files = sorted(embeddings_files)
        mapping_files = sorted(mapping_files)

        # Read the artifact files and concatenate them into:
        # 1. self.embeddings - with the entire index as a single pytorch tensor
        # 2. self.unittest_names - a single Dict of the form {idx: test_name}
        embeddings = []
        self.unittest_names = []
        for i in range(len(embeddings_files)):
            embeddings.append(torch.load(embeddings_files[i]))

            with open(mapping_files[i]) as f:
                test_map = json.load(f)
            self.unittest_names.extend(test_map["mapping"])

        self.embeddings = torch.cat(embeddings).to("cuda:0")
        print(self.embeddings.shape)

        # self.tokenizer = PTTokenizer("bert-base-uncased")
        # self.model = AutoModelForCausalLM.from_pretrained(
        #     "bert-base-uncased"
        # ).to("cuda:0")
        generator = Llama.build(
            ckpt_dir="/home/osalpekar/codellama/CodeLlama-7b-Python/",
            tokenizer_path="/home/osalpekar/codellama/CodeLlama-7b-Python/tokenizer.model",
            max_seq_len=CONTEXT_LENGTH,
            max_batch_size=600,
            use_kv_cache=False,
            model_parallel_size=1,
        )
        self.model = generator.model
        self.tokenizer = PTTokenizer()

    def retrieve(self):
        # parse and tokenize input (function from a file)
        # run model forward on each chunk of the embeddings
        # cosine similarity per chunk
        # rank and print most/least match
        functions = get_functions("/home/osalpekar/pytorch/torch/distributed/fsdp/fully_sharded_data_parallel.py")
        for signature in functions:
            if "__init__" in signature:
                function_body = functions[signature]
                tokens = self.tokenizer.encode(function_body).to("cuda:0")

        self.model.eval()
        with torch.no_grad():
            # full_model_states = self.model(
            #     tokens, output_hidden_states=True
            # )
            # test_embedding = full_model_states.hidden_states[-1].detach()

            _, func_embedding = self.model(tokens, output_hidden_states=True)

            similarity_matrix = F.cosine_similarity(
                self.embeddings, func_embedding
            )
            print(similarity_matrix)
            sorted_indices = torch.argsort(similarity_matrix, descending=False)
            print(sorted_indices)
            for ind in sorted_indices:
                print(ind)
                print(self.unittest_names[int(ind.item())])


def main():
    parser = ArgumentParser("Retriever")
    parser.add_argument(
        "--experiment-name",
        type=str,
        required=True,
        help="Uses artifacts from the specified Indexer Experiment")
    args = parser.parse_args()

    start = time.time()
    retriever = Retriever(args.experiment_name)
    retriever.retrieve()
    end = time.time()

    print(f"Total time to retreieve: {end-start} seconds")

if __name__ == "__main__":
    main()
