[
  {
    "number": 105503,
    "title": "[composable API] Fix the replicate_device_id test case to avoid copy replicated models.",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105503\n\nWe should not `replicate` deeocopy.copy(a already replicated model).\n\nDifferential Revision: [D47566678](https://our.internmc.facebook.com/intern/diff/D47566678/)",
    "merge_commit_sha": "1577569e20ac6104e9f5838f92de7368c3abd3db",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105503",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105503/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105503.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105503.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105503/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105503/comments",
    "labels": [
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T21:30:38.787014Z",
    "state": "open",
    "patch": "From 67a6e4e3ba219abd3039ed7cf2f4ec5582401e9a Mon Sep 17 00:00:00 2001\nFrom: Chien-Chin Huang <chienchin@fb.com>\nDate: Tue, 18 Jul 2023 14:30:31 -0700\nSubject: [PATCH] [composable API] Fix the replicate_device_id test case to\n avoid copy replicated models.\n\nWe should not `replicate` deeocopy.copy(a already replicated model).\n\nDifferential Revision: [D47566678](https://our.internmc.facebook.com/intern/diff/D47566678/)\n\n[ghstack-poisoned]\n---\n test/distributed/_composable/test_replicate.py | 11 +++++------\n 1 file changed, 5 insertions(+), 6 deletions(-)\n\ndiff --git a/test/distributed/_composable/test_replicate.py b/test/distributed/_composable/test_replicate.py\nindex 4a1f1cddc59338..30e530b1cd1135 100644\n--- a/test/distributed/_composable/test_replicate.py\n+++ b/test/distributed/_composable/test_replicate.py\n@@ -223,6 +223,8 @@ def test_replicate_device_id(self):\n             store=dist.FileStore(self.file_name, self.world_size),\n         )\n         model = Net()\n+        model_cuda = deepcopy(model).cuda()\n+        model_cuda2 = deepcopy(model_cuda)\n         replicate(model, device_id=torch.device(\"cpu\"))\n         # DDP instance is attached in first pre forward\n         model(torch.randn(2, 2))\n@@ -230,19 +232,16 @@ def test_replicate_device_id(self):\n         # Should be None for CPU training\n         self.assertEqual(None, replicate_ddp_weakref.device_ids)\n \n-        model.cuda()\n-        model_cuda = deepcopy(model)\n         replicate(model_cuda, device_id=torch.device(torch.cuda.current_device()))\n         # DDP instance is attached in first pre forward\n         model_cuda(torch.randn(2, 2))\n         replicate_ddp_weakref = replicate.state(model_cuda)._ddp_weakref()\n         self.assertEqual([0], replicate_ddp_weakref.device_ids)\n         # Pass in int as device_id\n-        model_cuda = deepcopy(model_cuda)\n-        replicate(model_cuda, device_id=int(torch.cuda.current_device()))\n+        replicate(model_cuda2, device_id=int(torch.cuda.current_device()))\n         # DDP instance is attached in first pre forward\n-        model_cuda(torch.randn(2, 2))\n-        replicate_ddp_weakref = replicate.state(model_cuda)._ddp_weakref()\n+        model_cuda2(torch.randn(2, 2))\n+        replicate_ddp_weakref = replicate.state(model_cuda2)._ddp_weakref()\n         self.assertEqual([0], replicate_ddp_weakref.device_ids)\n \n     def test_replicate_wrong_device_id_type(self):\n"
  },
  {
    "number": 105501,
    "title": "Fix test failure in TestCudaMultiGPU.test_cuda_device_memory_allocated",
    "body": "The test\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f508d3564c8a472ba2f74878dbdf67f09eaae2d3/test/test_cuda_multigpu.py#L1282-L1290\r\n\r\nTorch cuda caching allocator may cache the allocation and cause the \"new_alloc\" being the same as the \"old_alloc\". \r\n```python\r\n     self.assertGreater(memory_allocated(0), current_alloc[0]) \r\n```\r\n\r\nI suggest that we use `assertGreaterEqual` instead of `assertGreater` in the test.\r\n\r\nIndividually running only this test does not make it fail but running it together with other tests from the same test module will make it fail. ",
    "merge_commit_sha": "e72dbd98fbca1e5a49c2b859e7d4a36a3de41096",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105501",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105501/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105501.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105501.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105501/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105501/comments",
    "labels": [
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T21:27:06.965192Z",
    "state": "open",
    "patch": "From 3909e802b4e69c585652f67b9e953e0b76b53bb2 Mon Sep 17 00:00:00 2001\nFrom: Xiao Wang <24860335+xwang233@users.noreply.github.com>\nDate: Tue, 18 Jul 2023 14:22:06 -0700\nSubject: [PATCH] assertGreaterEqual\n\n---\n test/test_cuda_multigpu.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/test/test_cuda_multigpu.py b/test/test_cuda_multigpu.py\nindex 9a118c26ec9786..b00b7e501b43b9 100644\n--- a/test/test_cuda_multigpu.py\n+++ b/test/test_cuda_multigpu.py\n@@ -1285,7 +1285,7 @@ def test_cuda_device_memory_allocated(self):\n         device_count = torch.cuda.device_count()\n         current_alloc = [memory_allocated(idx) for idx in range(device_count)]\n         x = torch.ones(10, device=\"cuda:0\")\n-        self.assertGreater(memory_allocated(0), current_alloc[0])\n+        self.assertGreaterEqual(memory_allocated(0), current_alloc[0])\n         self.assertTrue(all(memory_allocated(torch.cuda.device(idx)) == current_alloc[idx] for idx in range(1, device_count)))\n \n \n"
  },
  {
    "number": 105500,
    "title": "Report guard failures with recompiles logging",
    "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "ee5f772aad854f6832be5158060ca24b3ab3672a",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105500",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105500/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105500.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105500.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105500/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105500/comments",
    "labels": [
      "module: dynamo",
      "ciflow/inductor",
      "ciflow/trunk",
      "merging",
      "release notes: dynamo"
    ],
    "_event_time": "2023-07-18T21:20:42.092058Z",
    "state": "open",
    "patch": "From e125fc7b56ca5cda73014b07d3a1a97a72cc27dd Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Tue, 18 Jul 2023 14:20:08 -0700\nSubject: [PATCH] Report guard failures with recompiles logging\n\n---\n torch/_dynamo/convert_frame.py | 3 ++-\n torch/_dynamo/guards.py        | 3 ++-\n torch/_dynamo/utils.py         | 7 +++++++\n 3 files changed, 11 insertions(+), 2 deletions(-)\n\ndiff --git a/torch/_dynamo/convert_frame.py b/torch/_dynamo/convert_frame.py\nindex 7ede910589cefc..1634b25a7f6761 100644\n--- a/torch/_dynamo/convert_frame.py\n+++ b/torch/_dynamo/convert_frame.py\n@@ -51,6 +51,7 @@\n     gen_record_file_name,\n     guard_failures,\n     increment_frame,\n+    is_guard_failure_reporting_enabled,\n     is_namedtuple,\n     istype,\n     LazyString,\n@@ -243,7 +244,7 @@ def _convert_frame_assert(\n         if code in input_codes and (\n             recompiles_log.isEnabledFor(logging.DEBUG) or config.error_on_recompile\n         ):\n-            if config.report_guard_failures:\n+            if is_guard_failure_reporting_enabled():\n                 message = (\n                     f\"Recompiling function {code.co_name} in {code.co_filename}:{code.co_firstlineno}\",\n                     f\"triggered by the following guard failure: {str(guard_failures[code][-1])}\",\ndiff --git a/torch/_dynamo/guards.py b/torch/_dynamo/guards.py\nindex 7599140e61308d..5df8adae09cecb 100644\n--- a/torch/_dynamo/guards.py\n+++ b/torch/_dynamo/guards.py\n@@ -51,6 +51,7 @@\n     dict_param_key_ids,\n     guard_failures,\n     HAS_NUMPY,\n+    is_guard_failure_reporting_enabled,\n     istype,\n     np,\n     orig_code_map,\n@@ -1000,7 +1001,7 @@ def convert(size_or_stride):\n         if os.environ.get(\"TORCHDYNAMO_PRINT_GUARDS\", None) == \"1\":\n             print(\"GUARDS\", guard_body)\n \n-        if config.report_guard_failures or guard_fail_fn is not None:\n+        if is_guard_failure_reporting_enabled() or guard_fail_fn is not None:\n             # Guard fail hook is called everytime guard eval fails. For a cache\n             # lookup where there are multiple entries in the same cache line,\n             # this can lead to very high performance overhead. So, we have\ndiff --git a/torch/_dynamo/utils.py b/torch/_dynamo/utils.py\nindex 0c01d6563bd6ee..afce7dbc73a292 100644\n--- a/torch/_dynamo/utils.py\n+++ b/torch/_dynamo/utils.py\n@@ -1743,3 +1743,10 @@ def is_compile_supported(device_type):\n     else:\n         compile_supported = False\n     return compile_supported\n+\n+\n+def is_guard_failure_reporting_enabled():\n+    return (\n+        config.report_guard_failures\n+        or torch._logging._internal.log_state.is_artifact_enabled(\"recompiles\")\n+    )\n"
  },
  {
    "number": 105498,
    "title": "[BE]: pyupgrade Python to 3.8 - remove extraneous parentheses only - `torch/` only",
    "body": "Same as #101606 but only for the `torch/` folder.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @anijain2305",
    "merge_commit_sha": "c5e8af018e5772cc27fef966fabc9677369861f1",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105498",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105498/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105498.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105498.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105498/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105498/comments",
    "labels": [
      "release notes: AO frontend",
      "open source",
      "ciflow/inductor",
      "module: dynamo",
      "release notes: quantization",
      "module: inductor"
    ],
    "_event_time": "2023-07-18T20:41:22.851841Z",
    "state": "open",
    "patch": "From 01a1f6f1a10ea61bd746f6fe1d506ff82379fc9b Mon Sep 17 00:00:00 2001\nFrom: Santiago Castro <santi.1410@hotmail.com>\nDate: Tue, 18 Jul 2023 16:38:07 -0400\nSubject: [PATCH] [BE]: pyupgrade Python to 3.8 - remove extraneous parentheses\n only - torch/ only\n\n---\n torch/_decomp/decompositions.py               |  6 +-\n torch/_dynamo/debug_utils.py                  | 16 ++---\n torch/_dynamo/symbolic_convert.py             |  2 +-\n torch/_dynamo/variables/builder.py            | 10 ++--\n torch/_functorch/eager_transforms.py          |  2 +-\n torch/_inductor/lowering.py                   |  6 +-\n torch/_refs/nn/functional/__init__.py         |  8 +--\n .../ao/quantization/quantization_mappings.py  |  8 +--\n torch/autograd/gradcheck.py                   | 16 ++---\n torch/cuda/_memory_viz.py                     |  2 +-\n .../examples/fsdp_checkpoint_example.py       |  2 +-\n torch/distributions/kl.py                     |  2 +-\n .../constraint_transformation.py              |  4 +-\n torch/masked/maskedtensor/core.py             |  2 +-\n torch/nn/functional.py                        | 12 ++--\n torch/nn/modules/container.py                 |  2 +-\n torch/nn/parallel/distributed.py              |  2 +-\n torch/onnx/symbolic_opset11.py                |  2 +-\n torch/onnx/symbolic_opset17.py                |  8 +--\n torch/optim/optimizer.py                      |  4 +-\n torch/package/package_exporter.py             | 20 +++----\n torch/profiler/_memory_profiler.py            |  4 +-\n torch/serialization.py                        |  4 +-\n torch/storage.py                              | 12 ++--\n torch/testing/_internal/common_device_type.py |  2 +-\n .../_internal/common_methods_invocations.py   | 10 ++--\n torch/testing/_internal/common_modules.py     | 58 +++++++++----------\n torch/testing/_internal/common_nn.py          |  2 +-\n .../testing/_internal/common_quantization.py  |  2 +-\n torch/testing/_internal/opinfo/core.py        |  5 +-\n .../_internal/opinfo/definitions/linalg.py    |  2 +-\n .../_internal/opinfo/definitions/sparse.py    |  2 +-\n torch/torch_version.py                        |  2 +-\n torch/utils/jit/__init__.py                   |  1 -\n 34 files changed, 112 insertions(+), 130 deletions(-)\n\ndiff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py\nindex 70c69ff5cef47a..63dd85d39b31aa 100644\n--- a/torch/_decomp/decompositions.py\n+++ b/torch/_decomp/decompositions.py\n@@ -3123,7 +3123,7 @@ def get_coeff(ofs: int) -> Tensor:\n             )\n             return _upsample_cubic_interp1d(cs, tx.unsqueeze(1))\n \n-        coeffs = tuple((get_coeff(ofs) for ofs in range(4)))\n+        coeffs = tuple(get_coeff(ofs) for ofs in range(4))\n         return _upsample_cubic_interp1d(coeffs, ty.unsqueeze(1))\n \n \n@@ -3371,10 +3371,10 @@ def load_bounded(ys, xs):\n         return aten._unsafe_index(a, [N_idx, C_idx, y_idx, x_idx])\n \n     def get_x_interp(y):\n-        coeffs_x = tuple((load_bounded(y, x_ofs) for x_ofs in ixs_ofs))\n+        coeffs_x = tuple(load_bounded(y, x_ofs) for x_ofs in ixs_ofs)\n         return _upsample_cubic_interp1d(coeffs_x, t_x)\n \n-    coeffs_y = tuple((get_x_interp(y_ofs) for y_ofs in iys_ofs))\n+    coeffs_y = tuple(get_x_interp(y_ofs) for y_ofs in iys_ofs)\n     result = _upsample_cubic_interp1d(coeffs_y, t_y)\n \n     # convert output to correct memory format, if necessary\ndiff --git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py\nindex 4d7c98aa222536..bf7a61b0793654 100644\n--- a/torch/_dynamo/debug_utils.py\n+++ b/torch/_dynamo/debug_utils.py\n@@ -385,11 +385,9 @@ def same_two_models(\n         # This means that the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return True.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph.\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph.\"\n         )\n         return True\n \n@@ -465,11 +463,9 @@ def backend_accuracy_fails(\n         # This means that the the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return False.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph\"\n         )\n         return False\n \ndiff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py\nindex 75a44965b63a3e..86c0188bcfc7ab 100644\n--- a/torch/_dynamo/symbolic_convert.py\n+++ b/torch/_dynamo/symbolic_convert.py\n@@ -1840,7 +1840,7 @@ def format_frame_summary(self, additional_stack_frames=None):\n             additional_stack_frames = []\n         return \"\".join(\n             traceback.format_list(\n-                ([self.frame_summary()] + list(reversed(additional_stack_frames)))\n+                [self.frame_summary()] + list(reversed(additional_stack_frames))\n             )\n         )\n \ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex 25f7f097b39a1b..e17f5a2b9e3bc7 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -379,12 +379,10 @@ def _wrap(self, value):\n         elif istype(\n             value, (dict, collections.defaultdict, collections.OrderedDict)\n         ) and all(\n-            (\n-                ConstantVariable.is_literal(k)\n-                or self.tensor_can_be_dict_key(k)\n-                or isinstance(k, enum.Enum)\n-                for k in value.keys()\n-            )\n+            ConstantVariable.is_literal(k)\n+            or self.tensor_can_be_dict_key(k)\n+            or isinstance(k, enum.Enum)\n+            for k in value.keys()\n         ):\n             if not value and self.get_source().is_nn_module():\n                 # It is faster to guard on 'false' property than to guard\ndiff --git a/torch/_functorch/eager_transforms.py b/torch/_functorch/eager_transforms.py\nindex a25a7bc456bec1..4ba72eb2152c6b 100644\n--- a/torch/_functorch/eager_transforms.py\n+++ b/torch/_functorch/eager_transforms.py\n@@ -548,7 +548,7 @@ def compute_jacobian_stacked():\n             # Iterate and concat the jacobians of different\n             # inputs.\n             for idx in range(len(flat_primals)):\n-                r = tuple((r_[idx] for r_ in chunked_results))\n+                r = tuple(r_[idx] for r_ in chunked_results)\n                 flat_results.append(torch.cat(r, 0))\n \n             return flat_results\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 0283ed1c2025ae..c7ba9d863a5c2f 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -2865,11 +2865,11 @@ def load_bounded(fy, fx):\n \n         iy = ops.to_dtype(in_y, get_int_dtype(iH + 1))\n         ix = ops.to_dtype(in_x, get_int_dtype(iW + 1))\n-        iys_ofs = tuple((ops.add(iy, ofs) for ofs in (-1, 0, 1, 2)))\n-        ixs_ofs = tuple((ops.add(ix, ofs) for ofs in (-1, 0, 1, 2)))\n+        iys_ofs = tuple(ops.add(iy, ofs) for ofs in (-1, 0, 1, 2))\n+        ixs_ofs = tuple(ops.add(ix, ofs) for ofs in (-1, 0, 1, 2))\n \n         def get_x_interp(y):\n-            coeffs_x = tuple((load_bounded(y, x) for x in ixs_ofs))\n+            coeffs_x = tuple(load_bounded(y, x) for x in ixs_ofs)\n             return cubic_interp1d(coeffs_x, t_x)\n \n         coeffs_y = tuple(get_x_interp(y) for y in iys_ofs)\ndiff --git a/torch/_refs/nn/functional/__init__.py b/torch/_refs/nn/functional/__init__.py\nindex eaa6618379f356..2da72ad72bb831 100644\n--- a/torch/_refs/nn/functional/__init__.py\n+++ b/torch/_refs/nn/functional/__init__.py\n@@ -610,11 +610,9 @@ def margin_ranking_loss(\n     # loss_without_reduction = max(0, \u2212target * (input1 \u2212 input2) + margin)\n     if input1.ndim != input2.ndim or input1.ndim != target.ndim:\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(\n-                    input1.shape, input2.shape, target.shape\n-                )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(\n+                input1.shape, input2.shape, target.shape\n             )\n         )\n     _check_reduction_value(reduction)\ndiff --git a/torch/ao/quantization/quantization_mappings.py b/torch/ao/quantization/quantization_mappings.py\nindex 96db52624acd34..2dae1965e653a2 100644\n--- a/torch/ao/quantization/quantization_mappings.py\n+++ b/torch/ao/quantization/quantization_mappings.py\n@@ -300,10 +300,10 @@ def get_default_qconfig_propagation_list() -> Set[Callable]:\n     attribute to in prepare\n     '''\n     QCONFIG_PROPAGATE_MODULE_CLASS_LIST = (\n-        (set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n-         set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         _INCLUDE_QCONFIG_PROPAGATE_LIST)\n+        set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        _INCLUDE_QCONFIG_PROPAGATE_LIST\n     )\n     return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n \ndiff --git a/torch/autograd/gradcheck.py b/torch/autograd/gradcheck.py\nindex 18382806c0d4df..bf605a561e34b6 100644\n--- a/torch/autograd/gradcheck.py\n+++ b/torch/autograd/gradcheck.py\n@@ -1035,11 +1035,11 @@ def _test_undefined_backward_mode(func, outputs, inputs) -> bool:\n         raise GradcheckError(\"no Tensors requiring grad found in input\")\n \n     def warn_bc_breaking():\n-        warnings.warn((\n+        warnings.warn(\n             'Backwards compatibility: New undefined gradient support checking '\n             'feature is enabled by default, but it may break existing callers '\n             'of this function. If this is true for you, you can call this '\n-            'function with \"check_undefined_grad=False\" to disable the feature'))\n+            'function with \"check_undefined_grad=False\" to disable the feature')\n \n     def check_undefined_grad_support(output_to_check):\n         grads_output = [torch.zeros_like(o, memory_format=torch.legacy_contiguous_format) for o in output_to_check]\n@@ -1048,18 +1048,18 @@ def check_undefined_grad_support(output_to_check):\n                                               grads_output, allow_unused=True)\n         except RuntimeError as e:\n             warn_bc_breaking()\n-            raise GradcheckError((\n+            raise GradcheckError(\n                 'Expected backward function to handle undefined output grads. '\n                 'Please look at \"Notes about undefined output gradients\" in '\n-                '\"tools/autograd/derivatives.yaml\"')) from e\n+                '\"tools/autograd/derivatives.yaml\"') from e\n \n         for gi, i in zip(grads_input, diff_input_list):\n             if (gi is not None) and (not gi.eq(0).all()):\n                 warn_bc_breaking()\n-                raise GradcheckError((\n+                raise GradcheckError(\n                     'Expected all input grads to be undefined or zero when all output grads are undefined '\n                     'or zero. Please look at \"Notes about undefined output gradients\" in '\n-                    '\"tools/autograd/derivatives.yaml\"'))\n+                    '\"tools/autograd/derivatives.yaml\"')\n         return True\n \n     # All backward functions must work properly if all output grads are undefined\n@@ -1525,9 +1525,9 @@ def gradcheck(\n         else:\n             check_sparse_nnz = masked\n     else:\n-        warnings.warn((\n+        warnings.warn(\n             'Backwards compatibility: check_sparse_nnz is deprecated, it will be removed in a future version of PyTorch.'\n-            f' Use masked={check_sparse_nnz} instead.'))\n+            f' Use masked={check_sparse_nnz} instead.')\n         if masked is None:\n             masked = check_sparse_nnz\n         elif check_sparse_nnz != masked:\ndiff --git a/torch/cuda/_memory_viz.py b/torch/cuda/_memory_viz.py\nindex f32fc361c28f64..0af30d3ac61d83 100644\n--- a/torch/cuda/_memory_viz.py\n+++ b/torch/cuda/_memory_viz.py\n@@ -248,7 +248,7 @@ def segsum(data):\n             finish_ = (start_ + size)\n             start = start_ // PAGE_SIZE\n             finish = (finish_ - 1) // PAGE_SIZE + 1\n-            m = chr((ord('a' if active else 'A') + (i % 26)))\n+            m = chr(ord('a' if active else 'A') + (i % 26))\n             for j in range(start, finish):\n                 s = max(start_, j * PAGE_SIZE)\n                 e = min(finish_, (j + 1) * PAGE_SIZE)\ndiff --git a/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py b/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py\nindex b8553997f756f0..d363fdc1a107d3 100644\n--- a/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py\n+++ b/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py\n@@ -24,7 +24,7 @@\n \n \n def opt_at(opt, idx):\n-    return list((opt.state.values()))[idx]\n+    return list(opt.state.values())[idx]\n \n \n def init_model():\ndiff --git a/torch/distributions/kl.py b/torch/distributions/kl.py\nindex 26d7b47d2f51a8..9c214b89ee0c9a 100644\n--- a/torch/distributions/kl.py\n+++ b/torch/distributions/kl.py\n@@ -735,7 +735,7 @@ def _kl_uniform_beta(p, q):\n     common_term = p.high - p.low\n     t1 = torch.log(common_term)\n     t2 = (q.concentration1 - 1) * (_x_log_x(p.high) - _x_log_x(p.low) - common_term) / common_term\n-    t3 = (q.concentration0 - 1) * (_x_log_x((1 - p.high)) - _x_log_x((1 - p.low)) + common_term) / common_term\n+    t3 = (q.concentration0 - 1) * (_x_log_x(1 - p.high) - _x_log_x(1 - p.low) + common_term) / common_term\n     t4 = q.concentration1.lgamma() + q.concentration0.lgamma() - (q.concentration1 + q.concentration0).lgamma()\n     result = t3 + t4 - t1 - t2\n     result[(p.high > q.support.upper_bound) | (p.low < q.support.lower_bound)] = inf\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\nindex fc1fae790d8300..153a8407fc4113 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n@@ -75,7 +75,7 @@ def transform_index_select(constraint, counter):\n     # if the index is valid then replace the input dimension with the new dimension\n     # otherwise the dimension will not be replaced and the clause will contain False\n     if is_valid_index == T():\n-        new_dims = copy.deepcopy((dims))\n+        new_dims = copy.deepcopy(dims)\n         new_dims[constraint.index] = constraint.dim_replace\n \n     transformed_constraint = Conj([BinConstraintT(constraint.input_var, TensorType(dims), op_eq),\n@@ -803,7 +803,7 @@ def apply_padding(e1_var: TVar,\n         broadcast_padding = []\n \n         # for every padding size, we also consider broadcasting\n-        for j in range((len(d2) - i)):\n+        for j in range(len(d2) - i):\n             broadcast_padding.append(broadcast_dim(simulate_padding, d2, d11, d12, j, True))\n \n         # we consider the possibilities for broadcasting for every dimension. Since we already\ndiff --git a/torch/masked/maskedtensor/core.py b/torch/masked/maskedtensor/core.py\nindex 3b15fa9f7802a5..42321bb959c4f5 100644\n--- a/torch/masked/maskedtensor/core.py\n+++ b/torch/masked/maskedtensor/core.py\n@@ -109,7 +109,7 @@ def _masked_tensor_str(data, mask, formatter):\n             for d in data\n         ]\n         max_len = max(\n-            (8 if x[1] else len(x[0]) for x in zip(formatted_elements, ~mask))\n+            8 if x[1] else len(x[0]) for x in zip(formatted_elements, ~mask)\n         )\n         return (\n             \"[\"\ndiff --git a/torch/nn/functional.py b/torch/nn/functional.py\nindex e1b1227d235d6c..dd3bc7f06bfeea 100644\n--- a/torch/nn/functional.py\n+++ b/torch/nn/functional.py\n@@ -3368,10 +3368,8 @@ def margin_ranking_loss(\n         reduction_enum = _Reduction.get_enum(reduction)\n     if (input1.dim() != input2.dim() or input1.dim() != target.dim()):\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(input1.size(), input2.size(), target.size())\n-            )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(input1.size(), input2.size(), target.size())\n         )\n     return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)\n \n@@ -4677,9 +4675,9 @@ def triplet_margin_with_distance_loss(\n     n_dim = negative.ndim\n     if not (a_dim == p_dim and p_dim == n_dim):\n         raise RuntimeError(\n-            (f\"The anchor, positive, and negative tensors are expected to have \"\n-             f\"the same number of dimensions, but got: anchor {a_dim}D, \"\n-             f\"positive {p_dim}D, and negative {n_dim}D inputs\"))\n+            f\"The anchor, positive, and negative tensors are expected to have \"\n+            f\"the same number of dimensions, but got: anchor {a_dim}D, \"\n+            f\"positive {p_dim}D, and negative {n_dim}D inputs\")\n \n     # Calculate loss\n     if distance_function is None:\ndiff --git a/torch/nn/modules/container.py b/torch/nn/modules/container.py\nindex 03ce028b7256cf..72d7b0f59cc0b7 100644\n--- a/torch/nn/modules/container.py\n+++ b/torch/nn/modules/container.py\n@@ -834,7 +834,7 @@ def fromkeys(self, keys: Iterable[str], default: Optional[Any] = None) -> 'Param\n             keys (iterable, string): keys to make the new ParameterDict from\n             default (Parameter, optional): value to set for all keys\n         \"\"\"\n-        return ParameterDict(((k, default) for k in keys))\n+        return ParameterDict((k, default) for k in keys)\n \n     def keys(self) -> Iterable[str]:\n         r\"\"\"Return an iterable of the ParameterDict keys.\ndiff --git a/torch/nn/parallel/distributed.py b/torch/nn/parallel/distributed.py\nindex 6f9fdcca018ac5..d96dfb3e5f6bd2 100644\n--- a/torch/nn/parallel/distributed.py\n+++ b/torch/nn/parallel/distributed.py\n@@ -671,7 +671,7 @@ def __init__(\n             for n, p in module.named_parameters()\n             if n not in self.parameters_to_ignore\n         ]\n-        if not any((p.requires_grad for p in self._module_parameters)):\n+        if not any(p.requires_grad for p in self._module_parameters):\n             if len(self._delay_all_reduce_params):\n                 logger.info(\"Delay the AllReduce of all parameters.\")\n             else:\ndiff --git a/torch/onnx/symbolic_opset11.py b/torch/onnx/symbolic_opset11.py\nindex b432244c42aaa7..3bb63e0e8fa36b 100644\n--- a/torch/onnx/symbolic_opset11.py\n+++ b/torch/onnx/symbolic_opset11.py\n@@ -888,7 +888,7 @@ def _get_arange_dtype(dtype):\n         dtype = symbolic_helper._maybe_get_const(dtype, \"i\")\n         return dtype\n \n-    if len(args) == 2 and all((isinstance(val, int) for val in args)):\n+    if len(args) == 2 and all(isinstance(val, int) for val in args):\n         # aten::arange(Scalar start, Scalar end)\n         dtype = torch.int64\n         # Start index.\ndiff --git a/torch/onnx/symbolic_opset17.py b/torch/onnx/symbolic_opset17.py\nindex 92151a5e58d977..3fa03ab8e11915 100644\n--- a/torch/onnx/symbolic_opset17.py\n+++ b/torch/onnx/symbolic_opset17.py\n@@ -144,8 +144,8 @@ def stft(\n         # Center window around zeros if needed (required by ONNX's STFT)\n         if n_win < n_fft:\n             left, right = _compute_edge_sizes(n_fft, n_win)\n-            left_win = g.op(\"Constant\", value_t=torch.zeros((left)))\n-            right_win = g.op(\"Constant\", value_t=torch.zeros((right)))\n+            left_win = g.op(\"Constant\", value_t=torch.zeros(left))\n+            right_win = g.op(\"Constant\", value_t=torch.zeros(right))\n             window = g.op(\"Concat\", left_win, window, right_win, axis_i=0)\n \n     # Create window, if needed\n@@ -161,11 +161,11 @@ def stft(\n             # Center window, if needed\n             left, right = _compute_edge_sizes(n_fft, win_length)\n             torch_window = torch.hstack(\n-                (torch.zeros((left)), torch.ones((win_length)), torch.zeros((right)))\n+                (torch.zeros(left), torch.ones(win_length), torch.zeros(right))\n             )\n         else:\n             # Rectangle window\n-            torch_window = torch.ones((n_fft))\n+            torch_window = torch.ones(n_fft)\n         assert torch_window.shape[0] == n_fft\n         window = g.op(\"Constant\", value_t=torch_window)\n     window = g.op(\ndiff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py\nindex 34d27bdaca6058..90d95ecb8d3c34 100644\n--- a/torch/optim/optimizer.py\n+++ b/torch/optim/optimizer.py\n@@ -468,8 +468,8 @@ def load_state_dict(self, state_dict):\n                              \"that doesn't match the size of optimizer's group\")\n \n         # Update the state\n-        id_map = dict(zip(chain.from_iterable((g['params'] for g in saved_groups)),\n-                      chain.from_iterable((g['params'] for g in groups))))\n+        id_map = dict(zip(chain.from_iterable(g['params'] for g in saved_groups),\n+                      chain.from_iterable(g['params'] for g in groups)))\n \n         def cast(param, value, param_id=None, param_groups=None, key=None):\n             r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex f9478b66327605..d9eeb7c5a1177b 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -156,14 +156,12 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-                        (\n-                            \"      Note: While we usually use modules in the python standard library \"\n-                            f\"from the local environment, `{module_name}` has a lot of system \"\n-                            \"level access and therefore can pose a security risk. We heavily \"\n-                            f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n-                            \"is not possible, add it to the extern list by calling \"\n-                            f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-                        )\n+                        \"      Note: While we usually use modules in the python standard library \"\n+                        f\"from the local environment, `{module_name}` has a lot of system \"\n+                        \"level access and therefore can pose a security risk. We heavily \"\n+                        f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n+                        \"is not possible, add it to the extern list by calling \"\n+                        f'PackageExporter.extern(\"`{module_name}`\")\\n'\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +171,8 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-                (\n-                    \"Set debug=True when invoking PackageExporter for a visualization of where \"\n-                    \"broken modules are coming from!\\n\"\n-                )\n+                \"Set debug=True when invoking PackageExporter for a visualization of where \"\n+                \"broken modules are coming from!\\n\"\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex 7ade85a85caa11..fbbcd4d67b7889 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -738,11 +738,11 @@ def _category_snapshot(self) -> Dict[TensorAndID, Optional[Category]]:\n \n         for node in self._data_flow_graph.flow_nodes:\n             all_tensor_versions.update(((k, v) for k, (_, v) in node.inputs.items()))\n-            all_tensor_versions.update(((key, 0) for key in node.intermediates))\n+            all_tensor_versions.update((key, 0) for key in node.intermediates)\n             all_tensor_versions.update(node.outputs.items())\n \n         for i in self._categories._values.values():\n-            all_tensor_versions.update(((key, 0) for key in i._by_id_keyset))\n+            all_tensor_versions.update((key, 0) for key in i._by_id_keyset)\n \n         return {\n             (key, version): self._categories.get(key, version)\ndiff --git a/torch/serialization.py b/torch/serialization.py\nindex ca362851687147..e5b069540cfe02 100644\n--- a/torch/serialization.py\n+++ b/torch/serialization.py\n@@ -559,9 +559,9 @@ def _check_dill_version(pickle_module) -> None:\n \n def _check_save_filelike(f):\n     if not isinstance(f, (str, os.PathLike)) and not hasattr(f, 'write'):\n-        raise AttributeError((\n+        raise AttributeError(\n             \"expected 'f' to be string, path, or a file-like object with \"\n-            \"a 'write' attribute\"))\n+            \"a 'write' attribute\")\n \n \n def save(\ndiff --git a/torch/storage.py b/torch/storage.py\nindex 43c4b89a2279de..5c9dd90f5bd6c9 100644\n--- a/torch/storage.py\n+++ b/torch/storage.py\n@@ -99,7 +99,7 @@ def __repr__(self):\n         return str(self)\n \n     def __iter__(self):\n-        return iter((self[i] for i in range(self.size())))\n+        return iter(self[i] for i in range(self.size()))\n \n     def __copy__(self):\n         return self.clone()\n@@ -784,7 +784,7 @@ def __repr__(self):\n \n     def __iter__(self):\n         _warn_typed_storage_removal()\n-        return iter((self[i] for i in range(self.size())))\n+        return iter(self[i] for i in range(self.size()))\n \n     def __copy__(self):\n         _warn_typed_storage_removal()\n@@ -943,13 +943,13 @@ def _from_buffer(cls, *args, dtype=None, device=None, **kwargs):\n \n         else:\n             if dtype is not None or len(args) == 5:\n-                raise RuntimeError((\n+                raise RuntimeError(\n                     \"from_buffer: 'dtype' can only be specified in \"\n-                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\"))\n+                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\")\n             if device is not None:\n-                raise RuntimeError((\n+                raise RuntimeError(\n                     \"from_buffer: 'device' can only be specified in \"\n-                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\"))\n+                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\")\n \n             dtype = cls._dtype\n             untyped_storage = torch.UntypedStorage.from_buffer(*args, dtype=dtype, **kwargs)\ndiff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py\nindex d9c362c332479d..4fd0e5fe19edb0 100644\n--- a/torch/testing/_internal/common_device_type.py\n+++ b/torch/testing/_internal/common_device_type.py\n@@ -276,7 +276,7 @@ def _dtype_test_suffix(dtypes):\n     if isinstance(dtypes, (list, tuple)):\n         if len(dtypes) == 0:\n             return ''\n-        return '_' + '_'.join((dtype_name(d) for d in dtypes))\n+        return '_' + '_'.join(dtype_name(d) for d in dtypes)\n     elif dtypes:\n         return '_{}'.format(dtype_name(dtypes))\n     else:\ndiff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py\nindex 3f7318e64b0b6d..55bfaa91cc3236 100644\n--- a/torch/testing/_internal/common_methods_invocations.py\n+++ b/torch/testing/_internal/common_methods_invocations.py\n@@ -1889,9 +1889,9 @@ def sample_inputs_logcumsumexp(self, device, dtype, requires_grad, **kwargs):\n             yield SampleInput(t, dim)\n \n def sample_inputs_trace(self, device, dtype, requires_grad, **kwargs):\n-    yield SampleInput((make_tensor((S, S), dtype=dtype, device=device,\n-                                   low=None, high=None,\n-                                   requires_grad=requires_grad)))\n+    yield SampleInput(make_tensor((S, S), dtype=dtype, device=device,\n+                                  low=None, high=None,\n+                                  requires_grad=requires_grad))\n \n \n def error_inputs_trace(op, device):\n@@ -4020,7 +4020,7 @@ def error_inputs_group_norm(opinfo, device, **kwargs):\n \n     # check that input has minimum number of dimensions\n     err_msg1 = \"Expected at least 2 dimensions for input tensor but received\"\n-    s1 = SampleInput(make_arg((1)), args=(1,))\n+    s1 = SampleInput(make_arg(1), args=(1,))\n     yield ErrorInput(s1, error_regex=err_msg1)\n \n     # check that the channels dimension is compatible with number of groups\n@@ -6950,7 +6950,7 @@ def make_bool_mask(shape):\n \n         if mask_t.sum() == 0:\n             def random_index(shape):\n-                return tuple((random.randrange(0, max_idx) for max_idx in shape))\n+                return tuple(random.randrange(0, max_idx) for max_idx in shape)\n \n             mask_t[random_index(mask_t.shape)] = True\n             return mask_t\ndiff --git a/torch/testing/_internal/common_modules.py b/torch/testing/_internal/common_modules.py\nindex 2119678a33f5ef..094460e23cadf8 100644\n--- a/torch/testing/_internal/common_modules.py\n+++ b/torch/testing/_internal/common_modules.py\n@@ -252,7 +252,7 @@ def bilinear_reference_fn(m, p, x1, x2, bias=True):\n                     desc='no_bias',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n         ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n-                    forward_input=FunctionInput(make_input((2)), make_input((3))),\n+                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                     desc='no_batch_dim',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n     ]\n@@ -312,9 +312,9 @@ def module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_\n     for desc, constructor_kwargs in cases:\n         module_inputs.append(\n             ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n-                        forward_input=FunctionInput(make_input((3)),\n-                                                    make_target((3)),\n-                                                    make_input((1)).abs()),\n+                        forward_input=FunctionInput(make_input(3),\n+                                                    make_target(3),\n+                                                    make_input(1).abs()),\n                         desc=desc,\n                         reference_fn=no_batch_dim_reference_fn)\n         )\n@@ -752,7 +752,7 @@ def module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, tra\n     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n     conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n     kernel_size, C_in, C_out = 3, 4, 5\n-    input_no_batch_shape = (C_in,) + tuple((i + 3 for i in range(N)))\n+    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n     input_batch_shape = (2,) + input_no_batch_shape\n     return [\n         ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n@@ -878,7 +878,7 @@ def module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad,\n         ModuleInput(constructor_input=FunctionInput(),\n                     forward_input=FunctionInput(make_input((3, 2, 5)))),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(0.5),\n@@ -897,10 +897,10 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n \n     return [\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(()))),\n+                    forward_input=FunctionInput(make_input(())),\n                     desc='scalar'),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -908,7 +908,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -916,7 +916,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -924,7 +924,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5, 6)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d_multiparam')]\n \n@@ -1216,11 +1216,11 @@ def module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3, 6, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 6, 5)))),\n+            forward_input=FunctionInput(make_input((4, 6, 5))),\n             desc='1d_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput(3, 12, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 12)))),\n+            forward_input=FunctionInput(make_input((4, 12))),\n             desc='1d_affine_GN'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 6, 1e-3),\n@@ -1334,13 +1334,13 @@ def module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_g\n             constructor_input=(\n                 FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape)))),\n+            forward_input=FunctionInput(make_input(input_batch_shape))),\n         ModuleInput(\n             constructor_input=(\n                 FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                 FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape))),\n+            forward_input=FunctionInput(make_input(input_batch_shape)),\n             desc='tracking_stats'),\n         ModuleInput(\n             constructor_input=(\n@@ -1365,7 +1365,7 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n@@ -1373,7 +1373,7 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n             desc='1d_elementwise_affine_large_batch'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3, False),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_no_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([2, 2, 5], 1e-3),\n@@ -1396,11 +1396,11 @@ def module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, require\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7))),\n             desc='1d'),\n         ModuleInput(\n             constructor_input=FunctionInput(2,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n             desc='2d_uneven_pad'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 1., 0.5, 2.),\n@@ -1415,7 +1415,7 @@ def module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, t\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(1.5, 2),\n-            forward_input=FunctionInput(make_input(((1, 3, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 7))),\n             desc='norm'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, 2, 3),\n@@ -1449,7 +1449,7 @@ def module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(4),\n-            forward_input=FunctionInput(make_input(((2, 10, 4)))),\n+            forward_input=FunctionInput(make_input((2, 10, 4))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput(4, 4),\n@@ -1468,7 +1468,7 @@ def module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n-            forward_input=FunctionInput(make_input(((3, 7, 7)))),\n+            forward_input=FunctionInput(make_input((3, 7, 7))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n@@ -1486,7 +1486,7 @@ def module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2)),\n-            forward_input=FunctionInput(make_input(((2, 3, 5, 5, 5))))),\n+            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n         ModuleInput(\n             constructor_input=FunctionInput(2, (2, 2, 2)),\n             forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n@@ -1511,7 +1511,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n@@ -1521,11 +1521,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((3, 5, 7))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\n@@ -1545,7 +1545,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n@@ -1559,11 +1559,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\ndiff --git a/torch/testing/_internal/common_nn.py b/torch/testing/_internal/common_nn.py\nindex 6bc41ab20f3fcd..a75e4b9f080a3e 100644\n--- a/torch/testing/_internal/common_nn.py\n+++ b/torch/testing/_internal/common_nn.py\n@@ -4281,7 +4281,7 @@ def test_cuda(self, test_case):\n         type_map = {torch.double: torch.float}\n         cpu_input_tuple = cpu_input if isinstance(cpu_input, tuple) else (cpu_input,)\n \n-        is_any_input_complex = any((isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple))\n+        is_any_input_complex = any(isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple)\n \n         gpu_input_tuple = to_gpu(cpu_input_tuple, type_map=type_map)\n \ndiff --git a/torch/testing/_internal/common_quantization.py b/torch/testing/_internal/common_quantization.py\nindex 95686be4511264..937241e54f54dd 100644\n--- a/torch/testing/_internal/common_quantization.py\n+++ b/torch/testing/_internal/common_quantization.py\n@@ -1601,7 +1601,7 @@ def __init__(self):\n         super().__init__()\n         self.quant = torch.ao.quantization.QuantStub()\n         self.fc1 = torch.nn.Linear(5, 8).to(dtype=torch.float)\n-        self.layer_norm = torch.nn.LayerNorm((8))\n+        self.layer_norm = torch.nn.LayerNorm(8)\n         self.group_norm = torch.nn.GroupNorm(2, 8)\n         self.instance_norm1d = torch.nn.InstanceNorm1d(8)\n         self.instance_norm2d = torch.nn.InstanceNorm2d(8)\ndiff --git a/torch/testing/_internal/opinfo/core.py b/torch/testing/_internal/opinfo/core.py\nindex 8f86cbd06af61a..1148b86c542cf8 100644\n--- a/torch/testing/_internal/opinfo/core.py\n+++ b/torch/testing/_internal/opinfo/core.py\n@@ -874,10 +874,7 @@ def __post_init__(self):\n \n         # Attribute to verify dynamic_dtypes are used.\n         self.dynamic_dtypes = any(\n-            (\n-                isinstance(dtypes, utils._dynamic_dispatch_dtypes)\n-                for dtypes in dtypes_args\n-            )\n+            isinstance(dtypes, utils._dynamic_dispatch_dtypes) for dtypes in dtypes_args\n         )\n \n         if self.dynamic_dtypes:\ndiff --git a/torch/testing/_internal/opinfo/definitions/linalg.py b/torch/testing/_internal/opinfo/definitions/linalg.py\nindex ca84eca5d3d027..a8c29dbf09309d 100644\n--- a/torch/testing/_internal/opinfo/definitions/linalg.py\n+++ b/torch/testing/_internal/opinfo/definitions/linalg.py\n@@ -1007,7 +1007,7 @@ def sample_inputs_linalg_solve(\n         nrhs = [(1,), (3,)]\n \n     for n, batch, rhs in product(ns, batches, nrhs):\n-        yield SampleInput(make_a(*batch, n, n), args=(make_b((batch + (n,) + rhs)),))\n+        yield SampleInput(make_a(*batch, n, n), args=(make_b(batch + (n,) + rhs),))\n \n \n def sample_inputs_linalg_solve_triangular(\ndiff --git a/torch/testing/_internal/opinfo/definitions/sparse.py b/torch/testing/_internal/opinfo/definitions/sparse.py\nindex 6baff3b2f86fea..570b2c546f099a 100644\n--- a/torch/testing/_internal/opinfo/definitions/sparse.py\n+++ b/torch/testing/_internal/opinfo/definitions/sparse.py\n@@ -331,7 +331,7 @@ def _validate_sample_input_sparse_reduction_sum(sample, check_validate=False):\n     }:\n         if (isinstance(dim, int) and (t_inp.dim() != 2 or keepdim)) or (\n             isinstance(dim, (list, tuple))\n-            and (((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim))\n+            and ((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim)\n         ):\n             if layout in {torch.sparse_bsr, torch.sparse_bsc}:\n                 return ErrorInput(\ndiff --git a/torch/torch_version.py b/torch/torch_version.py\nindex 745595f1df15bd..f9445ce82c413c 100644\n--- a/torch/torch_version.py\n+++ b/torch/torch_version.py\n@@ -68,7 +68,7 @@ def _convert_to_version(self, inp: Any) -> Any:\n             #   * (1)         -> Version(\"1\")\n             #   * (1, 20)     -> Version(\"1.20\")\n             #   * (1, 20, 1)  -> Version(\"1.20.1\")\n-            return Version('.'.join((str(item) for item in inp)))\n+            return Version('.'.join(str(item) for item in inp))\n         else:\n             raise InvalidVersion(inp)\n \ndiff --git a/torch/utils/jit/__init__.py b/torch/utils/jit/__init__.py\nindex 8b137891791fe9..e69de29bb2d1d6 100644\n--- a/torch/utils/jit/__init__.py\n+++ b/torch/utils/jit/__init__.py\n@@ -1 +0,0 @@\n-\n"
  },
  {
    "number": 105497,
    "title": "freezing w aot",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105497\n* #105480\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "7e3c7b500060d32841b80ad23a6845e19c4963bf",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105497",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105497/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105497.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105497.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105497/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105497/comments",
    "labels": [
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T20:39:27.167584Z",
    "state": "open",
    "patch": "From 456d73e9bbd8f27502a6afe9f522224a95eb1108 Mon Sep 17 00:00:00 2001\nFrom: Elias Ellison <elias.ellison@gmail.com>\nDate: Tue, 18 Jul 2023 13:39:21 -0700\nSubject: [PATCH] freezing w aot\n\n[ghstack-poisoned]\n---\n test/inductor/test_inductor_freezing.py | 15 ++++++++++\n torch/_inductor/compile_fx.py           | 39 +++++++++++++++++--------\n 2 files changed, 42 insertions(+), 12 deletions(-)\n\ndiff --git a/test/inductor/test_inductor_freezing.py b/test/inductor/test_inductor_freezing.py\nindex 68ee3f7213cd09..a506f2767fa55b 100644\n--- a/test/inductor/test_inductor_freezing.py\n+++ b/test/inductor/test_inductor_freezing.py\n@@ -364,6 +364,21 @@ def foo(mod, inp):\n             mod_eager = mod(x)\n             self.assertEqual(foo(mod, x), mod_eager)\n \n+    def test_cpp_wrapper(self):\n+        mod = ConvBN(3, 32, kernel_size=3, stride=2).eval().to(self.device)\n+\n+        x = torch.rand(3, 3, 32, 32).to(self.device)\n+\n+        @torch.compile(options={\"cpp_wrapper\": True})\n+        def foo(mod, x):\n+            return mod(x)\n+\n+        out_eager = mod(x)\n+\n+        with torch.no_grad():\n+            self.assertEqual(foo(mod, x), out_eager)\n+            self.assertEqual(foo(mod, x), out_eager)\n+\n     def test_conv_layout_convert_with_view(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\ndiff --git a/torch/_inductor/compile_fx.py b/torch/_inductor/compile_fx.py\nindex 3f50944882e00a..dea52ae347f699 100644\n--- a/torch/_inductor/compile_fx.py\n+++ b/torch/_inductor/compile_fx.py\n@@ -218,7 +218,11 @@ def materialize(x):\n                 real_inputs = [\n                     materialize(x)\n                     for x in [\n-                        *torch._guards.TracingContext.get().params_flat,\n+                        *[\n+                            param\n+                            for param in torch._guards.TracingContext.get().params_flat\n+                            if param is not None\n+                        ],\n                         *V.real_inputs,\n                     ]\n                 ]\n@@ -760,6 +764,9 @@ def is_saved_tensor(x):\n     return len(static_arg_idxs)\n \n \n+_in_aot_compilation = BoxedBool(False)\n+\n+\n def compile_fx_aot(\n     model_: torch.fx.GraphModule,\n     example_inputs_: List[torch.Tensor],\n@@ -771,12 +778,13 @@ def compile_fx_aot(\n         if config_patches is None\n         else {**config_patches, \"cpp_wrapper\": True}\n     )\n-    return compile_fx(\n-        model_,\n-        example_inputs_,\n-        inner_compile=functools.partial(inner_compile, aot_mode=True),\n-        config_patches=config_patches,\n-    )\n+    with unittest.mock.patch.object(_in_aot_compilation, \"value\", True):\n+        return compile_fx(\n+            model_,\n+            example_inputs_,\n+            inner_compile=functools.partial(inner_compile, aot_mode=True),\n+            config_patches=config_patches,\n+        )\n \n \n _graph_counter = itertools.count(0)\n@@ -791,6 +799,7 @@ def fw_compiler_freezing(\n     cudagraphs,\n     graph_id,\n     forward_device,\n+    cpp_wrapper,\n ):\n     from torch._inductor.freezing import convert_conv_weights_to_channels_last, freeze\n \n@@ -820,6 +829,11 @@ def fw_compiler_freezing(\n     user_visible_outputs = [n.name for n in model_outputs]\n \n     # constant params will be real tensors, not fake\n+    params_flat = torch._guards.TracingContext.get().params_flat\n+    for i in range(len(params_flat)):\n+        if i not in preserved_arg_indices:\n+            params_flat[i] = None\n+\n     with unittest.mock.patch.object(fake_mode, \"allow_non_fake_inputs\", True):\n         optimized_function = inner_compile(\n             opt_model,\n@@ -833,11 +847,10 @@ def fw_compiler_freezing(\n             user_visible_outputs=user_visible_outputs,\n         )\n \n-    # Need to drop the args we have constant-ified.\n-    params_flat = torch._guards.TracingContext.get().params_flat\n-    for i in range(len(params_flat)):\n-        if i not in preserved_arg_indices:\n-            params_flat[i] = None\n+    # aot_inductor codegens a call that takes in just the inputs, so we don't return a wrapper\n+    # that drops constant-ified params\n+    if _in_aot_compilation:\n+        return optimized_function\n \n     def wrapper(args):\n         args_new = [args[i] for i in preserved_arg_indices]\n@@ -845,6 +858,7 @@ def wrapper(args):\n         return optimized_function(args_new)\n \n     wrapper._boxed_call = True\n+\n     return wrapper\n \n \n@@ -1009,6 +1023,7 @@ def fw_compiler_base(model: torch.fx.GraphModule, example_inputs, is_inference):\n             cudagraphs=cudagraphs,\n             graph_id=graph_id,\n             forward_device=forward_device,\n+            cpp_wrapper=config.cpp_wrapper\n         )\n     else:\n         inference_compiler = functools.partial(fw_compiler_base, is_inference=True)\n"
  },
  {
    "number": 105496,
    "title": "[inductor] Fix an AOTInductor missing output issue",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105496\n\nSummary: When an output buffer is reused instead of directly referring to the passed-in output, we need to explictly make a copy\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "ab974eeed5920dbb81202af1e9d700fb6bba6f85",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105496",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105496/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105496.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105496.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105496/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105496/comments",
    "labels": [
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T20:36:09.459449Z",
    "state": "open",
    "patch": "From c40856aeea373b6b2e9db9a0581b6d5dde06e75c Mon Sep 17 00:00:00 2001\nFrom: Bin Bao <binbao@meta.com>\nDate: Tue, 18 Jul 2023 13:36:00 -0700\nSubject: [PATCH] [inductor] Fix an AOTInductor missing output issue\n\nSummary: When an output buffer is reused instead of directly referring to the passed-in output, we need to explictly make a copy\n\n[ghstack-poisoned]\n---\n torch/_inductor/codegen/wrapper.py | 11 ++++++++---\n 1 file changed, 8 insertions(+), 3 deletions(-)\n\ndiff --git a/torch/_inductor/codegen/wrapper.py b/torch/_inductor/codegen/wrapper.py\nindex 4cf1bc9da75707..c5639ce782c970 100644\n--- a/torch/_inductor/codegen/wrapper.py\n+++ b/torch/_inductor/codegen/wrapper.py\n@@ -1058,6 +1058,12 @@ def define_kernel(\n     def generate_return(self, output_refs):\n         # Output tensors are allocated by the AOT runtime.\n         if V.graph.aot_mode:\n+            for idx, output in enumerate(V.graph.graph_outputs):\n+                if output.get_name() in self.reuses:\n+                    # buffer was reused, so we need to explicitly copy it to the output tensor\n+                    self.wrapper_call.writeline(\n+                        f\"outputs[{idx}].copy_({output.get_name()});\"\n+                    )\n             self.wrapper_call.writeline(\"\\n}\")\n         else:\n             self.wrapper_call.writeline(f\"return {{{', '.join(output_refs)}}};\\n}}\")\n@@ -1197,11 +1203,10 @@ def codegen_tensor_option(self, device, dtype):\n     def make_buffer_allocation(self, buffer):\n         output_idx = None\n         for idx, output in enumerate(V.graph.graph_outputs):\n-            if isinstance(output, (ir.NoneAsConstantBuffer, ir.ShapeAsConstantBuffer)):\n-                continue\n-            if buffer == output.data:\n+            if hasattr(output, \"get_name\") and buffer.get_name() == output.get_name():\n                 output_idx = idx\n                 break\n+\n         if output_idx is not None and V.graph.aot_mode:\n             # In aot_mode, output buffers are managed by the AOT runtime.\n             return (\n"
  },
  {
    "number": 105495,
    "title": "[PyTorch] [Memory profiler] Early return if qualified name is invalid",
    "body": "Summary: Return early if we can easily determine the operator qualified name is invalid before attempting to retrieve the schema. In particular \"::\" should always be present. Quick estimate shows that this is >50x faster (100 us -> 2 us).\n\nTest Plan: CI\n\nDifferential Revision: D47562587\n\n",
    "merge_commit_sha": "71d181240408c223907be1ee407da078c9e029ba",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105495",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105495/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105495.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105495.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105495/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105495/comments",
    "labels": [
      "fb-exported",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T20:29:02.854484Z",
    "state": "open",
    "patch": "From 596614254b6623a0e7cd65407a53819aabaaba86 Mon Sep 17 00:00:00 2001\nFrom: Howard Cheng <chowar@meta.com>\nDate: Tue, 18 Jul 2023 14:33:09 -0700\nSubject: [PATCH] [PyTorch] [Memory profiler] Early return if qualified name is\n invalid (#105495)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105495\n\nReturn early if we can easily determine the operator qualified name is invalid before attempting to retrieve the schema. In particular \"::\" should always be present. Quick estimate shows that this is >50x faster (100 us -> 2 us).\n\nTest Plan: CI\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D47562587\n\nfbshipit-source-id: b05e027b7b145fa481dfe114b1026f6003f35220\n---\n torch/profiler/_memory_profiler.py | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex fbbcd4d67b7889..12e5959cc9b48b 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -310,6 +310,8 @@ def lookup_schemas(name: str) -> Optional[Tuple[FunctionSchema, ...]]:\n             # Note that record_function annotations also go through this path,\n             # so it is expected that some names will not correspond to PyTorch\n             # operators.\n+            if \"::\" not in name:\n+                return None\n             return tuple(torch._C._jit_get_schemas_for_operator(name))\n         except RuntimeError:\n             return None\n"
  },
  {
    "number": 105492,
    "title": "[inductor][fx passes] batch layernom",
    "body": "Summary: Batch layernorm. Fuse independent horizontal layernorm with same size into one.\n\nTest Plan:\n# unit test\n```\nbuck test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion\nFile changed: fbcode//caffe2/test/inductor/test_group_batch_fusion.py\nBuck UI: https://www.internalfb.com/buck2/68eb51e1-bdbc-4847-aabf-e50737d8485b\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/5066549764442206\nNetwork: Up: 0 B  Down: 0 B\nJobs completed: 10. Time elapsed: 1:07.2s.\nTests finished: Pass 3. Fail 0. Fatal 0. Skip 0. Build failure 0\n```\n\nDifferential Revision: D47447542\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "4a30f12f9c761310859d64fbd982f3f962af22c1",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105492",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105492/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105492.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105492.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105492/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105492/comments",
    "labels": [
      "fb-exported",
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T19:47:49.791246Z",
    "state": "open",
    "patch": "From fa112eac82dc405891392c532846559103dcbfbf Mon Sep 17 00:00:00 2001\nFrom: \"Jackie (Jiaqi) Xu\" <jackiexu0313@meta.com>\nDate: Tue, 18 Jul 2023 14:34:37 -0700\nSubject: [PATCH] [inductor][fx passes] batch layernom (#105492)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105492\n\nBatch layernorm. Fuse independent horizontal layernorm with same size into one.\n\nTest Plan:\n# unit test\n```\nbuck test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion\nFile changed: fbcode//caffe2/test/inductor/test_group_batch_fusion.py\nBuck UI: https://www.internalfb.com/buck2/68eb51e1-bdbc-4847-aabf-e50737d8485b\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/5066549764442206\nNetwork: Up: 0 B  Down: 0 B\nJobs completed: 10. Time elapsed: 1:07.2s.\nTests finished: Pass 3. Fail 0. Fatal 0. Skip 0. Build failure 0\n```\n\nDifferential Revision: D47447542\n\nfbshipit-source-id: e1d021e80f1b603d7c87d9916e4b4ea2abc5eac9\n---\n test/inductor/test_group_batch_fusion.py      | 98 +++++++++++++++++--\n torch/_inductor/config.py                     |  3 +\n .../_inductor/fx_passes/group_batch_fusion.py | 92 ++++++++++++++++-\n torch/_inductor/fx_passes/post_grad.py        |  4 +-\n torch/_inductor/fx_passes/pre_grad.py         |  2 +\n 5 files changed, 186 insertions(+), 13 deletions(-)\n\ndiff --git a/test/inductor/test_group_batch_fusion.py b/test/inductor/test_group_batch_fusion.py\nindex 554b795fd1768b..da325c33d05401 100644\n--- a/test/inductor/test_group_batch_fusion.py\n+++ b/test/inductor/test_group_batch_fusion.py\n@@ -91,8 +91,30 @@ def forward(\n         return d0 + d1\n \n \n+class MyModule3(torch.nn.Module):\n+    def __init__(self, device):\n+        super().__init__()\n+        self.device = device\n+        self.scale0 = torch.nn.ParameterList([torch.nn.Parameter(torch.randn(10)) for _ in range(5)]).to(self.device)\n+        self.bias0 = torch.nn.ParameterList([torch.nn.Parameter(torch.randn(10)) for _ in range(5)]).to(self.device)\n+        self.scale1 = torch.nn.ParameterList([torch.nn.Parameter(torch.randn(5, 10)) for _ in range(5)]).to(self.device)\n+        self.bias1 = torch.nn.ParameterList([torch.nn.Parameter(torch.randn(5, 10)) for _ in range(5)]).to(self.device)\n+\n+    def forward(self, x):\n+        l1_out = torch.split(x.to(self.device), 10, dim=2)\n+        post_l1 = [torch.nn.functional.layer_norm(l1_out[i], (10,), weight=self.scale0[i], bias=self.bias0[i])\n+                    for i in range(len(l1_out))]\n+        l1_out = torch.cat(post_l1, dim=2)\n+\n+        l2_out = torch.split(l1_out, 10, dim=2)\n+        post_l2 = [torch.nn.functional.layer_norm(l2_out[i], (5, 10), weight=self.scale1[i], bias=self.bias1[i])\n+                    for i in range(len(l2_out))]\n+\n+        return torch.cat(post_l2, dim=2)\n+\n+\n @unittest.skipIf(not has_fbgemm, \"requires fbgemm\")\n-@torch._inductor.config.patch(group_fusion=True)\n+@torch._inductor.config.patch(group_fusion=True, batch_fusion=True)\n class TestGroupFusion(TestCase):\n     def compare_dict_tensors(self, ref_dict, res_dict):\n         if len(set(ref_dict.keys())) != len(set(res_dict.keys())):\n@@ -125,9 +147,9 @@ def test_group_linear_fusion(self):\n             counters.clear()\n             module = MyModule(z, has_bias).eval().to(\"cuda\")\n             input = [\n-                torch.randn(4, z).to(\"cuda\"),\n-                torch.randn(4, z).to(\"cuda\"),\n-                torch.randn(4, z).to(\"cuda\"),\n+                torch.randn(4, z, device=\"cuda\"),\n+                torch.randn(4, z, device=\"cuda\"),\n+                torch.randn(4, z, device=\"cuda\"),\n             ]\n             traced = torch.compile(module)\n             ref = module(*input)\n@@ -137,6 +159,10 @@ def test_group_linear_fusion(self):\n                 counters[\"inductor\"][\"group_fusion\"],\n                 2 if has_bias else 0,\n             )\n+            self.assertEqual(\n+                counters[\"inductor\"][\"batch_fusion\"],\n+                0,\n+            )\n             ref.sum().backward()\n             res.sum().backward()\n             self.compare_parameters(module, traced)\n@@ -145,15 +171,19 @@ def test_group_linear_fusion(self):\n                 counters[\"inductor\"][\"group_fusion\"],\n                 2 if has_bias else 0,\n             )\n+            self.assertEqual(\n+                counters[\"inductor\"][\"batch_fusion\"],\n+                0,\n+            )\n             counters.clear()\n \n     def test_group_linear_fusion_different_shapes(self):\n         counters.clear()\n         module = MyModule2().eval().to(\"cuda\")\n         input = [\n-            torch.randn(4, 6).to(\"cuda\"),\n-            torch.randn(4, 8).to(\"cuda\"),\n-            torch.randn(4, 10).to(\"cuda\"),\n+            torch.randn(4, 6, device=\"cuda\"),\n+            torch.randn(4, 8, device=\"cuda\"),\n+            torch.randn(4, 10, device=\"cuda\"),\n         ]\n         traced = torch.compile(module)\n         ref = module(*input)\n@@ -163,6 +193,10 @@ def test_group_linear_fusion_different_shapes(self):\n             counters[\"inductor\"][\"group_fusion\"],\n             1,\n         )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"batch_fusion\"],\n+            0,\n+        )\n         ref.sum().backward()\n         res.sum().backward()\n         self.compare_parameters(module, traced)\n@@ -171,6 +205,56 @@ def test_group_linear_fusion_different_shapes(self):\n             counters[\"inductor\"][\"group_fusion\"],\n             1,\n         )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"batch_fusion\"],\n+            0,\n+        )\n+        counters.clear()\n+\n+    def test_batch_layer_norm_fusion(self):\n+        counters.clear()\n+        module = MyModule3(\"cuda\").eval().to(\"cuda\")\n+        input = [torch.randn(2, 5, 50, device=\"cuda\")]\n+        traced = torch.compile(module)\n+        ref = module(*input)\n+        res = traced(*input)\n+        self.compare_pred(module, traced, input)\n+        self.assertEqual(\n+            counters[\"inductor\"][\"group_fusion\"],\n+            0,\n+        )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"batch_fusion\"],\n+            2\n+        )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"scmerge_split_removed\"],\n+            3,\n+        )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"scmerge_cat_removed\"],\n+            3,\n+        )\n+        ref.sum().backward()\n+        res.sum().backward()\n+        self.compare_parameters(module, traced)\n+        self.compare_gradients(module, traced)\n+        self.assertEqual(\n+            counters[\"inductor\"][\"group_fusion\"],\n+            0,\n+        )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"batch_fusion\"],\n+            2\n+        )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"scmerge_split_removed\"],\n+            3,\n+        )\n+        self.assertEqual(\n+            counters[\"inductor\"][\"scmerge_cat_removed\"],\n+            3,\n+        )\n         counters.clear()\n \n \ndiff --git a/torch/_inductor/config.py b/torch/_inductor/config.py\nindex 98a675ebb5d836..7648274cc12ee0 100644\n--- a/torch/_inductor/config.py\n+++ b/torch/_inductor/config.py\n@@ -51,6 +51,9 @@\n # enable pattern match with group fusion (using fbgemm)\n group_fusion = False\n \n+# enable pattern match with batch fusion (using torch op)\n+batch_fusion = True\n+\n # enable reordering pass\n reordering = True\n \ndiff --git a/torch/_inductor/fx_passes/group_batch_fusion.py b/torch/_inductor/fx_passes/group_batch_fusion.py\nindex 63dd0193abad33..8718e0308da5a9 100644\n--- a/torch/_inductor/fx_passes/group_batch_fusion.py\n+++ b/torch/_inductor/fx_passes/group_batch_fusion.py\n@@ -6,7 +6,7 @@\n from torch._dynamo.utils import counters\n \n from .. import config\n-from ..pattern_matcher import CallFunctionVarArgs\n+from ..pattern_matcher import CallFunctionVarArgs, get_arg_value\n \n try:\n     # importing this will register fbgemm lowerings for inductor\n@@ -21,7 +21,7 @@\n \n log = logging.getLogger(__name__)\n \n-maximum_group_size = 50\n+maximum_group_size = 150\n \n \n def _has_path(src_node, dest_node, cache):\n@@ -156,6 +156,78 @@ def fuse(self, graph, subset):\n             graph.erase_node(original_mm)\n \n \n+class BatchLayernormFusion(BatchFusion):\n+    \"\"\"\n+    Batch layer norm fusion in pre grad pass\n+    \"\"\"\n+\n+    def match(self, node):\n+        if CallFunctionVarArgs(torch.nn.functional.layer_norm).match(node):\n+            input_node = get_arg_value(node, 0, \"input\")\n+            group_key = (\n+                \"batch_layernorm\",\n+                str(input_node.meta[\"example_value\"].shape)\n+                if \"example_value\" in input_node.meta\n+                else \"\",\n+                str(get_arg_value(node, 1, \"normalized_shape\")),\n+                str(get_arg_value(node, 4, \"eps\")),\n+            )\n+        else:\n+            group_key = None\n+        return group_key\n+\n+    def fuse(self, graph, subset):\n+        group_inputs = []\n+        group_shapes = []\n+        group_weights = []\n+        group_biases = []\n+        group_epss = []\n+        group_nodes = []\n+        for node in subset:\n+            group_nodes.append(node)\n+            group_inputs.append(get_arg_value(node, 0, \"input\"))\n+            group_shapes.append(get_arg_value(node, 1, \"normalized_shape\"))\n+            group_weights.append(get_arg_value(node, 2, \"weight\"))\n+            group_biases.append(get_arg_value(node, 3, \"bias\"))\n+            eps = get_arg_value(node, 4, \"eps\")\n+            if eps is None:\n+                eps = 1e-5\n+            group_epss.append(eps)\n+        stack_dim = -1 - len(group_shapes[-1])\n+\n+        with graph.inserting_before(subset[0]):\n+            stack_input = graph.call_function(\n+                torch.stack, args=(group_inputs, stack_dim)\n+            )\n+            stack_weight = graph.call_function(torch.stack, args=(group_weights,))\n+            stack_bias = graph.call_function(torch.stack, args=(group_biases,))\n+\n+            batch_layer_norm = graph.call_function(\n+                torch.nn.functional.layer_norm,\n+                args=(stack_input, group_shapes[-1]),\n+                kwargs={\"eps\": group_epss[-1]},\n+            )\n+\n+            batch_layer_norm_addcmul = graph.call_function(\n+                torch.addcmul, args=(stack_bias, stack_weight, batch_layer_norm)\n+            )\n+\n+            batch_layer_norm_unbind = graph.call_function(\n+                torch.unbind,\n+                args=(batch_layer_norm_addcmul,),\n+                kwargs={\"dim\": stack_dim},\n+            )\n+\n+        for i, node in enumerate(group_nodes):\n+            with graph.inserting_after(batch_layer_norm_unbind):\n+                new_node = graph.call_function(\n+                    operator.getitem, args=(batch_layer_norm_unbind, i)\n+                )\n+            node.replace_all_uses_with(new_node)\n+            new_node.meta.update(node.meta)\n+            graph.erase_node(node)\n+\n+\n def apply_group_batch_fusion(graph, rule):\n     fusible_groups = collections.defaultdict(list)\n \n@@ -180,11 +252,23 @@ def apply_group_batch_fusion(graph, rule):\n                 counters[\"inductor\"][\"batch_fusion\"] += 1\n \n \n-def group_batch_fusion_passes(graph: torch.fx.Graph):\n+def group_batch_fusion_post_grad_passes(graph: torch.fx.Graph):\n     fusions = []\n-\n     if config.group_fusion and has_fbgemm:\n         fusions += [GroupLinearFusion()]\n \n     for rule in fusions:\n         apply_group_batch_fusion(graph, rule)\n+\n+    log.debug(f\"Total number of group fusion {counters['inductor']['group_fusion']}\")\n+    log.debug(f\"Total number of batch fusion {counters['inductor']['batch_fusion']}\")\n+\n+\n+def group_batch_fusion_pre_grad_passes(graph: torch.fx.Graph):\n+    fusions = []\n+\n+    if config.batch_fusion and has_fbgemm:\n+        fusions += [BatchLayernormFusion()]\n+\n+    for rule in fusions:\n+        apply_group_batch_fusion(graph, rule)\ndiff --git a/torch/_inductor/fx_passes/post_grad.py b/torch/_inductor/fx_passes/post_grad.py\nindex 27cd64c02304a9..746cb7db543896 100644\n--- a/torch/_inductor/fx_passes/post_grad.py\n+++ b/torch/_inductor/fx_passes/post_grad.py\n@@ -28,7 +28,7 @@\n     stable_topological_sort,\n )\n from ..virtualized import V\n-from .group_batch_fusion import group_batch_fusion_passes\n+from .group_batch_fusion import group_batch_fusion_post_grad_passes\n \n \n log = logging.getLogger(__name__)\n@@ -67,7 +67,7 @@ def post_grad_passes(gm: torch.fx.GraphModule, is_inference: bool):\n         if is_inference:\n             inference_patterns.apply(gm.graph)\n \n-        group_batch_fusion_passes(gm.graph)\n+        group_batch_fusion_post_grad_passes(gm.graph)\n \n     stable_topological_sort(gm.graph)\n     gm.recompile()\ndiff --git a/torch/_inductor/fx_passes/pre_grad.py b/torch/_inductor/fx_passes/pre_grad.py\nindex dd3de1a1f18773..21e287ec06b6bc 100644\n--- a/torch/_inductor/fx_passes/pre_grad.py\n+++ b/torch/_inductor/fx_passes/pre_grad.py\n@@ -22,6 +22,7 @@\n     stable_topological_sort,\n )\n from ..utils import is_cpu_device\n+from .group_batch_fusion import group_batch_fusion_pre_grad_passes\n \n log = logging.getLogger(__name__)\n \n@@ -62,6 +63,7 @@ def pre_grad_passes(gm, example_inputs):\n     if config.pattern_matcher:\n         lazy_init()\n         gm = fuse_fx(gm, example_inputs)\n+        group_batch_fusion_pre_grad_passes(gm.graph)\n         for pattern_matcher_pass in pattern_matcher_passes:\n             pattern_matcher_pass.apply(gm.graph)\n \n"
  },
  {
    "number": 105491,
    "title": "Reland: Value range refinement using multi-variate expressions",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105491\n\nTrying to re-land: #97964.",
    "merge_commit_sha": "b7baf51354481a259d1836a3229084a208c2165b",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105491",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105491/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105491.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105491.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105491/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105491/comments",
    "labels": [
      "open source",
      "release notes: fx",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T19:38:34.308565Z",
    "state": "open",
    "patch": "From 573ee692e371471a1abb1c968157515e73496c99 Mon Sep 17 00:00:00 2001\nFrom: Yukio Siraichi <yukio.siraichi@gmail.com>\nDate: Tue, 18 Jul 2023 16:38:26 -0300\nSubject: [PATCH] Reland: Value range refinement using multi-variate\n expressions\n\nTrying to re-land: #97964.\n\n[ghstack-poisoned]\n---\n test/test_proxy_tensor.py                | 22 ++++++++++++++++++++++\n torch/fx/experimental/symbolic_shapes.py |  4 ----\n 2 files changed, 22 insertions(+), 4 deletions(-)\n\ndiff --git a/test/test_proxy_tensor.py b/test/test_proxy_tensor.py\nindex 1b5f2b173e0242..6c971c1b5c2572 100644\n--- a/test/test_proxy_tensor.py\n+++ b/test/test_proxy_tensor.py\n@@ -1360,6 +1360,28 @@ def f(a):\n         tensor = make_fx(f, tracing_mode=\"symbolic\")(torch.randn(15))\n         self.assertExpectedInline(show_guards(tensor), \"\"\"L['a'].size()[0] < 20\"\"\")\n \n+    def test_guard_upperbound_range_refinement_multivariate(self):\n+        def f(a):\n+            assert a.shape[0] > 5 and a.shape[0] > 12\n+            assert a.shape[1] > 5 and a.shape[1] > a.shape[0]\n+            return a.cos()\n+        tensor = make_fx(f, tracing_mode=\"symbolic\")(torch.randn((15, 20)))\n+        self.assertExpectedInline(show_guards(tensor), \"\"\"\\\n+L['a'].size()[1] > L['a'].size()[0]\n+L['a'].size()[0] > 12\"\"\")\n+\n+    def test_guard_lowerbound_range_refinement_multivariate(self):\n+        def f(a):\n+            assert a.shape[0] < 20 and a.shape[0] < 30\n+            assert a.shape[1] < 30 and a.shape[1] < a.shape[0]\n+            return a.cos()\n+        tensor = make_fx(f, tracing_mode=\"symbolic\")(torch.randn((15, 5)))\n+        self.assertExpectedInline(\n+            show_guards(tensor),\n+            \"\"\"\\\n+L['a'].size()[1] < L['a'].size()[0]\n+L['a'].size()[0] < 20\"\"\")\n+\n     def test_sym_storage_offset(self):\n         def f(x, y):\n             return x + y\ndiff --git a/torch/fx/experimental/symbolic_shapes.py b/torch/fx/experimental/symbolic_shapes.py\nindex 90a3c519b3c44b..6a989abc42ceed 100644\n--- a/torch/fx/experimental/symbolic_shapes.py\n+++ b/torch/fx/experimental/symbolic_shapes.py\n@@ -3415,10 +3415,6 @@ def simplify_until(expr: sympy.Expr, max_iterations: int = 10) -> sympy.Expr:\n             ):\n                 continue\n \n-            # Use only univariate functions.\n-            if len(expr.rhs.free_symbols) > 0:\n-                continue\n-\n             # Update the value range of the left-hand side, if the\n             # right-hand side provides a better range.\n             symbol = expr.lhs\n"
  },
  {
    "number": 105489,
    "title": "[WIP] torch.compile + selective activation checkpointing",
    "body": "Summary:\nThis PR implements torch.compile + selective activation checkpoint integration, by extending Animesh's earlier implementation of tagging-based torch.compile + AC integration.\n\nThe key changes are:\n1. Introducing another argument `context_fn_dynamo` to `torch.utils.checkpoint` API to accept SAC context func that's understandable by Dynamo.\n2. If user decides to checkpoint an op node by setting `node.meta[\"recompute\"] = unique_graph_id`, we will always checkpoint this node, by adding this node into `required_bw_nodes` in min-cut partitioner.\n\nTest Plan: `buck2 run mode/opt fbcode//caffe2/test/dynamo:test_dynamo -- 'caffe2/test/dynamo:test_dynamo - test_activation_checkpointing.py::ActivationCheckpointingViaTagsTests'`\n\nDifferential Revision: D47497145\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "25d35359684dcc9409ec282da7c05c16038fd388",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105489",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105489/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105489.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105489.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105489/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105489/comments",
    "labels": [
      "fb-exported",
      "ciflow/inductor",
      "module: dynamo"
    ],
    "_event_time": "2023-07-18T19:35:58.828230Z",
    "state": "open",
    "patch": "From fb757ea441666ddf35327905bf4860e6589b3ce6 Mon Sep 17 00:00:00 2001\nFrom: Will Feng <willfeng@meta.com>\nDate: Tue, 18 Jul 2023 12:33:02 -0700\nSubject: [PATCH] [WIP] torch.compile + selective activation checkpointing\n\nSummary:\nThis PR implements torch.compile + selective activation checkpoint integration, by extending Animesh's earlier implementation of tagging-based torch.compile + AC integration.\n\nThe key changes are:\n1. Introducing another argument `context_fn_dynamo` to `torch.utils.checkpoint` API to accept SAC context func that's understandable by Dynamo.\n2. If user decides to checkpoint an op node by setting `node.meta[\"recompute\"] = unique_graph_id`, we will always checkpoint this node, by adding this node into `required_bw_nodes` in min-cut partitioner.\n\nTest Plan: `buck2 run mode/opt fbcode//caffe2/test/dynamo:test_dynamo -- 'caffe2/test/dynamo:test_dynamo - test_activation_checkpointing.py::ActivationCheckpointingViaTagsTests'`\n\nDifferential Revision: D47497145\n\nfbshipit-source-id: 709bad4dbbd9799c831868029ef3e159e2fb1680\n---\n test/dynamo/test_activation_checkpointing.py | 122 ++++++++++++++++++-\n torch/_dynamo/variables/higher_order_ops.py  |   4 +\n torch/_functorch/partitioners.py             |   4 +-\n torch/_higher_order_ops/wrap.py              |  22 ++--\n torch/utils/checkpoint.py                    |  10 ++\n 5 files changed, 150 insertions(+), 12 deletions(-)\n\ndiff --git a/test/dynamo/test_activation_checkpointing.py b/test/dynamo/test_activation_checkpointing.py\nindex 4ea3aeefeef333..ffa973f24a4245 100644\n--- a/test/dynamo/test_activation_checkpointing.py\n+++ b/test/dynamo/test_activation_checkpointing.py\n@@ -19,7 +19,10 @@\n \n \n def count_ops(gm, args, freq, op):\n-    assert [node.target for node in gm.graph.nodes].count(op) == freq\n+    actual_count = [node.target for node in gm.graph.nodes].count(op)\n+    assert (\n+        actual_count == freq\n+    ), f\"In graph {gm}, expected {op} to have occurred {freq} times in the graph, but got {actual_count}.\"\n     return gm\n \n \n@@ -331,6 +334,123 @@ def fn(x, y, z):\n         body_function = getattr(cnt.graphs[0], wrap_node.args[0].name)\n         self.assertEqual(op_count(body_function), 2)\n \n+    @requires_cuda()\n+    def test_tags_selective_checkpoint_gemm_only():\n+        ops_to_checkpoint = [\n+            torch.matmul,\n+        ]\n+\n+        def context_fn_dynamo(unique_graph_id, gmod):\n+            for node in gmod.graph.nodes:\n+                if node.op == \"call_function\" and node.target in ops_to_checkpoint:\n+                    node.meta[\"recompute\"] = unique_graph_id\n+\n+        def gn(x, y):\n+            return torch.sigmoid(torch.sigmoid(torch.matmul(x, y) * y))\n+\n+        def fn(x, y):\n+            return torch.utils.checkpoint.checkpoint(\n+                gn,\n+                torch.sin(x),\n+                y,\n+                use_reentrant=False,\n+                context_fn_dynamo=context_fn_dynamo,\n+            )\n+\n+        x = torch.randn(4, 4, device=\"cuda\", requires_grad=True)\n+        y = torch.randn(4, 4, device=\"cuda\", requires_grad=True)\n+\n+        fw_compiler = functools.partial(\n+            count_ops,\n+            freq=1,\n+            op=torch.ops.aten.mm.default,\n+        )\n+        bw_compiler = functools.partial(\n+            count_ops,\n+            freq=3,\n+            op=torch.ops.aten.mm.default,\n+        )  # mm is recomputed in the bwd\n+        backend = aot_autograd(fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n+        self._validate(fn, backend, x, y)\n+\n+    @requires_cuda()\n+    def test_tags_selective_checkpoint_custom_rule():\n+        def context_fn_dynamo(unique_graph_id, gmod):\n+            op_count = 0\n+            for node in gmod.graph.nodes:\n+                if node.op == \"call_function\" and node.target == torch.matmul:\n+                    op_count += 1\n+                    if op_count == 2:  # Only checkpoint the second matmul\n+                        node.meta[\"recompute\"] = unique_graph_id\n+\n+        def gn(x, y):\n+            return torch.sigmoid(torch.sigmoid(torch.matmul(torch.matmul(x, y), y)))\n+\n+        def fn(x, y):\n+            return torch.utils.checkpoint.checkpoint(\n+                gn,\n+                torch.sin(x),\n+                y,\n+                use_reentrant=False,\n+                context_fn_dynamo=context_fn_dynamo,\n+            )\n+\n+        x = torch.randn(4, 4, device=\"cuda\", requires_grad=True)\n+        y = torch.randn(4, 4, device=\"cuda\", requires_grad=True)\n+\n+        fw_compiler = functools.partial(\n+            count_ops,\n+            freq=2,\n+            op=torch.ops.aten.mm.default,\n+        )\n+        bw_compiler = functools.partial(\n+            count_ops,\n+            freq=5,  # if both matmuls are recomputed, this would be 6 instead of 5\n+            op=torch.ops.aten.mm.default,\n+        )\n+        backend = aot_autograd(fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n+        self._validate(fn, backend, x, y)\n+\n+    @requires_cuda()\n+    def test_tags_selective_checkpoint_with_inplace_op():\n+        ops_to_checkpoint = [\n+            torch.selu_,\n+        ]\n+\n+        def context_fn_dynamo(unique_graph_id, gmod):\n+            for node in gmod.graph.nodes:\n+                if node.op == \"call_function\" and node.target in ops_to_checkpoint:\n+                    node.meta[\"recompute\"] = unique_graph_id\n+\n+        def gn(x, y):\n+            return torch.sigmoid(torch.selu_(torch.matmul(torch.matmul(x, y), y)))\n+\n+        def fn(x, y):\n+            return torch.utils.checkpoint.checkpoint(\n+                gn,\n+                torch.sin(x),\n+                y,\n+                use_reentrant=False,\n+                context_fn_dynamo=context_fn_dynamo,\n+            )\n+\n+        x = torch.randn(4, 4, device=\"cuda\", requires_grad=True)\n+        y = torch.randn(4, 4, device=\"cuda\", requires_grad=True)\n+\n+        fw_compiler = functools.partial(\n+            count_ops,\n+            freq=2,\n+            op=torch.ops.aten.mm.default,\n+        )\n+        bw_compiler = functools.partial(\n+            count_ops,\n+            freq=4,\n+            op=torch.ops.aten.mm.default,\n+        )\n+        backend = aot_autograd(fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n+        # Check that saving input of in-place op doesn't affect accuracy\n+        self._validate(fn, backend, x, y)\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\ndiff --git a/torch/_dynamo/variables/higher_order_ops.py b/torch/_dynamo/variables/higher_order_ops.py\nindex 95250fe0f39ac8..6ce7db7cc93028 100644\n--- a/torch/_dynamo/variables/higher_order_ops.py\n+++ b/torch/_dynamo/variables/higher_order_ops.py\n@@ -893,6 +893,10 @@ def call_function(\n         from torch._higher_order_ops.wrap import TagActivationCheckpoint\n         from .builder import wrap_fx_proxy\n \n+        if \"context_fn_dynamo\" in kwargs:\n+            context_fn_dynamo = kwargs.pop(\"context_fn_dynamo\")\n+            self.value.context_fn_dynamo = context_fn_dynamo.fn\n+\n         checkpoint_kwargs, gmod_kwargs = TagActivationCheckpoint.divide_kwargs(kwargs)\n \n         # Here we use checkpoint_kwargs (and not gmod kwargs). gmod_kwargs are\ndiff --git a/torch/_functorch/partitioners.py b/torch/_functorch/partitioners.py\nindex b774c80a61db06..152d87ea1433fa 100644\n--- a/torch/_functorch/partitioners.py\n+++ b/torch/_functorch/partitioners.py\n@@ -23,7 +23,7 @@\n \n \n def must_recompute(node):\n-    return node.meta.get(\"recompute\", False)\n+    return node.meta.get(\"recompute\", 0) > 0\n \n def has_recomputable_ops(fx_g):\n     found = False\n@@ -657,7 +657,7 @@ def min_cut_rematerialization_partition(\n     def classify_nodes(joint_module):\n         required_bw_nodes = set()\n         for node in joint_module.graph.nodes:\n-            if node.op == 'placeholder' and \"tangents\" in node.target:\n+            if (node.op == 'placeholder' and \"tangents\" in node.target) or must_recompute(node):\n                 required_bw_nodes.add(node)\n             if node in required_bw_nodes:\n                 for user in node.users:\ndiff --git a/torch/_higher_order_ops/wrap.py b/torch/_higher_order_ops/wrap.py\nindex c55143484c3eda..f3c51ae3fcfa23 100644\n--- a/torch/_higher_order_ops/wrap.py\n+++ b/torch/_higher_order_ops/wrap.py\n@@ -1,3 +1,4 @@\n+import torch\n from torch._ops import HigherOrderOperator\n from torch.utils.checkpoint import checkpoint\n from itertools import count\n@@ -5,6 +6,9 @@\n \n uid = count(1)\n \n+def get_unique_graph_id():\n+    return next(uid)\n+\n # Used for testing the HigherOrderOperator mechanism\n class Wrap(HigherOrderOperator):\n     def __init__(self):\n@@ -75,6 +79,7 @@ class TagActivationCheckpoint(HigherOrderOperator):\n \n     def __init__(self):\n         super().__init__(\"tag_activation_checkpoint\")\n+        self.context_fn_dynamo = None\n \n     @staticmethod\n     def divide_kwargs(kwargs):\n@@ -110,19 +115,18 @@ def divide_kwargs(kwargs):\n         return checkpoint_kwargs, gmod_kwargs\n \n     def tag_nodes(self, gmod):\n-        # TODO - This needs major investigation. Currently, we are tagging all\n-        # the forward nodes as recomputable. However, torch.utils.checkpoint\n-        # provides a custom function to selectively recompute. We will have to\n-        # figure out how to tag seletively.\n-        unique_graph_id = next(uid)\n-        for node in gmod.graph.nodes:\n-            if node.op in (\"call_function\", \"call_method\", \"call_module\"):\n-                node.meta[\"recompute\"] = unique_graph_id\n+        unique_graph_id = get_unique_graph_id()\n+        if self.context_fn_dynamo:\n+            self.context_fn_dynamo(unique_graph_id, gmod)\n+        else:\n+            for node in gmod.graph.nodes:\n+                if node.op in (\"call_function\", \"call_method\", \"call_module\"):\n+                    node.meta[\"recompute\"] = unique_graph_id\n         return gmod\n \n     def __call__(self, gmod, *args, **kwargs):\n         if \"context_fn\" in kwargs:\n-            raise RuntimeError(\"Tagged Activation checkpointing does not support selective checkpointing yet.\")\n+            raise RuntimeError(\"Tagged Activation checkpointing does not support `context_fn` selective checkpointing, please use `context_fn_dynamo` following documentation.\")\n         import torch.fx.traceback as fx_traceback\n         from torch.fx import Interpreter\n         gmod = self.tag_nodes(gmod)\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..647bbc6af5ac68 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -311,6 +311,7 @@ def checkpoint(\n     *args,\n     use_reentrant: Optional[bool] = None,\n     context_fn: Callable[[], Tuple[ContextManager, ContextManager]] = noop_context_fn,\n+    context_fn_dynamo: Callable[[\"GraphModule\"], None] = None,\n     determinism_check: str = \"default\",\n     debug: bool = False,\n     **kwargs\n@@ -405,6 +406,15 @@ def checkpoint(\n             context managers. The function and its recomputation will be run\n             under the first and second context managers respectively.\n             This argument is only supported if ``use_reentrant=False``.\n+        context_fn_dynamo(Callable, optional): A callable taking GraphModule as input\n+            and walks through the graph nodes to decide whether to checkpoint a node\n+            (i.e. saves its input and recomputes the node in backward pass).\n+            If a node is marked as \"recompute\", it's guaranteed to be recomputed.\n+            This argument is only supported if ``use_reentrant=False`` (TODO: confirm if we want this).\n+            This argument is only supported if we are using `torch.compile()`.\n+            Note that `torch.compile()` will make additional decision on whether\n+            to recompute other ops (e.g. bandwidth-bound ops such as pointwise).\n+            (TODO: give example of how to use `context_fn_dynamo`)\n         determinism_check(str, optional): A string specifying the determinism\n             check to perform. By default it is set to ``\"default\"`` which\n             compares the shapes, dtypes, and devices of the recomputed tensors\n"
  },
  {
    "number": 105487,
    "title": "Remove unnecessary seen check in get_current_graph_task_execution_order",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #105487\r\n\r\nhttps://github.com/pytorch/pytorch/pull/105353#discussion_r1266977015",
    "merge_commit_sha": "494d1b8e2e621d42d6fb658e9585d370da4fe79c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105487",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105487/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105487.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105487.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105487/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105487/comments",
    "labels": [
      "merging",
      "topic: not user facing",
      "ciflow/trunk"
    ],
    "_event_time": "2023-07-18T18:33:34.640240Z",
    "state": "open",
    "patch": "From 4f894af25c6e6b6d12bc0311e30f74759d981193 Mon Sep 17 00:00:00 2001\nFrom: soulitzer <soulitzer@gmail.com>\nDate: Tue, 18 Jul 2023 14:33:27 -0400\nSubject: [PATCH] Remove unnecessary seen check in\n get_current_graph_task_execution_order\n\n[ghstack-poisoned]\n---\n torch/csrc/autograd/engine.cpp | 6 ------\n 1 file changed, 6 deletions(-)\n\ndiff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp\nindex 5010382551df0a..5b2d4415801218 100644\n--- a/torch/csrc/autograd/engine.cpp\n+++ b/torch/csrc/autograd/engine.cpp\n@@ -414,7 +414,6 @@ std::vector<Node*> get_current_graph_task_execution_order() {\n \n   const bool check_exec_info = !task->exec_info_.empty();\n   std::vector<Node*> out{};\n-  std::unordered_set<Node*> seen{};\n   // Do a copy since we mutate it later\n   std::unordered_map<Node*, int> dependencies = task->dependencies_;\n \n@@ -437,11 +436,6 @@ std::vector<Node*> get_current_graph_task_execution_order() {\n     Node* fn = heap.top();\n     heap.pop();\n \n-    const bool was_inserted = seen.insert(fn).second;\n-    if (!was_inserted) {\n-      continue;\n-    }\n-\n     out.push_back(fn);\n     for (const auto& edge : fn->next_edges()) {\n       Node* next_ptr = edge.function.get();\n"
  },
  {
    "number": 105486,
    "title": "Update core_aten_decomposition_table",
    "body": "Summary:\nUpdated the decomposition table based on the existing [Core ATen IR](https://pytorch.org/docs/stable/ir.html) list.\n\nAlso propose to add `empty_like` and `full_like` as core ops.\n\nTest Plan: CI\n\nDifferential Revision: D47531383\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "5ecbe2849173169354a8bfb48ae8e6fbb5e0fc26",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105486",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105486/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105486.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105486.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105486/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105486/comments",
    "labels": [
      "fb-exported",
      "ciflow/inductor-perf-compare",
      "ciflow/inductor",
      "module: inductor",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T18:01:08.048554Z",
    "state": "open",
    "patch": "From 02d279d60f5928303f7ab350d004a2cd565959f6 Mon Sep 17 00:00:00 2001\nFrom: Angela Yi <angelayi@meta.com>\nDate: Tue, 18 Jul 2023 10:58:04 -0700\nSubject: [PATCH] Update core_aten_decomposition_table\n\nSummary:\nUpdated the decomposition table based on the existing [Core ATen IR](https://pytorch.org/docs/stable/ir.html) list.\n\nAlso propose to add `empty_like` and `full_like` as core ops.\n\nTest Plan: CI\n\nDifferential Revision: D47531383\n\nfbshipit-source-id: 6c1775f8bcaaa88a93fbd47c486d8422452c7adf\n---\n aten/src/ATen/native/native_functions.yaml |  2 ++\n torch/_decomp/__init__.py                  | 21 ---------------------\n torch/_inductor/decomposition.py           | 21 +++++++++++++++++++++\n 3 files changed, 23 insertions(+), 21 deletions(-)\n\ndiff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml\nindex e4a4673dba0f92..9742e3b324140b 100644\n--- a/aten/src/ATen/native/native_functions.yaml\n+++ b/aten/src/ATen/native/native_functions.yaml\n@@ -2396,6 +2396,7 @@\n     SparseCsrCPU, SparseCsrCUDA: empty_like_sparse_csr\n     NestedTensorCPU, NestedTensorCUDA: empty_like_nested\n   autogen: empty_like.out\n+  tags: core\n \n - func: empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n   dispatch:\n@@ -2725,6 +2726,7 @@\n     # non-differentiable so NonFunctional doesn't apply\n     CompositeExplicitAutograd: full_like\n   autogen: full_like.out\n+  tags: core\n \n - func: from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n   dispatch:\ndiff --git a/torch/_decomp/__init__.py b/torch/_decomp/__init__.py\nindex 99f3bebefb2023..0a502cd66ee85f 100644\n--- a/torch/_decomp/__init__.py\n+++ b/torch/_decomp/__init__.py\n@@ -181,7 +181,6 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n     aten = torch.ops.aten\n     return get_decompositions(\n         [\n-            aten._adaptive_avg_pool2d_backward,\n             aten.addcdiv,\n             aten.addcdiv_,\n             aten.addcmul,\n@@ -220,9 +219,6 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.frac,\n             aten.frac_,\n             aten._fused_moving_avg_obs_fq_helper,\n-            aten.gelu,\n-            aten.gelu_,\n-            aten.gelu_backward,\n             aten.glu_backward,\n             aten.grid_sampler_2d,\n             aten.hardshrink,\n@@ -233,8 +229,6 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.hardswish,\n             aten.hardswish_,\n             aten.hardswish_backward,\n-            aten.hardtanh,\n-            aten.hardtanh_,\n             aten.hardtanh_backward,\n             aten.heaviside,\n             aten.heaviside_,\n@@ -247,12 +241,9 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.index_copy_,\n             aten.index_fill,\n             aten.index_fill_,\n-            aten.index_select,\n             aten.isneginf,\n             aten.isposinf,\n             aten.l1_loss,\n-            aten.leaky_relu,\n-            aten.leaky_relu_,\n             aten.leaky_relu_backward,\n             aten.lerp,\n             aten.lerp_,\n@@ -264,13 +255,11 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.logit_backward,\n             aten.log_sigmoid_backward,\n             aten.log_sigmoid_forward,\n-            aten._log_softmax,\n             aten._log_softmax_backward_data,\n             aten.logspace,\n             aten.logsumexp.default,\n             aten.masked_fill,\n             aten.masked_fill_,\n-            aten.max_pool2d_with_indices_backward,\n             aten.mish,\n             aten.mish_,\n             aten.mse_loss,\n@@ -283,16 +272,8 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.nan_to_num,\n             aten.nan_to_num_,\n             aten.narrow,\n-            aten.native_batch_norm,\n             aten.native_batch_norm_backward,\n-            aten._native_batch_norm_legit,\n-            aten._native_batch_norm_legit_functional,\n-            aten._native_batch_norm_legit_no_training,\n             aten.native_dropout_backward,\n-            aten.native_group_norm,\n-            aten.native_group_norm_backward,\n-            aten.native_layer_norm,\n-            aten.native_layer_norm_backward,\n             aten.new_empty,\n             aten.new_full,\n             aten.new_ones,\n@@ -327,7 +308,6 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.smooth_l1_loss_backward,\n             aten.soft_margin_loss,\n             aten.soft_margin_loss_backward,\n-            aten._softmax,\n             aten._softmax_backward_data,\n             aten.softplus,\n             aten.softplus_backward,\n@@ -351,7 +331,6 @@ def core_aten_decompositions() -> Dict[OpOverload, Callable]:\n             aten.unfold_backward,\n             aten.unfold_copy,\n             aten.upsample_bilinear2d,\n-            aten.upsample_bilinear2d.vec,\n             aten.upsample_nearest2d_backward,\n             aten.xlogy,\n             aten.xlogy_,\ndiff --git a/torch/_inductor/decomposition.py b/torch/_inductor/decomposition.py\nindex bfba644d243bbe..bc6f2d93c8024e 100644\n--- a/torch/_inductor/decomposition.py\n+++ b/torch/_inductor/decomposition.py\n@@ -19,14 +19,34 @@\n \n inductor_decompositions = get_decompositions(\n     [\n+        aten._adaptive_avg_pool2d_backward,\n         aten.arange,\n         aten.bitwise_and_,\n         aten.bitwise_or_,\n         aten.clamp_min_,\n         aten.empty_like,\n         aten.flip,\n+        aten.gelu,\n+        aten.gelu_,\n+        aten.gelu_backward,\n+        aten.hardtanh,\n+        aten.hardtanh_,\n+        aten.index_select,\n         aten.lcm,\n+        aten.leaky_relu,\n+        aten.leaky_relu_,\n         aten.linalg_vector_norm,\n+        aten._log_softmax,\n+        aten.max_pool2d_with_indices_backward,\n+        aten._native_batch_norm_legit,\n+        aten._native_batch_norm_legit_functional,\n+        aten._native_batch_norm_legit_no_training,\n+        aten.native_batch_norm,\n+        aten.native_group_norm,\n+        aten.native_group_norm_backward,\n+        aten.native_layer_norm,\n+        aten.native_layer_norm_backward,\n+        aten._softmax,\n         aten.sin_,\n         aten.sqrt_,\n         aten.std,\n@@ -35,6 +55,7 @@\n         aten.tril_indices,\n         aten.triu_indices,\n         aten.unsafe_split,\n+        aten.upsample_bilinear2d.vec,\n     ]\n )\n decompositions = {**core_aten_decompositions(), **inductor_decompositions}\n"
  },
  {
    "number": 105484,
    "title": "[quant][pt2e][be] Rename prepare_pt2e_quantizer to prepare_pt2e",
    "body": "Summary: att\n\nTest Plan: sandcastle and OSS CI\n\nReviewed By: andrewor14\n\nDifferential Revision: D47422892\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "c96dcbb1560a6569abd38d9fcbcf4e281951d9a9",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105484",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105484/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105484.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105484.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105484/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105484/comments",
    "labels": [
      "fb-exported",
      "suppress-bc-linter",
      "release notes: quantization",
      "module: inductor"
    ],
    "_event_time": "2023-07-18T17:56:24.197739Z",
    "state": "open",
    "patch": "From a4b3629a626897f186df8ee2e7e2c0c91f8780ab Mon Sep 17 00:00:00 2001\nFrom: Jerry Zhang <jerryzh@meta.com>\nDate: Tue, 18 Jul 2023 14:58:45 -0700\nSubject: [PATCH] [quant][pt2e][be] Rename prepare_pt2e_quantizer to\n prepare_pt2e (#105484)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105484\n\natt\n\nTest Plan: sandcastle and OSS CI\n\nReviewed By: andrewor14\n\nDifferential Revision: D47422892\n\nfbshipit-source-id: e47c0d045a9074bc74e298edfb9655883003adbd\n---\n test/inductor/test_inductor_freezing.py       |  4 +--\n test/quantization/pt2e/test_quantize_pt2e.py  | 32 +++++++++----------\n .../pt2e/test_x86inductor_quantizer.py        |  4 +--\n torch/ao/quantization/quantize_pt2e.py        | 11 +++----\n 4 files changed, 24 insertions(+), 27 deletions(-)\n\ndiff --git a/test/inductor/test_inductor_freezing.py b/test/inductor/test_inductor_freezing.py\nindex 614c56f3d4bae2..0fbb8ed1c9cdd5 100644\n--- a/test/inductor/test_inductor_freezing.py\n+++ b/test/inductor/test_inductor_freezing.py\n@@ -18,7 +18,7 @@\n from torch._inductor.compile_fx import compile_fx\n from torch._inductor.utils import override_lowering, run_and_get_code\n from torch.ao.quantization.pt2e.quantizer import X86InductorQuantizer\n-from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e_quantizer\n+from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e\n from torch.testing import FileCheck\n from torch.testing._internal.common_quantization import (\n     skipIfNoDynamoSupport,\n@@ -555,7 +555,7 @@ def test_functional_constant_folding_after_dynamo_export(self):\n             # int8_weight -> dequant_per_channel -> convolution\n             self.assertTrue(torch._inductor.config.freezing)\n \n-            prepare_model = prepare_pt2e_quantizer(export_model, quantizer)\n+            prepare_model = prepare_pt2e(export_model, quantizer)\n             prepare_model(*example_inputs)\n \n             convert_model = convert_pt2e(prepare_model)\ndiff --git a/test/quantization/pt2e/test_quantize_pt2e.py b/test/quantization/pt2e/test_quantize_pt2e.py\nindex df66771989f451..0a64c04bb75b89 100644\n--- a/test/quantization/pt2e/test_quantize_pt2e.py\n+++ b/test/quantization/pt2e/test_quantize_pt2e.py\n@@ -36,8 +36,8 @@\n from torch.ao.quantization.quantize_pt2e import (\n     _convert_to_reference_decomposed_fx,\n     convert_pt2e,\n-    prepare_pt2e_quantizer,\n-    prepare_qat_pt2e_quantizer,\n+    prepare_pt2e,\n+    prepare_qat_pt2e,\n )\n from torch.ao.quantization.backend_config import (\n     get_executorch_backend_config,\n@@ -235,7 +235,7 @@ def _test_quantizer(\n                 tracing_mode=\"symbolic\" if export_with_dynamic_shape else \"real\",\n             )\n \n-        m = prepare_pt2e_quantizer(m, quantizer)\n+        m = prepare_pt2e(m, quantizer)\n         # Calibrate\n         m(*example_inputs)\n         m = convert_pt2e(m)\n@@ -302,7 +302,7 @@ def _verify_symmetric_qnnpack_qat_numerics(\n             *copy.deepcopy(example_inputs),\n             aten_graph=True,\n         )\n-        model_pt2e = prepare_qat_pt2e_quantizer(model_pt2e, quantizer)\n+        model_pt2e = prepare_qat_pt2e(model_pt2e, quantizer)\n         torch.manual_seed(MANUAL_SEED)\n         after_prepare_result_pt2e = model_pt2e(*example_inputs)\n \n@@ -364,7 +364,7 @@ def _verify_symmetric_qnnpack_qat_graph(\n             aten_graph=True,\n             tracing_mode=\"real\",\n         )\n-        m = prepare_qat_pt2e_quantizer(m, quantizer)\n+        m = prepare_qat_pt2e(m, quantizer)\n         m(*example_inputs)\n \n         # Verify: getitem output activation fake quantize\n@@ -495,7 +495,7 @@ def _test_representation(\n             aten_graph=True,\n         )\n \n-        model = prepare_pt2e_quantizer(model, quantizer)\n+        model = prepare_pt2e(model, quantizer)\n         # Calibrate\n         model(*example_inputs)\n         model = convert_pt2e(model, use_reference_representation=True)\n@@ -505,7 +505,7 @@ def _test_representation(\n \n         # TODO: torchdynamo times out when we do this, we can enable numerical checking\n         # after that is fixed\n-        # model_copy = prepare_pt2e_quantizer(model_copy, quantizer)\n+        # model_copy = prepare_pt2e(model_copy, quantizer)\n         # # Calibrate\n         # model_copy(*example_inputs)\n         # model_copy = convert_pt2e(model_copy, use_reference_representation=False)\n@@ -660,7 +660,7 @@ def get_supported_operators(cls) -> List[OperatorConfig]:\n             *copy.deepcopy(example_inputs),\n             aten_graph=True,\n         )\n-        m = prepare_pt2e_quantizer(m, BackendAQuantizer())\n+        m = prepare_pt2e(m, BackendAQuantizer())\n         m(*example_inputs)\n         m = convert_pt2e(m)\n         # Ensure the conv has no observer inserted at output\n@@ -763,7 +763,7 @@ def get_supported_operators(cls) -> List[OperatorConfig]:\n             *copy.deepcopy(example_inputs),\n             aten_graph=True,\n         )\n-        m = prepare_pt2e_quantizer(m, BackendAQuantizer())\n+        m = prepare_pt2e(m, BackendAQuantizer())\n         m(*example_inputs)\n         m = convert_pt2e(m)\n         node_occurrence = {\n@@ -869,7 +869,7 @@ def get_supported_operators(cls) -> List[OperatorConfig]:\n             *copy.deepcopy(example_inputs),\n             aten_graph=True,\n         )\n-        m = prepare_pt2e_quantizer(m, BackendAQuantizer())\n+        m = prepare_pt2e(m, BackendAQuantizer())\n         m(*example_inputs)\n         m = convert_pt2e(m)\n         node_occurrence = {\n@@ -946,7 +946,7 @@ def get_supported_operators(cls) -> List[OperatorConfig]:\n             *copy.deepcopy(example_inputs),\n             aten_graph=True,\n         )\n-        m = prepare_pt2e_quantizer(m, BackendAQuantizer())\n+        m = prepare_pt2e(m, BackendAQuantizer())\n         m(*example_inputs)\n         m = convert_pt2e(m)\n         fixed_scale = 1.0 / 256.0\n@@ -1170,7 +1170,7 @@ def test_propagate_annotation(self):\n             aten_graph=True,\n         )\n \n-        m = prepare_pt2e_quantizer(m, quantizer)\n+        m = prepare_pt2e(m, quantizer)\n         m(*example_inputs)\n         self.assertEqual(\n             id(m.activation_post_process_2), id(m.activation_post_process_3)\n@@ -1709,7 +1709,7 @@ def _get_getitem_nodes(m: torch.fx.GraphModule):\n         quantizer.set_global(\n             get_symmetric_quantization_config(is_per_channel=False, is_qat=True)\n         )\n-        m = prepare_qat_pt2e_quantizer(m, quantizer)\n+        m = prepare_qat_pt2e(m, quantizer)\n         (maxpool_getitem_node, conv_bn_getitem_node) = _get_getitem_nodes(m)\n \n         # Verify that the metadata was copied from `conv_bn_getitem`, not `maxpool_getitem`\n@@ -1885,7 +1885,7 @@ def forward(self, input_tensor, hidden_tensor):\n                 is_per_channel=False, is_dynamic=False\n             )\n             quantizer.set_global(operator_config)\n-            model_graph = prepare_pt2e_quantizer(model_graph, quantizer)\n+            model_graph = prepare_pt2e(model_graph, quantizer)\n             model_graph(*example_inputs)\n             model_graph = convert_pt2e(model_graph)\n             self.assertEqual(model_fx(*example_inputs), model_graph(*example_inputs))\n@@ -1949,7 +1949,7 @@ def forward(self, input_tensor, hidden_tensor):\n                 is_per_channel=False, is_dynamic=False\n             )\n             quantizer.set_global(operator_config)\n-            model_graph = prepare_pt2e_quantizer(model_graph, quantizer)\n+            model_graph = prepare_pt2e(model_graph, quantizer)\n             model_graph(*example_inputs)\n             model_graph = convert_pt2e(model_graph)\n             self.assertEqual(model_fx(*example_inputs), model_graph(*example_inputs))\n@@ -1975,7 +1975,7 @@ def test_resnet18_with_quantizer_api(self):\n             quantizer = QNNPackQuantizer()\n             operator_config = get_symmetric_quantization_config(is_per_channel=True)\n             quantizer.set_global(operator_config)\n-            m = prepare_pt2e_quantizer(m, quantizer)\n+            m = prepare_pt2e(m, quantizer)\n             # checking that we inserted observers correctly for maxpool operator (input and\n             # output share observer instance)\n             self.assertEqual(\ndiff --git a/test/quantization/pt2e/test_x86inductor_quantizer.py b/test/quantization/pt2e/test_x86inductor_quantizer.py\nindex 6d8e9b0cbd1dcc..6f02d6a94e4b5b 100644\n--- a/test/quantization/pt2e/test_x86inductor_quantizer.py\n+++ b/test/quantization/pt2e/test_x86inductor_quantizer.py\n@@ -8,7 +8,7 @@\n )\n from torch.ao.quantization.quantize_pt2e import (\n     convert_pt2e,\n-    prepare_pt2e_quantizer,\n+    prepare_pt2e,\n )\n from torch.testing._internal.common_quantization import (\n     NodeSpec as ns,\n@@ -172,7 +172,7 @@ def _test_quantizer(\n             *copy.deepcopy(example_inputs),\n             aten_graph=True,\n         )\n-        m = prepare_pt2e_quantizer(m, quantizer)\n+        m = prepare_pt2e(m, quantizer)\n         # Calibrate\n         m(*example_inputs)\n         m = convert_pt2e(m)\ndiff --git a/torch/ao/quantization/quantize_pt2e.py b/torch/ao/quantization/quantize_pt2e.py\nindex d78904ebffae99..d8aaec7c14a4e3 100644\n--- a/torch/ao/quantization/quantize_pt2e.py\n+++ b/torch/ao/quantization/quantize_pt2e.py\n@@ -46,8 +46,8 @@\n from typing import Any, Tuple\n \n __all__ = [\n-    \"prepare_pt2e_quantizer\",\n-    \"prepare_qat_pt2e_quantizer\",\n+    \"prepare_pt2e\",\n+    \"prepare_qat_pt2e\",\n     \"convert_pt2e\",\n ]\n \n@@ -77,9 +77,7 @@ def _prepare_pt2e_deprecated(\n     _rearrange_weight_observer_for_decomposed_linear(model)\n     return model\n \n-# TODO: update this to prepare_pt2e after we have a usable quantizer\n-# implemented\n-def prepare_pt2e_quantizer(\n+def prepare_pt2e(\n     model: GraphModule,\n     quantizer: Quantizer,\n ) -> GraphModule:\n@@ -94,8 +92,7 @@ def prepare_pt2e_quantizer(\n     model = prepare(model, node_name_to_scope, is_qat=False)\n     return model\n \n-# TODO: update this to prepare_qat_pt2e\n-def prepare_qat_pt2e_quantizer(\n+def prepare_qat_pt2e(\n     model: GraphModule,\n     quantizer: Quantizer,\n ) -> GraphModule:\n"
  },
  {
    "number": 105480,
    "title": "Allow graph breaks in item() inductor opinfo tests",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105497\n* __->__ #105480\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "7cc5a69af864339f192e1b8fc18cb62898e3eb32",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105480",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105480/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105480.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105480.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105480/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105480/comments",
    "labels": [
      "module: inductor",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T17:28:47.044440Z",
    "state": "open",
    "patch": "From 22279f2d9c44168403249b3803798590c6e56699 Mon Sep 17 00:00:00 2001\nFrom: Elias Ellison <elias.ellison@gmail.com>\nDate: Tue, 18 Jul 2023 10:28:40 -0700\nSubject: [PATCH] Allow graph breaks in item() inductor opinfo tests\n\n[ghstack-poisoned]\n---\n test/inductor/test_torchinductor_opinfo.py | 13 +++++++------\n 1 file changed, 7 insertions(+), 6 deletions(-)\n\ndiff --git a/test/inductor/test_torchinductor_opinfo.py b/test/inductor/test_torchinductor_opinfo.py\nindex 9251d4d6f5d434..b49f0c017be67e 100644\n--- a/test/inductor/test_torchinductor_opinfo.py\n+++ b/test/inductor/test_torchinductor_opinfo.py\n@@ -178,8 +178,6 @@ def process(device_type):\n     \"index_add\": {f16},\n     \"index_reduce\": {f16, f32, f64},\n     \"istft\": {f32, f64},\n-    # Unsupported: data dependent operator: aten._local_scalar_dense.default\n-    \"item\": {b8, f16, f32, f64, i32, i64},\n     \"linalg.eig\": {f32, f64},\n     \"linalg.eigh\": {f32, f64},\n     \"linalg.eigvals\": {f32, f64},\n@@ -282,8 +280,6 @@ def process(device_type):\n     \"equal\": {b8, f16, f32, f64, i32, i64},\n     \"index_reduce\": {f16, f32, f64},\n     \"istft\": {f32, f64},\n-    # Unsupported: data dependent operator: aten._local_scalar_dense.default\n-    \"item\": {b8, f16, f32, f64, i32, i64},\n     \"linalg.eig\": {f32, f64},\n     \"linalg.eigh\": {f32, f64},\n     \"linalg.eigvals\": {f32, f64},\n@@ -504,6 +500,10 @@ def wrapper_set_seed(op, *args, **kwargs):\n     \"triu\",\n }\n \n+allow_graph_breaks = {\n+    \"item\",\n+}\n+\n \n class TestInductorOpInfo(TestCase):\n     check_model = check_model\n@@ -588,6 +588,7 @@ def fn(*args, **kwargs):\n             else:\n                 samples = [next(samples)]\n \n+        nopython = op_name not in allow_graph_breaks\n         try:\n             for sample_input in samples:\n                 args = [sample_input.input] + list(sample_input.args)\n@@ -601,7 +602,7 @@ def fn(*args, **kwargs):\n                     # so we don't need do additional copy by setting copy_to_cuda=False\n                     adjusted_kwargs = {\n                         \"check_lowp\": False,\n-                        \"nopython\": True,\n+                        \"nopython\": nopython,\n                         \"copy_to_cuda\": False,\n                         \"reference_in_float\": False,\n                         \"check_gradient\": requires_grad,\n@@ -616,7 +617,7 @@ def fn(*args, **kwargs):\n                 elif device_type == \"cpu\":\n                     adjusted_kwargs = {\n                         \"check_lowp\": False,\n-                        \"nopython\": True,\n+                        \"nopython\": nopython,\n                         # skip checking gradient on CPU for now\n                         \"check_gradient\": False,\n                     }\n"
  },
  {
    "number": 105479,
    "title": "Allow graph breaks in item() inductor opinfo tests",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105479\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "2f5db103043ed34df9f621ef3dac3e7b2aebc513",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105479",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105479/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105479.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105479.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105479/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105479/comments",
    "labels": [
      "module: inductor",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T17:25:47.174287Z",
    "state": "closed",
    "patch": "From 6f873c1ecd9685a06d02fba4f57677ec32c55ed6 Mon Sep 17 00:00:00 2001\nFrom: eellison <eellison@devgpu001.ash8.facebook.com>\nDate: Tue, 18 Jul 2023 10:25:40 -0700\nSubject: [PATCH] Allow graph breaks in item() inductor opinfo tests\n\n[ghstack-poisoned]\n---\n test/inductor/test_torchinductor_opinfo.py | 13 +++++++------\n 1 file changed, 7 insertions(+), 6 deletions(-)\n\ndiff --git a/test/inductor/test_torchinductor_opinfo.py b/test/inductor/test_torchinductor_opinfo.py\nindex 9251d4d6f5d434..b49f0c017be67e 100644\n--- a/test/inductor/test_torchinductor_opinfo.py\n+++ b/test/inductor/test_torchinductor_opinfo.py\n@@ -178,8 +178,6 @@ def process(device_type):\n     \"index_add\": {f16},\n     \"index_reduce\": {f16, f32, f64},\n     \"istft\": {f32, f64},\n-    # Unsupported: data dependent operator: aten._local_scalar_dense.default\n-    \"item\": {b8, f16, f32, f64, i32, i64},\n     \"linalg.eig\": {f32, f64},\n     \"linalg.eigh\": {f32, f64},\n     \"linalg.eigvals\": {f32, f64},\n@@ -282,8 +280,6 @@ def process(device_type):\n     \"equal\": {b8, f16, f32, f64, i32, i64},\n     \"index_reduce\": {f16, f32, f64},\n     \"istft\": {f32, f64},\n-    # Unsupported: data dependent operator: aten._local_scalar_dense.default\n-    \"item\": {b8, f16, f32, f64, i32, i64},\n     \"linalg.eig\": {f32, f64},\n     \"linalg.eigh\": {f32, f64},\n     \"linalg.eigvals\": {f32, f64},\n@@ -504,6 +500,10 @@ def wrapper_set_seed(op, *args, **kwargs):\n     \"triu\",\n }\n \n+allow_graph_breaks = {\n+    \"item\",\n+}\n+\n \n class TestInductorOpInfo(TestCase):\n     check_model = check_model\n@@ -588,6 +588,7 @@ def fn(*args, **kwargs):\n             else:\n                 samples = [next(samples)]\n \n+        nopython = op_name not in allow_graph_breaks\n         try:\n             for sample_input in samples:\n                 args = [sample_input.input] + list(sample_input.args)\n@@ -601,7 +602,7 @@ def fn(*args, **kwargs):\n                     # so we don't need do additional copy by setting copy_to_cuda=False\n                     adjusted_kwargs = {\n                         \"check_lowp\": False,\n-                        \"nopython\": True,\n+                        \"nopython\": nopython,\n                         \"copy_to_cuda\": False,\n                         \"reference_in_float\": False,\n                         \"check_gradient\": requires_grad,\n@@ -616,7 +617,7 @@ def fn(*args, **kwargs):\n                 elif device_type == \"cpu\":\n                     adjusted_kwargs = {\n                         \"check_lowp\": False,\n-                        \"nopython\": True,\n+                        \"nopython\": nopython,\n                         # skip checking gradient on CPU for now\n                         \"check_gradient\": False,\n                     }\n"
  },
  {
    "number": 105478,
    "title": "downgrade unhandle value range from warning to info",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105478\n\n",
    "merge_commit_sha": "e31791647366319cd9bb709f117f91029ef76381",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105478",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105478/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105478.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105478.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105478/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105478/comments",
    "labels": [],
    "_event_time": "2023-07-18T17:17:19.200010Z",
    "state": "closed",
    "patch": "From c2d618f635f0820a7ec121600c4fc1cf18d33ed3 Mon Sep 17 00:00:00 2001\nFrom: eellison <eellison@devgpu001.ash8.facebook.com>\nDate: Tue, 18 Jul 2023 10:17:13 -0700\nSubject: [PATCH] downgrade unhandle value range from warning to info\n\n[ghstack-poisoned]\n---\n torch/utils/_sympy/value_ranges.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/torch/utils/_sympy/value_ranges.py b/torch/utils/_sympy/value_ranges.py\nindex 26d5ea978d4b66..db55fe17afd1c0 100644\n--- a/torch/utils/_sympy/value_ranges.py\n+++ b/torch/utils/_sympy/value_ranges.py\n@@ -541,7 +541,7 @@ def where(a, b, c):\n             return ValueRanges(sympy.Min(b.lower, c.lower), sympy.Max(b.upper, c.upper))\n \n     def __getattr__(self, name):\n-        log.warning(\"unhandled ValueRange op %s\", name)\n+        log.info(\"unhandled ValueRange op %s\", name)\n         return self.default_handler\n \n \n"
  },
  {
    "number": 105477,
    "title": "Support torch.onnx.dynamo_export within FakeTensorMode",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105477\n* #105247\n* #105246\n\r\nCurrently, exporting a model to ONNX with fake tensor mode requires the\r\nuser to load data and model within `torch.onnx.enable_fake_mode` context,\r\nbut the actual call to `torch.onnx.dynamo_export` is done outside such\r\ncontext.\r\n\r\nWith this PR, we enable `torch.onnx.dynamo_export` to be called either\r\nwithin `torch.onnx.enable_fake_mode` or outside of it. This feature\r\nrequired changes to the core PyTorch Dynamo, which were greatly\r\nsupported by @ezyang\r\n\r\nIn future steps we will determine which scenario we are going to\r\nsupport, but for now we can use either to explore different options and\r\nscenarios and asses their pros and cons.\r\n\r\nThis PR also creates a separate suite of tests for fake mode specific\r\nscenarios (`TestFxToOnnxFakeTensorWithOnnxRuntime`).\r\nIt was done separately to decrease the test time, but we\r\ncould merge it with the default `TestFxToOnnxWithOnnxRuntime`. The\r\nadditional parameters are `load_checkpoint_during_init` and\r\n`export_within_fake_mode`\r\n\r\nWith the newly added supported of nested export within fake mode, the\r\nfollowing scenarios are now supported:\r\n\r\n```python\r\nimport torch\r\n\r\nwith torch.onnx.enable_fake_mode() as fake_context:\r\n    fake_args = create_args()\r\n    fake_kwargs = create_kwargs()\r\n    fake_model = create_model()\r\n    fake_model.load_state_dict(torch.load(tmp_checkpoint_file.name))\r\n\r\n    export_options = torch.onnx.ExportOptions(fake_context=fake_context)\r\n\r\n    # `torch.onnx.dynamo_export` called WITHIN `torch.onnx.enable_fake_mode`\r\n    export_output = torch.onnx.dynamo_export(\r\n        fake_model,\r\n        *fake_args,\r\n        **fake_kwargs,\r\n        export_options=export_options,\r\n    )\r\n\r\n    export_output.save(\"/path/to/model.onnx\", model_state_dict=create_model())\r\n```\r\n\r\nIf we decide to only support scenarios in which `torch._dynamo.export` is called within `FakeTensorMode`, then we can remove `fake_mode` argument from `torch._dynamo.export` as a follow-up task\r\n\r\nps: https://github.com/pytorch/pytorch/issues/105464 tracks pending tasks/limitations from this PR\r\n\r\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "d2851fea9b3c6970c7c857d109de5c8e20a50d94",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105477",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105477/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105477.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105477.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105477/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105477/comments",
    "labels": [
      "open source",
      "module: dynamo",
      "ciflow/inductor",
      "module: onnx",
      "release notes: onnx"
    ],
    "_event_time": "2023-07-18T17:10:33.207288Z",
    "state": "open",
    "patch": "From 4894638f58c310500f267acaa0479e23c903bf83 Mon Sep 17 00:00:00 2001\nFrom: Thiago Crepaldi <thiago.crepaldi@microsoft.com>\nDate: Tue, 18 Jul 2023 17:10:24 +0000\nSubject: [PATCH 1/2] Support torch.onnx.dynamo_export within FakeTensorMode\n\nCurrently, exporting a model to ONNX with fake tensor mode requires the\nuser to load data and model within `torch.onnx.enable_fake_mode` context,\nbut the actual call to `torch.onnx.dynamo_export` is done outside such\ncontext.\n\nWith this PR, we enable `torch.onnx.dynamo_export` to be called either\nwithin `torch.onnx.enable_fake_mode` or outside of it. This feature\nrequired changes to the core PyTorch Dynamo, which were greatly\nsupported by @ezyang\n\nIn future steps we will determine which scenario we are going to\nsupport, but for now we can use either to explore different options and\nscenarios and asses their pros and cons.\n\nThis PR also creates a separate suite of tests for fake mode specific\nscenarios (`TestFxToOnnxFakeTensorWithOnnxRuntime`).\nIt was done separately to decrease the test time, but we\ncould merge it with the default `TestFxToOnnxWithOnnxRuntime`. The\nadditional parameters are `load_checkpoint_during_init` and\n`export_within_fake_mode`\n\nWith the newly added supported of nested export within fake mode, the\nfollowing scenarios are now supported:\n\n```python\nimport torch\n\nwith torch.onnx.enable_fake_mode() as fake_context:\n    fake_args = create_args()\n    fake_kwargs = create_kwargs()\n    fake_model = create_model()\n    fake_model.load_state_dict(torch.load(tmp_checkpoint_file.name))\n\n    export_options = torch.onnx.ExportOptions(fake_context=fake_context)\n\n    # `torch.onnx.dynamo_export` called WITHIN `torch.onnx.enable_fake_mode`\n    export_output = torch.onnx.dynamo_export(\n        fake_model,\n        *fake_args,\n        **fake_kwargs,\n        export_options=export_options,\n    )\n\n    export_output.save(\"/path/to/model.onnx\", model_state_dict=create_model())\n```\n\n[ghstack-poisoned]\n---\n test/onnx/test_fx_to_onnx_with_onnxruntime.py | 121 +++++++++++++-----\n torch/_dynamo/eval_frame.py                   |   5 +-\n torch/_dynamo/variables/builder.py            |  14 +-\n torch/onnx/_internal/exporter.py              |   6 +-\n 4 files changed, 104 insertions(+), 42 deletions(-)\n\ndiff --git a/test/onnx/test_fx_to_onnx_with_onnxruntime.py b/test/onnx/test_fx_to_onnx_with_onnxruntime.py\nindex 1cbf60c20af535..977dff769baeff 100644\n--- a/test/onnx/test_fx_to_onnx_with_onnxruntime.py\n+++ b/test/onnx/test_fx_to_onnx_with_onnxruntime.py\n@@ -36,6 +36,8 @@\n     HAS_TORCHVISION = False\n skip_if_no_torchvision = unittest.skipIf(not HAS_TORCHVISION, \"no torchvision\")\n \n+ONNX_OPSET_VERSION_TO_TEST = 18\n+\n \n def _parameterized_class_attrs_and_values():\n     input_values = []\n@@ -73,7 +75,7 @@ class TestFxToOnnxWithOnnxRuntime(onnx_test_common._TestONNXRuntime):\n \n     def setUp(self):\n         super().setUp()\n-        self.opset_version = 18\n+        self.opset_version = ONNX_OPSET_VERSION_TO_TEST\n         self.ort_version = onnxruntime.__version__\n \n     @pytorch_test_common.skip_min_ort_version(\n@@ -824,6 +826,43 @@ def create_pytorch_only_extra_kwargs():\n             create_pytorch_only_extra_kwargs,\n         )\n \n+\n+def _parameterized_class_attrs_and_values_with_fake_options():\n+    input_values = []\n+    input_values.extend(\n+        itertools.product((True, False), (True, False), (True, False), (True, False))\n+    )\n+    return {\n+        \"attrs\": [\n+            \"op_level_debug\",\n+            \"dynamic_shapes\",\n+            \"load_checkpoint_during_init\",\n+            \"export_within_fake_mode\",\n+        ],\n+        \"input_values\": input_values,\n+    }\n+\n+\n+@parameterized.parameterized_class(\n+    **_parameterized_class_attrs_and_values_with_fake_options(),\n+    class_name_func=_parameterize_class_name,\n+)\n+class TestFxToOnnxFakeTensorWithOnnxRuntime(onnx_test_common._TestONNXRuntime):\n+    \"\"\"ONNX export test for specific Fake Tensor scenarios\n+\n+    TODO: Should we merge this with  `TestFxToOnnxWithOnnxRuntime`? Considerably increases export time\n+    \"\"\"\n+\n+    op_level_debug: bool\n+    dynamic_shapes: bool\n+    load_checkpoint_during_init: bool\n+    export_within_fake_mode: bool\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.opset_version = ONNX_OPSET_VERSION_TO_TEST\n+        self.ort_version = onnxruntime.__version__\n+\n     @_beartype.beartype\n     def _test_fake_tensor_mode_exporter(\n         self,\n@@ -832,6 +871,7 @@ def _test_fake_tensor_mode_exporter(\n         create_args: Callable,\n         create_kwargs: Callable,\n         load_checkpoint_during_init: bool,\n+        export_within_fake_mode: bool,\n     ):\n         \"\"\"Test helper for FakeTensorMode-enabled exporter.\n \n@@ -842,6 +882,7 @@ def _test_fake_tensor_mode_exporter(\n             create_kwargs: A function that creates keyword inputs for ther model.\n             load_checkpoint_during_init: Whether to load a checkpoint during model initialization.\n                 (after or during model creation, but before exporting starts)\n+            export_within_fake_mode: Whether to call torch.onnx._dynamo_export within torch._subclasses.FakeTensorMode\n \n         This test contains several steps.\n \n@@ -873,16 +914,26 @@ def _test_fake_tensor_mode_exporter(\n                 if load_checkpoint_during_init:\n                     fake_model.load_state_dict(torch.load(tmp_checkpoint_file.name))\n \n-            # Export the model with fake inputs and parameters\n-            export_options = torch.onnx.ExportOptions(\n-                opset_version=self.opset_version,\n-                dynamic_shapes=self.dynamic_shapes,\n-                op_level_debug=self.op_level_debug,\n-                fake_context=fake_context,\n-            )\n-            export_output = torch.onnx.dynamo_export(\n-                fake_model, *fake_args, **fake_kwargs, export_options=export_options\n-            )\n+                # Export the model with fake inputs and parameters\n+                export_options = torch.onnx.ExportOptions(\n+                    opset_version=self.opset_version,\n+                    dynamic_shapes=self.dynamic_shapes,\n+                    op_level_debug=self.op_level_debug,\n+                    fake_context=fake_context,\n+                )\n+\n+                if export_within_fake_mode:\n+                    export_output = torch.onnx.dynamo_export(\n+                        fake_model,\n+                        *fake_args,\n+                        **fake_kwargs,\n+                        export_options=export_options,\n+                    )\n+\n+            if not export_within_fake_mode:\n+                export_output = torch.onnx.dynamo_export(\n+                    fake_model, *fake_args, **fake_kwargs, export_options=export_options\n+                )\n \n             with tempfile.NamedTemporaryFile(suffix=\".onnx\") as tmp_onnx_file:\n                 export_output.save(\n@@ -933,14 +984,14 @@ def create_args():\n         def create_kwargs():\n             return {}\n \n-        for load_checkpoint_during_init in (True, False):\n-            self._test_fake_tensor_mode_exporter(\n-                \"simple\",\n-                create_model,\n-                create_args,\n-                create_kwargs,\n-                load_checkpoint_during_init=load_checkpoint_during_init,\n-            )\n+        self._test_fake_tensor_mode_exporter(\n+            \"simple\",\n+            create_model,\n+            create_args,\n+            create_kwargs,\n+            load_checkpoint_during_init=self.load_checkpoint_during_init,\n+            export_within_fake_mode=self.export_within_fake_mode,\n+        )\n \n     @pytorch_test_common.skip_op_level_debug_test(\n         \"op_level_debug_test does not support FakeTensor yet.\"\n@@ -961,14 +1012,14 @@ def create_args():\n         def create_kwargs():\n             return {\"return_dict\": False}\n \n-        for load_checkpoint_during_init in (True, False):\n-            self._test_fake_tensor_mode_exporter(\n-                \"tiny_gpt2\",\n-                create_model,\n-                create_args,\n-                create_kwargs,\n-                load_checkpoint_during_init=load_checkpoint_during_init,\n-            )\n+        self._test_fake_tensor_mode_exporter(\n+            \"tiny_gpt2\",\n+            create_model,\n+            create_args,\n+            create_kwargs,\n+            load_checkpoint_during_init=self.load_checkpoint_during_init,\n+            export_within_fake_mode=self.export_within_fake_mode,\n+        )\n \n     @pytorch_test_common.skip_op_level_debug_test(\n         \"op_level_debug_test does not support FakeTensor yet.\"\n@@ -1001,14 +1052,14 @@ def create_args():\n         def create_kwargs():\n             return {}\n \n-        for load_checkpoint_during_init in (True, False):\n-            self._test_fake_tensor_mode_exporter(\n-                \"toy_mlp1\",\n-                create_model,\n-                create_args,\n-                create_kwargs,\n-                load_checkpoint_during_init=load_checkpoint_during_init,\n-            )\n+        self._test_fake_tensor_mode_exporter(\n+            \"toy_mlp1\",\n+            create_model,\n+            create_args,\n+            create_kwargs,\n+            load_checkpoint_during_init=self.load_checkpoint_during_init,\n+            export_within_fake_mode=self.export_within_fake_mode,\n+        )\n \n \n if __name__ == \"__main__\":\ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..bb22386aa3ead2 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -57,6 +57,7 @@\n log = logging.getLogger(__name__)\n \n from torch._dispatch.python import enable_python_dispatcher\n+from torch.utils._python_dispatch import _disable_current_modes\n \n always_optimize_code_objects = utils.ExactWeakKeyDictionary()\n null_context = contextlib.nullcontext\n@@ -443,7 +444,7 @@ def catch_errors(frame, cache_size, frame_state):\n                     )\n                     return hijacked_callback(frame, cache_size, hooks, frame_state)\n \n-        with compile_lock:\n+        with compile_lock, _disable_current_modes():\n             return callback(frame, cache_size, hooks, frame_state)\n \n     catch_errors._torchdynamo_orig_callable = callback  # type: ignore[attr-defined]\n@@ -908,7 +909,7 @@ def dynamo_normalization_capturing_compiler(\n         graph = gm\n \n         nonlocal fake_mode, example_inputs\n-        fake_mode = fake_mode or _guards.detect_fake_mode(inner_example_inputs)\n+        fake_mode = fake_mode or _guards.detect_fake_mode()\n         example_inputs = inner_example_inputs\n \n         def result_capturing_wrapper(*graph_inputs):\ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex 60d9f09f8e2847..69607dd8ec961c 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -1086,6 +1086,7 @@ def wrap_unspecialized_primitive(self, value):\n                     example_value = unspec_var.proxy.node.meta[\"example_value\"]\n                 if isinstance(example_value, torch._subclasses.fake_tensor.FakeTensor):\n                     fake_tensor_value = example_value\n+                    assert fake_tensor_value.fake_mode is self.tx.fake_mode\n                 proxy.node.meta[\"grapharg\"] = GraphArg(\n                     self.get_source(),\n                     wrapped_value,\n@@ -1193,7 +1194,10 @@ def _clone_input(value):\n             example_value = get_fake_value(proxy.node, tx)\n \n         # Handle recursive calls here\n-        elif isinstance(example_value, FakeTensor):\n+        elif (\n+            isinstance(example_value, FakeTensor)\n+            and example_value.fake_mode is tx.fake_mode\n+        ):\n             pass\n \n         elif isinstance(example_value, torch.Tensor):\n@@ -1233,7 +1237,11 @@ def _clone_input(value):\n         example_value = _clone_input(example_value)\n         proxy.node.meta[\"example_value\"] = example_value\n         specialized_props = target_cls.specialize(example_value)\n-        if isinstance(example_value, torch._subclasses.fake_tensor.FakeTensor):\n+        # TODO: not sure about this fake mode test\n+        if (\n+            isinstance(example_value, torch._subclasses.fake_tensor.FakeTensor)\n+            and example_value.fake_mode is tx.fake_mode\n+        ):\n             # NB: This will be wrong for ignore_subclass; fix it up later!\n             specialized_props[\"class_type\"] = (\n                 torch.nn.Parameter if is_parameter else torch.Tensor\n@@ -1469,7 +1477,7 @@ def update_dim2constraint(dim, constraint_range):\n def wrap_to_fake_tensor_and_record(\n     e, tx, ignore_subclass=False, *, source: Optional[Source], is_tensor: bool\n ):\n-    if type(e) in (torch.Tensor, torch.nn.Buffer, torch.nn.Parameter) or (\n+    if type(e) in (torch.Tensor, torch.nn.Buffer, torch.nn.Parameter, FakeTensor) or (\n         ignore_subclass and isinstance(e, torch.Tensor)\n     ):\n         assert source is not None\ndiff --git a/torch/onnx/_internal/exporter.py b/torch/onnx/_internal/exporter.py\nindex 6985eacde6f5a3..f01336200eee0f 100644\n--- a/torch/onnx/_internal/exporter.py\n+++ b/torch/onnx/_internal/exporter.py\n@@ -275,10 +275,12 @@ def enable_fake_mode():\n     from torch.fx.experimental.symbolic_shapes import ShapeEnv\n \n     # This overrides the internal `FakeTensorMode` instance created by `torch._dynamo.export`[1].\n-    # Ideally we should keep them in sync to preserve the same default behavior\n+    # It is a good idea to keep them in sync (constructor args) to maintain the same default behavior\n     # [1] `torch/_dynamo/output_graph.py::InstructionTranslator::OutputGraph.__init__`\n+    # Mixed fake/real tensors are only allowed when `torch.onnx.dynamo_export` is not called within `FakeTensorMode`\n+    # This is needed because models can create new parameters during `forward(self, *args, **kwargs)` run\n     fake_mode = fake_tensor.FakeTensorMode(\n-        allow_non_fake_inputs=True,  # https://github.com/pytorch/pytorch/issues/105077\n+        allow_non_fake_inputs=not torch._guards.detect_fake_mode(),\n         shape_env=ShapeEnv(\n             allow_scalar_outputs=False, allow_dynamic_output_shape_ops=False\n         ),\n\nFrom e0478e2e7d322d5bc561e9542c3de1e6af031a3d Mon Sep 17 00:00:00 2001\nFrom: Thiago Crepaldi <thiago.crepaldi@microsoft.com>\nDate: Tue, 18 Jul 2023 18:39:02 +0000\nSubject: [PATCH 2/2] Update on \"Support torch.onnx.dynamo_export within\n FakeTensorMode\"\n\nCurrently, exporting a model to ONNX with fake tensor mode requires the\nuser to load data and model within `torch.onnx.enable_fake_mode` context,\nbut the actual call to `torch.onnx.dynamo_export` is done outside such\ncontext.\n\nWith this PR, we enable `torch.onnx.dynamo_export` to be called either\nwithin `torch.onnx.enable_fake_mode` or outside of it. This feature\nrequired changes to the core PyTorch Dynamo, which were greatly\nsupported by ezyang\n\nIn future steps we will determine which scenario we are going to\nsupport, but for now we can use either to explore different options and\nscenarios and asses their pros and cons.\n\nThis PR also creates a separate suite of tests for fake mode specific\nscenarios (`TestFxToOnnxFakeTensorWithOnnxRuntime`).\nIt was done separately to decrease the test time, but we\ncould merge it with the default `TestFxToOnnxWithOnnxRuntime`. The\nadditional parameters are `load_checkpoint_during_init` and\n`export_within_fake_mode`\n\nWith the newly added supported of nested export within fake mode, the\nfollowing scenarios are now supported:\n\n```python\nimport torch\n\nwith torch.onnx.enable_fake_mode() as fake_context:\n    fake_args = create_args()\n    fake_kwargs = create_kwargs()\n    fake_model = create_model()\n    fake_model.load_state_dict(torch.load(tmp_checkpoint_file.name))\n\n    export_options = torch.onnx.ExportOptions(fake_context=fake_context)\n\n    # `torch.onnx.dynamo_export` called WITHIN `torch.onnx.enable_fake_mode`\n    export_output = torch.onnx.dynamo_export(\n        fake_model,\n        *fake_args,\n        **fake_kwargs,\n        export_options=export_options,\n    )\n\n    export_output.save(\"/path/to/model.onnx\", model_state_dict=create_model())\n```\n\nIf we decide to only support scenarios in which `torch._dynamo.export` is called within `FakeTensorMode`, then we can remove `fake_mode` argument from `torch._dynamo.export` as a follow-up task\n\ncc voznesenskym penguinwu anijain2305 EikanWang jgong5 Guobing-Chen XiaobingSuper zhuhaozhe blzheng Xia-Weiwen wenzhe-nrv jiayisunx ipiszy chenyang78 aakhundov\n\n[ghstack-poisoned]\n---\n test/onnx/test_fx_to_onnx.py | 21 ++-------------------\n 1 file changed, 2 insertions(+), 19 deletions(-)\n\ndiff --git a/test/onnx/test_fx_to_onnx.py b/test/onnx/test_fx_to_onnx.py\nindex 4c7de5180cba20..5f9587df224ef9 100644\n--- a/test/onnx/test_fx_to_onnx.py\n+++ b/test/onnx/test_fx_to_onnx.py\n@@ -339,37 +339,20 @@ def forward(self, x):\n                 fake_model, real_x, export_options=export_options\n             )\n \n-        # Scenario 3: Real model and fake input WITHOUT fake_context\n-        with pytest.raises(torch.onnx.OnnxExporterError):\n-            export_options = ExportOptions(fake_context=None)\n-            _ = torch.onnx.dynamo_export(\n-                real_model, fake_x, export_options=export_options\n-            )\n-\n-        # Scenario 4: Real model and real input WITH fake_context\n+        # Scenario 3: Real model and real input WITH fake_context\n         with pytest.raises(torch.onnx.OnnxExporterError):\n             export_options = ExportOptions(fake_context=fake_context)\n             _ = torch.onnx.dynamo_export(\n                 real_model, real_x, export_options=export_options\n             )\n \n-        # Scenario 5: Fake model and real input WITH fake_context\n+        # Scenario 4: Fake model and real input WITH fake_context\n         with pytest.raises(torch.onnx.OnnxExporterError):\n             export_options = ExportOptions(fake_context=fake_context)\n             _ = torch.onnx.dynamo_export(\n                 fake_model, real_x, export_options=export_options\n             )\n \n-        # Scenario 6: Real model and fake input WITH fake_context\n-        # TODO: Delete the test below if https://github.com/pytorch/pytorch/pull/105246\n-        #       Tracked by https://github.com/pytorch/pytorch/issues/105077\n-        # mixed mode (real+fake) will be permanently allowed.\n-        # with pytest.raises(torch.onnx.OnnxExporterError):\n-        #     export_options = ExportOptions(fake_context=fake_context)\n-        #     _ = torch.onnx.dynamo_export(\n-        #         real_model, fake_x, export_options=export_options\n-        #     )\n-\n \n if __name__ == \"__main__\":\n     common_utils.run_tests()\n"
  },
  {
    "number": 105476,
    "title": "Fix `isinstance` check in `quat_utils`",
    "body": "Calling `isinstance(x, Tuple[Node, Node])` would either fail, or raise a\r\ntype error on a more modern Python, as none of the tuples are actually\r\ninstances of `Tuple`\r\n\r\n```python\r\n>>> from typing import Tuple\r\n>>> from torch.fx import Node\r\n>>> edge_or_node=(Node(None, \"foo\", \"output\", \"foo\", None, None), Node(None, \"bar\", \"output\", \"bar\", None, None))\r\n>>> isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node)\r\nTrue\r\n>>> isinstance(edge_or_node, Tuple[Node, Node])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/malfet/miniconda3/lib/python3.10/typing.py\", line 994, in __instancecheck__\r\n    return self.__subclasscheck__(type(obj))\r\n  File \"/Users/malfet/miniconda3/lib/python3.10/typing.py\", line 997, in __subclasscheck__\r\n    raise TypeError(\"Subscripted generics cannot be used with\"\r\nTypeError: Subscripted generics cannot be used with class and instance checks\r\n```\r\n\r\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 40fa451</samp>\n\n> _Fix type annotation_\n> _Quantize nodes in the graph_\n> _Autumn leaves falling_\r\n",
    "merge_commit_sha": "2383e89652eb0c4efba67b5d8eeafe28b3761a6f",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105476",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105476/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105476.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105476.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105476/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105476/comments",
    "labels": [
      "release notes: AO frontend",
      "Merged",
      "ciflow/trunk",
      "topic: bug fixes",
      "release notes: quantization"
    ],
    "_event_time": "2023-07-18T16:59:47.701343Z",
    "state": "closed",
    "patch": "From 40fa451649cf922523019921d388269517940372 Mon Sep 17 00:00:00 2001\nFrom: Nikita Shulga <nikita.shulga@gmail.com>\nDate: Tue, 18 Jul 2023 09:56:53 -0700\nSubject: [PATCH 1/2] Fix `isinstance` check in `quat_utils`\n\nCalling `isinstance(x, Tuple[Node, Node])` would either fail, or raise a\ntype error on a more modern Python, as none of the tuples are actually\ninstances of `Tuple`\n\n```python\n>>> from typing import Tuple\n>>> from torch.fx import Node\n>>> edge_or_node=(Node(None, \"foo\", \"output\", \"foo\", None, None), Node(None, \"bar\", \"output\", \"bar\", None, None))\n>>> isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node)\nTrue\n>>> isinstance(edge_or_node, Tuple[Node, Node])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/malfet/miniconda3/lib/python3.10/typing.py\", line 994, in __instancecheck__\n    return self.__subclasscheck__(type(obj))\n  File \"/Users/malfet/miniconda3/lib/python3.10/typing.py\", line 997, in __subclasscheck__\n    raise TypeError(\"Subscripted generics cannot be used with\"\nTypeError: Subscripted generics cannot be used with class and instance checks\n```\n\ncopilot:poem\n---\n torch/ao/quantization/pt2e/qat_utils.py | 6 ++----\n 1 file changed, 2 insertions(+), 4 deletions(-)\n\ndiff --git a/torch/ao/quantization/pt2e/qat_utils.py b/torch/ao/quantization/pt2e/qat_utils.py\nindex dda784a8ed6ea1..ccbf903751222b 100644\n--- a/torch/ao/quantization/pt2e/qat_utils.py\n+++ b/torch/ao/quantization/pt2e/qat_utils.py\n@@ -458,10 +458,8 @@ def _get_new_edge_or_node(edge_or_node: EdgeOrNode):\n         if isinstance(edge_or_node, Node):\n             _node = edge_or_node\n             return original_to_replacement_node.get(_node, _node)\n-        # TODO: It's really should be\n-        # isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node)\n-        elif isinstance(edge_or_node, Tuple[Node, Node]):  # type: ignore[arg-type]\n-            src, dest = edge_or_node  # type: ignore[misc]\n+        elif isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node)\n+            src, dest = edge_or_node\n             return (\n                 original_to_replacement_node.get(src, src),\n                 original_to_replacement_node.get(dest, dest),\n\nFrom 513d7fd3a0841d5470da3ef16e264fe1a56aa3bb Mon Sep 17 00:00:00 2001\nFrom: Nikita Shulga <nshulga@meta.com>\nDate: Tue, 18 Jul 2023 10:30:57 -0700\nSubject: [PATCH 2/2] Update torch/ao/quantization/pt2e/qat_utils.py\n\n---\n torch/ao/quantization/pt2e/qat_utils.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/torch/ao/quantization/pt2e/qat_utils.py b/torch/ao/quantization/pt2e/qat_utils.py\nindex ccbf903751222b..dae5e9092c4ffb 100644\n--- a/torch/ao/quantization/pt2e/qat_utils.py\n+++ b/torch/ao/quantization/pt2e/qat_utils.py\n@@ -458,7 +458,7 @@ def _get_new_edge_or_node(edge_or_node: EdgeOrNode):\n         if isinstance(edge_or_node, Node):\n             _node = edge_or_node\n             return original_to_replacement_node.get(_node, _node)\n-        elif isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node)\n+        elif isinstance(edge_or_node, tuple) and len(edge_or_node) == 2 and all(isinstance(x, Node) for x in edge_or_node):\n             src, dest = edge_or_node\n             return (\n                 original_to_replacement_node.get(src, src),\n"
  },
  {
    "number": 105475,
    "title": "[MPS][BE] Use `Tensor::copy_`",
    "body": "Replace `const_cast<Tensor&>(x) = y;` with `x.copy_(y);`\r\n\r\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at d2b7a0d</samp>\n\n> _`copy_` not `clone`_\n> _MPS backend runs faster_\n> _Winter memory_\r\n",
    "merge_commit_sha": "000038b78bc2d3601ac5be5298b57f7d98288b8d",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105475",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105475/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105475.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105475.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105475/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105475/comments",
    "labels": [
      "ciflow/mps",
      "release notes: mps"
    ],
    "_event_time": "2023-07-18T16:51:51.227026Z",
    "state": "open",
    "patch": "From b93ebbf1a43870dc5fdfbb30e3f12f09ed42d8fd Mon Sep 17 00:00:00 2001\nFrom: Nikita Shulga <nikita.shulga@gmail.com>\nDate: Tue, 18 Jul 2023 09:50:24 -0700\nSubject: [PATCH] [MPS][BE] Use `Tensor::copy_`\n\nReplace `const_cast<Tensor&>(x) = y;` with `x.copy_(y)`\n---\n .../ATen/native/mps/operations/BinaryOps.mm   |  2 +-\n .../src/ATen/native/mps/operations/Pooling.mm | 12 ++---\n .../ATen/native/mps/operations/UpSample.mm    | 53 +++++++------------\n 3 files changed, 27 insertions(+), 40 deletions(-)\n\ndiff --git a/aten/src/ATen/native/mps/operations/BinaryOps.mm b/aten/src/ATen/native/mps/operations/BinaryOps.mm\nindex ca13f8d4424271..df91735a00623d 100644\n--- a/aten/src/ATen/native/mps/operations/BinaryOps.mm\n+++ b/aten/src/ATen/native/mps/operations/BinaryOps.mm\n@@ -262,7 +262,7 @@ void add_sub_template(const Tensor& self,\n                       std::string op_name) {\n   if (alpha.toDouble() == 0.0) {\n     if (!self.is_alias_of(output)) { // if inplace, no-op\n-      const_cast<Tensor&>(output) = self.clone();\n+      output.copy_(self);\n     }\n     return;\n   }\ndiff --git a/aten/src/ATen/native/mps/operations/Pooling.mm b/aten/src/ATen/native/mps/operations/Pooling.mm\nindex ddb4d8f6c13d4f..69b62c2b24d999 100644\n--- a/aten/src/ATen/native/mps/operations/Pooling.mm\n+++ b/aten/src/ATen/native/mps/operations/Pooling.mm\n@@ -224,7 +224,7 @@ static void pool2d_template(const Tensor& input,\n     runMPSGraph(mpsStream, cachedGraph->graph(), feeds, results);\n \n     if (output_memory_format != suggested_memory_format) {\n-      const_cast<Tensor&>(output) = output.to(suggested_memory_format);\n+      output.copy_(output.to(suggested_memory_format));\n     }\n   }\n }\n@@ -253,12 +253,12 @@ static void avg_pool2d_template(const Tensor& input,\n                     \"not supported on MPS backend. \",\n                     \"Falling back on CPU. This may have performance implications.\");\n     if (!is_backward_pass) {\n-      const_cast<Tensor&>(output) =\n+      output.copy_(\n           at::avg_pool2d(input.to(\"cpu\"), kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override)\n               .clone()\n-              .to(\"mps\");\n+              .to(\"mps\"));\n     } else {\n-      const_cast<Tensor&>(output) = at::avg_pool2d_backward(grad_output.to(\"cpu\"),\n+      output.copy_(at::avg_pool2d_backward(grad_output.to(\"cpu\"),\n                                                             input.to(\"cpu\"),\n                                                             kernel_size,\n                                                             stride,\n@@ -267,7 +267,7 @@ static void avg_pool2d_template(const Tensor& input,\n                                                             count_include_pad,\n                                                             divisor_override)\n                                         .clone()\n-                                        .to(\"mps\");\n+                                        .to(\"mps\"));\n     }\n     return;\n   }\n@@ -446,7 +446,7 @@ Tensor mps_max_pool2d_backward(const Tensor& grad_output,\n                        \"max_pool2d_indices\");\n \n   if (indices_memory_format == MemoryFormat::ChannelsLast) {\n-    const_cast<Tensor&>(indices) = indices.to(MemoryFormat::ChannelsLast);\n+    indices.copy_(indices.to(MemoryFormat::ChannelsLast));\n   }\n }\n \ndiff --git a/aten/src/ATen/native/mps/operations/UpSample.mm b/aten/src/ATen/native/mps/operations/UpSample.mm\nindex 26cb7761c997bc..5cde82684aa140 100644\n--- a/aten/src/ATen/native/mps/operations/UpSample.mm\n+++ b/aten/src/ATen/native/mps/operations/UpSample.mm\n@@ -264,8 +264,7 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"nearest\", scale)) {\n     mps::upsample_out_template(input, output_size, c10::nullopt, c10::nullopt, scale, output, false, \"nearest\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(output) = at::upsample_nearest1d(input.to(\"cpu\"), output_size, scale).clone().to(\"mps\");\n+    output.copy_(at::upsample_nearest1d(input.to(\"cpu\"), output_size, scale).clone().to(\"mps\"));\n   }\n }\n \n@@ -278,9 +277,8 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"nearest\", scale)) {\n     mps::upsample_out_template(grad_output, output_size, input_size, c10::nullopt, scale, grad_input, false, \"nearest\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(grad_input) =\n-        at::upsample_nearest1d_backward(grad_output.to(\"cpu\"), output_size, input_size, scale).clone().to(\"mps\");\n+    grad_input.copy_(\n+        at::upsample_nearest1d_backward(grad_output.to(\"cpu\"), output_size, input_size, scale).clone().to(\"mps\"));\n   }\n }\n \n@@ -289,8 +287,7 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"nearest-exact\", scale)) {\n     mps::upsample_out_template(input, output_size, c10::nullopt, c10::nullopt, scale, output, false, \"nearest-exact\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(output) = at::_upsample_nearest_exact1d(input.to(\"cpu\"), output_size, scale).clone().to(\"mps\");\n+    output.copy_(at::_upsample_nearest_exact1d(input.to(\"cpu\"), output_size, scale).clone().to(\"mps\"));\n   }\n }\n \n@@ -304,9 +301,9 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n     mps::upsample_out_template(\n         grad_output, output_size, input_size, c10::nullopt, scale, grad_input, false, \"nearest-exact\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(grad_input) =\n-        at::_upsample_nearest_exact1d_backward(grad_output.to(\"cpu\"), output_size, input_size, scale).clone().to(\"mps\");\n+    grad_input.copy_(at::_upsample_nearest_exact1d_backward(grad_output.to(\"cpu\"), output_size, input_size, scale)\n+                         .clone()\n+                         .to(\"mps\"));\n   }\n }\n \n@@ -319,9 +316,7 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"nearest\", scales_w)) {\n     mps::upsample_out_template(input, output_size, c10::nullopt, scales_h, scales_w, output, false, \"nearest\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(output) =\n-        at::upsample_nearest2d(input.to(\"cpu\"), output_size, scales_h, scales_w).clone().to(\"mps\");\n+    output.copy_(at::upsample_nearest2d(input.to(\"cpu\"), output_size, scales_h, scales_w).clone().to(\"mps\"));\n   }\n }\n \n@@ -335,11 +330,9 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"nearest\", scales_w)) {\n     mps::upsample_out_template(grad_output, output_size, input_size, scales_h, scales_w, grad_input, false, \"nearest\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(grad_input) =\n-        at::upsample_nearest2d_backward(grad_output.to(\"cpu\"), output_size, input_size, scales_h, scales_w)\n-            .clone()\n-            .to(\"mps\");\n+    grad_input.copy_(at::upsample_nearest2d_backward(grad_output.to(\"cpu\"), output_size, input_size, scales_h, scales_w)\n+                         .clone()\n+                         .to(\"mps\"));\n   }\n }\n \n@@ -352,9 +345,7 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"nearest-exact\", scales_w)) {\n     mps::upsample_out_template(input, output_size, c10::nullopt, scales_h, scales_w, output, false, \"nearest-exact\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(output) =\n-        at::_upsample_nearest_exact2d(input.to(\"cpu\"), output_size, scales_h, scales_w).clone().to(\"mps\");\n+    output.copy_(at::_upsample_nearest_exact2d(input.to(\"cpu\"), output_size, scales_h, scales_w).clone().to(\"mps\"));\n   }\n }\n \n@@ -369,11 +360,10 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n     mps::upsample_out_template(\n         grad_output, output_size, input_size, scales_h, scales_w, grad_input, false, \"nearest-exact\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(grad_input) =\n+    grad_input.copy_(\n         at::_upsample_nearest_exact2d_backward(grad_output.to(\"cpu\"), output_size, input_size, scales_h, scales_w)\n             .clone()\n-            .to(\"mps\");\n+            .to(\"mps\"));\n   }\n }\n \n@@ -387,9 +377,8 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n   if (check_mps_compatibility(\"bilinear\", scales_w)) {\n     mps::upsample_out_template(input, output_size, c10::nullopt, scales_h, scales_w, output, align_corners, \"bilinear\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(output) =\n-        at::upsample_bilinear2d(input.to(\"cpu\"), output_size, align_corners, scales_h, scales_w).clone().to(\"mps\");\n+    output.copy_(\n+        at::upsample_bilinear2d(input.to(\"cpu\"), output_size, align_corners, scales_h, scales_w).clone().to(\"mps\"));\n   }\n }\n \n@@ -405,12 +394,10 @@ static bool check_mps_compatibility(const c10::string_view resize_mode_str, c10:\n     mps::upsample_out_template(\n         grad_output, output_size, input_size, scales_h, scales_w, grad_input, align_corners, \"bilinear\");\n   } else {\n-    // NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)\n-    const_cast<Tensor&>(grad_input) =\n-        at::upsample_bilinear2d_backward(\n-            grad_output.to(\"cpu\"), output_size, input_size, align_corners, scales_h, scales_w)\n-            .clone()\n-            .to(\"mps\");\n+    grad_input.copy_(at::upsample_bilinear2d_backward(\n+                         grad_output.to(\"cpu\"), output_size, input_size, align_corners, scales_h, scales_w)\n+                         .clone()\n+                         .to(\"mps\"));\n   }\n }\n \n"
  },
  {
    "number": 105473,
    "title": "[BE] Enable ruff's UP rules and autoformat utils/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nSigned-off-by: Justin Chu <justinchu@microsoft.com>",
    "merge_commit_sha": "fd27b9428b2dc62bd05646ef35b9ddf6de8b3625",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105473",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105473/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105473.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105473.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105473/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105473/comments",
    "labels": [
      "release notes: dataloader"
    ],
    "_event_time": "2023-07-18T15:38:37.050603Z",
    "state": "closed",
    "patch": "From 370319428eee9b17970c22d10f8d38bbd8126ea3 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 08:38:29 -0700\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat utils/\n\nSigned-off-by: Justin Chu <justinchu@microsoft.com>\n\n[ghstack-poisoned]\n---\n torch/utils/_freeze.py                        |  2 +-\n torch/utils/benchmark/examples/end_to_end.py  |  5 ++--\n .../utils/benchmark/examples/op_benchmark.py  |  4 +--\n .../benchmark/examples/sparse/op_benchmark.py |  4 +--\n .../examples/spectral_ops_fuzz_test.py        |  2 +-\n torch/utils/benchmark/utils/common.py         |  4 +--\n torch/utils/benchmark/utils/cpp_jit.py        |  6 ++--\n .../utils/valgrind_wrapper/timer_interface.py | 13 +++++----\n torch/utils/bottleneck/__main__.py            |  6 ++--\n torch/utils/bundled_inputs.py                 |  8 +++---\n torch/utils/checkpoint.py                     |  2 +-\n torch/utils/collect_env.py                    | 26 ++++++++---------\n torch/utils/cpp_extension.py                  | 14 +++++-----\n torch/utils/data/_utils/pin_memory.py         |  2 +-\n torch/utils/data/_utils/worker.py             |  8 +++---\n torch/utils/data/dataloader.py                |  6 ++--\n torch/utils/data/datapipes/_decorator.py      |  2 +-\n torch/utils/data/datapipes/_typing.py         |  8 +++---\n .../data/datapipes/dataframe/dataframes.py    | 18 ++++++------\n torch/utils/data/datapipes/datapipe.py        | 10 +++----\n torch/utils/data/datapipes/gen_pyi.py         |  2 +-\n torch/utils/data/datapipes/iter/callable.py   |  2 +-\n .../data/datapipes/iter/combinatorics.py      |  4 +--\n torch/utils/data/datapipes/iter/combining.py  |  6 ++--\n torch/utils/data/datapipes/iter/filelister.py |  2 +-\n torch/utils/data/datapipes/iter/fileopener.py |  4 +--\n torch/utils/data/datapipes/iter/grouping.py   |  2 +-\n .../data/datapipes/iter/routeddecoder.py      |  2 +-\n torch/utils/data/datapipes/iter/sharding.py   |  2 +-\n torch/utils/data/datapipes/map/combining.py   |  2 +-\n torch/utils/data/datapipes/map/grouping.py    |  2 +-\n torch/utils/data/datapipes/utils/common.py    |  2 +-\n torch/utils/data/datapipes/utils/decoder.py   |  6 ++--\n torch/utils/data/graph.py                     |  2 +-\n torch/utils/dlpack.py                         |  2 +-\n torch/utils/hipify/cuda_to_hip_mappings.py    |  2 +-\n torch/utils/hipify/hipify_python.py           | 28 +++++++++----------\n torch/utils/jit/log_extract.py                |  2 +-\n torch/utils/mobile_optimizer.py               |  4 +--\n torch/utils/tensorboard/_caffe2_graph.py      |  4 +--\n torch/utils/tensorboard/_embedding.py         |  2 +-\n torch/utils/tensorboard/_pytorch_graph.py     |  6 ++--\n torch/utils/tensorboard/writer.py             |  2 +-\n torch/utils/throughput_benchmark.py           | 10 +++----\n torch/utils/viz/_cycles.py                    | 12 ++++----\n torch/utils/weak.py                           |  5 ++--\n 46 files changed, 135 insertions(+), 134 deletions(-)\n\ndiff --git a/torch/utils/_freeze.py b/torch/utils/_freeze.py\nindex 5245ac011e19ac..6590ff4b769e42 100644\n--- a/torch/utils/_freeze.py\n+++ b/torch/utils/_freeze.py\n@@ -237,7 +237,7 @@ def compile_file(self, path: Path, top_package_path: Path):\n         module_mangled_name = \"__\".join(module_qualname)\n         c_name = \"M_\" + module_mangled_name\n \n-        with open(path, \"r\") as src_file:\n+        with open(path) as src_file:\n             co = self.compile_string(src_file.read())\n \n         bytecode = marshal.dumps(co)\ndiff --git a/torch/utils/benchmark/examples/end_to_end.py b/torch/utils/benchmark/examples/end_to_end.py\nindex 5e0f42712d7c7a..a6d05a91c94253 100644\n--- a/torch/utils/benchmark/examples/end_to_end.py\n+++ b/torch/utils/benchmark/examples/end_to_end.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"End-to-end example to test a PR for regressions:\n \n $ python -m examples.end_to_end --pr 39850\n@@ -111,7 +110,7 @@ def parse_args():\n \n def construct_stmt_and_label(pr, params):\n     if pr == \"39850\":\n-        k0, k1, k2, dim = [params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"]]\n+        k0, k1, k2, dim = (params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"])\n         state = np.random.RandomState(params[\"random_value\"])\n         topk_dim = state.randint(low=0, high=dim)\n         dim_size = [k0, k1, k2][topk_dim]\n@@ -291,7 +290,7 @@ def construct_table(results, device_str, test_variance):\n     )\n \n     _, result_log_file = tempfile.mkstemp(suffix=\".log\")\n-    with open(result_log_file, \"wt\") as f:\n+    with open(result_log_file, \"w\") as f:\n         f.write(f\"{device_str}\\n\\n{column_labels}\\n\")\n         print(f\"\\n{column_labels}\\n[First twenty omitted (these tend to be noisy) ]\")\n         for key, (r_ref, r_pr), rel_diff in results:\ndiff --git a/torch/utils/benchmark/examples/op_benchmark.py b/torch/utils/benchmark/examples/op_benchmark.py\nindex 65b69d84b41f44..b7536b9ec26bb8 100644\n--- a/torch/utils/benchmark/examples/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/op_benchmark.py\n@@ -37,13 +37,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/sparse/op_benchmark.py b/torch/utils/benchmark/examples/sparse/op_benchmark.py\nindex f9ee17d5617e08..d7e97d33cc1101 100644\n--- a/torch/utils/benchmark/examples/sparse/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/sparse/op_benchmark.py\n@@ -32,13 +32,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\nindex d8284ee4187c49..c70395573adb2c 100644\n--- a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n+++ b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n@@ -27,7 +27,7 @@ def run_benchmark(name: str, function: object, dtype: torch.dtype, seed: int, de\n     results = []\n     for tensors, tensor_params, params in spectral_fuzzer.take(samples):\n         shape = [params['k0'], params['k1'], params['k2']][:params['ndim']]\n-        str_shape = ' x '.join([\"{:<4}\".format(s) for s in shape])\n+        str_shape = ' x '.join([f\"{s:<4}\" for s in shape])\n         sub_label = f\"{str_shape} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n         for dim in _dim_options(params['ndim']):\n             for nthreads in (1, 4, 16) if not cuda else (1,):\ndiff --git a/torch/utils/benchmark/utils/common.py b/torch/utils/benchmark/utils/common.py\nindex a8bbef3bfbeb4f..c1636ddb78a2bf 100644\n--- a/torch/utils/benchmark/utils/common.py\n+++ b/torch/utils/benchmark/utils/common.py\n@@ -325,7 +325,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n                 if not os.path.exists(owner_file):\n                     continue\n \n-                with open(owner_file, \"rt\") as f:\n+                with open(owner_file) as f:\n                     owner_pid = int(f.read())\n \n                 if owner_pid == os.getpid():\n@@ -349,7 +349,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n     os.makedirs(path, exist_ok=False)\n \n     if use_dev_shm:\n-        with open(os.path.join(path, \"owner.pid\"), \"wt\") as f:\n+        with open(os.path.join(path, \"owner.pid\"), \"w\") as f:\n             f.write(str(os.getpid()))\n \n     return path\ndiff --git a/torch/utils/benchmark/utils/cpp_jit.py b/torch/utils/benchmark/utils/cpp_jit.py\nindex 65b8c70ee43e6c..a09f1a00aace6f 100644\n--- a/torch/utils/benchmark/utils/cpp_jit.py\n+++ b/torch/utils/benchmark/utils/cpp_jit.py\n@@ -137,7 +137,7 @@ def _compile_template(\n         os.makedirs(build_dir, exist_ok=True)\n \n         src_path = os.path.join(build_dir, \"timer_src.cpp\")\n-        with open(src_path, \"wt\") as f:\n+        with open(src_path, \"w\") as f:\n             f.write(src)\n \n     # `cpp_extension` has its own locking scheme, so we don't need our lock.\n@@ -154,7 +154,7 @@ def _compile_template(\n \n def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> TimeitModuleType:\n     template_path: str = os.path.join(SOURCE_ROOT, \"timeit_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     module = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)\n@@ -164,7 +164,7 @@ def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> Time\n \n def compile_callgrind_template(*, stmt: str, setup: str, global_setup: str) -> str:\n     template_path: str = os.path.join(SOURCE_ROOT, \"valgrind_wrapper\", \"timer_callgrind_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     target = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 71753bd59548ae..61e43488e41c33 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -28,7 +28,10 @@\n     CompletedProcessType = subprocess.CompletedProcess\n \n \n-FunctionCount = NamedTuple(\"FunctionCount\", [(\"count\", int), (\"function\", str)])\n+class FunctionCount(NamedTuple):\n+    # TODO(#105471): Rename the count field\n+    count: int  # type: ignore[assignment]\n+    function: str\n \n \n @dataclasses.dataclass(repr=False, eq=False, frozen=True)\n@@ -598,7 +601,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     stderr=subprocess.STDOUT,\n                     **kwargs,\n                 )\n-                with open(stdout_stderr_log, \"rt\") as f:\n+                with open(stdout_stderr_log) as f:\n                     return invocation, f.read()\n             finally:\n                 f_stdout_stderr.close()\n@@ -612,7 +615,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     )\n \n                 script_file = os.path.join(working_dir, \"timer_callgrind.py\")\n-                with open(script_file, \"wt\") as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(self._construct_script(\n                         task_spec,\n                         globals=GlobalsBridge(globals, data_dir),\n@@ -652,7 +655,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n             if valgrind_invocation.returncode:\n                 error_report = \"\"\n                 if os.path.exists(error_log):\n-                    with open(error_log, \"rt\") as f:\n+                    with open(error_log) as f:\n                         error_report = f.read()\n                 if not error_report:\n                     error_report = \"Unknown error.\\n\" + valgrind_invocation_output\n@@ -724,7 +727,7 @@ def read_results(i: int) -> Tuple[FunctionCounts, FunctionCounts, Optional[str]]\n                 fpath = f\"{callgrind_out}.{i + 1}\"  # Callgrind one-indexes files.\n                 callgrind_out_contents: Optional[str] = None\n                 if retain_out_file:\n-                    with open(fpath, \"rt\") as f:\n+                    with open(fpath) as f:\n                         callgrind_out_contents = f.read()\n \n                 return (\ndiff --git a/torch/utils/bottleneck/__main__.py b/torch/utils/bottleneck/__main__.py\nindex 86c1af04baa0e6..f7fd209e1438fa 100644\n--- a/torch/utils/bottleneck/__main__.py\n+++ b/torch/utils/bottleneck/__main__.py\n@@ -16,7 +16,7 @@ def redirect_argv(new_argv):\n \n def compiled_with_cuda(sysinfo):\n     if sysinfo.cuda_compiled_version:\n-        return 'compiled w/ CUDA {}'.format(sysinfo.cuda_compiled_version)\n+        return f'compiled w/ CUDA {sysinfo.cuda_compiled_version}'\n     return 'not compiled w/ CUDA'\n \n \n@@ -59,7 +59,7 @@ def run_env_analysis():\n         'debug_str': debug_str,\n         'pytorch_version': info.torch_version,\n         'cuda_compiled': compiled_with_cuda(info),\n-        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'py_version': f'{sys.version_info[0]}.{sys.version_info[1]}',\n         'cuda_runtime': cuda_avail,\n         'pip_version': pip_version,\n         'pip_list_output': pip_list_output,\n@@ -138,7 +138,7 @@ def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\n \n     result = {\n         'mode': mode,\n-        'description': 'top {} events sorted by {}'.format(topk, sortby),\n+        'description': f'top {topk} events sorted by {sortby}',\n         'output': torch.autograd.profiler_util._build_table(topk_events),\n         'cuda_warning': cuda_warning\n     }\ndiff --git a/torch/utils/bundled_inputs.py b/torch/utils/bundled_inputs.py\nindex 4ae39733ff2e4b..ad34e15e6bfa17 100644\n--- a/torch/utils/bundled_inputs.py\n+++ b/torch/utils/bundled_inputs.py\n@@ -261,11 +261,11 @@ def augment_many_model_functions_with_bundled_inputs(\n \n \n         if input_list is not None and not isinstance(input_list, Sequence):\n-            raise TypeError(\"Error inputs for function {0} is not a Sequence\".format(function_name))\n+            raise TypeError(f\"Error inputs for function {function_name} is not a Sequence\")\n \n         function_arg_types = [arg.type for arg in function.schema.arguments[1:]]  # type: ignore[attr-defined]\n         deflated_inputs_type: ListType = ListType(TupleType(function_arg_types))\n-        model._c._register_attribute(\"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs_type, [])\n+        model._c._register_attribute(f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs_type, [])\n \n         if hasattr(model, \"_generate_bundled_inputs_for_\" + function_name):\n             if input_list is not None:\n@@ -290,7 +290,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             for inp_idx, args in enumerate(input_list):\n                 if not isinstance(args, Tuple) and not isinstance(args, List):  # type: ignore[arg-type]\n                     raise TypeError(\n-                        \"Error bundled input for function {0} idx: {1} is not a Tuple or a List\".format(function_name, inp_idx)\n+                        f\"Error bundled input for function {function_name} idx: {inp_idx} is not a Tuple or a List\"\n                     )\n                 deflated_args = []\n                 parts.append(\"(\")\n@@ -314,7 +314,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             # Back-channel return this expr for debugging.\n             if _receive_inflate_expr is not None:\n                 _receive_inflate_expr.append(expr)\n-            setattr(model, \"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs)\n+            setattr(model, f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs)\n             definition = textwrap.dedent(\"\"\"\n                 def _generate_bundled_inputs_for_{name}(self):\n                     deflated = self._bundled_inputs_deflated_{name}\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..4da281d32ac3bd 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -66,7 +66,7 @@ def _get_device_module(device=\"cuda\"):\n     return device_module\n \n \n-class DefaultDeviceType(object):\n+class DefaultDeviceType:\n     r\"\"\"\n     A class that manages the default device type for checkpointing.\n     If no non-CPU tensors are present, the default device type will\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex de03564a2b75d1..2266d64c1944d5 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n+    out = run_and_read_all(run_lambda, f\"{conda} list\")\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n+        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return 'Probably one of the following:\\n{}'.format(result)\n+    return f'Probably one of the following:\\n{result}'\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = '\"{}\"'.format(candidate_smi)\n+                smi = f'\"{candidate_smi}\"'\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n+    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return 'macOS {} ({})'.format(version, machine())\n+        return f'macOS {version} ({machine()})'\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n-        return '{} ({})'.format(platform, machine())\n+        return f'{platform} ({machine()})'\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n+        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return '\\n{}\\n'.format(string)\n+            return f'\\n{string}\\n'\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               '[{}] '.format(envinfo.pip_version))\n+                                               f'[{envinfo.pip_version}] ')\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n+            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..323a391a684ed8 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -150,11 +150,11 @@ def _join_rocm_home(*paths) -> str:\n     only once we need to get any ROCm-specific path.\n     '''\n     if ROCM_HOME is None:\n-        raise EnvironmentError('ROCM_HOME environment variable is not set. '\n-                               'Please set it to your ROCm install root.')\n+        raise OSError('ROCM_HOME environment variable is not set. '\n+                      'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n-        raise EnvironmentError('Building PyTorch extensions using '\n-                               'ROCm and Windows is not supported.')\n+        raise OSError('Building PyTorch extensions using '\n+                      'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n \n@@ -264,7 +264,7 @@ def _maybe_write(filename, new_content):\n     if it already had the right content (to avoid triggering recompile).\n     '''\n     if os.path.exists(filename):\n-        with open(filename, 'r') as f:\n+        with open(filename) as f:\n             content = f.read()\n \n         if content == new_content:\n@@ -2247,8 +2247,8 @@ def _join_cuda_home(*paths) -> str:\n     only once we need to get any CUDA-specific path.\n     '''\n     if CUDA_HOME is None:\n-        raise EnvironmentError('CUDA_HOME environment variable is not set. '\n-                               'Please set it to your CUDA install root.')\n+        raise OSError('CUDA_HOME environment variable is not set. '\n+                      'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \n \ndiff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py\nindex 074b89b624b9d3..cdd53c2d9ea2b1 100644\n--- a/torch/utils/data/_utils/pin_memory.py\n+++ b/torch/utils/data/_utils/pin_memory.py\n@@ -37,7 +37,7 @@ def do_one_step():\n                 data = pin_memory(data, device)\n             except Exception:\n                 data = ExceptionWrapper(\n-                    where=\"in pin memory thread for device {}\".format(device_id))\n+                    where=f\"in pin memory thread for device {device_id}\")\n             r = (idx, data)\n         while not done_event.is_set():\n             try:\ndiff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py\nindex b4fc8e0748f0f1..0d43f63a6a2f20 100644\n--- a/torch/utils/data/_utils/worker.py\n+++ b/torch/utils/data/_utils/worker.py\n@@ -76,13 +76,13 @@ def __init__(self, **kwargs):\n \n     def __setattr__(self, key, val):\n         if self.__initialized:\n-            raise RuntimeError(\"Cannot assign attributes to {} objects\".format(self.__class__.__name__))\n+            raise RuntimeError(f\"Cannot assign attributes to {self.__class__.__name__} objects\")\n         return super().__setattr__(key, val)\n \n     def __repr__(self):\n         items = []\n         for k in self.__keys:\n-            items.append('{}={}'.format(k, getattr(self, k)))\n+            items.append(f'{k}={getattr(self, k)}')\n         return '{}({})'.format(self.__class__.__name__, ', '.join(items))\n \n \n@@ -252,7 +252,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n             fetcher = _DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)\n         except Exception:\n             init_exception = ExceptionWrapper(\n-                where=\"in DataLoader worker process {}\".format(worker_id))\n+                where=f\"in DataLoader worker process {worker_id}\")\n \n         # When using Iterable mode, some worker can exit earlier than others due\n         # to the IterableDataset behaving differently for different workers.\n@@ -318,7 +318,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n                         # `ExceptionWrapper` does the correct thing.\n                         # See NOTE [ Python Traceback Reference Cycle Problem ]\n                         data = ExceptionWrapper(\n-                            where=\"in DataLoader worker process {}\".format(worker_id))\n+                            where=f\"in DataLoader worker process {worker_id}\")\n             data_queue.put((idx, data))\n             del data, idx, index, r  # save memory\n     except KeyboardInterrupt:\ndiff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py\nindex ec86f778023ba6..1c33592f02f146 100644\n--- a/torch/utils/data/dataloader.py\n+++ b/torch/utils/data/dataloader.py\n@@ -604,7 +604,7 @@ def __init__(self, loader: DataLoader) -> None:\n         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n         self._persistent_workers = loader.persistent_workers\n         self._num_yielded = 0\n-        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n+        self._profile_name = f\"enumerate(DataLoader)#{self.__class__.__name__}.__next__\"\n \n     def __iter__(self) -> '_BaseDataLoaderIter':\n         return self\n@@ -1145,7 +1145,7 @@ def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n                     self._mark_worker_as_unavailable(worker_id)\n             if len(failed_workers) > 0:\n                 pids_str = ', '.join(str(w.pid) for w in failed_workers)\n-                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n+                raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n             if isinstance(e, queue.Empty):\n                 return (False, None)\n             import tempfile\n@@ -1281,7 +1281,7 @@ def _get_data(self):\n             if success:\n                 return data\n             else:\n-                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n+                raise RuntimeError(f'DataLoader timed out after {self._timeout} seconds')\n         elif self._pin_memory:\n             while self._pin_memory_thread.is_alive():\n                 success, data = self._try_get_data()\ndiff --git a/torch/utils/data/datapipes/_decorator.py b/torch/utils/data/datapipes/_decorator.py\nindex e4cc9e4e59365d..96b7e00e076f02 100644\n--- a/torch/utils/data/datapipes/_decorator.py\n+++ b/torch/utils/data/datapipes/_decorator.py\n@@ -80,7 +80,7 @@ def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:\n         elif isinstance(arg, Callable):  # type:ignore[arg-type]\n             self.deterministic_fn = arg  # type: ignore[assignment, misc]\n         else:\n-            raise TypeError(\"{} can not be decorated by non_deterministic\".format(arg))\n+            raise TypeError(f\"{arg} can not be decorated by non_deterministic\")\n \n     def __call__(self, *args, **kwargs):\n         global _determinism\ndiff --git a/torch/utils/data/datapipes/_typing.py b/torch/utils/data/datapipes/_typing.py\nindex 6377a2ec940860..68049ba30d9018 100644\n--- a/torch/utils/data/datapipes/_typing.py\n+++ b/torch/utils/data/datapipes/_typing.py\n@@ -234,7 +234,7 @@ def issubtype(self, other):\n             return issubtype(self.param, other.param)\n         if isinstance(other, type):\n             return issubtype(self.param, other)\n-        raise TypeError(\"Expected '_DataPipeType' or 'type', but found {}\".format(type(other)))\n+        raise TypeError(f\"Expected '_DataPipeType' or 'type', but found {type(other)}\")\n \n     def issubtype_of_instance(self, other):\n         return issubinstance(other, self.param)\n@@ -279,13 +279,13 @@ def __init__(self, name, bases, namespace, **kwargs):\n     @_tp_cache\n     def _getitem_(self, params):\n         if params is None:\n-            raise TypeError('{}[t]: t can not be None'.format(self.__name__))\n+            raise TypeError(f'{self.__name__}[t]: t can not be None')\n         if isinstance(params, str):\n             params = ForwardRef(params)\n         if not isinstance(params, tuple):\n             params = (params, )\n \n-        msg = \"{}[t]: t must be a type\".format(self.__name__)\n+        msg = f\"{self.__name__}[t]: t must be a type\"\n         params = tuple(_type_check(p, msg) for p in params)\n \n         if isinstance(self.type.param, _GenericAlias):\n@@ -303,7 +303,7 @@ def _getitem_(self, params):\n                                        '__type_class__': True})\n \n         if len(params) > 1:\n-            raise TypeError('Too many parameters for {} actual {}, expected 1'.format(self, len(params)))\n+            raise TypeError(f'Too many parameters for {self} actual {len(params)}, expected 1')\n \n         t = _DataPipeType(params[0])\n \ndiff --git a/torch/utils/data/datapipes/dataframe/dataframes.py b/torch/utils/data/datapipes/dataframe/dataframes.py\nindex 06029e07851685..72d93cde66c3cb 100644\n--- a/torch/utils/data/datapipes/dataframe/dataframes.py\n+++ b/torch/utils/data/datapipes/dataframe/dataframes.py\n@@ -36,7 +36,7 @@ def disable_capture():\n     CaptureControl.disabled = True\n \n \n-class CaptureControl():\n+class CaptureControl:\n     disabled = False\n \n \n@@ -184,7 +184,7 @@ def execute(self):\n         return value\n \n \n-class CaptureLikeMock():\n+class CaptureLikeMock:\n     def __init__(self, name):\n         import unittest.mock as mock\n         # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n@@ -232,7 +232,7 @@ class CaptureVariableAssign(CaptureF):\n     def __str__(self):\n         variable = self.kwargs['variable']\n         value = self.kwargs['value']\n-        return \"{variable} = {value}\".format(variable=variable, value=value)\n+        return f\"{variable} = {value}\"\n \n     def execute(self):\n         self.kwargs['variable'].calculated_value = self.kwargs['value'].execute()\n@@ -272,7 +272,7 @@ def __init__(self, left, key, ctx):\n         self.key = key\n \n     def __str__(self):\n-        return \"%s[%s]\" % (self.left, get_val(self.key))\n+        return f\"{self.left}[{get_val(self.key)}]\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -287,7 +287,7 @@ def __init__(self, left, key, value, ctx):\n         self.value = value\n \n     def __str__(self):\n-        return \"%s[%s] = %s\" % (self.left, get_val(self.key), self.value)\n+        return f\"{self.left}[{get_val(self.key)}] = {self.value}\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -302,7 +302,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s + %s\" % (self.left, self.right)\n+        return f\"{self.left} + {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) + get_val(self.right)\n@@ -315,7 +315,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s * %s\" % (self.left, self.right)\n+        return f\"{self.left} * {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) * get_val(self.right)\n@@ -328,7 +328,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s - %s\" % (self.left, self.right)\n+        return f\"{self.left} - {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) - get_val(self.right)\n@@ -341,7 +341,7 @@ def __init__(self, src, name, ctx):\n         self.name = name\n \n     def __str__(self):\n-        return \"%s.%s\" % (self.src, self.name)\n+        return f\"{self.src}.{self.name}\"\n \n     def execute(self):\n         val = get_val(self.src)\ndiff --git a/torch/utils/data/datapipes/datapipe.py b/torch/utils/data/datapipes/datapipe.py\nindex 445400ecb59c32..1017b52af0fbce 100644\n--- a/torch/utils/data/datapipes/datapipe.py\n+++ b/torch/utils/data/datapipes/datapipe.py\n@@ -126,7 +126,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -135,7 +135,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register, enable_df_api_tracing=False):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, enable_df_api_tracing, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -265,7 +265,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -274,7 +274,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -363,7 +363,7 @@ def __len__(self):\n             return len(self._datapipe)\n         except Exception as e:\n             raise TypeError(\n-                \"{} instance doesn't have valid length\".format(type(self).__name__)\n+                f\"{type(self).__name__} instance doesn't have valid length\"\n             ) from e\n \n \ndiff --git a/torch/utils/data/datapipes/gen_pyi.py b/torch/utils/data/datapipes/gen_pyi.py\nindex 1b77fbfecf0290..ed3e75bc5da12e 100644\n--- a/torch/utils/data/datapipes/gen_pyi.py\n+++ b/torch/utils/data/datapipes/gen_pyi.py\n@@ -19,7 +19,7 @@ def gen_from_template(dir: str, template_name: str, output_name: str, replacemen\n     template_path = os.path.join(dir, template_name)\n     output_path = os.path.join(dir, output_name)\n \n-    with open(template_path, \"r\") as f:\n+    with open(template_path) as f:\n         content = f.read()\n     for placeholder, lines, indentation in replacements:\n         with open(output_path, \"w\") as f:\ndiff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py\nindex 4e3dce4b82d1dd..9916b094e408d1 100644\n--- a/torch/utils/data/datapipes/iter/callable.py\n+++ b/torch/utils/data/datapipes/iter/callable.py\n@@ -126,7 +126,7 @@ def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n         raise TypeError(\n-            \"{} instance doesn't have valid length\".format(type(self).__name__)\n+            f\"{type(self).__name__} instance doesn't have valid length\"\n         )\n \n \ndiff --git a/torch/utils/data/datapipes/iter/combinatorics.py b/torch/utils/data/datapipes/iter/combinatorics.py\nindex 30b569e329b654..4d2973bbc5a2e9 100644\n--- a/torch/utils/data/datapipes/iter/combinatorics.py\n+++ b/torch/utils/data/datapipes/iter/combinatorics.py\n@@ -48,7 +48,7 @@ def __len__(self) -> int:\n         # Dataset has been tested as `Sized`\n         if isinstance(self.sampler, Sized):\n             return len(self.sampler)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('shuffle')\n@@ -137,7 +137,7 @@ def __iter__(self) -> Iterator[T_co]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self._buffer = []\ndiff --git a/torch/utils/data/datapipes/iter/combining.py b/torch/utils/data/datapipes/iter/combining.py\nindex 7c76e986b230d4..4fe05ea717cf16 100644\n--- a/torch/utils/data/datapipes/iter/combining.py\n+++ b/torch/utils/data/datapipes/iter/combining.py\n@@ -56,7 +56,7 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return sum(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('fork')\n@@ -567,7 +567,7 @@ def __len__(self):\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes) * len(self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self.buffer = []\n@@ -627,4 +627,4 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/filelister.py b/torch/utils/data/datapipes/iter/filelister.py\nindex b2ecd71b5ce9c9..22e2cd432d6a3a 100644\n--- a/torch/utils/data/datapipes/iter/filelister.py\n+++ b/torch/utils/data/datapipes/iter/filelister.py\n@@ -61,5 +61,5 @@ def __iter__(self) -> Iterator[str] :\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/fileopener.py b/torch/utils/data/datapipes/iter/fileopener.py\nindex 03d5761a9f164c..50737d9b587b25 100644\n--- a/torch/utils/data/datapipes/iter/fileopener.py\n+++ b/torch/utils/data/datapipes/iter/fileopener.py\n@@ -51,7 +51,7 @@ def __init__(\n         self.encoding: Optional[str] = encoding\n \n         if self.mode not in ('b', 't', 'rb', 'rt', 'r'):\n-            raise ValueError(\"Invalid mode {}\".format(mode))\n+            raise ValueError(f\"Invalid mode {mode}\")\n         # TODO: enforce typing for each instance based on mode, otherwise\n         #       `argument_validation` with this DataPipe may be potentially broken\n \n@@ -68,5 +68,5 @@ def __iter__(self):\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/grouping.py b/torch/utils/data/datapipes/iter/grouping.py\nindex c83bd2748b78fb..b26847d7319740 100644\n--- a/torch/utils/data/datapipes/iter/grouping.py\n+++ b/torch/utils/data/datapipes/iter/grouping.py\n@@ -83,7 +83,7 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('unbatch')\ndiff --git a/torch/utils/data/datapipes/iter/routeddecoder.py b/torch/utils/data/datapipes/iter/routeddecoder.py\nindex 8bfbe1442180ab..5e68ae133e05ab 100644\n--- a/torch/utils/data/datapipes/iter/routeddecoder.py\n+++ b/torch/utils/data/datapipes/iter/routeddecoder.py\n@@ -62,4 +62,4 @@ def __iter__(self) -> Iterator[Tuple[str, Any]]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/sharding.py b/torch/utils/data/datapipes/iter/sharding.py\nindex 730caeaf7d4da3..1f4a3a291bd11f 100644\n--- a/torch/utils/data/datapipes/iter/sharding.py\n+++ b/torch/utils/data/datapipes/iter/sharding.py\n@@ -80,4 +80,4 @@ def __len__(self):\n         if isinstance(self.source_datapipe, Sized):\n             return len(self.source_datapipe) // self.num_of_instances +\\\n                 (1 if (self.instance_id < len(self.source_datapipe) % self.num_of_instances) else 0)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/map/combining.py b/torch/utils/data/datapipes/map/combining.py\nindex 85146f8345cbdc..4a4a785eff78e5 100644\n--- a/torch/utils/data/datapipes/map/combining.py\n+++ b/torch/utils/data/datapipes/map/combining.py\n@@ -47,7 +47,7 @@ def __getitem__(self, index) -> T_co:  # type: ignore[type-var]\n                 return dp[index - offset]\n             else:\n                 offset += len(dp)\n-        raise IndexError(\"Index {} is out of range.\".format(index))\n+        raise IndexError(f\"Index {index} is out of range.\")\n \n     def __len__(self) -> int:\n         return sum(len(dp) for dp in self.datapipes)\ndiff --git a/torch/utils/data/datapipes/map/grouping.py b/torch/utils/data/datapipes/map/grouping.py\nindex da3cf5688a1bb0..65b30d8eba1f40 100644\n--- a/torch/utils/data/datapipes/map/grouping.py\n+++ b/torch/utils/data/datapipes/map/grouping.py\n@@ -64,4 +64,4 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/utils/common.py b/torch/utils/data/datapipes/utils/common.py\nindex e39d67ee6c81b9..99ae0cb4cbd024 100644\n--- a/torch/utils/data/datapipes/utils/common.py\n+++ b/torch/utils/data/datapipes/utils/common.py\n@@ -305,7 +305,7 @@ def __init__(self, file_obj, parent_stream=None, name=None):\n         self.closed = False\n         if parent_stream is not None:\n             if not isinstance(parent_stream, StreamWrapper):\n-                raise RuntimeError('Parent stream should be StreamWrapper, {} was given'.format(type(parent_stream)))\n+                raise RuntimeError(f'Parent stream should be StreamWrapper, {type(parent_stream)} was given')\n             parent_stream.child_counter += 1\n             self.parent_stream = parent_stream\n         if StreamWrapper.debug_unclosed_streams:\ndiff --git a/torch/utils/data/datapipes/utils/decoder.py b/torch/utils/data/datapipes/utils/decoder.py\nindex 4da810c3276684..8a7cb71b619de4 100644\n--- a/torch/utils/data/datapipes/utils/decoder.py\n+++ b/torch/utils/data/datapipes/utils/decoder.py\n@@ -137,7 +137,7 @@ class ImageHandler:\n     - pilrgba: pil None rgba\n     \"\"\"\n     def __init__(self, imagespec):\n-        assert imagespec in list(imagespecs.keys()), \"unknown image specification: {}\".format(imagespec)\n+        assert imagespec in list(imagespecs.keys()), f\"unknown image specification: {imagespec}\"\n         self.imagespec = imagespec.lower()\n \n     def __call__(self, extension, data):\n@@ -167,14 +167,14 @@ def __call__(self, extension, data):\n                 return img\n             elif atype == \"numpy\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n                 if etype == \"uint8\":\n                     return result\n                 else:\n                     return result.astype(\"f\") / 255.0\n             elif atype == \"torch\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n \n                 if etype == \"uint8\":\n                     result = np.array(result.transpose(2, 0, 1))\ndiff --git a/torch/utils/data/graph.py b/torch/utils/data/graph.py\nindex 2769e326c03e3b..7fc95d58fa2198 100644\n--- a/torch/utils/data/graph.py\n+++ b/torch/utils/data/graph.py\n@@ -130,7 +130,7 @@ def traverse(datapipe: DataPipe, only_datapipe: Optional[bool] = None) -> DataPi\n # Add cache here to prevent infinite recursion on DataPipe\n def _traverse_helper(datapipe: DataPipe, only_datapipe: bool, cache: Set[int]) -> DataPipeGraph:\n     if not isinstance(datapipe, (IterDataPipe, MapDataPipe)):\n-        raise RuntimeError(\"Expected `IterDataPipe` or `MapDataPipe`, but {} is found\".format(type(datapipe)))\n+        raise RuntimeError(f\"Expected `IterDataPipe` or `MapDataPipe`, but {type(datapipe)} is found\")\n \n     dp_id = id(datapipe)\n     if dp_id in cache:\ndiff --git a/torch/utils/dlpack.py b/torch/utils/dlpack.py\nindex f903de94eb67b2..a987bca6dcd51b 100644\n--- a/torch/utils/dlpack.py\n+++ b/torch/utils/dlpack.py\n@@ -102,7 +102,7 @@ def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n         # device is either CUDA or ROCm, we need to pass the current\n         # stream\n         if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n-            stream = torch.cuda.current_stream('cuda:{}'.format(device[1]))\n+            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n             # cuda_stream is the pointer to the stream and it is a public\n             # attribute, but it is not documented\n             # The array API specify that the default legacy stream must be passed\ndiff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py\nindex 3b583dbf790109..163f3649c41279 100644\n--- a/torch/utils/hipify/cuda_to_hip_mappings.py\n+++ b/torch/utils/hipify/cuda_to_hip_mappings.py\n@@ -46,7 +46,7 @@\n     RE_MINOR = re.compile(r\"#define\\s+ROCM_VERSION_MINOR\\s+(\\d+)\")\n     RE_PATCH = re.compile(r\"#define\\s+ROCM_VERSION_PATCH\\s+(\\d+)\")\n     major, minor, patch = 0, 0, 0\n-    for line in open(rocm_version_h, \"r\"):\n+    for line in open(rocm_version_h):\n         match = RE_MAJOR.search(line)\n         if match:\n             major = int(match.group(1))\ndiff --git a/torch/utils/hipify/hipify_python.py b/torch/utils/hipify/hipify_python.py\nindex 34a066750e1cdc..fa800659595bd7 100755\n--- a/torch/utils/hipify/hipify_python.py\n+++ b/torch/utils/hipify/hipify_python.py\n@@ -219,13 +219,13 @@ def compute_stats(stats):\n     unsupported_calls = {cuda_call for (cuda_call, _filepath) in stats[\"unsupported_calls\"]}\n \n     # Print the number of unsupported calls\n-    print(\"Total number of unsupported CUDA function calls: {0:d}\".format(len(unsupported_calls)))\n+    print(f\"Total number of unsupported CUDA function calls: {len(unsupported_calls):d}\")\n \n     # Print the list of unsupported calls\n     print(\", \".join(unsupported_calls))\n \n     # Print the number of kernel launches\n-    print(\"\\nTotal number of replaced kernel launches: {0:d}\".format(len(stats[\"kernel_launches\"])))\n+    print(\"\\nTotal number of replaced kernel launches: {:d}\".format(len(stats[\"kernel_launches\"])))\n \n \n def add_dim3(kernel_string, cuda_kernel):\n@@ -254,8 +254,8 @@ def add_dim3(kernel_string, cuda_kernel):\n     first_arg_clean = kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace(\"\\n\", \"\").strip(\" \")\n     second_arg_clean = kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace(\"\\n\", \"\").strip(\" \")\n \n-    first_arg_dim3 = \"dim3({})\".format(first_arg_clean)\n-    second_arg_dim3 = \"dim3({})\".format(second_arg_clean)\n+    first_arg_dim3 = f\"dim3({first_arg_clean})\"\n+    second_arg_dim3 = f\"dim3({second_arg_clean})\"\n \n     first_arg_raw_dim3 = first_arg_raw.replace(first_arg_clean, first_arg_dim3)\n     second_arg_raw_dim3 = second_arg_raw.replace(second_arg_clean, second_arg_dim3)\n@@ -269,7 +269,7 @@ def add_dim3(kernel_string, cuda_kernel):\n def processKernelLaunches(string, stats):\n     \"\"\" Replace the CUDA style Kernel launches with the HIP style kernel launches.\"\"\"\n     # Concat the namespace with the kernel names. (Find cleaner way of doing this later).\n-    string = RE_KERNEL_LAUNCH.sub(lambda inp: \"{0}{1}::\".format(inp.group(1), inp.group(2)), string)\n+    string = RE_KERNEL_LAUNCH.sub(lambda inp: f\"{inp.group(1)}{inp.group(2)}::\", string)\n \n     def grab_method_and_template(in_kernel):\n         # The positions for relevant kernel components.\n@@ -482,7 +482,7 @@ def replace_math_functions(input_string):\n     \"\"\"\n     output_string = input_string\n     for func in MATH_TRANSPILATIONS:\n-        output_string = output_string.replace(r'{}('.format(func), '{}('.format(MATH_TRANSPILATIONS[func]))\n+        output_string = output_string.replace(fr'{func}(', f'{MATH_TRANSPILATIONS[func]}(')\n \n     return output_string\n \n@@ -531,7 +531,7 @@ def replace_extern_shared(input_string):\n     \"\"\"\n     output_string = input_string\n     output_string = RE_EXTERN_SHARED.sub(\n-        lambda inp: \"HIP_DYNAMIC_SHARED({0} {1}, {2})\".format(\n+        lambda inp: \"HIP_DYNAMIC_SHARED({} {}, {})\".format(\n             inp.group(1) or \"\", inp.group(2), inp.group(3)), output_string)\n \n     return output_string\n@@ -657,7 +657,7 @@ def is_caffe2_gpu_file(rel_filepath):\n \n \n # Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n-class Trie():\n+class Trie:\n     \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n     The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n \n@@ -750,7 +750,7 @@ def pattern(self):\n             CAFFE2_TRIE.add(src)\n             CAFFE2_MAP[src] = dst\n RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.pattern())\n-RE_PYTORCH_PREPROCESSOR = re.compile(r'(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))\n+RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\\W)({PYTORCH_TRIE.pattern()})(?=\\W)')\n \n RE_QUOTE_HEADER = re.compile(r'#include \"([^\"]+)\"')\n RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')\n@@ -789,7 +789,7 @@ def preprocessor(\n \n     rel_filepath = os.path.relpath(filepath, output_directory)\n \n-    with open(fin_path, 'r', encoding='utf-8') as fin:\n+    with open(fin_path, encoding='utf-8') as fin:\n         if fin.readline() == HIPIFY_C_BREADCRUMB:\n             hipify_result.hipified_path = None\n             hipify_result.status = \"[ignored, input is hipified output]\"\n@@ -929,7 +929,7 @@ def repl(m):\n \n     do_write = True\n     if os.path.exists(fout_path):\n-        with open(fout_path, 'r', encoding='utf-8') as fout_old:\n+        with open(fout_path, encoding='utf-8') as fout_old:\n             do_write = fout_old.read() != output_source\n     if do_write:\n         try:\n@@ -956,7 +956,7 @@ def file_specific_replacement(filepath, search_string, replace_string, strict=Fa\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if strict:\n-            contents = re.sub(r'\\b({0})\\b'.format(re.escape(search_string)), lambda x: replace_string, contents)\n+            contents = re.sub(fr'\\b({re.escape(search_string)})\\b', lambda x: replace_string, contents)\n         else:\n             contents = contents.replace(search_string, replace_string)\n         f.seek(0)\n@@ -968,8 +968,8 @@ def file_add_header(filepath, header):\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if header[0] != \"<\" and header[-1] != \">\":\n-            header = '\"{0}\"'.format(header)\n-        contents = ('#include {0} \\n'.format(header)) + contents\n+            header = f'\"{header}\"'\n+        contents = (f'#include {header} \\n') + contents\n         f.seek(0)\n         f.write(contents)\n         f.truncate()\ndiff --git a/torch/utils/jit/log_extract.py b/torch/utils/jit/log_extract.py\nindex d9d0e442c1dbf6..2e89a769eff0c8 100644\n--- a/torch/utils/jit/log_extract.py\n+++ b/torch/utils/jit/log_extract.py\n@@ -11,7 +11,7 @@ def extract_ir(filename: str) -> List[str]:\n     pfx = None\n     current = \"\"\n     graphs = []\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         split_strs = f.read().split(BEGIN)\n         for i, split_str in enumerate(split_strs):\n             if i == 0:\ndiff --git a/torch/utils/mobile_optimizer.py b/torch/utils/mobile_optimizer.py\nindex ec200423e10c5b..66d57a2372baf9 100644\n--- a/torch/utils/mobile_optimizer.py\n+++ b/torch/utils/mobile_optimizer.py\n@@ -31,7 +31,7 @@ def optimize_for_mobile(\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     if optimization_blocklist is None:\n         optimization_blocklist = set()\n@@ -86,7 +86,7 @@ def generate_mobile_module_lints(script_module: torch.jit.ScriptModule):\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     lint_list = []\n \ndiff --git a/torch/utils/tensorboard/_caffe2_graph.py b/torch/utils/tensorboard/_caffe2_graph.py\nindex 8bba2aeffddef2..2aa162af7ad5c3 100644\n--- a/torch/utils/tensorboard/_caffe2_graph.py\n+++ b/torch/utils/tensorboard/_caffe2_graph.py\n@@ -232,7 +232,7 @@ def _add_gradient_scope(shapes, blob_name_tracker, ops):\n \n     def f(name):\n         if \"_grad\" in name:\n-            return \"GRADIENTS/{}\".format(name)\n+            return f\"GRADIENTS/{name}\"\n         else:\n             return name\n \n@@ -317,7 +317,7 @@ def _tf_device(device_option):\n     ):\n         return \"/cpu:*\"\n     if device_option.device_type == caffe2_pb2.CUDA:\n-        return \"/gpu:{}\".format(device_option.device_id)\n+        return f\"/gpu:{device_option.device_id}\"\n     raise Exception(\"Unhandled device\", device_option)\n \n \ndiff --git a/torch/utils/tensorboard/_embedding.py b/torch/utils/tensorboard/_embedding.py\nindex f172e092608337..afbe68191aa98f 100644\n--- a/torch/utils/tensorboard/_embedding.py\n+++ b/torch/utils/tensorboard/_embedding.py\n@@ -62,7 +62,7 @@ def make_sprite(label_img, save_path):\n \n def get_embedding_info(metadata, label_img, subdir, global_step, tag):\n     info = EmbeddingInfo()\n-    info.tensor_name = \"{}:{}\".format(tag, str(global_step).zfill(5))\n+    info.tensor_name = f\"{tag}:{str(global_step).zfill(5)}\"\n     info.tensor_path = _gfile_join(subdir, \"tensors.tsv\")\n     if metadata is not None:\n         info.metadata_path = _gfile_join(subdir, \"metadata.tsv\")\ndiff --git a/torch/utils/tensorboard/_pytorch_graph.py b/torch/utils/tensorboard/_pytorch_graph.py\nindex f03812b603e1c0..280b503c515c0b 100644\n--- a/torch/utils/tensorboard/_pytorch_graph.py\n+++ b/torch/utils/tensorboard/_pytorch_graph.py\n@@ -275,7 +275,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n                     parent_scope, attr_scope, attr_name\n                 )\n             else:\n-                attr_to_scope[attr_key] = \"__module.{}\".format(attr_name)\n+                attr_to_scope[attr_key] = f\"__module.{attr_name}\"\n             # We don't need classtype nodes; scope will provide this information\n             if node.output().type().kind() != CLASSTYPE_KIND:\n                 node_py = NodePyOP(node)\n@@ -286,7 +286,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n \n     for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n         node_pyio = NodePyIO(node, \"output\")\n-        node_pyio.debugName = \"output.{}\".format(i + 1)\n+        node_pyio.debugName = f\"output.{i + 1}\"\n         node_pyio.inputs = [node.debugName()]\n         nodes_py.append(node_pyio)\n \n@@ -302,7 +302,7 @@ def parse_traced_name(module):\n     for name, module in trace.named_modules(prefix=\"__module\"):\n         mod_name = parse_traced_name(module)\n         attr_name = name.split(\".\")[-1]\n-        alias_to_name[name] = \"{}[{}]\".format(mod_name, attr_name)\n+        alias_to_name[name] = f\"{mod_name}[{attr_name}]\"\n \n     for node in nodes_py.nodes_op:\n         module_aliases = node.scopeName.split(\"/\")\ndiff --git a/torch/utils/tensorboard/writer.py b/torch/utils/tensorboard/writer.py\nindex 2fa34d05e727d9..b592707df2c1e1 100644\n--- a/torch/utils/tensorboard/writer.py\n+++ b/torch/utils/tensorboard/writer.py\n@@ -953,7 +953,7 @@ def add_embedding(\n \n         # Maybe we should encode the tag so slashes don't trip us up?\n         # I don't think this will mess us up, but better safe than sorry.\n-        subdir = \"%s/%s\" % (str(global_step).zfill(5), self._encode(tag))\n+        subdir = f\"{str(global_step).zfill(5)}/{self._encode(tag)}\"\n         save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n \n         fs = tf.io.gfile\ndiff --git a/torch/utils/throughput_benchmark.py b/torch/utils/throughput_benchmark.py\nindex 8b2fd1a76ca8a4..2dc3ce8543a9b6 100644\n--- a/torch/utils/throughput_benchmark.py\n+++ b/torch/utils/throughput_benchmark.py\n@@ -18,10 +18,10 @@ def format_time(time_us=None, time_ms=None, time_s=None):\n             raise AssertionError(\"Shouldn't reach here :)\")\n \n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n \n class ExecutionStats:\n@@ -52,8 +52,8 @@ def total_time_seconds(self):\n     def __str__(self):\n         return '\\n'.join([\n             \"Average latency per example: \" + format_time(time_ms=self.latency_avg_ms),\n-            \"Total number of iterations: {}\".format(self.num_iters),\n-            \"Total number of iterations per second (across all threads): {:.2f}\".format(self.iters_per_second),\n+            f\"Total number of iterations: {self.num_iters}\",\n+            f\"Total number of iterations per second (across all threads): {self.iters_per_second:.2f}\",\n             \"Total time: \" + format_time(time_s=self.total_time_seconds)\n         ])\n \ndiff --git a/torch/utils/viz/_cycles.py b/torch/utils/viz/_cycles.py\nindex a64d5e9c35830a..13a425cd1b8285 100644\n--- a/torch/utils/viz/_cycles.py\n+++ b/torch/utils/viz/_cycles.py\n@@ -220,29 +220,29 @@ def format_sequence(obj):\n     if isinstance(obj, BASE_TYPES):\n         return repr(obj)\n     if type(obj).__name__ == 'function':\n-        return \"function\\n{}\".format(obj.__name__)\n+        return f\"function\\n{obj.__name__}\"\n     elif isinstance(obj, types.MethodType):\n         try:\n             func_name = obj.__func__.__qualname__\n         except AttributeError:\n             func_name = \"<anonymous>\"\n-        return \"instancemethod\\n{}\".format(func_name)\n+        return f\"instancemethod\\n{func_name}\"\n     elif isinstance(obj, list):\n         return f\"[{format_sequence(obj)}]\"\n     elif isinstance(obj, tuple):\n         return f\"({format_sequence(obj)})\"\n     elif isinstance(obj, dict):\n-        return \"dict[{}]\".format(len(obj))\n+        return f\"dict[{len(obj)}]\"\n     elif isinstance(obj, types.ModuleType):\n-        return \"module\\n{}\".format(obj.__name__)\n+        return f\"module\\n{obj.__name__}\"\n     elif isinstance(obj, type):\n-        return \"type\\n{}\".format(obj.__name__)\n+        return f\"type\\n{obj.__name__}\"\n     elif isinstance(obj, weakref.ref):\n         referent = obj()\n         if referent is None:\n             return \"weakref (dead referent)\"\n         else:\n-            return \"weakref to id 0x{:x}\".format(id(referent))\n+            return f\"weakref to id 0x{id(referent):x}\"\n     elif isinstance(obj, types.FrameType):\n         filename = obj.f_code.co_filename\n         if len(filename) > FRAME_FILENAME_LIMIT:\ndiff --git a/torch/utils/weak.py b/torch/utils/weak.py\nindex 2a7d597c4f2a06..bcd3025bc68e3a 100644\n--- a/torch/utils/weak.py\n+++ b/torch/utils/weak.py\n@@ -4,7 +4,6 @@\n from weakref import ref\n from _weakrefset import _IterationGuard  # type: ignore[attr-defined]\n from collections.abc import MutableMapping, Mapping\n-from typing import Dict\n from torch import Tensor\n import collections.abc as _collections_abc\n \n@@ -83,7 +82,7 @@ def __eq__(self, other):\n \n # This is directly adapted from cpython/Lib/weakref.py\n class WeakIdKeyDictionary(MutableMapping):\n-    data: Dict[WeakIdRef, object]\n+    data: dict[WeakIdRef, object]\n \n     def __init__(self, dict=None):\n         self.data = {}\n@@ -144,7 +143,7 @@ def __len__(self):\n         return len(self.data) - len(self._pending_removals)\n \n     def __repr__(self):\n-        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n+        return f\"<{self.__class__.__name__} at {id(self):#x}>\"\n \n     def __setitem__(self, key, value):\n         self.data[WeakIdRef(key, self._remove)] = value  # CHANGED\n"
  },
  {
    "number": 105472,
    "title": "[BE] Enable ruff's UP rules and autoformat torchgen/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105473\n* __->__ #105472\n\n",
    "merge_commit_sha": "af54df43fb18a5a6bc3b73534b18926c13fb8f4c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105472",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105472/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105472.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105472.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105472/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105472/comments",
    "labels": [
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T15:38:32.109303Z",
    "state": "closed",
    "patch": "From ec5b09614768e79e3540a1cbc23ae34bff255caa Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 08:38:22 -0700\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat torchgen/\n\n[ghstack-poisoned]\n---\n torchgen/api/python.py               | 16 ++++++++--------\n torchgen/code_template.py            |  2 +-\n torchgen/executorch/parse.py         |  2 +-\n torchgen/gen.py                      |  4 ++--\n torchgen/gen_backend_stubs.py        |  6 +++---\n torchgen/gen_executorch.py           |  6 +++---\n torchgen/gen_lazy_tensor.py          |  6 +++---\n torchgen/model.py                    |  2 +-\n torchgen/selective_build/operator.py |  2 +-\n torchgen/selective_build/selector.py |  4 ++--\n torchgen/utils.py                    | 10 +++++-----\n 11 files changed, 30 insertions(+), 30 deletions(-)\n\ndiff --git a/torchgen/api/python.py b/torchgen/api/python.py\nindex b4da5d1113dce6..96aa43be1060b5 100644\n--- a/torchgen/api/python.py\n+++ b/torchgen/api/python.py\n@@ -315,7 +315,7 @@ def from_outputs(\n                 outputs=outputs,\n             )\n         elif size > 1:\n-            if any((not a.type.is_tensor_like() for a in outputs)):\n+            if any(not a.type.is_tensor_like() for a in outputs):\n                 raise RuntimeError(f\"Unsupported output type: {outputs}\")\n             return PythonOutArgument(\n                 name=\"out\",\n@@ -882,10 +882,10 @@ def topt_default_init(name: str) -> Optional[str]:\n \n \n def namedtuple_fieldnames(returns: Tuple[Return, ...]) -> List[str]:\n-    if len(returns) <= 1 or all((r.name is None for r in returns)):\n+    if len(returns) <= 1 or all(r.name is None for r in returns):\n         return []\n     else:\n-        if any((r.name is None for r in returns)):\n+        if any(r.name is None for r in returns):\n             # When building on Windows, `PyStructSequence_UnnamedField` could not be\n             # resolved by the linker for some reason, which cause error in building:\n             #\n@@ -1163,7 +1163,7 @@ def dispatch_lambda_return_str(f: NativeFunction) -> str:\n     # mutable reference to temporary.  Maybe we could assign it to a\n     # variable itself.)\n     returns_without_annotation = tuple(\n-        (Return(r.name, r.type, None) for r in f.func.returns)\n+        Return(r.name, r.type, None) for r in f.func.returns\n     )\n     return_str = cpp.returns_type(returns_without_annotation, symint=True).cpp_type()\n     if return_str not in SUPPORTED_RETURN_TYPES:\n@@ -1195,7 +1195,7 @@ def cpp_dispatch_exprs(\n     exprs: Tuple[str, ...] = tuple()\n     if not isinstance(python_signature, PythonSignatureDeprecated):\n         # By default the exprs are consistent with the C++ signature.\n-        exprs = tuple((a.name for a in cpp_args))\n+        exprs = tuple(a.name for a in cpp_args)\n     else:\n         # For deprecated python signature we may need fill in some constants.\n         exprs = tuple(\n@@ -1426,7 +1426,7 @@ def dispatch_lambda_exprs(\n                     f\"{f.func}: unrecognized type '{str(a.type)}' for tensor options field '{a.name}'\"\n                 )\n         if not all(\n-            (a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys())\n+            a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys()\n         ):\n             raise RuntimeError(\n                 f\"{f.func}: incomplete tensor options args: {tensor_options_args_names}\"\n@@ -1454,7 +1454,7 @@ def dispatch_lambda_exprs(\n                 raise RuntimeError(\n                     f\"{f.func}: dtype in tensor_options_args without output arg\"\n                 )\n-            if not all((a in tensor_options_args_names for a in (\"layout\", \"device\"))):\n+            if not all(a in tensor_options_args_names for a in (\"layout\", \"device\")):\n                 raise RuntimeError(\n                     f\"{f.func}: incomplete tensor options for output check\"\n                 )\n@@ -1473,6 +1473,6 @@ def dispatch_lambda_exprs(\n             )\n \n     return DispatchLambdaArgumentExprs(\n-        exprs=tuple((lambda_args_exprs[a.name] for a in lambda_args)),\n+        exprs=tuple(lambda_args_exprs[a.name] for a in lambda_args),\n         inits=inits,\n     )\ndiff --git a/torchgen/code_template.py b/torchgen/code_template.py\nindex 9f877771afe9be..b932a94ecc9192 100644\n--- a/torchgen/code_template.py\n+++ b/torchgen/code_template.py\n@@ -20,7 +20,7 @@ class CodeTemplate:\n \n     @staticmethod\n     def from_file(filename: str) -> \"CodeTemplate\":\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             return CodeTemplate(f.read(), filename)\n \n     def __init__(self, pattern: str, filename: str = \"\") -> None:\ndiff --git a/torchgen/executorch/parse.py b/torchgen/executorch/parse.py\nindex f6f30b4554aafb..89b4b93558a6a2 100644\n--- a/torchgen/executorch/parse.py\n+++ b/torchgen/executorch/parse.py\n@@ -124,7 +124,7 @@ def parse_et_yaml(\n     \"\"\"Parse native_functions.yaml into NativeFunctions and an Operator Indexed Dict\n     of fields to persist from native_functions.yaml to functions.yaml\n     \"\"\"\n-    with open(path, \"r\") as f:\n+    with open(path) as f:\n         es = yaml.load(f, Loader=LineLoader)\n \n     et_kernel = extract_kernel_fields(es)\ndiff --git a/torchgen/gen.py b/torchgen/gen.py\nindex dcdd0945dff019..9766c8af5bc0f5 100644\n--- a/torchgen/gen.py\n+++ b/torchgen/gen.py\n@@ -212,7 +212,7 @@ def parse_tags_yaml_struct(es: object, path: str = \"<stdin>\") -> Set[str]:\n def parse_tags_yaml(path: str) -> Set[str]:\n     global _GLOBAL_PARSE_TAGS_YAML_CACHE\n     if path not in _GLOBAL_PARSE_TAGS_YAML_CACHE:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n             _GLOBAL_PARSE_TAGS_YAML_CACHE[path] = parse_tags_yaml_struct(es, path=path)\n \n@@ -233,7 +233,7 @@ def parse_native_yaml(\n \n         # if a loaded yaml is provided, use that instead of reading from path\n         if loaded_yaml is None:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 es = yaml.load(f, Loader=LineLoader)\n         else:\n             es = loaded_yaml\ndiff --git a/torchgen/gen_backend_stubs.py b/torchgen/gen_backend_stubs.py\nindex 7322daa5dc7602..ff23aa9be39713 100644\n--- a/torchgen/gen_backend_stubs.py\n+++ b/torchgen/gen_backend_stubs.py\n@@ -47,7 +47,7 @@ def parse_backend_yaml(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -253,9 +253,9 @@ def error_on_missing_kernels(\n     full_codegen: Optional[List[OperatorName]] = None,\n ) -> None:\n     try:\n-        with open(kernel_defn_file_path, \"r\") as f:\n+        with open(kernel_defn_file_path) as f:\n             backend_defns = f.read()\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified impl_path file: {kernel_defn_file_path}\"\n         ) from e\ndiff --git a/torchgen/gen_executorch.py b/torchgen/gen_executorch.py\nindex bfd42a7985e49d..6f5df46944f0f6 100644\n--- a/torchgen/gen_executorch.py\n+++ b/torchgen/gen_executorch.py\n@@ -575,7 +575,7 @@ def translate_native_yaml(\n         None\n     \"\"\"\n     if use_aten_lib:\n-        with open(aten_yaml_path, \"r\") as aten_yaml:\n+        with open(aten_yaml_path) as aten_yaml:\n             out_file.writelines(aten_yaml.readlines())\n         return\n \n@@ -604,7 +604,7 @@ def translate_native_yaml(\n         or os.stat(native_yaml_path).st_size == 0\n     ):\n         return\n-    with open(native_yaml_path, \"r\") as native_yaml:\n+    with open(native_yaml_path) as native_yaml:\n         native_es = yaml.load(native_yaml, Loader=LineLoader)\n         if not native_es:\n             return\n@@ -641,7 +641,7 @@ def parse_yaml(\n     Union[Dict[DispatchKey, Dict[OperatorName, BackendMetadata]], ETKernelIndex],\n ]:\n     if path and os.path.exists(path) and os.stat(path).st_size > 0:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n \n         # Check for kernel index structure\ndiff --git a/torchgen/gen_lazy_tensor.py b/torchgen/gen_lazy_tensor.py\nindex f995bdb2619838..3e4e4b0414277c 100644\n--- a/torchgen/gen_lazy_tensor.py\n+++ b/torchgen/gen_lazy_tensor.py\n@@ -115,7 +115,7 @@ def parse_native_functions_keys(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -134,10 +134,10 @@ def validate_shape_inference_header(\n     shape_inference_hdr: str, expected_shape_infr_decls: List[str]\n ) -> None:\n     try:\n-        with open(shape_inference_hdr, \"r\") as f:\n+        with open(shape_inference_hdr) as f:\n             shape_infr_decls = f.read()\n             shape_infr_decl_lines = set(shape_infr_decls.split(\"\\n\"))\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified shape_inference_hdr file: {shape_inference_hdr}\"\n         ) from e\ndiff --git a/torchgen/model.py b/torchgen/model.py\nindex 151fb02bb2c908..0b44732455ea2e 100644\n--- a/torchgen/model.py\n+++ b/torchgen/model.py\n@@ -40,7 +40,7 @@ class Location:\n     line: int\n \n     def __str__(self) -> str:\n-        return \"{}:{}\".format(self.file, self.line)\n+        return f\"{self.file}:{self.line}\"\n \n \n # Valid values of the 'variants' field in native_functions.yaml\ndiff --git a/torchgen/selective_build/operator.py b/torchgen/selective_build/operator.py\nindex 52fdcb74fca84b..d7f5c56f63a60d 100644\n--- a/torchgen/selective_build/operator.py\n+++ b/torchgen/selective_build/operator.py\n@@ -83,7 +83,7 @@ def from_yaml_dict(\n         if \"debug_info\" in op_info:\n             di_list = op_info[\"debug_info\"]\n             assert isinstance(di_list, list)\n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         return SelectiveBuildOperator(\n             name=op_name,\ndiff --git a/torchgen/selective_build/selector.py b/torchgen/selective_build/selector.py\nindex 1d4a00e5968950..4fdc513534444d 100644\n--- a/torchgen/selective_build/selector.py\n+++ b/torchgen/selective_build/selector.py\n@@ -93,7 +93,7 @@ def from_yaml_dict(data: Dict[str, object]) -> \"SelectiveBuilder\":\n             di_list = data[\"debug_info\"]\n             assert isinstance(di_list, list)\n \n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         operators = {}\n         operators_dict = data.get(\"operators\", {})\n@@ -141,7 +141,7 @@ def from_yaml_str(config_contents: str) -> \"SelectiveBuilder\":\n \n     @staticmethod\n     def from_yaml_path(config_path: str) -> \"SelectiveBuilder\":\n-        with open(config_path, \"r\") as f:\n+        with open(config_path) as f:\n             contents = yaml.safe_load(f)\n             return SelectiveBuilder.from_yaml_dict(contents)\n \ndiff --git a/torchgen/utils.py b/torchgen/utils.py\nindex dd187c737c93ed..0729645ef10b92 100644\n--- a/torchgen/utils.py\n+++ b/torchgen/utils.py\n@@ -105,7 +105,7 @@ def context(msg_fn: Callable[[], str]) -> Iterator[None]:\n # for getting mypy to do exhaustiveness checking\n # TODO: put this somewhere else, maybe\n def assert_never(x: NoReturn) -> NoReturn:\n-    raise AssertionError(\"Unhandled type: {}\".format(type(x).__name__))\n+    raise AssertionError(f\"Unhandled type: {type(x).__name__}\")\n \n \n @functools.lru_cache(maxsize=None)\n@@ -137,9 +137,9 @@ def __init__(self, install_dir: str, template_dir: str, dry_run: bool) -> None:\n     def _write_if_changed(self, filename: str, contents: str) -> None:\n         old_contents: Optional[str]\n         try:\n-            with open(filename, \"r\") as f:\n+            with open(filename) as f:\n                 old_contents = f.read()\n-        except IOError:\n+        except OSError:\n             old_contents = None\n         if contents != old_contents:\n             # Create output directory if it doesn't exist\n@@ -157,7 +157,7 @@ def substitute_with_template(\n             # TODO: Update the comment reference to the correct location\n             if \"generated_comment\" not in env:\n                 comment = \"@\" + \"generated by torchgen/gen.py\"\n-                comment += \" from {}\".format(os.path.basename(template_path))\n+                comment += f\" from {os.path.basename(template_path)}\"\n                 env[\"generated_comment\"] = comment\n             template = _read_template(template_path)\n             return template.substitute(env)\n@@ -172,7 +172,7 @@ def write_with_template(\n         template_fn: str,\n         env_callable: Callable[[], Union[str, Dict[str, Any]]],\n     ) -> None:\n-        filename = \"{}/{}\".format(self.install_dir, filename)\n+        filename = f\"{self.install_dir}/{filename}\"\n         assert filename not in self.filenames, \"duplicate file write {filename}\"\n         self.filenames.add(filename)\n         if not self.dry_run:\n"
  },
  {
    "number": 105470,
    "title": "[MPS] Add lerp implementation",
    "body": "lerp.Scalar fits very well into binary op template\r\nAdd a very naive implementation for `lerp.Tensor` as `add_out(self, weights.mul(end.sub(self)))`\r\n\r\nEnable `lerp` testing in `test_mps`\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/105382\r\n",
    "merge_commit_sha": "33633e47df6f1d01140dc068705bf9566f3514ba",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105470",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105470/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105470.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105470.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105470/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105470/comments",
    "labels": [
      "ciflow/mps",
      "Merged",
      "topic: improvements",
      "release notes: mps"
    ],
    "_event_time": "2023-07-18T15:28:09.947987Z",
    "state": "closed",
    "patch": "From d6686b9ce3e968386b2efd72f0363aa12a1c9140 Mon Sep 17 00:00:00 2001\nFrom: Nikita Shulga <nikita.shulga@gmail.com>\nDate: Tue, 18 Jul 2023 08:26:40 -0700\nSubject: [PATCH 1/3] [MPS] Add lerp implementation\n\nlerp.Scalar fits very well into binary op template\nAdd a very naive implementation for lerp.Tensor\n\nEnable `lerp` testing in `test_mps`\n---\n .../ATen/native/mps/operations/BinaryOps.mm   | 42 +++++++++++++------\n aten/src/ATen/native/mps/operations/Lerp.mm   | 19 +++++++++\n aten/src/ATen/native/native_functions.yaml    |  2 +\n test/test_mps.py                              |  1 -\n 4 files changed, 51 insertions(+), 13 deletions(-)\n create mode 100644 aten/src/ATen/native/mps/operations/Lerp.mm\n\ndiff --git a/aten/src/ATen/native/mps/operations/BinaryOps.mm b/aten/src/ATen/native/mps/operations/BinaryOps.mm\nindex ca13f8d4424271..b8091686cffa0c 100644\n--- a/aten/src/ATen/native/mps/operations/BinaryOps.mm\n+++ b/aten/src/ATen/native/mps/operations/BinaryOps.mm\n@@ -18,6 +18,7 @@\n #include <ATen/ops/gt_native.h>\n #include <ATen/ops/hypot_native.h>\n #include <ATen/ops/le_native.h>\n+#include <ATen/ops/lerp_native.h>\n #include <ATen/ops/logaddexp2_native.h>\n #include <ATen/ops/logaddexp_native.h>\n #include <ATen/ops/lt_native.h>\n@@ -46,7 +47,7 @@\n #define BinaryOpFn(graph, primary, secondary) \\\n   MPSGraphTensor*(mps::BinaryOpCachedGraph * graph, MPSGraphTensor * primary, MPSGraphTensor * secondary)\n \n-// alpha is always 1.0 except when this function is called from add_sub_template()\n+// alpha is always 1.0 except when this function is called from add_sub_lerp_template()\n void binaryOpTensor(const Tensor& self,\n                     const Tensor& other,\n                     const Scalar& alpha,\n@@ -173,7 +174,7 @@ void binaryOpTensor(const Tensor& self,\n       feeds[otherPlaceholder.getMPSGraphTensor()] = otherPlaceholder.getMPSGraphTensorData();\n     }\n \n-    // 'cachedGraph->alphaTensor' is not nil only if add_sub_template() was called with an alpha value != 1.0\n+    // 'cachedGraph->alphaTensor' is not nil only if add_sub_lerp_template() was called with an alpha value != 1.0\n     if (cachedGraph->alphaTensor) {\n       alpha_scalar = getMPSScalar(alpha, other.scalar_type());\n       feeds[cachedGraph->alphaTensor] = getMPSGraphTensorFromScalar(mpsStream, alpha_scalar);\n@@ -255,11 +256,11 @@ void div_mode_template(const Tensor& self,\n                  div_mode_op_block);\n }\n \n-void add_sub_template(const Tensor& self,\n-                      const Tensor& other,\n-                      const Scalar& alpha,\n-                      const Tensor& output,\n-                      std::string op_name) {\n+void add_sub_lerp_template(const Tensor& self,\n+                           const Tensor& other,\n+                           const Scalar& alpha,\n+                           const Tensor& output,\n+                            std::string op_name) {\n   if (alpha.toDouble() == 0.0) {\n     if (!self.is_alias_of(output)) { // if inplace, no-op\n       const_cast<Tensor&>(output) = self.clone();\n@@ -273,10 +274,23 @@ void add_sub_template(const Tensor& self,\n     at::native::alpha_check(commonDtype, alpha);\n   }\n \n-  BinaryOpBlock add_sub_op_block = ^BinaryOpFn(cachedGraph, primaryCastTensor, secondaryCastTensor) {\n+  if (!alpha_has_value && op_name == \"lerp\") {\n+    if (!self.is_alias_of(other)) { // if inplace, no-op\n+      const_cast<Tensor&>(output) = other.clone();\n+    }\n+    return;\n+  }\n+\n+  BinaryOpBlock add_sub_lerp_op_block = ^BinaryOpFn(cachedGraph, primaryCastTensor, secondaryCastTensor) {\n     MPSGraph* mpsGraph = cachedGraph->graph();\n     MPSGraphTensor* secondaryTensor = secondaryCastTensor;\n \n+    if (op_name == \"lerp\") {\n+        secondaryCastTensor = [mpsGraph subtractionWithPrimaryTensor: secondaryCastTensor\n+                                                     secondaryTensor: primaryCastTensor\n+                                                                name: nil];\n+    }\n+\n     // if alpha is 1.0, then we don't bother adding another multiply to graph\n     if (alpha_has_value) {\n       cachedGraph->alphaTensor = mpsGraphRankedPlaceHolder(mpsGraph, getMPSScalarType(other.scalar_type()), @[ @1 ]);\n@@ -284,7 +298,7 @@ void add_sub_template(const Tensor& self,\n                                                   secondaryTensor:cachedGraph->alphaTensor\n                                                              name:nil];\n     }\n-    if (op_name == \"add\")\n+    if (op_name == \"add\" || op_name == \"lerp\")\n       return [mpsGraph additionWithPrimaryTensor:primaryCastTensor secondaryTensor:secondaryTensor name:nil];\n     else\n       return [mpsGraph subtractionWithPrimaryTensor:primaryCastTensor secondaryTensor:secondaryTensor name:nil];\n@@ -295,7 +309,7 @@ void add_sub_template(const Tensor& self,\n                  alpha,\n                  output,\n                  op_name + \"_out_mps:\" + (alpha_has_value ? getMPSTypeString(alpha.type()) : \"\"),\n-                 add_sub_op_block);\n+                 add_sub_lerp_op_block);\n }\n \n } // namespace mps\n@@ -389,11 +403,11 @@ void add_sub_template(const Tensor& self,\n }\n \n TORCH_IMPL_FUNC(add_out_mps)(const Tensor& self, const Tensor& other, const Scalar& alpha, const Tensor& output) {\n-  mps::add_sub_template(self, other, alpha, output, \"add\");\n+  mps::add_sub_lerp_template(self, other, alpha, output, \"add\");\n }\n \n TORCH_IMPL_FUNC(sub_out_mps)(const Tensor& self, const Tensor& other, const Scalar& alpha, const Tensor& output) {\n-  mps::add_sub_template(self, other, alpha, output, \"sub\");\n+  mps::add_sub_lerp_template(self, other, alpha, output, \"sub\");\n }\n \n TORCH_IMPL_FUNC(pow_Scalar_out_mps)(const Scalar& base, const Tensor& exp, const Tensor& out) {\n@@ -492,4 +506,8 @@ Tensor floor_divide_mps(const Tensor& self, const Tensor& other) {\n   mps::binaryOpTensor(self, other, Scalar(1.0), output, \"xlogy_out_mps\", xlogy_op_block);\n }\n \n+TORCH_IMPL_FUNC(lerp_Scalar_mps)(\n+    const Tensor& self, const Tensor& end, const Scalar& weight, const Tensor& out) {\n+  mps::add_sub_lerp_template(self, end, weight, out, \"lerp\");\n+}\n } // namespace at::native\ndiff --git a/aten/src/ATen/native/mps/operations/Lerp.mm b/aten/src/ATen/native/mps/operations/Lerp.mm\nnew file mode 100644\nindex 00000000000000..a584b8166e3a58\n--- /dev/null\n+++ b/aten/src/ATen/native/mps/operations/Lerp.mm\n@@ -0,0 +1,19 @@\n+#define TORCH_ASSERT_ONLY_METHOD_OPERATORS\n+#include <ATen/core/Tensor.h>\n+\n+#ifndef AT_PER_OPERATOR_HEADERS\n+#include <ATen/Functions.h>\n+#include <ATen/NativeFunctions.h>\n+#else\n+#include <ATen/ops/lerp_native.h>\n+#include <ATen/ops/add.h>\n+#endif\n+\n+namespace at::native {\n+TORCH_IMPL_FUNC(lerp_Tensor_mps)(\n+    const Tensor& self, const Tensor& end, const Tensor& weight, const Tensor& out) {\n+  // TODO: Write a much better implementation\n+  at::add_out(const_cast<Tensor&>(out), self, weight.mul(end.sub(self)));\n+}\n+\n+} // namespace at::native\ndiff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml\nindex 9709d5083950fb..e5a7020186662a 100644\n--- a/aten/src/ATen/native/native_functions.yaml\n+++ b/aten/src/ATen/native/native_functions.yaml\n@@ -9237,6 +9237,7 @@\n   structured_inherits: TensorIteratorBase\n   dispatch:\n     CPU, CUDA: lerp_Scalar\n+    MPS: lerp_Scalar_mps\n   tags: pointwise\n \n - func: lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)\n@@ -9245,6 +9246,7 @@\n   structured_inherits: TensorIteratorBase\n   dispatch:\n     CPU, CUDA: lerp_Tensor\n+    MPS: lerp_Tensor_mps\n   tags: pointwise\n \n - func: lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor\ndiff --git a/test/test_mps.py b/test/test_mps.py\nindex 8f16af44d378be..211b5b43cadbd4 100644\n--- a/test/test_mps.py\n+++ b/test/test_mps.py\n@@ -424,7 +424,6 @@ def mps_ops_modifier(ops):\n         'isposinf': None,\n         'kthvalue': None,\n         'lcm': None,\n-        'lerp': None,\n         'lgamma': None,\n         'linalg.cholesky': None,\n         'linalg.cholesky_ex': None,\n\nFrom e1b35b297c4814ba12fe37a211118aed5e5aaa76 Mon Sep 17 00:00:00 2001\nFrom: Nikita Shulga <nikita.shulga@gmail.com>\nDate: Tue, 18 Jul 2023 09:37:49 -0700\nSubject: [PATCH 2/3] Fix lint\n\n---\n aten/src/ATen/native/mps/operations/BinaryOps.mm | 11 +++++------\n aten/src/ATen/native/mps/operations/Lerp.mm      |  5 ++---\n 2 files changed, 7 insertions(+), 9 deletions(-)\n\ndiff --git a/aten/src/ATen/native/mps/operations/BinaryOps.mm b/aten/src/ATen/native/mps/operations/BinaryOps.mm\nindex b8091686cffa0c..b8753d343bced7 100644\n--- a/aten/src/ATen/native/mps/operations/BinaryOps.mm\n+++ b/aten/src/ATen/native/mps/operations/BinaryOps.mm\n@@ -260,7 +260,7 @@ void add_sub_lerp_template(const Tensor& self,\n                            const Tensor& other,\n                            const Scalar& alpha,\n                            const Tensor& output,\n-                            std::string op_name) {\n+                           std::string op_name) {\n   if (alpha.toDouble() == 0.0) {\n     if (!self.is_alias_of(output)) { // if inplace, no-op\n       const_cast<Tensor&>(output) = self.clone();\n@@ -286,9 +286,9 @@ void add_sub_lerp_template(const Tensor& self,\n     MPSGraphTensor* secondaryTensor = secondaryCastTensor;\n \n     if (op_name == \"lerp\") {\n-        secondaryCastTensor = [mpsGraph subtractionWithPrimaryTensor: secondaryCastTensor\n-                                                     secondaryTensor: primaryCastTensor\n-                                                                name: nil];\n+      secondaryCastTensor = [mpsGraph subtractionWithPrimaryTensor:secondaryCastTensor\n+                                                   secondaryTensor:primaryCastTensor\n+                                                              name:nil];\n     }\n \n     // if alpha is 1.0, then we don't bother adding another multiply to graph\n@@ -506,8 +506,7 @@ Tensor floor_divide_mps(const Tensor& self, const Tensor& other) {\n   mps::binaryOpTensor(self, other, Scalar(1.0), output, \"xlogy_out_mps\", xlogy_op_block);\n }\n \n-TORCH_IMPL_FUNC(lerp_Scalar_mps)(\n-    const Tensor& self, const Tensor& end, const Scalar& weight, const Tensor& out) {\n+TORCH_IMPL_FUNC(lerp_Scalar_mps)(const Tensor& self, const Tensor& end, const Scalar& weight, const Tensor& out) {\n   mps::add_sub_lerp_template(self, end, weight, out, \"lerp\");\n }\n } // namespace at::native\ndiff --git a/aten/src/ATen/native/mps/operations/Lerp.mm b/aten/src/ATen/native/mps/operations/Lerp.mm\nindex a584b8166e3a58..ca674336a907f5 100644\n--- a/aten/src/ATen/native/mps/operations/Lerp.mm\n+++ b/aten/src/ATen/native/mps/operations/Lerp.mm\n@@ -5,13 +5,12 @@\n #include <ATen/Functions.h>\n #include <ATen/NativeFunctions.h>\n #else\n-#include <ATen/ops/lerp_native.h>\n #include <ATen/ops/add.h>\n+#include <ATen/ops/lerp_native.h>\n #endif\n \n namespace at::native {\n-TORCH_IMPL_FUNC(lerp_Tensor_mps)(\n-    const Tensor& self, const Tensor& end, const Tensor& weight, const Tensor& out) {\n+TORCH_IMPL_FUNC(lerp_Tensor_mps)(const Tensor& self, const Tensor& end, const Tensor& weight, const Tensor& out) {\n   // TODO: Write a much better implementation\n   at::add_out(const_cast<Tensor&>(out), self, weight.mul(end.sub(self)));\n }\n\nFrom d47b05d97a6a6823c0a82a4288a7f67863c7c504 Mon Sep 17 00:00:00 2001\nFrom: Nikita Shulga <nikita.shulga@gmail.com>\nDate: Tue, 18 Jul 2023 09:42:12 -0700\nSubject: [PATCH 3/3] Review feedback\n\n---\n aten/src/ATen/native/mps/operations/BinaryOps.mm | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/aten/src/ATen/native/mps/operations/BinaryOps.mm b/aten/src/ATen/native/mps/operations/BinaryOps.mm\nindex b8753d343bced7..2e05afb8e719ef 100644\n--- a/aten/src/ATen/native/mps/operations/BinaryOps.mm\n+++ b/aten/src/ATen/native/mps/operations/BinaryOps.mm\n@@ -276,7 +276,7 @@ void add_sub_lerp_template(const Tensor& self,\n \n   if (!alpha_has_value && op_name == \"lerp\") {\n     if (!self.is_alias_of(other)) { // if inplace, no-op\n-      const_cast<Tensor&>(output) = other.clone();\n+      output.copy_(other);\n     }\n     return;\n   }\n"
  },
  {
    "number": 105469,
    "title": "[pt2][inductor] fix LoweringException: TypeError: '<' not supported between instances of 'ExternKernelCaller' and 'ExternKernelCaller'",
    "body": "Summary: `sort_keys=True` for autotuning results fails because we can't compare ExternKernelCaller objects. besides, it isn't really necessary to sort the keys, either for the autotuning results or the sysinfo. let's just drop sorting all together\n\nTest Plan: sandcastle + CI\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D47544587\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "79fff20fdab17a986205ee280fe0943c0e7bcd54",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105469",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105469/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105469.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105469.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105469/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105469/comments",
    "labels": [
      "Merged",
      "fb-exported",
      "ciflow/trunk",
      "ciflow/inductor",
      "module: inductor",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T15:24:23.216246Z",
    "state": "closed",
    "patch": "From 2c856d16615e41449cc3e043dbb3883cec259af6 Mon Sep 17 00:00:00 2001\nFrom: Nicolas Macchioni <nmacchioni@meta.com>\nDate: Tue, 18 Jul 2023 08:21:35 -0700\nSubject: [PATCH] [pt2][inductor] fix LoweringException: TypeError: '<' not\n supported between instances of 'ExternKernelCaller' and 'ExternKernelCaller'\n\nSummary: `sort_keys=True` for autotuning results fails because we can't compare ExternKernelCaller objects. besides, it isn't really necessary to sort the keys, either for the autotuning results or the sysinfo. let's just drop sorting all together\n\nTest Plan: sandcastle + CI\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D47544587\n\nfbshipit-source-id: 719b6cfa9899489faf65ff7f5aeb991509dd9ee0\n---\n torch/_inductor/codecache.py | 5 ++++-\n 1 file changed, 4 insertions(+), 1 deletion(-)\n\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex b5cc885678a591..2f0cc88c29bbf2 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -271,7 +271,10 @@ def check_cache(cache, callback=None):\n                 self.update_local_cache(local_cache)\n \n                 if use_global_cache():\n-                    log_vals(timings)\n+                    timings_to_log = {\n+                        choice.hash_key(): timings[choice] for choice in choices\n+                    }\n+                    log_vals(timings_to_log)\n         elif use_global_cache():\n             # only check global cache, not local one\n             check_cache(self.get_global_cache(), callback=log_stats)\n"
  },
  {
    "number": 105468,
    "title": "Distinguish between outer and inner fake mode in Dynamo",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105468\n\r\nPossibly fixes https://github.com/pytorch/pytorch/issues/105077\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "f7436f88710e3da5d024f61f83535d682b568826",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105468",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105468/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105468.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105468.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105468/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105468/comments",
    "labels": [
      "ciflow/inductor",
      "module: dynamo"
    ],
    "_event_time": "2023-07-18T15:06:56.216758Z",
    "state": "open",
    "patch": "From 161cb3bf43a72f279b3d91ebcca47d966745dae0 Mon Sep 17 00:00:00 2001\nFrom: \"Edward Z. Yang\" <ezyang@meta.com>\nDate: Tue, 18 Jul 2023 08:06:47 -0700\nSubject: [PATCH 1/2] Distinguish between outer and inner fake mode in Dynamo\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\n[ghstack-poisoned]\n---\n torch/_dynamo/eval_frame.py        | 2 +-\n torch/_dynamo/variables/builder.py | 8 +++++---\n 2 files changed, 6 insertions(+), 4 deletions(-)\n\ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..9f410118c0098c 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -908,7 +908,7 @@ def dynamo_normalization_capturing_compiler(\n         graph = gm\n \n         nonlocal fake_mode, example_inputs\n-        fake_mode = fake_mode or _guards.detect_fake_mode(inner_example_inputs)\n+        fake_mode = fake_mode or _guards.detect_fake_mode()\n         example_inputs = inner_example_inputs\n \n         def result_capturing_wrapper(*graph_inputs):\ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex 3a934ba24ef58a..18584843296f1b 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -1071,6 +1071,7 @@ def wrap_unspecialized_primitive(self, value):\n                     example_value = unspec_var.proxy.node.meta[\"example_value\"]\n                 if isinstance(example_value, torch._subclasses.fake_tensor.FakeTensor):\n                     fake_tensor_value = example_value\n+                    assert fake_tensor_value.fake_mode is self.tx.fake_mode\n                 proxy.node.meta[\"grapharg\"] = GraphArg(\n                     self.get_source(),\n                     wrapped_value,\n@@ -1178,7 +1179,7 @@ def _clone_input(value):\n             example_value = get_fake_value(proxy.node, tx)\n \n         # Handle recursive calls here\n-        elif isinstance(example_value, FakeTensor):\n+        elif isinstance(example_value, FakeTensor) and example_value.fake_mode is tx.fake_mode:\n             pass\n \n         elif isinstance(example_value, torch.Tensor):\n@@ -1218,7 +1219,8 @@ def _clone_input(value):\n         example_value = _clone_input(example_value)\n         proxy.node.meta[\"example_value\"] = example_value\n         specialized_props = target_cls.specialize(example_value)\n-        if isinstance(example_value, torch._subclasses.fake_tensor.FakeTensor):\n+        # TODO: not sure about this fake mode test\n+        if isinstance(example_value, torch._subclasses.fake_tensor.FakeTensor) and example_value.fake_mode is tx.fake_mode:\n             # NB: This will be wrong for ignore_subclass; fix it up later!\n             specialized_props[\"class_type\"] = (\n                 torch.nn.Parameter if is_parameter else torch.Tensor\n@@ -1456,7 +1458,7 @@ def update_dim2constraint(dim, constraint_range):\n def wrap_to_fake_tensor_and_record(\n     e, tx, ignore_subclass=False, *, source: Optional[Source], is_tensor: bool\n ):\n-    if type(e) in (torch.Tensor, torch.nn.Parameter) or (\n+    if type(e) in (torch.Tensor, torch.nn.Parameter, FakeTensor) or (\n         ignore_subclass and isinstance(e, torch.Tensor)\n     ):\n         assert source is not None\n\nFrom 94891d44d52c4937b81a963056ca066c8363bab9 Mon Sep 17 00:00:00 2001\nFrom: \"Edward Z. Yang\" <ezyang@meta.com>\nDate: Tue, 18 Jul 2023 08:40:51 -0700\nSubject: [PATCH 2/2] Update on \"Distinguish between outer and inner fake mode\n in Dynamo\"\n\nPossibly fixes https://github.com/pytorch/pytorch/issues/105077\n\nSigned-off-by: Edward Z. Yang <ezyangmeta.com>\n\ncc voznesenskym penguinwu anijain2305 EikanWang jgong5 Guobing-Chen XiaobingSuper zhuhaozhe blzheng Xia-Weiwen wenzhe-nrv jiayisunx ipiszy chenyang78 aakhundov\n\n[ghstack-poisoned]\n---\n torch/_dynamo/eval_frame.py | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex 9f410118c0098c..bb22386aa3ead2 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -57,6 +57,7 @@\n log = logging.getLogger(__name__)\n \n from torch._dispatch.python import enable_python_dispatcher\n+from torch.utils._python_dispatch import _disable_current_modes\n \n always_optimize_code_objects = utils.ExactWeakKeyDictionary()\n null_context = contextlib.nullcontext\n@@ -443,7 +444,7 @@ def catch_errors(frame, cache_size, frame_state):\n                     )\n                     return hijacked_callback(frame, cache_size, hooks, frame_state)\n \n-        with compile_lock:\n+        with compile_lock, _disable_current_modes():\n             return callback(frame, cache_size, hooks, frame_state)\n \n     catch_errors._torchdynamo_orig_callable = callback  # type: ignore[attr-defined]\n"
  },
  {
    "number": 105466,
    "title": "[inductor] Allow specify a subdir to store .so and .cubin files",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105466\n\nSummary: The subdir is used to store .so and .cubin files generated by AOTInductor. It can either be specified, or created based on hash of the input graph.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov\n\nDifferential Revision: [D47556730](https://our.internmc.facebook.com/intern/diff/D47556730)",
    "merge_commit_sha": "28bfd9e26d9dd28ea2a524fbd709da4664df48f7",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105466",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105466/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105466.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105466.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105466/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105466/comments",
    "labels": [
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T14:42:21.049585Z",
    "state": "open",
    "patch": "From c09f824da0928bc54d8016e6b729d95c81400327 Mon Sep 17 00:00:00 2001\nFrom: Bin Bao <binbao@meta.com>\nDate: Tue, 18 Jul 2023 07:42:14 -0700\nSubject: [PATCH 1/2] [inductor] Allow specify a subdir to store .so and .cubin\n files\n\nSummary: The subdir is used to store .so and .cubin files generated by AOTInductor. It can either be specified, or created based on hash of the input graph.\n\n[ghstack-poisoned]\n---\n torch/_inductor/codecache.py  | 40 +++++++++++++++++++++++------------\n torch/_inductor/compile_fx.py | 11 +++++++++-\n torch/_inductor/config.py     |  7 ++++--\n 3 files changed, 42 insertions(+), 16 deletions(-)\n\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex b5cc885678a591..a95680fccfc0b2 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -299,8 +299,14 @@ def code_hash(code, extra=\"\"):\n     )\n \n \n-def get_path(basename: str, extension: str):\n-    subdir = os.path.join(cache_dir(), basename[1:3])\n+def get_path(basename: str, extension: str, specified_dir: str = \"\"):\n+    if specified_dir:\n+        if os.path.isabs(specified_dir):\n+            subdir = specified_dir\n+        else:\n+            subdir = os.path.join(cache_dir(), specified_dir)\n+    else:\n+        subdir = os.path.join(cache_dir(), basename[1:3])\n     path = os.path.join(subdir, f\"{basename}.{extension}\")\n     return basename, subdir, path\n \n@@ -314,10 +320,14 @@ def get_hash(content: Union[str, bytes], extra=\"\", hash_type=\"code\"):\n \n \n def write(\n-    content: Union[str, bytes], extension: str, extra=\"\", hash_type: str = \"code\"\n+    content: Union[str, bytes],\n+    extension: str,\n+    extra=\"\",\n+    hash_type: str = \"code\",\n+    specified_dir: str = \"\",\n ):\n     key: str = get_hash(content, extra, hash_type)\n-    basename, subdir, path = get_path(key, extension)\n+    basename, subdir, path = get_path(key, extension, specified_dir)\n     if not os.path.exists(subdir):\n         os.makedirs(subdir, exist_ok=True)\n     if not os.path.exists(path):\n@@ -791,7 +801,12 @@ class CudaKernelParamCache:\n \n     @classmethod\n     def set(cls, key, params, cubin):\n-        _, path = write(cubin, \"cubin\", hash_type=\"cubin\")\n+        _, path = write(\n+            cubin,\n+            \"cubin\",\n+            hash_type=\"cubin\",\n+            specified_dir=config.aot_inductor_output_path,\n+        )\n         params[\"cubin_path\"] = path\n         cls.cache[key] = params\n \n@@ -813,20 +828,19 @@ def compile(cls, graph, source_code, cuda):\n                 \"i\", \"o\", vec_isa=picked_vec_isa, cuda=cuda, aot_mode=graph.aot_mode\n             )\n         )\n-        key, input_path = write(source_code, \"cpp\", extra=cpp_command)\n+        key, input_path = write(\n+            source_code,\n+            \"cpp\",\n+            extra=cpp_command,\n+            specified_dir=config.aot_inductor_output_path,\n+        )\n         if key not in cls.cache:\n             from filelock import FileLock\n \n             lock_dir = get_lock_dir()\n             lock = FileLock(os.path.join(lock_dir, key + \".lock\"), timeout=LOCK_TIMEOUT)\n             with lock:\n-                # Place the generated .so into a sub-folder with the full hex-hash to avoid\n-                # any name collision.\n-                output_so_dir = os.path.splitext(input_path)[0]\n-                if not os.path.exists(output_so_dir):\n-                    os.makedirs(output_so_dir, exist_ok=False)\n-                so_name = f\"{config.dll_name}.so\"\n-                output_so = os.path.join(output_so_dir, so_name)\n+                output_so = f\"{input_path[:-4]}.so\"\n                 if not os.path.exists(output_so):\n                     cmd = cpp_compile_command(\n                         input=input_path,\ndiff --git a/torch/_inductor/compile_fx.py b/torch/_inductor/compile_fx.py\nindex 3f50944882e00a..014352ec5f2598 100644\n--- a/torch/_inductor/compile_fx.py\n+++ b/torch/_inductor/compile_fx.py\n@@ -19,7 +19,7 @@\n from torch._dynamo import logging as dynamo_logging, utils as dynamo_utils\n from torch._dynamo.utils import detect_fake_mode\n from torch._functorch.aot_autograd import make_boxed_func\n-from torch._inductor.codecache import CompiledFxGraph\n+from torch._inductor.codecache import code_hash, CompiledFxGraph\n from torch._ops import OpOverload\n from torch._subclasses.fake_tensor import FakeTensor\n from torch.fx.passes.fake_tensor_prop import FakeTensorProp\n@@ -771,6 +771,15 @@ def compile_fx_aot(\n         if config_patches is None\n         else {**config_patches, \"cpp_wrapper\": True}\n     )\n+    if (\n+        \"aot_inductor_output_path\" not in config_patches\n+        and not config.aot_inductor_output_path\n+    ):\n+        config_patches = {\n+            **config_patches,\n+            \"aot_inductor_output_path\": code_hash(model_.code),\n+        }\n+\n     return compile_fx(\n         model_,\n         example_inputs_,\ndiff --git a/torch/_inductor/config.py b/torch/_inductor/config.py\nindex 98a675ebb5d836..91b8801a5323ed 100644\n--- a/torch/_inductor/config.py\n+++ b/torch/_inductor/config.py\n@@ -54,8 +54,11 @@\n # enable reordering pass\n reordering = True\n \n-# inductor engine name\n-dll_name = \"inductor_engine.so\"\n+# AOTInductor output path\n+# If an absolute path is specified, the generated lib files will be stored under the directory;\n+# If a relative path is specified, it will be used as a subdirectory under the default caching path;\n+# If not specified, a temp directory will be created under the default caching path\n+aot_inductor_output_path = \"\"\n \n # enable slow autotuning passes to select algorithms\n max_autotune = os.environ.get(\"TORCHINDUCTOR_MAX_AUTOTUNE\") == \"1\"\n\nFrom 49c310af002d967659d686e19c164a53053295e9 Mon Sep 17 00:00:00 2001\nFrom: Bin Bao <binbao@meta.com>\nDate: Tue, 18 Jul 2023 13:56:36 -0700\nSubject: [PATCH 2/2] Update on \"[inductor] Allow specify a subdir to store .so\n and .cubin files\"\n\nSummary: The subdir is used to store .so and .cubin files generated by AOTInductor. It can either be specified, or created based on hash of the input graph.\n\ncc voznesenskym penguinwu EikanWang jgong5 Guobing-Chen XiaobingSuper zhuhaozhe blzheng Xia-Weiwen wenzhe-nrv jiayisunx peterbell10 ipiszy ngimel yf225 chenyang78 kadeng muchulee8 aakhundov\n\nDifferential Revision: [D47556730](https://our.internmc.facebook.com/intern/diff/D47556730)\n\n[ghstack-poisoned]\n---\n torch/_inductor/codecache.py | 9 +++++----\n 1 file changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex a95680fccfc0b2..9f98b0c2797597 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -287,7 +287,7 @@ def get_lock_dir():\n     return lock_dir\n \n \n-def code_hash(code, extra=\"\"):\n+def code_hash(code, extra: str = \"\"):\n     hashing_str = code\n     if extra != \"\":\n         hashing_str = hashing_str + \"||\" + extra\n@@ -311,7 +311,7 @@ def get_path(basename: str, extension: str, specified_dir: str = \"\"):\n     return basename, subdir, path\n \n \n-def get_hash(content: Union[str, bytes], extra=\"\", hash_type=\"code\"):\n+def get_hash(content: Union[str, bytes], extra: str = \"\", hash_type: str = \"code\"):\n     assert hash_type in [\"code\", \"cubin\"], \"Hash type not supported\"\n     if hash_type == \"code\":\n         return code_hash(content, extra)\n@@ -322,7 +322,7 @@ def get_hash(content: Union[str, bytes], extra=\"\", hash_type=\"code\"):\n def write(\n     content: Union[str, bytes],\n     extension: str,\n-    extra=\"\",\n+    extra: str = \"\",\n     hash_type: str = \"code\",\n     specified_dir: str = \"\",\n ):\n@@ -840,7 +840,8 @@ def compile(cls, graph, source_code, cuda):\n             lock_dir = get_lock_dir()\n             lock = FileLock(os.path.join(lock_dir, key + \".lock\"), timeout=LOCK_TIMEOUT)\n             with lock:\n-                output_so = f\"{input_path[:-4]}.so\"\n+                output_so = os.path.splitext(input_path)[0] + \".so\"\n+\n                 if not os.path.exists(output_so):\n                     cmd = cpp_compile_command(\n                         input=input_path,\n"
  },
  {
    "number": 105463,
    "title": "Upgrade triton pin",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105463\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>",
    "merge_commit_sha": "35bd649a236a3223bb08541a197b386f6bc19d7c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105463",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105463/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105463.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105463.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105463/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105463/comments",
    "labels": [
      "ciflow/inductor",
      "topic: not user facing",
      "ciflow/trunk"
    ],
    "_event_time": "2023-07-18T14:31:27.045768Z",
    "state": "open",
    "patch": "From d49114dd8d6cd46be6de915c6fb53c7e6cf4f517 Mon Sep 17 00:00:00 2001\nFrom: \"Edward Z. Yang\" <ezyang@meta.com>\nDate: Tue, 18 Jul 2023 10:31:19 -0400\nSubject: [PATCH] Upgrade triton pin\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\n[ghstack-poisoned]\n---\n .ci/docker/ci_commit_pins/triton.txt | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/.ci/docker/ci_commit_pins/triton.txt b/.ci/docker/ci_commit_pins/triton.txt\nindex 2283b4142e0484..5fc6f2dc3d6d4d 100644\n--- a/.ci/docker/ci_commit_pins/triton.txt\n+++ b/.ci/docker/ci_commit_pins/triton.txt\n@@ -1 +1 @@\n-3c400e78185b4a35129d1a8857e554f48370f2c9\n+9e3e10c5edb4a062cf547ae73e6ebfb19aad7bdf\n"
  },
  {
    "number": 105461,
    "title": "Add peterbell10 to core reviewers",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105461\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>",
    "merge_commit_sha": "bb3899cfddc6b366e23c5ef5f432e4afa005df93",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105461",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105461/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105461.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105461.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105461/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105461/comments",
    "labels": [
      "Merged",
      "topic: not user facing",
      "ciflow/trunk"
    ],
    "_event_time": "2023-07-18T13:31:32.045762Z",
    "state": "closed",
    "patch": "From 8d22289e405cfd17a8e826c60b6ba8101f1066bb Mon Sep 17 00:00:00 2001\nFrom: \"Edward Z. Yang\" <ezyang@meta.com>\nDate: Tue, 18 Jul 2023 09:31:23 -0400\nSubject: [PATCH] Add peterbell10 to core reviewers\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\n[ghstack-poisoned]\n---\n .github/merge_rules.yaml | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/.github/merge_rules.yaml b/.github/merge_rules.yaml\nindex 8309d45fda44a6..ed24d909d80676 100644\n--- a/.github/merge_rules.yaml\n+++ b/.github/merge_rules.yaml\n@@ -415,6 +415,7 @@\n   - lezcano\n   - Skylion007\n   - ngimel\n+  - peterbell10\n   mandatory_checks_name:\n   - EasyCLA\n   - Lint\n"
  },
  {
    "number": 105459,
    "title": "Adding documentation an diagram on code base",
    "body": "Fixes #104962\r\n",
    "merge_commit_sha": "d48d4fc75849e528523beda2ae99de55da4a8925",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105459",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105459/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105459.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105459.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105459/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105459/comments",
    "labels": [
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T12:07:01.973291Z",
    "state": "open",
    "patch": "From af9e03da77c3b20751c662aebbbb458b3a19b950 Mon Sep 17 00:00:00 2001\nFrom: David Radley <david_radley@uk.ibm.com>\nDate: Tue, 18 Jul 2023 13:06:29 +0100\nSubject: [PATCH] Adding documentation an diagram on code base\n\nSigned-off-by: David Radley <david_radley@uk.ibm.com>\n---\n CONTRIBUTING.md                   |  5 +++++\n aten/src/README.md                | 37 +++++++++++++++++++++++++++++--\n simplified-codebase-structure.svg |  0\n 3 files changed, 40 insertions(+), 2 deletions(-)\n create mode 100644 simplified-codebase-structure.svg\n\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex 80311a17c99ffa..73fdc6e30587d1 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -28,6 +28,7 @@ aspects of contributing to PyTorch.\n     - [Tips](#tips)\n     - [Building C++ Documentation](#building-c-documentation)\n   - [Previewing changes locally](#previewing-changes-locally)\n+  - [Adding diagrams](#adding-diagrams)\n   - [Previewing documentation on PRs](#previewing-documentation-on-prs)\n   - [Adding documentation tests](#adding-documentation-tests)\n - [Profiling with `py-spy`](#profiling-with-py-spy)\n@@ -186,6 +187,10 @@ into the repo directory.\n \n ## Codebase structure\n \n+A diagram of the important parts of the codebase\n+![code base structure](simplified-codebase-structure.svg)\n+\n+\n * [c10](c10) - Core library files that work everywhere, both server\n   and mobile. We are slowly moving pieces from [ATen/core](aten/src/ATen/core)\n   here. This library is intended only to contain essential functionality,\ndiff --git a/aten/src/README.md b/aten/src/README.md\nindex 3127ed5c8c3993..6b97aec20b8d3c 100644\n--- a/aten/src/README.md\n+++ b/aten/src/README.md\n@@ -1,6 +1,8 @@\n-This directory contains the low-level tensor libraries for PyTorch,\n-as well as the new ATen C++ bindings.\n+# ATen (A tensor library)\n \n+This directory contains the low-level tensor libraries for PyTorch, as well as the new ATen C++ bindings.\n+\n+## Low-level tensor libraries for PyTorch\n The low-level libraries trace their lineage from the original Torch.  There are\n multiple variants of the library, summarized here:\n \n@@ -12,6 +14,37 @@ multiple variants of the library, summarized here:\n \n (You'll also see these abbreviations show up in symbol names.)\n \n+## New ATen C++ bindings.\n+\n+The new ATen C++ bindings definitions are in the native folder.\n+\n+### Native folder\n+\n+#### The native functions yaml file\n+The Native folder contains [native-functions.yaml](native-functions.yaml). This file contains definitions \n+of the PyTorch operations, for example:\n+\n+`func: hardsigmoid(Tensor self) -> Tensor\n+  structured_delegate: hardsigmoid.out\n+  device_check: NoCheck   # TensorIterator\n+  python_module: nn\n+  dispatch:\n+  QuantizedCPU: hardsigmoid_quantized_cpu`\n+\n+It defines operations and how they are dispatched, and other related information required to run the operations for each kernel.\n+The frontend operations (what the PyTorch developer writes) are written without regard for which device operations will be run on. \n+The yaml file is the way to map operations so they can be run on particualr kernels; it does this by declaring\n+how each operation will be implemented; including which devices an operation can run on and how it will be dispatched.\n+The yaml file is used as input to generate code by the [setup.py](../../setup.py). The code is generated in the ATen\n+folder in the build folder.\n+\n+### Kernel implmentation specific folders\n+There are folders per kernel implmentation; cpu, cuda, cudnn, metal, mps, mkl. These have kernel specific code that does not need to be generated.\n+\n+### Core\n+This folder contains non kernel specific information; the intention is to move the contents of this folder to\n+the [c10/core](../../c10/core), which does not contain kernel implementation information.\n+\n ## Reference counting\n \n PyTorch employs reference counting in order to permit tensors to provide\ndiff --git a/simplified-codebase-structure.svg b/simplified-codebase-structure.svg\nnew file mode 100644\nindex 00000000000000..e69de29bb2d1d6\n"
  },
  {
    "number": 105456,
    "title": "[Quant][Inductor] Enable quantization conv_binary(add/add_relu) pattern fusion inside inductor",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #105456\r\n* #105455\r\n* #104590\r\n* #104588\r\n* #104581\r\n* #104580\r\n* #104573\r\n* #105109\r\n* #105039\r\n\r\n**Summary**\r\nEnable the `dequant-conv2d-binary_postop(add)-unary_postop(relu)-quant` pattern fusion and lowering inside inductor.\r\n\r\n**Test Plan**\r\n```\r\nclear && python -u -m pytest -s -v test_mkldnn_pattern_matcher.py -k test_qconv2d_binary\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "7c860347c5305fd03011db49357786eb7738df9c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105456",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105456/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105456.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105456.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105456/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105456/comments",
    "labels": [
      "release notes: AO frontend",
      "open source",
      "ciflow/inductor",
      "ciflow/trunk",
      "release notes: quantization",
      "module: inductor"
    ],
    "_event_time": "2023-07-18T09:46:07.711919Z",
    "state": "open",
    "patch": "From e67e6520fc6bb5718e40e84291d1600541bb6518 Mon Sep 17 00:00:00 2001\nFrom: leslie-fang-intel <leslie.fang@intel.com>\nDate: Tue, 18 Jul 2023 17:49:27 +0800\nSubject: [PATCH] [Quant][Inductor] Enable quantization\n conv_binary(add/add_relu) pattern fusion inside inductor\n\n[ghstack-poisoned]\n---\n test/inductor/test_mkldnn_pattern_matcher.py |  75 ++++++++-\n torch/_inductor/fx_passes/quantization.py    | 131 +++++++++++++++\n torch/_inductor/graph.py                     |   1 +\n torch/_inductor/ir.py                        | 162 +++++++++++++++++++\n torch/_inductor/lowering.py                  |  54 +++++++\n torch/ao/quantization/pt2e/graph_utils.py    |   2 +-\n 6 files changed, 419 insertions(+), 6 deletions(-)\n\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex aa43beef1d7faa..1a604e5dab287d 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -61,6 +61,12 @@\n     lambda x, y: x.sub_(y): (1, 2, True),  # call_method\n }\n \n+quantization_binary_list = [\n+    lambda x, y: torch.add(x, y),\n+    lambda x, y: x.add(y),\n+    lambda x, y: x.add_(y),\n+]\n+\n \n @config.patch({\"freezing\": True})\n class TestPatternMatcherBase(TestCase):\n@@ -364,6 +370,64 @@ def forward(self, x, y):\n                     mod, (v, other), match_count, match_nodes, rtol=1e-2, atol=1e-2\n                 )\n \n+    @skipIfNoDynamoSupport\n+    @skipIfNoONEDNN\n+    def test_qconv2d_binary(self):\n+        class M(torch.nn.Module):\n+            def __init__(\n+                self,\n+                binary_fn,\n+                has_relu,\n+                **kwargs,\n+            ):\n+                super().__init__()\n+                self.conv1 = torch.nn.Conv2d(3, 6, kernel_size=3, stride=1)\n+                self.conv2 = torch.nn.Conv2d(3, 6, kernel_size=3, stride=1)\n+                self.binary_fn = binary_fn\n+                self.has_relu = has_relu\n+                self.relu = torch.nn.ReLU()\n+\n+            def forward(self, x):\n+                x1 = self.conv1(x)\n+                x2 = self.conv2(x)\n+                if self.has_relu:\n+                    return self.relu(self.binary_fn(x1, x2))\n+                else:\n+                    return self.binary_fn(x1, x2)\n+\n+        options = itertools.product(\n+            quantization_binary_list,\n+            [True, False],  # has_relu\n+        )\n+\n+        for binary_fn, has_relu in options:\n+            has_relu = False\n+            mod = M(binary_fn, has_relu=has_relu).eval()\n+            v = torch.randn((1, 3, 8, 8), dtype=torch.float32, requires_grad=False).add(\n+                1\n+            )\n+            # Totally 9 pattern_matcher_count, 39 pattern_matcher_nodes + 1 optional(unary post op)\n+            # 1. Pair of to_int8 and to_fp32 at conv input * 2, extra input of add * 1, and graph output * 1\n+            #    matched in pointless_convert pass at\n+            #    torch/_inductor/fx_passes/joint_graph.py: [convert_element_type, convert_element_type_1]\n+            # 2. Dequant pattern matcher for dequant promotion * 1\n+            #    [convert_element_type_3, sub_1, mul_3]\n+            # 3. Dequant-conv pattern matched in quantization weight prepack * 2\n+            #    [convert_element_type_1, sub, mul_1, dequantize_per_channel, convolution]\n+            # 4. Quantization fusion in post-grad fusion pass * 1\n+            #    [qconv2d_pointwise_default, div_1, round_2, add_1, clamp_min_1, clamp_max_1, convert_element_type_2]\n+            # 5. Qconv2d_add * 1\n+            #    [qconv2d_pointwise_default_1, convert_element_type_5, sub_2, mul_5, add_3, optional(relu),\n+            #     mul_6, round_4, add_4, clamp_min_3, clamp_max_3, convert_element_type_6]\n+            self._test_common(\n+                mod,\n+                (v,),\n+                9,\n+                40 if has_relu else 39,\n+                check_quantization=True,\n+            )\n+            return\n+\n     @skipIfNoDynamoSupport\n     @skipIfNoONEDNN\n     def test_qconv2d_unary(self):\n@@ -442,9 +506,7 @@ def forward(self, x):\n \n         mod = M().eval()\n         v = torch.randn((1, 3, 8, 8), dtype=torch.float32, requires_grad=False).add(1)\n-        # For now, we have annotated conv_add in x86InductorQuantizer. But we didn't implement the lowering.\n-        # TODO <leslie>: Modify the pattern matcher count after we implement the qconv2d_add lowering.\n-        # Totally 10 pattern_matcher_count, 40 pattern_matcher_nodes\n+        # Totally 11 pattern_matcher_count, 51 pattern_matcher_nodes\n         # 1. Pair of to_int8 and to_fp32 at conv input * 2, extra input of add * 1, and graph output * 1\n         #    matched in pointless_convert pass at\n         #    torch/_inductor/fx_passes/joint_graph.py: [convert_element_type, convert_element_type_1]\n@@ -454,11 +516,14 @@ def forward(self, x):\n         #    [convert_element_type_1, sub, mul_1, dequantize_per_channel, convolution]\n         # 4. Quantization fusion in post-grad fusion pass * 2\n         #    [qconv2d_pointwise_default, div_1, round_2, add_1, clamp_min_1, clamp_max_1, convert_element_type_2]\n+        # 5. Qconv2d_add * 1\n+        #    [qconv2d_pointwise_default_1, convert_element_type_5, sub_2, mul_5, add_3, mul_6, round_4, add_4,\n+        #     clamp_min_3, clamp_max_3, convert_element_type_6]\n         self._test_common(\n             mod,\n             (v,),\n-            10,\n-            40,\n+            11,\n+            51,\n             check_quantization=True,\n         )\n \ndiff --git a/torch/_inductor/fx_passes/quantization.py b/torch/_inductor/fx_passes/quantization.py\nindex 0cebd6b31a1194..a4cd36a9967d3e 100644\n--- a/torch/_inductor/fx_passes/quantization.py\n+++ b/torch/_inductor/fx_passes/quantization.py\n@@ -69,6 +69,28 @@\n     Arg(),  # algorithm\n )\n \n+dequantize_accum_pattern = CallFunction(\n+    aten.mul.Tensor,\n+    CallFunction(\n+        aten.sub.Tensor,\n+        CallFunction(\n+            prims.convert_element_type.default,\n+            KeywordArg(\"accum\"),\n+            KeywordArg(\"accum_dq_dtype\"),\n+        ),\n+        KeywordArg(\"accum_zp\"),\n+    ),\n+    KeywordArg(\"accum_scale\"),\n+)\n+\n+\n+def generate_pattern_with_binary(binary_post_op, computation_call, extra_input_pattern):\n+    return CallFunction(\n+        binary_post_op,\n+        computation_call,\n+        extra_input_pattern,\n+    )\n+\n \n def generate_pattern_with_unary(computation_call, unary_post_op):\n     if unary_post_op is not None:\n@@ -179,6 +201,67 @@ def qconv(match: Match, *args, **kwargs):\n     return qconv\n \n \n+def _register_quantized_conv_binary_lowering(\n+    pattern,\n+    pass_number,\n+    computation_op,\n+    fp32_output,\n+    binary_unary_attr,\n+):\n+    @register_lowering_pattern(pattern, pass_number=pass_number)\n+    def qconv_binary(match: Match, *args, **kwargs):\n+        x, x_scale, x_zp = kwargs[\"x\"], kwargs[\"x_scale\"], kwargs[\"x_zp\"]\n+        accum, accum_scale, accum_zp = (\n+            kwargs[\"accum\"],\n+            kwargs[\"accum_scale\"],\n+            kwargs[\"accum_zp\"],\n+        )\n+        packed_weight, w_scale, w_zp = (\n+            kwargs[\"packed_weight\"],\n+            kwargs[\"w_scale\"],\n+            kwargs[\"w_zp\"],\n+        )\n+        b, stride, padding, dilation, groups = (\n+            kwargs[\"b\"],\n+            kwargs[\"stride\"],\n+            kwargs[\"padding\"],\n+            kwargs[\"dilation\"],\n+            kwargs[\"groups\"],\n+        )\n+        o_inv_scale, o_zero_point = (\n+            kwargs[\"o_inv_scale\"],\n+            kwargs[\"o_zp\"],\n+        )\n+\n+        computation_args = (\n+            x,\n+            x_scale,\n+            x_zp,\n+            accum,\n+            accum_scale,\n+            accum_zp,\n+            packed_weight,\n+            w_scale,\n+            w_zp,\n+            b,\n+            stride,\n+            padding,\n+            dilation,\n+            groups,\n+            o_inv_scale,\n+            o_zero_point,\n+            fp32_output,\n+            binary_unary_attr.binary_op_name,\n+            binary_unary_attr.alpha,\n+            binary_unary_attr.unary_op_name,\n+            binary_unary_attr.scalars_attr,\n+            binary_unary_attr.algorithm_attr,\n+        )\n+        return L[computation_op](*computation_args)\n+\n+    return qconv_binary\n+\n+\n def _register_quantization_unary_fusion():\n     class UnaryAttr:\n         def __init__(self, op_name: str, scalars_attr=None, algorithm_attr=None):\n@@ -208,8 +291,56 @@ def __init__(self, op_name: str, scalars_attr=None, algorithm_attr=None):\n         )\n \n \n+def _register_quantization_binary_fusion():\n+    class BinaryUnaryAttr:\n+        def __init__(\n+            self,\n+            binary_op_name: str,\n+            alpha=None,\n+            unary_op_name: str = \"none\",\n+            scalars_attr=None,\n+            algorithm_attr=None,\n+        ):\n+            self.binary_op_name = binary_op_name\n+            self.alpha = alpha if scalars_attr else 1.0\n+            self.unary_op_name = unary_op_name\n+            self.scalars_attr = scalars_attr if scalars_attr else []\n+            self.algorithm_attr = algorithm_attr if algorithm_attr else \"\"\n+\n+    binary_replace_patterns = {\n+        BinaryUnaryAttr(\"add\", 1.0, \"none\", [], \"\"): generate_pattern_with_output_quant(\n+            generate_pattern_with_binary(\n+                aten.add.Tensor,\n+                dequantize_qconv_pt2e_pattern,\n+                dequantize_accum_pattern,\n+            )\n+        ),\n+        BinaryUnaryAttr(\"add\", 1.0, \"relu\", [], \"\"): generate_pattern_with_output_quant(\n+            generate_pattern_with_unary(\n+                generate_pattern_with_binary(\n+                    aten.add.Tensor,\n+                    dequantize_qconv_pt2e_pattern,\n+                    dequantize_accum_pattern,\n+                ),\n+                aten.relu.default,\n+            )\n+        ),\n+    }\n+\n+    for binary_unary_attr, patterns in binary_replace_patterns.items():\n+        # Register qconv2d_binary_unary pattern for ExternKernel Lowering\n+        _register_quantized_conv_binary_lowering(\n+            patterns,\n+            0 if binary_unary_attr.unary_op_name != \"none\" else 1,  # pass_number\n+            torch.ops.onednn.qconv2d_pointwise.binary,  # computation_op\n+            False,  # fp32_output\n+            binary_unary_attr,  # binary_unary_attr\n+        )\n+\n+\n def _register_quantization_lowerings():\n     _register_quantization_unary_fusion()\n+    _register_quantization_binary_fusion()\n \n \n def _is_valid_dequant_promotion_pattern(match):\ndiff --git a/torch/_inductor/graph.py b/torch/_inductor/graph.py\nindex 5593f0e057d19e..ae9b191e838c7b 100644\n--- a/torch/_inductor/graph.py\n+++ b/torch/_inductor/graph.py\n@@ -737,6 +737,7 @@ def run_node(self, n: torch.fx.Node):\n                                 torch.ops.mkldnn._linear_pointwise.default,\n                                 torch.ops.mkldnn._linear_pointwise.binary,\n                                 torch.ops.onednn.qconv2d_pointwise.default,\n+                                torch.ops.onednn.qconv2d_pointwise.binary,\n                             ]\n                             if torch._C.has_mkl:\n                                 need_fixed_layout += [torch.ops.mkl._mkl_linear.default]\ndiff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py\nindex b50253ee410b4d..d0252002b6b090 100644\n--- a/torch/_inductor/ir.py\n+++ b/torch/_inductor/ir.py\n@@ -4270,6 +4270,168 @@ def create(\n         )\n \n \n+class QConvPointWiseBinaryPT2E(ExternKernelAlloc):\n+    def __init__(\n+        self,\n+        layout,\n+        inputs,\n+        constant_args=(),\n+    ):\n+        \"\"\"\n+        Needs input/weight/output qparams\n+        if bias is not None\n+            - inputs = [x, w, b, accum, w_scale, w_zp]\n+            - const_args = [stride, padding, dilation, groups, x_scale, x_zp, accum_scale, accum_zp, o_inv_scale, o_zp,\n+            fp32_output, binary_attr, aplha, unary_attr, unary_scalars, unary_algorithm]\n+        else\n+            - inputs = [x, w, accum, w_scale, w_zp]\n+            - const_args = const_args is: [bias, stride, padding, dilation, groups, x_scale, x_zp, accum_scale,\n+            accum_zp, o_inv_scale, o_zp, fp32_output, binary_attr, aplha, unary_attr, unary_scalars, unary_algorithm]\n+        \"\"\"\n+        self.has_bias = len(inputs) == 6\n+        super().__init__(layout, inputs, constant_args)\n+\n+    def codegen(self, wrapper):\n+        # Parser the inputs and constant\n+        args = [x.codegen_reference() for x in self.inputs]\n+        const_args = []\n+        const_args.extend(self.codegen_const_args())\n+\n+        x = args[0]\n+        packed_weight = args[1]\n+        bias = args[2] if self.has_bias else const_args[0]\n+        accum, w_scale, w_zp = args[-3], args[-2], args[-1]\n+        (\n+            stride,\n+            padding,\n+            dilation,\n+            groups,\n+            x_scale,\n+            x_zp,\n+            accum_scale,\n+            accum_zp,\n+            o_inv_scale,\n+            o_zp,\n+            fp32_output,\n+            binary_attr,\n+            alpha,\n+            unary_attr,\n+            unary_scalars,\n+            unary_algorithm,\n+        ) = const_args[-16:]\n+        self.kernel = \"torch.ops.onednn.qconv2d_pointwise.binary\"\n+        conv_args = (\n+            \"{}\".format(x)\n+            + \", {}\".format(x_scale)\n+            + \", {}\".format(x_zp)\n+            + \", {}\".format(accum)\n+            + \", {}\".format(accum_scale)\n+            + \", {}\".format(accum_zp)\n+            + \", {}\".format(packed_weight)\n+            + \", {}\".format(w_scale)\n+            + \", {}\".format(w_zp)\n+            + \", {}\".format(bias)\n+            + \", {}\".format(stride)\n+            + \", {}\".format(padding)\n+            + \", {}\".format(dilation)\n+            + \", {}\".format(groups)\n+            + \", {}\".format(o_inv_scale)\n+            + \", {}\".format(o_zp)\n+            + \", {}\".format(fp32_output)\n+            + \", {}\".format(binary_attr)\n+            + \", {}\".format(alpha)\n+            + \", {}\".format(unary_attr)\n+            + \", {}\".format(unary_scalars)\n+            + \", {}\".format(unary_algorithm)\n+        )\n+        wrapper.writeline(f\"{self.get_name()} = {self.kernel}({conv_args})\")\n+        if isinstance(self.layout, Layout):\n+            self.codegen_size_asserts(wrapper)\n+\n+    @classmethod\n+    def create(\n+        cls,\n+        x: \"TensorBox\",\n+        x_scale,\n+        x_zp,\n+        accum: \"TensorBox\",\n+        accum_scale,\n+        accum_zp,\n+        weight: \"TensorBox\",  # packed_weight\n+        w_scale,\n+        w_zp,\n+        bias: \"TensorBox\",\n+        stride_: List[int],\n+        padding_: List[int],\n+        dilation_: List[int],\n+        groups: int,\n+        o_inv_scale: \"TensorBox\",\n+        output_zero_point: \"TensorBox\",\n+        fp32_output,\n+        binary_attr,\n+        alpha,\n+        unary_attr,\n+        unary_scalars,\n+        unary_algorithm,\n+    ):\n+        transposed = False\n+        output_padding = None\n+        (\n+            inputs,\n+            constant_args,\n+            kernel_layout,\n+            req_stride_order,\n+        ) = _prepare_convolution_fusion_create(\n+            cls,\n+            x,\n+            weight,\n+            bias,\n+            padding_,\n+            stride_,\n+            dilation_,\n+            groups,\n+            transposed,\n+            output_padding,\n+        )\n+\n+        accum = cls.require_stride_order(accum, req_stride_order)\n+        inputs.append(accum)\n+\n+        # swap padding and stride to align with functional conv arg order\n+        if bias is None:\n+            constant_args[1], constant_args[2] = constant_args[2], constant_args[1]\n+        else:\n+            constant_args[0], constant_args[1] = constant_args[1], constant_args[0]\n+\n+        w_scale.realize()\n+        w_zp.realize()\n+        inputs = inputs + [w_scale, w_zp]\n+        constant_args = constant_args + [\n+            x_scale,\n+            x_zp,\n+            accum_scale,\n+            accum_zp,\n+            o_inv_scale,\n+            output_zero_point,\n+            fp32_output,\n+            binary_attr,\n+            alpha,\n+            unary_attr,\n+            unary_scalars,\n+            unary_algorithm,\n+        ]\n+        if fp32_output:\n+            # in _prepare_convolution_fusion_create, we use x.dtype (uint8) to create kernel_layout\n+            # if we set fp32_output, the output buf should be dtype float32 instead of uint8.\n+            kernel_layout.dtype = torch.float32\n+\n+        return QConvPointWiseBinaryPT2E(\n+            layout=kernel_layout,\n+            inputs=inputs,\n+            constant_args=constant_args,\n+        )\n+\n+\n @dataclasses.dataclass\n class MutableBox(IRNode):\n     \"\"\"\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 51c8c583f286b5..5bc33080eb0b4b 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -1302,6 +1302,60 @@ def qconvolution_unary(\n                 )\n             )\n \n+        @register_lowering(\n+            torch.ops.onednn.qconv2d_pointwise.binary, type_promotion_kind=None\n+        )\n+        def qconvolution_binary(\n+            x: TensorBox,\n+            x_scale,\n+            x_zp,\n+            accum: TensorBox,\n+            accum_scale,\n+            accum_zp,\n+            packed_weight: TensorBox,\n+            w_scale: TensorBox,\n+            w_zp: TensorBox,\n+            bias: TensorBox,\n+            stride,\n+            padding,\n+            dilation,\n+            groups,\n+            o_inv_scale,\n+            o_zero_point,\n+            fp32_output,\n+            binary_attr,\n+            alpha,\n+            unary_attr,\n+            unary_scalars,\n+            unary_algorithmm,\n+        ):\n+            return TensorBox.create(\n+                ir.QConvPointWiseBinaryPT2E.create(\n+                    x,\n+                    x_scale,\n+                    x_zp,\n+                    accum,\n+                    accum_scale,\n+                    accum_zp,\n+                    packed_weight,\n+                    w_scale,\n+                    w_zp,\n+                    bias,\n+                    stride,\n+                    padding,\n+                    dilation,\n+                    groups,\n+                    o_inv_scale,\n+                    o_zero_point,\n+                    fp32_output,\n+                    binary_attr,\n+                    alpha,\n+                    unary_attr,\n+                    unary_scalars,\n+                    unary_algorithmm,\n+                )\n+            )\n+\n         if torch._C.has_mkl:\n             cpu_needs_realized_inputs.append(torch.ops.mkl._mkl_linear)\n \ndiff --git a/torch/ao/quantization/pt2e/graph_utils.py b/torch/ao/quantization/pt2e/graph_utils.py\nindex 0776e6db56e19e..f30620b9c13fa0 100644\n--- a/torch/ao/quantization/pt2e/graph_utils.py\n+++ b/torch/ao/quantization/pt2e/graph_utils.py\n@@ -22,7 +22,7 @@\n     {torch.nn.ReLU, torch.nn.functional.relu, torch.nn.functional.relu_},\n     {torch.nn.BatchNorm2d, torch.nn.functional.batch_norm},\n     {torch.nn.Hardtanh, torch.nn.functional.hardtanh, torch.nn.functional.hardtanh_},\n-    {torch.add, operator.add, operator.iadd},\n+    {torch.add, operator.add, operator.iadd, \"add\", \"add_\"},\n ]\n \n \n"
  },
  {
    "number": 105455,
    "title": "[Quant][Inductor] Enable quantization conv_unary(relu) pattern fusion inside inductor",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #105456\r\n* __->__ #105455\r\n* #104590\r\n* #104588\r\n* #104581\r\n* #104580\r\n* #104573\r\n* #105109\r\n* #105039\r\n\r\n**Summary**\r\nEnable the `dequant-conv2d-unary_postop(relu)-quant` pattern fusion and lowering inside inductor.\r\n\r\n**Test Plan**\r\n```\r\nclear && python -u -m pytest -s -v test_mkldnn_pattern_matcher.py -k test_qconv2d_unary\r\n```\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "79531706ef9989bdd1d52ab376b1cc0908293601",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105455",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105455/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105455.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105455.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105455/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105455/comments",
    "labels": [
      "open source",
      "ciflow/inductor",
      "module: inductor",
      "ciflow/trunk"
    ],
    "_event_time": "2023-07-18T09:45:59.180875Z",
    "state": "open",
    "patch": "From ce2ce8293586b9f6860afb6dfb49c16c1c90abe4 Mon Sep 17 00:00:00 2001\nFrom: leslie-fang-intel <leslie.fang@intel.com>\nDate: Tue, 18 Jul 2023 17:49:19 +0800\nSubject: [PATCH] [Quant][Inductor] Enable quantization conv_unary(relu)\n pattern fusion inside inductor\n\n[ghstack-poisoned]\n---\n test/inductor/test_mkldnn_pattern_matcher.py | 40 +++++++++++++----\n torch/_inductor/fx_passes/quantization.py    | 46 +++++++++++++++-----\n 2 files changed, 66 insertions(+), 20 deletions(-)\n\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex 8f0c3e49d9d026..aa43beef1d7faa 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -45,6 +45,11 @@\n     torch.nn.Tanh,\n ]\n \n+quantization_unary_list = {\n+    None: 0,\n+    torch.nn.ReLU(): 1,\n+}\n+\n # The dict value is (match_count, match_nodes, inplace)\n binary_list = {\n     lambda x, y: torch.add(x, y): (1, 2, False),  # call_function\n@@ -365,23 +370,40 @@ def test_qconv2d_unary(self):\n         class M(torch.nn.Module):\n             def __init__(\n                 self,\n-                auto_insert_channel_last_node=False,\n+                unary_fn,\n+                **kwargs,\n             ):\n                 super().__init__()\n-                if auto_insert_channel_last_node:\n+                if (\n+                    \"auto_insert_channel_last_node\" in kwargs\n+                    and kwargs[\"auto_insert_channel_last_node\"]\n+                ):\n                     self.conv = torch.nn.Conv2d(3, 128, kernel_size=3, stride=1)\n                 else:\n                     self.conv = torch.nn.Conv2d(3, 6, kernel_size=3, stride=1)\n+                self.unary_fn = unary_fn\n \n             def forward(self, x):\n-                return self.conv(x)\n+                x = self.conv(x)\n+                return self.unary_fn(x) if self.unary_fn else x\n+\n+        options = itertools.product(\n+            quantization_unary_list.keys(),\n+            [True, False],  # auto_insert_channel_last_node\n+        )\n \n-        for auto_insert_channel_last_node in [True, False]:\n-            mod = M(auto_insert_channel_last_node).eval()\n+        for unary_fn, auto_insert_channel_last_node in options:\n+            if auto_insert_channel_last_node and unary_fn is not None:\n+                # Skip trivial test combinations to reduce test time.\n+                continue\n+            mod = M(\n+                unary_fn, auto_insert_channel_last_node=auto_insert_channel_last_node\n+            ).eval()\n             v = torch.randn((1, 3, 8, 8), dtype=torch.float32, requires_grad=False).add(\n                 1\n             )\n-            # Totally 4 pattern_matcher_count, 16 or 17 pattern_matcher_nodes\n+            # Totally pattern_matcher_count 4,\n+            # pattern_matcher_nodes 16 + 1 for optional(clone) + 1 for optional(unary_post_op)\n             # 1. pair of to_int8 and to_fp32 at conv input matched in pointless_convert pass\n             #    at torch/_inductor/fx_passes/joint_graph.py: [convert_element_type, convert_element_type_1]\n             # 2. dequant-conv pattern matched in quantization weight prepack\n@@ -390,12 +412,14 @@ def forward(self, x):\n             # 3. pair of to_int8 and to_fp32 at conv output matched in pointless_convert pass\n             #    at torch/_inductor/fx_passes/joint_graph.py: [convert_element_type_2, convert_element_type_3]\n             # 4. Quantization fusion in post-grad fusion pass\n-            #    [qconv2d_pointwise_default, div_1, round_2, add_1, clamp_min_1, clamp_max_1, convert_element_type_2]\n+            #    [qconv2d_pointwise_default, optional(unary_post_op), div_1, round_2, add_1,\n+            #     clamp_min_1, clamp_max_1, convert_element_type_2]\n             self._test_common(\n                 mod,\n                 (v,),\n                 4,\n-                17 if auto_insert_channel_last_node else 16,\n+                (17 if auto_insert_channel_last_node else 16)\n+                + quantization_unary_list[unary_fn],\n                 check_quantization=True,\n             )\n \ndiff --git a/torch/_inductor/fx_passes/quantization.py b/torch/_inductor/fx_passes/quantization.py\nindex 221ce0749202c8..0cebd6b31a1194 100644\n--- a/torch/_inductor/fx_passes/quantization.py\n+++ b/torch/_inductor/fx_passes/quantization.py\n@@ -70,6 +70,15 @@\n )\n \n \n+def generate_pattern_with_unary(computation_call, unary_post_op):\n+    if unary_post_op is not None:\n+        return CallFunction(\n+            unary_post_op,\n+            computation_call,\n+        )\n+    return computation_call\n+\n+\n def generate_pattern_with_output_quant(computation_call):\n     \"\"\"\n     quantize output:\n@@ -170,24 +179,37 @@ def qconv(match: Match, *args, **kwargs):\n     return qconv\n \n \n-def _register_quantization_lowerings():\n+def _register_quantization_unary_fusion():\n     class UnaryAttr:\n         def __init__(self, op_name: str, scalars_attr=None, algorithm_attr=None):\n             self.op_name = op_name\n             self.scalars_attr = scalars_attr if scalars_attr else []\n             self.algorithm_attr = algorithm_attr if algorithm_attr else \"\"\n \n-    # Register dq-conv2d-q pattern for ExternKernel Lowering\n-    quantize_conv_output_pattern_pt2e = generate_pattern_with_output_quant(\n-        dequantize_qconv_pt2e_pattern\n-    )\n-    _register_quantized_conv_lowering(\n-        quantize_conv_output_pattern_pt2e,\n-        2,  # pass_number\n-        torch.ops.onednn.qconv2d_pointwise,  # computation_op\n-        False,  # fp32_output\n-        UnaryAttr(\"none\", [], \"\"),  # unary_attr\n-    )\n+    unary_replace_patterns = {\n+        UnaryAttr(\"none\", [], \"\"): generate_pattern_with_output_quant(\n+            dequantize_qconv_pt2e_pattern\n+        ),\n+        UnaryAttr(\"relu\", [], \"\"): generate_pattern_with_output_quant(\n+            generate_pattern_with_unary(\n+                dequantize_qconv_pt2e_pattern, aten.relu.default\n+            )\n+        ),\n+    }\n+\n+    for unary_attr, patterns in unary_replace_patterns.items():\n+        # Register qconv2d pattern for ExternKernel Lowering\n+        _register_quantized_conv_lowering(\n+            patterns,\n+            1 if unary_attr.op_name != \"none\" else 2,  # pass_number\n+            torch.ops.onednn.qconv2d_pointwise,  # computation_op\n+            False,  # fp32_output\n+            unary_attr,  # unary_attr\n+        )\n+\n+\n+def _register_quantization_lowerings():\n+    _register_quantization_unary_fusion()\n \n \n def _is_valid_dequant_promotion_pattern(match):\n"
  },
  {
    "number": 105453,
    "title": "[torch_np] update test to use ones_like instead of empty_like",
    "body": "This test fails locally (probably because deterministic mode is not on by default). \r\n\r\nWe replace the use of `empty_like` to `ones_like` as this test doesn't need `empty_like`.\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "bd0f35415796199d6b3d635ff434732b5df9abcc",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105453",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105453/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105453.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105453.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105453/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105453/comments",
    "labels": [
      "Merged",
      "open source",
      "module: dynamo",
      "ciflow/trunk",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T08:51:46.292918Z",
    "state": "closed",
    "patch": "From 93f1cbd38cb98cb3e8bcf5a61fbab662d28a7fcf Mon Sep 17 00:00:00 2001\nFrom: kshitij12345 <kshitijkalambarkar@gmail.com>\nDate: Tue, 18 Jul 2023 08:46:49 +0000\nSubject: [PATCH] [torch_np] update test to use ones_like instead of empty_like\n\n---\n test/dynamo/test_functions.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/test/dynamo/test_functions.py b/test/dynamo/test_functions.py\nindex b8e9729c303ec1..e51f0c9038bbd3 100644\n--- a/test/dynamo/test_functions.py\n+++ b/test/dynamo/test_functions.py\n@@ -1130,7 +1130,7 @@ def test_ndarray_builtin_functions(x):\n     def test_numpy_dtype_argument_to_function(x):\n         import numpy as np\n \n-        return np.empty_like(x, dtype=np.float64)\n+        return np.ones_like(x, dtype=np.float64)\n \n \n def global_func_with_default_tensor_args(\n"
  },
  {
    "number": 105451,
    "title": "[VMAP] Add linspace and logspace batch rules",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105451\n* #104889\n\n",
    "merge_commit_sha": "f537f779bdbd2da634ecd4c0d971509c5db2688d",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105451",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105451/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105451.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105451.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105451/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105451/comments",
    "labels": [
      "open source"
    ],
    "_event_time": "2023-07-18T08:25:45.415999Z",
    "state": "open",
    "patch": "From c9a19aeef438f00d998d0cb9dc493e00c3e3708b Mon Sep 17 00:00:00 2001\nFrom: \"Li-Huai (Allan) Lin\" <qqaatw@gmail.com>\nDate: Tue, 18 Jul 2023 16:25:37 +0800\nSubject: [PATCH] [VMAP] Add linspace and logspace batch rules\n\n[ghstack-poisoned]\n---\n aten/src/ATen/functorch/BatchRulesFactory.cpp | 58 +++++++++++++++++++\n aten/src/ATen/native/native_functions.yaml    |  4 +-\n test/functorch/test_vmap.py                   |  2 -\n test/test_decomp.py                           | 10 ++--\n 4 files changed, 65 insertions(+), 9 deletions(-)\n\ndiff --git a/aten/src/ATen/functorch/BatchRulesFactory.cpp b/aten/src/ATen/functorch/BatchRulesFactory.cpp\nindex bde6842342b2db..c5159691f14e5b 100644\n--- a/aten/src/ATen/functorch/BatchRulesFactory.cpp\n+++ b/aten/src/ATen/functorch/BatchRulesFactory.cpp\n@@ -103,6 +103,62 @@ static std::tuple<Tensor,optional<int64_t>> _new_zeros_with_same_feature_meta_ba\n   return std::make_tuple(result, 0);\n }\n \n+static std::tuple<Tensor,optional<int64_t>> linspace_logspace_batch_rule_helper(\n+    const at::Tensor& start, optional<int64_t> start_bdim,\n+    const at::Tensor& end, optional<int64_t> end_bdim,\n+    int64_t steps,\n+    c10::optional<double> base,\n+    c10::optional<at::ScalarType> dtype,\n+    c10::optional<at::Layout> layout,\n+    c10::optional<at::Device> device,\n+    c10::optional<bool> pin_memory)\n+{\n+  auto batch_size = get_bdim_size2(start, start_bdim, end, end_bdim);\n+  auto start_ = ensure_has_bdim(start, start_bdim.has_value(), batch_size);\n+  auto end_ = ensure_has_bdim(end, end_bdim.has_value(), batch_size);\n+  start_ = moveBatchDimToFront(start_, start_bdim);\n+  end_ = moveBatchDimToFront(end_, end_bdim);\n+\n+  auto tensor_options = at::TensorOptions().dtype(dtype).layout(layout).device(device).pinned_memory(pin_memory);\n+\n+  Tensor result;\n+  if (steps == 0){\n+    result = at::full({batch_size, 0}, 0, tensor_options);\n+  } else if (steps == 1){\n+    result = at::empty({start_.size(0), 1}, tensor_options).copy_(start_.unsqueeze_(1));\n+  } else {\n+    result = (start_ + at::arange(0, steps, tensor_options).unsqueeze_(1) * (end_ - start_) / (steps - 1)).transpose(0, 1);\n+  }\n+\n+  if (base){\n+    result = at::pow(*base, result);\n+  }\n+  return std::make_tuple(result, 0);\n+}\n+\n+static std::tuple<Tensor,optional<int64_t>> linspace_batch_rule(\n+    const at::Tensor& start, optional<int64_t> start_bdim,\n+    const at::Tensor& end, optional<int64_t> end_bdim,\n+    int64_t steps,\n+    c10::optional<at::ScalarType> dtype,\n+    c10::optional<at::Layout> layout,\n+    c10::optional<at::Device> device,\n+    c10::optional<bool> pin_memory){\n+  return linspace_logspace_batch_rule_helper(start, start_bdim, end, end_bdim, steps, c10::nullopt, dtype, layout, device, pin_memory);\n+}\n+\n+static std::tuple<Tensor,optional<int64_t>> logspace_batch_rule(\n+    const at::Tensor& start, optional<int64_t> start_bdim,\n+    const at::Tensor& end, optional<int64_t> end_bdim,\n+    int64_t steps,\n+    double base,\n+    c10::optional<at::ScalarType> dtype,\n+    c10::optional<at::Layout> layout,\n+    c10::optional<at::Device> device,\n+    c10::optional<bool> pin_memory){\n+  return linspace_logspace_batch_rule_helper(start, start_bdim, end, end_bdim, steps, c10::make_optional(base), dtype, layout, device, pin_memory);\n+}\n+\n static bool _has_same_storage_numel_batch_rule(const Tensor& a, const Tensor& b) {\n   return true;\n }\n@@ -119,6 +175,8 @@ TORCH_LIBRARY_IMPL(aten, FuncTorchBatched, m) {\n   VMAP_SUPPORT(new_zeros, NEW_BLAH_BATCH_RULE_SYMINT(ATEN_FN(new_zeros)));\n   VMAP_SUPPORT(new_ones, NEW_BLAH_BATCH_RULE_SYMINT(ATEN_FN(new_ones)));\n   VMAP_SUPPORT(new_full, NEW_BLAH_BATCH_RULE_SYMINT(ATEN_FN(new_full)));\n+  VMAP_SUPPORT2(linspace, Tensor, linspace_batch_rule);\n+  VMAP_SUPPORT2(logspace, Tensor, logspace_batch_rule);\n   VMAP_SUPPORT(_new_zeros_with_same_feature_meta, _new_zeros_with_same_feature_meta_batch_rule);\n   // Not sure how to add the ones with irregular args to the mix cleanly (i.e. randint takes an extra int parameter)\n }\ndiff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml\nindex 701f6a97cb04ac..def2bfb279b4ca 100644\n--- a/aten/src/ATen/native/native_functions.yaml\n+++ b/aten/src/ATen/native/native_functions.yaml\n@@ -3263,7 +3263,7 @@\n - func: ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)\n   tags: pointwise\n \n-- func: linspace.Scalar(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n+- func: linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n   dispatch:\n     CompositeExplicitAutograd: linspace\n \n@@ -3272,7 +3272,7 @@\n   dispatch:\n     CompositeExplicitAutograd: linspace\n \n-- func: linspace.Scalar_out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)\n+- func: linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)\n   dispatch:\n     CPU, Meta: linspace_out\n     CUDA: linspace_cuda_out\ndiff --git a/test/functorch/test_vmap.py b/test/functorch/test_vmap.py\nindex bd9aaf105421ed..8a504d4dca5c2a 100644\n--- a/test/functorch/test_vmap.py\n+++ b/test/functorch/test_vmap.py\n@@ -3736,8 +3736,6 @@ def test_vmap_exhaustive(self, device, dtype, op):\n         xfail('as_strided_scatter', ''),\n         xfail('equal', ''),\n         xfail('linalg.lu', ''),\n-        xfail('linspace', ''),\n-        xfail('logspace', ''),\n         skip('linalg.ldl_solve', ''),\n         skip('_softmax_backward_data'),\n         # UBSAN: runtime error: shift exponent -1 is negative\ndiff --git a/test/test_decomp.py b/test/test_decomp.py\nindex 2dc0f807c056f5..21530532d42859 100644\n--- a/test/test_decomp.py\n+++ b/test/test_decomp.py\n@@ -219,11 +219,11 @@ def op_assert_equal(test_case, op, test_dtype, orig, decomp, args, kwargs):\n         # there's an off-by-one error. See\n         # https://github.com/pytorch/pytorch/issues/81996\n         # https://github.com/pytorch/pytorch/issues/82230\n-        (torch.int8, torch.ops.aten.linspace.Scalar) : (0, 1),\n-        (torch.uint8, torch.ops.aten.linspace.Scalar) : (0, 1),\n-        (torch.int16, torch.ops.aten.linspace.Scalar) : (0, 1),\n-        (torch.int32, torch.ops.aten.linspace.Scalar) : (0, 1),\n-        (torch.int64, torch.ops.aten.linspace.Scalar) : (0, 1),\n+        (torch.int8, torch.ops.aten.linspace.default) : (0, 1),\n+        (torch.uint8, torch.ops.aten.linspace.default) : (0, 1),\n+        (torch.int16, torch.ops.aten.linspace.default) : (0, 1),\n+        (torch.int32, torch.ops.aten.linspace.default) : (0, 1),\n+        (torch.int64, torch.ops.aten.linspace.default) : (0, 1),\n         (torch.int8, torch.ops.aten.linspace.Tensor) : (0, 1),\n         (torch.uint8, torch.ops.aten.linspace.Tensor) : (0, 1),\n         (torch.int16, torch.ops.aten.linspace.Tensor) : (0, 1),\n"
  },
  {
    "number": 105450,
    "title": "Xu compiler recommender",
    "body": "For discuss and review.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "71da28ed2c9c70546f8949700a7474a312a8217f",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105450",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105450/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105450.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105450.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105450/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105450/comments",
    "labels": [
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T08:08:46.930399Z",
    "state": "closed",
    "patch": "From 8172ffd7475009a19c30f6d972999687fc6d545b Mon Sep 17 00:00:00 2001\nFrom: \"Han, Xu\" <xu.han@intel.com>\nDate: Wed, 19 Jul 2023 09:56:18 +0000\nSubject: [PATCH 1/2] unify code for get cpp compiler\n\n---\n torch/utils/cpp_extension.py | 26 +++++++++++---------------\n 1 file changed, 11 insertions(+), 15 deletions(-)\n\ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..eaf1766a8b5130 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -43,6 +43,13 @@\n MINIMUM_GCC_VERSION = (5, 0, 0)\n MINIMUM_MSVC_VERSION = (19, 0, 24215)\n \n+def get_cpp_compiler():\n+    if IS_WINDOWS:\n+        compiler = os.environ.get('CXX', 'cl')\n+    else:\n+        compiler = os.environ.get('CXX', 'c++')\n+    return compiler\n+\n VersionRange = Tuple[Tuple[int, ...], Tuple[int, ...]]\n VersionMap = Dict[str, VersionRange]\n # The following values were taken from the following GitHub gist that\n@@ -883,10 +890,8 @@ def _check_abi(self) -> Tuple[str, TorchVersion]:\n         # On some platforms, like Windows, compiler_cxx is not available.\n         if hasattr(self.compiler, 'compiler_cxx'):\n             compiler = self.compiler.compiler_cxx[0]\n-        elif IS_WINDOWS:\n-            compiler = os.environ.get('CXX', 'cl')\n         else:\n-            compiler = os.environ.get('CXX', 'c++')\n+            compiler = get_cpp_compiler()\n         _, version = get_compiler_abi_compatibility_and_version(compiler)\n         # Warn user if VC env is activated but `DISTUILS_USE_SDK` is not set.\n         if IS_WINDOWS and 'VSCMD_ARG_TGT_ARCH' in os.environ and 'DISTUTILS_USE_SDK' not in os.environ:\n@@ -1562,10 +1567,7 @@ def _write_ninja_file_and_compile_objects(\n         verbose: bool,\n         with_cuda: Optional[bool]) -> None:\n     verify_ninja_availability()\n-    if IS_WINDOWS:\n-        compiler = os.environ.get('CXX', 'cl')\n-    else:\n-        compiler = os.environ.get('CXX', 'c++')\n+    compiler = get_cpp_compiler()\n     get_compiler_abi_compatibility_and_version(compiler)\n     if with_cuda is None:\n         with_cuda = any(map(_is_cuda_file, sources))\n@@ -1606,10 +1608,7 @@ def _write_ninja_file_and_build_library(\n         with_cuda: Optional[bool],\n         is_standalone: bool = False) -> None:\n     verify_ninja_availability()\n-    if IS_WINDOWS:\n-        compiler = os.environ.get('CXX', 'cl')\n-    else:\n-        compiler = os.environ.get('CXX', 'c++')\n+    compiler = get_cpp_compiler()\n     get_compiler_abi_compatibility_and_version(compiler)\n     if with_cuda is None:\n         with_cuda = any(map(_is_cuda_file, sources))\n@@ -2126,10 +2125,7 @@ def sanitize_flags(flags):\n     assert len(sources) == len(objects)\n     assert len(sources) > 0\n \n-    if IS_WINDOWS:\n-        compiler = os.environ.get('CXX', 'cl')\n-    else:\n-        compiler = os.environ.get('CXX', 'c++')\n+    compiler = get_cpp_compiler()\n \n     # Version 1.3 is required for the `deps` directive.\n     config = ['ninja_required_version = 1.3']\n\nFrom c235f5fb4d01f8de7853dee1ff3fb95d8a9c775e Mon Sep 17 00:00:00 2001\nFrom: \"Han, Xu\" <xu.han@intel.com>\nDate: Wed, 19 Jul 2023 15:12:18 +0000\nSubject: [PATCH 2/2] recommand clang++ in Linux when cpp_wrapper build.\n\n---\n torch/_inductor/codecache.py | 15 +++++++++++++++\n 1 file changed, 15 insertions(+)\n\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex d890586e79d87b..848ed62cd53bb0 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -35,6 +35,10 @@\n from torch._inductor.utils import developer_warning\n from torch.hub import _Faketqdm, tqdm\n \n+from torch.utils.cpp_extension import get_cpp_compiler\n+import platform\n+import os.path\n+\n _HERE = os.path.abspath(__file__)\n _TORCH_PATH = os.path.dirname(os.path.dirname(_HERE))\n \n@@ -73,6 +77,16 @@ def _compile_end():\n \n log = logging.getLogger(__name__)\n \n+IS_LINUX = platform.system() == \"Linux\"\n+\n+def cpp_compiler_check():\n+    compiler = get_cpp_compiler()\n+    \n+    if IS_LINUX is True:\n+        if compiler.startswith('clang') is False:\n+            print(\"warning: recommend to use clang++, and then get better compile performance.\")\n+    \n+    return\n \n @functools.lru_cache(None)\n def cache_dir():\n@@ -999,6 +1013,7 @@ def load(cls, source_code, func_name, key, cuda):\n                     extra_ldflags = f\"{_shared} {_lpaths} {_libs} -ffast-math\"\n                     extra_include_paths = f\"{_ipaths}\"\n \n+                    cpp_compiler_check()\n                     mod = torch.utils.cpp_extension.load_inline(\n                         name=name,\n                         build_directory=cpp_wrapper_dir,\n"
  },
  {
    "number": 105449,
    "title": "inductor: fix bug in nn.Linear when in_feature size is zero",
    "body": "Fix #104937 \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #105449\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "6636268c20fd8f80ec26a742d84d339f4a3da497",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105449",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105449/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105449.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105449.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105449/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105449/comments",
    "labels": [
      "open source",
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T07:27:18.926659Z",
    "state": "open",
    "patch": "From 5fa99c90ae0a27b079dee23b280eeb8ef16849e4 Mon Sep 17 00:00:00 2001\nFrom: blzheng <beilei.zheng@intel.com>\nDate: Tue, 18 Jul 2023 00:27:05 -0700\nSubject: [PATCH] inductor: fix bug in nn.Linear when in_feature size is zero\n\n[ghstack-poisoned]\n---\n test/inductor/test_cuda_repro.py    | 8 ++++++++\n torch/_inductor/fx_passes/pad_mm.py | 5 ++++-\n 2 files changed, 12 insertions(+), 1 deletion(-)\n\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..ab534bf74ce7c5 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -928,6 +928,14 @@ def fn(arg7_1, add_1, permute_2, select_scatter, slice_8):\n         ref = compiled(list(args))\n         assert same(ref, correct)\n \n+    def test_issue104937(self):\n+        m = nn.Linear(in_features=0, out_features=0, bias=True).to(\"cuda\")\n+        x = torch.rand(1, 1, 0, device=\"cuda\")\n+        expect = m(x)\n+        opt_fn = torch.compile(m)\n+        actual = opt_fn(x)\n+        self.assertEqual(expect, actual)\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\ndiff --git a/torch/_inductor/fx_passes/pad_mm.py b/torch/_inductor/fx_passes/pad_mm.py\nindex 9ad33a7c60cff6..f2c27e574cd0b0 100644\n--- a/torch/_inductor/fx_passes/pad_mm.py\n+++ b/torch/_inductor/fx_passes/pad_mm.py\n@@ -164,7 +164,10 @@ def get_flops(dtype):\n def is_mm_compute_bound(M, K, N, dtype):\n     from triton.testing import get_dram_gbps\n \n-    arithmetic_intensity = (M * N * K) / (M * K + N * K + M * N)\n+    denominator = M * K + N * K + M * N\n+    if denominator == 0:\n+        return True\n+    arithmetic_intensity = (M * N * K) / denominator\n \n     # Fails with AMD\n     try:\n"
  },
  {
    "number": 105446,
    "title": "inductor: add support for 0 repeats",
    "body": "Fix #104948\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #105446\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "bec2ec9bb59e8107687e2f13b9ed99d2ca4268eb",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105446",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105446/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105446.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105446.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105446/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105446/comments",
    "labels": [
      "open source",
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T04:18:31.647187Z",
    "state": "open",
    "patch": "From f6a7bb51005184c5d09cf7849cb1930b37accbd2 Mon Sep 17 00:00:00 2001\nFrom: blzheng <beilei.zheng@intel.com>\nDate: Mon, 17 Jul 2023 21:18:19 -0700\nSubject: [PATCH] inductor: add support for 0 repeats\n\n[ghstack-poisoned]\n---\n test/inductor/test_torchinductor.py | 1 +\n torch/_inductor/lowering.py         | 6 +++++-\n 2 files changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py\nindex cf8675d776da42..6a2af635b5299c 100644\n--- a/test/inductor/test_torchinductor.py\n+++ b/test/inductor/test_torchinductor.py\n@@ -2653,6 +2653,7 @@ def fn(x):\n     def test_repeat(self):\n         def fn(x):\n             return (\n+                x.repeat(0, 1, 1, 1),\n                 x.repeat(2, 2, 3, 1),\n                 x.repeat(8, 1, 1, 1),\n                 x.repeat(2, 1, 1, 1, 1, 1),\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 0283ed1c2025ae..b1c63159950858 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -779,11 +779,15 @@ def repeat(x, repeats):\n \n     new_size = list(x.get_size())\n \n+    zero_tensor = False\n     for i in range(len(repeats)):\n-        assert repeats[i] != 0\n+        if repeats[i] == 0:\n+            zero_tensor = True\n         if repeats[i] != 1:\n             new_size[i] = new_size[i] * repeats[i]\n \n+    if zero_tensor:\n+        return empty(new_size, dtype=x.get_dtype(), device=x.get_device())\n     if all((a == 1 or b == 1) for a, b in zip(repeats, old_size)):\n         return expand(x, new_size)\n \n"
  },
  {
    "number": 105443,
    "title": "fix torchtun script for custom device",
    "body": "Fixes #ISSUE_NUMBER\r\nas the title,add torchrun support for custom device",
    "merge_commit_sha": "84e3ef7f8f885d42b6ef940a10878276f01ad1b7",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105443",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105443/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105443.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105443.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105443/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105443/comments",
    "labels": [
      "open source"
    ],
    "_event_time": "2023-07-18T03:10:12.279122Z",
    "state": "open",
    "patch": "From 50d1fae13036575328a15d3d130c3010d0246732 Mon Sep 17 00:00:00 2001\nFrom: shibo19 <18207133434@163.com>\nDate: Tue, 18 Jul 2023 10:58:01 +0800\nSubject: [PATCH] fix torchtun script for custom device\n\n---\n torch/distributed/run.py | 11 ++++++++++-\n 1 file changed, 10 insertions(+), 1 deletion(-)\n\ndiff --git a/torch/distributed/run.py b/torch/distributed/run.py\nindex 4e3cdbdab82137..ebccba448b40b0 100644\n--- a/torch/distributed/run.py\n+++ b/torch/distributed/run.py\n@@ -384,7 +384,7 @@ def main():\n from torch.distributed.elastic.utils import macros\n from torch.distributed.elastic.utils.logging import get_logger\n from torch.distributed.launcher.api import LaunchConfig, elastic_launch\n-\n+from torch.utils.backend_registration import _get_custom_mod_func\n \n log = get_logger(__name__)\n \n@@ -639,10 +639,19 @@ def determine_local_world_size(nproc_per_node: str):\n                 raise ValueError(\"Cuda is not available.\") from e\n             device_type = \"gpu\"\n             num_proc = torch.cuda.device_count()\n+        elif nproc_per_node == torch._C._get_privateuse1_backend_name():\n+            if not _get_custom_mod_func(\"is_available\")():\n+                raise ValueError(f\"{nproc_per_node} is not available.\") from e\n+            device_type = nproc_per_node\n+            num_proc = _get_custom_mod_func(\"device_count\")()\n         elif nproc_per_node == \"auto\":\n             if torch.cuda.is_available():\n                 num_proc = torch.cuda.device_count()\n                 device_type = \"gpu\"\n+            elif hasattr(torch, torch._C._get_privateuse1_backend_name()) and \\\n+                    _get_custom_mod_func(\"is_available\")():\n+                num_proc = _get_custom_mod_func(\"device_count\")()\n+                device_type = torch._C._get_privateuse1_backend_name()\n             else:\n                 num_proc = os.cpu_count()\n                 device_type = \"cpu\"\n"
  },
  {
    "number": 105441,
    "title": "Expose Some FunctionsManual Api",
    "body": "Fixes #ISSUE_NUMBER\r\nExporse some function api in FunctionsManual.h for custom devices. This can be used in codegen features",
    "merge_commit_sha": "b6e7c279a871b3f3f80e4981e939524a6d9c3f51",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105441",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105441/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105441.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105441.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105441/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105441/comments",
    "labels": [
      "open source"
    ],
    "_event_time": "2023-07-18T02:54:49.975645Z",
    "state": "open",
    "patch": "From 30ee7dd53ac6152e607ca9072fdeec9aae9dc5da Mon Sep 17 00:00:00 2001\nFrom: wgb <wgb_strive@163.com>\nDate: Tue, 18 Jul 2023 11:34:24 +0800\nSubject: [PATCH] Expose FunctionsManual Api\n\n---\n torch/csrc/autograd/FunctionsManual.h | 33 +++++++++++++--------------\n 1 file changed, 16 insertions(+), 17 deletions(-)\n\ndiff --git a/torch/csrc/autograd/FunctionsManual.h b/torch/csrc/autograd/FunctionsManual.h\nindex d15b8ec65d4f35..527528555df635 100644\n--- a/torch/csrc/autograd/FunctionsManual.h\n+++ b/torch/csrc/autograd/FunctionsManual.h\n@@ -64,17 +64,13 @@ TORCH_API at::Tensor not_implemented(const char* name, const char* reason = \"\");\n TORCH_API std::vector<Tensor> not_implemented_list(\n     const char* name,\n     const char* reason = \"\");\n-at::Tensor handle_r_to_c(ScalarType self_st, Tensor gradient_result);\n-at::Tensor maybe_multiply(const at::Tensor& t, const at::Scalar& s);\n-int64_t _safe_size(IntArrayRef sizes, IntArrayRef dim);\n-Tensor restore_reduced_dims(\n-    const Tensor& output,\n-    IntArrayRef dims,\n-    bool keepdim);\n-Tensor scale_grad_by_count(\n-    const Tensor& grad,\n-    const Tensor& mask,\n-    IntArrayRef dims);\n+TORCH_API at::Tensor handle_r_to_c(ScalarType self_st, Tensor gradient_result);\n+TORCH_API at::Tensor maybe_multiply(const at::Tensor& t, const at::Scalar& s);\n+TORCH_API int64_t _safe_size(IntArrayRef sizes, IntArrayRef dim);\n+TORCH_API Tensor\n+restore_reduced_dims(const Tensor& output, IntArrayRef dims, bool keepdim);\n+TORCH_API Tensor\n+scale_grad_by_count(const Tensor& grad, const Tensor& mask, IntArrayRef dims);\n at::Tensor norm_backward(\n     const at::Tensor& grad,\n     const at::Tensor& self,\n@@ -183,9 +179,10 @@ at::Tensor nansum_backward(\n     const at::Tensor& self,\n     at::OptionalIntArrayRef dims,\n     bool keepdim);\n-std::vector<int64_t> reverse_list(const at::IntArrayRef list);\n-std::vector<c10::SymInt> reverse_list_symint(const c10::SymIntArrayRef list);\n-at::Tensor reverse_dim(const at::Tensor& t, int64_t dim);\n+TORCH_API std::vector<int64_t> reverse_list(const at::IntArrayRef list);\n+TORCH_API std::vector<c10::SymInt> reverse_list_symint(\n+    const c10::SymIntArrayRef list);\n+TORCH_API at::Tensor reverse_dim(const at::Tensor& t, int64_t dim);\n at::Tensor prod_safe_zeros_backward(\n     const at::Tensor& grad,\n     const at::Tensor& inp,\n@@ -236,12 +233,14 @@ at::Tensor logcumsumexp_jvp(\n     const at::Tensor& self_t,\n     int64_t dim);\n at::Tensor unbind_backward(const variable_list& grads, int64_t dim);\n-at::Tensor unsqueeze_to(const at::Tensor& self, c10::SymIntArrayRef sym_sizes);\n-at::Tensor unsqueeze_to(\n+TORCH_API at::Tensor unsqueeze_to(\n+    const at::Tensor& self,\n+    c10::SymIntArrayRef sym_sizes);\n+TORCH_API at::Tensor unsqueeze_to(\n     const at::Tensor& self,\n     int64_t dim,\n     c10::SymIntArrayRef sym_sizes);\n-at::Tensor unsqueeze_to(\n+TORCH_API at::Tensor unsqueeze_to(\n     const at::Tensor& self,\n     IntArrayRef dim,\n     c10::SymIntArrayRef sym_sizes);\n"
  },
  {
    "number": 105440,
    "title": "inductor: promote half/bfloat16 constant to float for cpu vectorization path",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #105440\r\n\r\nAs scalar path, we should also promote half/bfloat16 constant to float for better accuracy, after this PR, the TIMM ```dm_nfnet``` model amp path can be passed.\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "a8943b611244ee73cde37ba6ff0a89c4b6fe69b9",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105440",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105440/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105440.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105440.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105440/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105440/comments",
    "labels": [
      "module: cpu inductor",
      "release notes: inductor",
      "open source",
      "ciflow/inductor",
      "ciflow/trunk",
      "module: inductor"
    ],
    "_event_time": "2023-07-18T02:11:33.978759Z",
    "state": "open",
    "patch": "From 0605cd5e250ba78d838918478192e4d4365589a0 Mon Sep 17 00:00:00 2001\nFrom: XiaobingSuper <xiaobing.zhang@intel.com>\nDate: Mon, 17 Jul 2023 21:31:36 -0400\nSubject: [PATCH 1/2] inductor: promote half/bfloat16 constant to float for cpu\n vectorization path\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py | 8 ++++++++\n torch/_inductor/codegen/cpp.py  | 4 ++++\n 2 files changed, 12 insertions(+)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 89ea4315b1de6c..483353bfa09558 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -1968,6 +1968,14 @@ def f(a):\n         x = torch.rand(16)\n         self.common(f, (x,))\n \n+    def test_scalar_mul_low_precison(self):\n+        def f(x):\n+            return torch.ops.aten.mul.Tensor(x, 1.7015043497085571)\n+\n+        for dtype in [torch.float16, torch.bfloat16]:\n+            x = torch.randn(4, 5, dtype=dtype)\n+            self.common(f, (x,))\n+\n     def test_to_channels_last_bfloat16(self):\n         def f(a):\n             return a.to(memory_format=torch.channels_last)\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex bb2469ebffd33d..e86e6c4ee2144f 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -2092,6 +2092,10 @@ def store_reduction(name, index, value):\n \n             @staticmethod\n             def constant(val, dtype):\n+                if dtype in (torch.float16, torch.bfloat16):\n+                    # Since load promotes all half-precision inputs to float, constants\n+                    # must be promoted as well\n+                    dtype = torch.float32\n                 with RecordOptimizationContext(__name__) as node_ctx:\n                     opt_ctx: OptimizationContext = node_ctx.get_opt_ctx()\n                     assert opt_ctx\n\nFrom f769ac6dd66de51616d235f77d5ad3e4ce352686 Mon Sep 17 00:00:00 2001\nFrom: XiaobingSuper <xiaobing.zhang@intel.com>\nDate: Mon, 17 Jul 2023 22:07:35 -0400\nSubject: [PATCH 2/2] Update on \"inductor: promote half/bfloat16 constant to\n float for cpu vectorization path\"\n\nAs scalar code, we should also promote half/bfloat16 constant to float for better accuracy, after this PR, the TIMM ```dm_nfnet``` model amp path can be passed.\n\n\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py | 9 +++++----\n torch/_inductor/codegen/cpp.py  | 4 ++--\n 2 files changed, 7 insertions(+), 6 deletions(-)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 483353bfa09558..fe43fbcb614c54 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -1968,13 +1968,14 @@ def f(a):\n         x = torch.rand(16)\n         self.common(f, (x,))\n \n-    def test_scalar_mul_low_precison(self):\n+    def test_scalar_mul_bfloat16(self):\n         def f(x):\n             return torch.ops.aten.mul.Tensor(x, 1.7015043497085571)\n \n-        for dtype in [torch.float16, torch.bfloat16]:\n-            x = torch.randn(4, 5, dtype=dtype)\n-            self.common(f, (x,))\n+        metrics.reset()\n+        x = torch.randn(4, 5, dtype=torch.bfloat16)\n+        self.common(f, (x,))\n+        assert metrics.generated_cpp_vec_kernel_count == 1\n \n     def test_to_channels_last_bfloat16(self):\n         def f(a):\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex e86e6c4ee2144f..55df3276aaa8f2 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -2092,8 +2092,8 @@ def store_reduction(name, index, value):\n \n             @staticmethod\n             def constant(val, dtype):\n-                if dtype in (torch.float16, torch.bfloat16):\n-                    # Since load promotes all half-precision inputs to float, constants\n+                if dtype == torch.bfloat16:\n+                    # Since load promotes all bfloat16-precision inputs to float, constants\n                     # must be promoted as well\n                     dtype = torch.float32\n                 with RecordOptimizationContext(__name__) as node_ctx:\n"
  },
  {
    "number": 105439,
    "title": "[inductor] Mark index_put_ as unsafe for cudagraphs when accumulate=True",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #105439\r\n\r\nindex_put_ fails with cuda graphs when `accumulate=True` - likely for the same reason that it fails with deterministic_algorithms_enabled:\r\nhttps://github.com/pytorch/pytorch/blob/fcb7d4b35821d97142f1c0d8843dae0e98f8e965/aten/src/ATen/native/TensorAdvancedIndexing.cpp#L730\r\n\r\nThis PR causes cuda graphs to be skipped if index_put is encountered with accumulate=True.\r\n\r\nNote - we could just also just always skip index_put, but it's possible that might regress perf if it's used w/ cudagraphs in places where accumulate=False and deterministic_algorithms_enabled=False.\r\n\r\nExample of failure outside of PT2:\r\n\r\n```python\r\nimport torch\r\n\r\ndef fn(x, y, z):\r\n    x = torch.zeros_like(x)\r\n    return x.index_put_([y], z, True)\r\n    # return x + 1\r\n\r\nx = torch.zeros((512, 512), dtype=torch.bool, device='cuda')\r\ny = torch.arange(512, dtype=torch.int64, device='cuda')\r\nz = torch.ones((512, 512), dtype=torch.bool, device='cuda')\r\n\r\ns = torch.cuda.Stream()\r\ns.wait_stream(torch.cuda.current_stream())\r\nwith torch.cuda.stream(s):\r\n    for i in range(3):\r\n        fn(x, y, z)\r\ntorch.cuda.current_stream().wait_stream(s)\r\n\r\ng = torch.cuda.CUDAGraph()\r\nwith torch.cuda.graph(g):\r\n    fn(x, y, z)\r\n```\r\n\r\nfails with\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/users/dberard/scripts/graphed_index_put.py\", line 24, in <module>\r\n    fn(x, y, z)\r\n  File \"/data/users/dberard/scripts/graphed_index_put.py\", line 8, in fn\r\n    return x.index_put_([y], z, True)\r\nRuntimeError: CUDA error: operation not permitted when stream is capturing\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/users/dberard/scripts/graphed_index_put.py\", line 24, in <module>\r\n    fn(x, y, z)\r\n  File \"/data/users/dberard/pytorch/torch/cuda/graphs.py\", line 173, in __exit__\r\n    self.cuda_graph.capture_end()\r\n  File \"/data/users/dberard/pytorch/torch/cuda/graphs.py\", line 79, in capture_end\r\n    super().capture_end()\r\nRuntimeError: CUDA error: operation failed due to a previous error during capture\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov\n\nDifferential Revision: [D47538548](https://our.internmc.facebook.com/intern/diff/D47538548)",
    "merge_commit_sha": "df16a0796b64f48a73af6d84fd2012527b9968a8",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105439",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105439/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105439.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105439.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105439/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105439/comments",
    "labels": [
      "module: inductor",
      "topic: not user facing",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T02:04:56.764678Z",
    "state": "open",
    "patch": "From 9e38d7d43679f4463c58ca78b713e75c65b506e4 Mon Sep 17 00:00:00 2001\nFrom: David Berard <dberard@fb.com>\nDate: Mon, 17 Jul 2023 19:04:49 -0700\nSubject: [PATCH] [inductor] Mark index_put_ as always unsafe for cudagraphs\n\nindex_put_ fails with cuda graphs when `accumulate=True` - likely for the same reason that it fails with deterministic_algorithms_enabled:\nhttps://github.com/pytorch/pytorch/blob/fcb7d4b35821d97142f1c0d8843dae0e98f8e965/aten/src/ATen/native/TensorAdvancedIndexing.cpp#L730\n\nThis PR causes cuda graphs to be skipped if index_put is encountered with accumulate=True.\n\nNote - we could just also just always skip index_put, but it's possible that might regress perf if it's used w/ cudagraphs in places where accumulate=False and deterministic_algorithms_enabled=False.\n\nExample of failure outside of PT2:\n\n```python\nimport torch\n\ndef fn(x, y, z):\n    x = torch.zeros_like(x)\n    return x.index_put_([y], z, True)\n    # return x + 1\n\nx = torch.zeros((512, 512), dtype=torch.bool, device='cuda')\ny = torch.arange(512, dtype=torch.int64, device='cuda')\nz = torch.ones((512, 512), dtype=torch.bool, device='cuda')\n\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        fn(x, y, z)\ntorch.cuda.current_stream().wait_stream(s)\n\ng = torch.cuda.CUDAGraph()\nwith torch.cuda.graph(g):\n    fn(x, y, z)\n```\n\nfails with\n```\n```\n\n[ghstack-poisoned]\n---\n test/inductor/test_cuda_repro.py | 42 ++++++++++++++++++++++++++++++++\n torch/_inductor/utils.py         | 19 ++++++++++++---\n 2 files changed, 57 insertions(+), 4 deletions(-)\n\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..c535699cb05644 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -928,6 +928,48 @@ def fn(arg7_1, add_1, permute_2, select_scatter, slice_8):\n         ref = compiled(list(args))\n         assert same(ref, correct)\n \n+    @skipIfRocm\n+    @config.patch({\"triton.cudagraphs\": True})\n+    def test_index_put_inplace_cudagraph(self):\n+        def fn(x, y, z):\n+            x = torch.zeros_like(x)\n+            return x.index_put_([y], z, True)\n+\n+        x = torch.zeros((512, 512), device=\"cuda\", dtype=torch.bool)\n+        y = torch.zeros((512,), device=\"cuda\", dtype=torch.int64)\n+        z = torch.ones((512, 512), device=\"cuda\", dtype=torch.bool)\n+\n+        opt_fn = torch._dynamo.optimize(\"inductor\")(fn)\n+\n+        ref = fn(x, y, z)\n+\n+        # run it twice to test cuda graph issue\n+        res = opt_fn(x, y, z)\n+        res = opt_fn(x, y, z)\n+\n+        self.assertEqual(ref, res)\n+\n+    @skipIfRocm\n+    @config.patch({\"triton.cudagraphs\": True})\n+    def test_index_put_cudagraph(self):\n+        def fn(x, y, z):\n+            x = torch.zeros_like(x)\n+            return x.index_put([y], z, True)\n+\n+        x = torch.zeros((512, 512), device=\"cuda\", dtype=torch.bool)\n+        y = torch.zeros((512,), device=\"cuda\", dtype=torch.int64)\n+        z = torch.ones((512, 512), device=\"cuda\", dtype=torch.bool)\n+\n+        opt_fn = torch._dynamo.optimize(\"inductor\")(fn)\n+\n+        ref = fn(x, y, z)\n+\n+        # run it twice to test cuda graph issue\n+        res = opt_fn(x, y, z)\n+        res = opt_fn(x, y, z)\n+\n+        self.assertEqual(ref, res)\n+\n \n if __name__ == \"__main__\":\n     from torch._dynamo.test_case import run_tests\ndiff --git a/torch/_inductor/utils.py b/torch/_inductor/utils.py\nindex 538d0a2040fb93..bdfddb9227f746 100644\n--- a/torch/_inductor/utils.py\n+++ b/torch/_inductor/utils.py\n@@ -430,12 +430,16 @@ def has_incompatible_cudagraph_ops(gm):\n         \"run_with_rng_state\",\n         \"run_and_save_rng_state\",\n     }\n+\n+    index_put_ops = {\n+        \"aten._unsafe_index_put.default\",\n+        \"aten.index_put_.default\",\n+        \"aten.index_put.default\",\n+    }\n     if torch.are_deterministic_algorithms_enabled():\n         forbidden_set.update(\n             {\n-                \"aten._unsafe_index_put.default\",\n-                \"aten.index_put.default\",\n-                \"aten.index_put_.default\",\n+                *index_put_ops,\n                 \"aten.scatter.src\",\n                 \"aten.scatter.reduce\",\n                 \"aten.scatter.value_reduce\",\n@@ -446,8 +450,15 @@ def has_incompatible_cudagraph_ops(gm):\n                 \"aten.scatter_reduce.two_out\",\n             }\n         )\n+\n+    def is_bad_index_put(node):\n+        # index_put(..., accumulate=True) fails with CUDA graphs\n+        if str(node.target) not in index_put_ops:\n+            return False\n+        return len(node.args) >= 4 and node.args[3] is True\n+\n     for node in gm.graph.nodes:\n-        if str(node.target) in forbidden_set:\n+        if str(node.target) in forbidden_set or is_bad_index_put(node):\n             return True\n     return False\n \n"
  },
  {
    "number": 105438,
    "title": "Enable more e2e foreach optimizer compilation tests",
    "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @anijain2305",
    "merge_commit_sha": "3eddbbcc7a28bf52347c92b5d8cff34d730e042c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105438",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105438/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105438.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105438.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105438/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105438/comments",
    "labels": [
      "module: dynamo",
      "ciflow/inductor",
      "ciflow/trunk",
      "ciflow/inductor-perf-compare",
      "module: inductor",
      "release notes: dynamo"
    ],
    "_event_time": "2023-07-18T01:46:40.976265Z",
    "state": "open",
    "patch": "From 6ccf50ff7b22455f700c9b37fa9e4ff58e0c5be6 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Fri, 14 Jul 2023 19:14:11 -0700\nSubject: [PATCH 1/7] Enable all foreach optims\n\n---\n torch/_dynamo/eval_frame.py | 37 -------------------------------------\n 1 file changed, 37 deletions(-)\n\ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex 0e2c4b80649554..720a090cfef094 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -651,43 +651,6 @@ def patch():\n         ]\n \n         # Note: this excludes the optimizers that are unsupported in excluded_opts below\n-        from ..optim import (\n-            adadelta,\n-            adagrad,\n-            adamax,\n-            adamw,\n-            asgd,\n-            nadam,\n-            rmsprop,\n-            rprop,\n-            sgd,\n-        )\n-\n-        for opt_mod in (\n-            adadelta,\n-            adagrad,\n-            adamax,\n-            adamw,\n-            asgd,\n-            nadam,\n-            rmsprop,\n-            rprop,\n-            sgd,\n-        ):\n-            opt_name = opt_mod.__name__.split(\".\")[-1]\n-            multi_tensor_fn_name = f\"_multi_tensor_{opt_name}\"\n-            fused_fn_name = f\"_fused_{opt_name}\"\n-            if hasattr(opt_mod, multi_tensor_fn_name):\n-                setattr(\n-                    opt_mod,\n-                    multi_tensor_fn_name,\n-                    disable(getattr(opt_mod, multi_tensor_fn_name)),\n-                )\n-\n-            if hasattr(opt_mod, fused_fn_name):\n-                setattr(\n-                    opt_mod, fused_fn_name, disable(getattr(opt_mod, fused_fn_name))\n-                )\n \n         # Note: we don't support sparsity, data-dependent control, or tracing through backwards\n         excluded_opts = {torch.optim.SparseAdam, torch.optim.RAdam, torch.optim.LBFGS}\n\nFrom 510445d63087fa111d4e63983c1dd9413b40d4d2 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Fri, 14 Jul 2023 19:14:29 -0700\nSubject: [PATCH 2/7] Add recompile tests and accurate kernel counts for\n compiled optims\n\n---\n test/inductor/test_compiled_optimizers.py | 117 ++++++++++++++++++----\n 1 file changed, 95 insertions(+), 22 deletions(-)\n\ndiff --git a/test/inductor/test_compiled_optimizers.py b/test/inductor/test_compiled_optimizers.py\nindex f9bc5d681c6d3b..604d2236859c30 100644\n--- a/test/inductor/test_compiled_optimizers.py\n+++ b/test/inductor/test_compiled_optimizers.py\n@@ -27,9 +27,35 @@\n     raise\n \n \n-def make_test(optim_cls, closure=None, **kwargs):\n+def compile_opt(opt_compiled, closure=None):\n+    # run the patcher so that step has the expected structure\n+    torch._dynamo.eval_frame.TorchPatcher.patch()\n+\n+    # unwrap step to avoid a deliberate graph break due to\n+    # a limitation of functionalization/no_grad detection\n+    # see the [Note on graph break] in optimizer.py\n+    # This ignores the outer _use_grad_if_differentiable wrapper\n+    # and instead manually disables grad before calling step, which is fine\n+    # for now as dynamo does not support differentiable optimizers anyway\n+    step_fn = opt_compiled.step.__wrapped__\n+    if closure is not None:\n+\n+        def fn():\n+            step_fn(opt_compiled, closure)\n+\n+    else:\n+\n+        def fn():\n+            step_fn(opt_compiled)\n+\n+    return torch.compile(fn, backend=\"inductor\", fullgraph=True)\n+\n+\n+def make_test(optim_cls, closure=None, kernel_count=2, **kwargs):\n     @requires_cuda()\n     def test_fn(self):\n+        torch._dynamo.reset()\n+        torch._inductor.metrics.reset()\n         input = torch.ones([10, 10], device=\"cuda:0\")\n         model_eager = torch.nn.Sequential(\n             *[torch.nn.Linear(10, 10, device=\"cuda:0\") for _ in range(2)]\n@@ -42,37 +68,58 @@ def test_fn(self):\n \n         opt_eager = optim_cls(model_eager.parameters(), **kwargs)\n         opt_compiled = optim_cls(model_compiled.parameters(), **kwargs)\n-        # run the patcher so that step has the expected structure\n-        torch._dynamo.eval_frame.TorchPatcher.patch()\n-\n-        # unwrap step to avoid a deliberate graph break due to\n-        # a limitation of functionalization/no_grad detection\n-        # see the [Note on graph break] in optimizer.py\n-        # This ignores the outer _use_grad_if_differentiable wrapper\n-        # and instead manually disables grad before calling step, which is fine\n-        # for now as dynamo does not support differentiable optimizers anyway\n-        step_fn = opt_compiled.step.__wrapped__\n-        if closure is not None:\n-\n-            def fn():\n-                step_fn(opt_compiled, closure)\n-\n-        else:\n-\n-            def fn():\n-                step_fn(opt_compiled)\n+        compiled_step = compile_opt(opt_compiled, closure=closure)\n \n         with torch.set_grad_enabled(False):\n-            torch.compile(fn, backend=\"inductor\", fullgraph=True)()\n+            compiled_step()\n             opt_eager.step()\n \n         self.assertEqual(\n             list(model_eager.parameters()), list(model_compiled.parameters())\n         )\n+\n+        if self.check_kernel_count:\n+            # currently, we compile the step and the rest of the computation\n+            # separately because the step is a single element tensor\n+            # hence, the usual kernel count is 2\n+            self.assertEqual(\n+                torch._inductor.metrics.generated_kernel_count, kernel_count\n+            )\n+\n+    return test_fn\n+\n+\n+def make_recompile_test(optim_cls, closure=None, kernel_count=2, **kwargs):\n+    @requires_cuda()\n+    def test_fn(self):\n+        torch._dynamo.reset()\n+        torch._inductor.metrics.reset()\n+        input = torch.ones([10, 10], device=\"cuda:0\")\n+        model = torch.nn.Sequential(\n+            *[torch.nn.Linear(10, 10, device=\"cuda:0\") for _ in range(2)]\n+        )\n+        model(input).sum().backward()\n+\n+        opt_compiled = optim_cls(model.parameters(), **kwargs)\n+        compiled_step = compile_opt(opt_compiled)\n+\n+        # check no recompile here\n+        with torch.set_grad_enabled(False):\n+            compiled_step()\n+            compiled_step()\n+\n+            # perturb state to force recompile\n+            opt_compiled.state.clear()\n+            compiled_step()\n+\n         if self.check_kernel_count:\n             # currently, we compile the step and the rest of the computation\n             # separately because the step is a single element tensor\n-            self.assertEqual(torch._inductor.metrics.generated_kernel_count, 2)\n+            # hence, the usual kernel count is 2\n+            # multiply by 2 to account for the recompile\n+            self.assertEqual(\n+                torch._inductor.metrics.generated_kernel_count, 2 * kernel_count\n+            )\n \n     return test_fn\n \n@@ -92,6 +139,32 @@ def tearDown(self):\n \n     test_adam = make_test(torch.optim.Adam, lr=0.01)\n     test_adam_weight_decay = make_test(torch.optim.Adam, lr=0.01, weight_decay=0.01)\n+    test_adamw = make_test(torch.optim.AdamW, lr=0.01)\n+    test_adamax = make_test(torch.optim.Adamax, lr=0.01)\n+    test_nadam = make_test(torch.optim.NAdam, lr=0.01)\n+    test_rprop = make_test(torch.optim.Rprop, kernel_count=6, lr=0.01)\n+    test_rmsprop = make_test(torch.optim.RMSprop, kernel_count=1, lr=0.01)\n+    test_adadelta = make_test(torch.optim.Adadelta, kernel_count=5, lr=0.01)\n+    test_adagrad = make_test(torch.optim.Adagrad, kernel_count=5, lr=0.01)\n+    test_sgd = make_test(torch.optim.SGD, kernel_count=1, lr=0.01)\n+\n+    test_adam_recompile = make_recompile_test(torch.optim.Adam, lr=0.01)\n+    test_adamw_recompile = make_recompile_test(torch.optim.AdamW, lr=0.01)\n+    test_adamax_recompile = make_recompile_test(torch.optim.Adamax, lr=0.01)\n+    test_nadam_recompile = make_recompile_test(torch.optim.NAdam, lr=0.01)\n+    test_rprop_recompile = make_recompile_test(\n+        torch.optim.Rprop, kernel_count=6, lr=0.01\n+    )\n+    test_rmsprop_recompile = make_recompile_test(\n+        torch.optim.RMSprop, kernel_count=1, lr=0.01\n+    )\n+    test_adadelta_recompile = make_recompile_test(\n+        torch.optim.Adadelta, kernel_count=5, lr=0.01\n+    )\n+    test_adagrad_recompile = make_recompile_test(\n+        torch.optim.Adagrad, kernel_count=5, lr=0.01\n+    )\n+    test_sgd_recompile = make_recompile_test(torch.optim.SGD, kernel_count=1, lr=0.01)\n \n \n if __name__ == \"__main__\":\n\nFrom e7aee09cc6f198fc3a5928dd5a5537021fd33d79 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Fri, 14 Jul 2023 20:50:03 -0700\nSubject: [PATCH 3/7] Fix duplicate guards\n\n---\n test/inductor/test_compiled_optimizers.py |  5 +++++\n torch/_dynamo/variables/optimizer.py      | 10 +++-------\n 2 files changed, 8 insertions(+), 7 deletions(-)\n\ndiff --git a/test/inductor/test_compiled_optimizers.py b/test/inductor/test_compiled_optimizers.py\nindex 604d2236859c30..6d68092ea1c979 100644\n--- a/test/inductor/test_compiled_optimizers.py\n+++ b/test/inductor/test_compiled_optimizers.py\n@@ -92,6 +92,8 @@ def test_fn(self):\n def make_recompile_test(optim_cls, closure=None, kernel_count=2, **kwargs):\n     @requires_cuda()\n     def test_fn(self):\n+        import os\n+\n         torch._dynamo.reset()\n         torch._inductor.metrics.reset()\n         input = torch.ones([10, 10], device=\"cuda:0\")\n@@ -100,12 +102,15 @@ def test_fn(self):\n         )\n         model(input).sum().backward()\n \n+        os.environ[\"TORCHDYNAMO_REPORT_GUARD_FAILURES\"] = \"1\"\n         opt_compiled = optim_cls(model.parameters(), **kwargs)\n         compiled_step = compile_opt(opt_compiled)\n \n         # check no recompile here\n         with torch.set_grad_enabled(False):\n             compiled_step()\n+\n+            torch._logging.set_logs(recompiles=True)\n             compiled_step()\n \n             # perturb state to force recompile\ndiff --git a/torch/_dynamo/variables/optimizer.py b/torch/_dynamo/variables/optimizer.py\nindex 5bfb2cab46bad3..97bfa612fb7c0e 100644\n--- a/torch/_dynamo/variables/optimizer.py\n+++ b/torch/_dynamo/variables/optimizer.py\n@@ -45,6 +45,7 @@ def call_method(\n             try:\n                 py_args, py_kwargs = self.get_python_args(*args, **kwargs)\n                 self.value._init_group(*py_args, **py_kwargs)\n+                self.map_grads_to_sources()\n                 self.install_guards(tx)\n                 self.update_list_args(tx, args, kwargs, py_args, py_kwargs)\n                 return ConstantVariable(None)\n@@ -110,12 +111,8 @@ def install_guards(self, tx):\n             )\n             guards.add(p_state_source.make_guard(GuardBuilder.DICT_KEYS))\n             for k, v in value.items():\n-                if isinstance(v, torch.Tensor):\n-                    guards.add(\n-                        GetItemSource(p_state_source, k).make_guard(\n-                            GuardBuilder.TENSOR_MATCH\n-                        )\n-                    )\n+                if isinstance(v, torch.Tensor) and v not in self.grad_to_source:\n+                    continue  # we process tensors guards used in step when we call it\n                 elif v is None or isinstance(v, (bool, int, float, str)):\n                     guards.add(\n                         GetItemSource(p_state_source, k).make_guard(\n@@ -148,7 +145,6 @@ def wrap_tensor(self, tx, tensor_value):\n \n     def update_list_args(self, tx, args, kwargs, py_args, py_kwargs):\n         \"\"\"Update the args and kwargs to the traced optimizer call\"\"\"\n-        self.map_grads_to_sources()\n         for arg, py_arg in zip(args, py_args):\n             if isinstance(arg, ListVariable) and all(\n                 isinstance(t, torch.Tensor) for t in py_arg\n\nFrom 72d854a8c29fa1f18d4e29f80c1eb56e4f876757 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Mon, 17 Jul 2023 18:39:03 -0700\nSubject: [PATCH 4/7] Refactor optimizer guard installation and handling to not\n have guard duplicates\n\n---\n test/inductor/test_compiled_optimizers.py | 17 +++++--\n torch/_dynamo/variables/optimizer.py      | 62 ++++++++++++++---------\n 2 files changed, 50 insertions(+), 29 deletions(-)\n\ndiff --git a/test/inductor/test_compiled_optimizers.py b/test/inductor/test_compiled_optimizers.py\nindex 6d68092ea1c979..1800393d7521e0 100644\n--- a/test/inductor/test_compiled_optimizers.py\n+++ b/test/inductor/test_compiled_optimizers.py\n@@ -114,7 +114,12 @@ def test_fn(self):\n             compiled_step()\n \n             # perturb state to force recompile\n-            opt_compiled.state.clear()\n+            # Adagrad doesn't reinitialize state on each step\n+            if optim_cls is torch.optim.Adagrad:\n+                opt_compiled.param_groups[0][\"lr\"] = 0.02\n+            else:\n+                opt_compiled.state.clear()\n+\n             compiled_step()\n \n         if self.check_kernel_count:\n@@ -145,8 +150,9 @@ def tearDown(self):\n     test_adam = make_test(torch.optim.Adam, lr=0.01)\n     test_adam_weight_decay = make_test(torch.optim.Adam, lr=0.01, weight_decay=0.01)\n     test_adamw = make_test(torch.optim.AdamW, lr=0.01)\n-    test_adamax = make_test(torch.optim.Adamax, lr=0.01)\n-    test_nadam = make_test(torch.optim.NAdam, lr=0.01)\n+    # Need to an impl which does not use python scalars\n+    # test_adamax = make_test(torch.optim.Adamax, lr=0.01)\n+    # test_nadam = make_test(torch.optim.NAdam, lr=0.01)\n     test_rprop = make_test(torch.optim.Rprop, kernel_count=6, lr=0.01)\n     test_rmsprop = make_test(torch.optim.RMSprop, kernel_count=1, lr=0.01)\n     test_adadelta = make_test(torch.optim.Adadelta, kernel_count=5, lr=0.01)\n@@ -155,8 +161,9 @@ def tearDown(self):\n \n     test_adam_recompile = make_recompile_test(torch.optim.Adam, lr=0.01)\n     test_adamw_recompile = make_recompile_test(torch.optim.AdamW, lr=0.01)\n-    test_adamax_recompile = make_recompile_test(torch.optim.Adamax, lr=0.01)\n-    test_nadam_recompile = make_recompile_test(torch.optim.NAdam, lr=0.01)\n+    # Need an impl which does not use python scalars\n+    # test_adamax_recompile = make_recompile_test(torch.optim.Adamax, lr=0.01)\n+    # test_nadam_recompile = make_recompile_test(torch.optim.NAdam, lr=0.01)\n     test_rprop_recompile = make_recompile_test(\n         torch.optim.Rprop, kernel_count=6, lr=0.01\n     )\ndiff --git a/torch/_dynamo/variables/optimizer.py b/torch/_dynamo/variables/optimizer.py\nindex 97bfa612fb7c0e..e365b9ae89c075 100644\n--- a/torch/_dynamo/variables/optimizer.py\n+++ b/torch/_dynamo/variables/optimizer.py\n@@ -23,7 +23,7 @@ class GuardInstallException(Exception):\n \n \n class OptimizerVariable(UserDefinedObjectVariable):\n-    def __init__(self, value, grad_to_source=None, **kwargs):\n+    def __init__(self, value, grad_to_source=None, tensor_to_source=None, **kwargs):\n         super().__init__(value, **kwargs)\n \n         for group in self.value.param_groups:\n@@ -33,6 +33,9 @@ def __init__(self, value, grad_to_source=None, **kwargs):\n         if grad_to_source is None:\n             self.grad_to_source = {}\n \n+        if tensor_to_source is None:\n+            self.tensor_to_source = {}\n+\n     def call_method(\n         self,\n         tx,\n@@ -45,8 +48,7 @@ def call_method(\n             try:\n                 py_args, py_kwargs = self.get_python_args(*args, **kwargs)\n                 self.value._init_group(*py_args, **py_kwargs)\n-                self.map_grads_to_sources()\n-                self.install_guards(tx)\n+                self.map_sources_and_install_guards(tx)\n                 self.update_list_args(tx, args, kwargs, py_args, py_kwargs)\n                 return ConstantVariable(None)\n             except (ArgMappingException, GuardInstallException) as _:\n@@ -55,18 +57,6 @@ def call_method(\n \n         return super().call_method(tx, name, args, kwargs)\n \n-    def map_grads_to_sources(self):\n-        \"\"\"Map the optimizer's grads to their sources\"\"\"\n-        self.grad_to_source = {}\n-        for g_ind, group in enumerate(self.value.param_groups):\n-            group_source = GetItemSource(AttrSource(self.source, \"param_groups\"), g_ind)\n-            for p_ind, p in enumerate(group[\"params\"]):\n-                if p.grad is not None:\n-                    self.grad_to_source[p.grad] = AttrSource(\n-                        GetItemSource(GetItemSource(group_source, \"params\"), p_ind),\n-                        \"grad\",\n-                    )\n-\n     def var_getattr(self, tx, name):\n         if name == \"_init_group\":\n             return GetAttrVariable(self, name)\n@@ -96,9 +86,25 @@ def map_arg(arg):\n \n         return new_args, new_kwargs\n \n-    def install_guards(self, tx):\n+    def map_sources_and_install_guards(self, tx):\n         from .builder import VariableBuilder\n \n+        self.grad_to_source = {}\n+        self.tensor_to_source = {}\n+\n+        for g_ind, group in enumerate(self.value.param_groups):\n+            group_source = GetItemSource(AttrSource(self.source, \"param_groups\"), g_ind)\n+            for p_ind, p in enumerate(group[\"params\"]):\n+                param_source = GetItemSource(\n+                    GetItemSource(group_source, \"params\"), p_ind\n+                )\n+                self.tensor_to_source[p] = param_source\n+                if p.grad is not None:\n+                    self.grad_to_source[p.grad] = AttrSource(\n+                        param_source,\n+                        \"grad\",\n+                    )\n+\n         # state guards take a long time to generate\n         # so we manually generate them here\n         guards = set()\n@@ -106,13 +112,15 @@ def install_guards(self, tx):\n         guards.add(state_source.make_guard(GuardBuilder.DICT_KEYS))\n         for p, value in self.value.state.items():\n             tx.store_dict_key(global_key_name(p), p)\n-            p_state_source = GetItemSource(\n-                state_source, GlobalWeakRefSource(global_key_name(p))\n-            )\n+            p_state_source = GetItemSource(state_source, self.tensor_to_source[p])\n             guards.add(p_state_source.make_guard(GuardBuilder.DICT_KEYS))\n             for k, v in value.items():\n-                if isinstance(v, torch.Tensor) and v not in self.grad_to_source:\n-                    continue  # we process tensors guards used in step when we call it\n+                if (\n+                    isinstance(v, torch.Tensor)\n+                    and v not in self.grad_to_source\n+                    and v not in self.tensor_to_source\n+                ):\n+                    self.tensor_to_source[v] = GetItemSource(p_state_source, k)\n                 elif v is None or isinstance(v, (bool, int, float, str)):\n                     guards.add(\n                         GetItemSource(p_state_source, k).make_guard(\n@@ -133,9 +141,15 @@ def wrap_tensor(self, tx, tensor_value):\n         \"\"\"Wrap state tensor in a TensorVariable\"\"\"\n         from .builder import VariableBuilder\n \n-        # don't add weakref guards for grads, they will possibly change on\n-        # each iteration\n-        if tensor_value in self.grad_to_source:\n+        # If we have a source for a tensor already use it,\n+        # if we have not seen a tensor before, stash and use a\n+        # global weak ref source, since it must be an optimizer tensor\n+        # that we have missed\n+        if tensor_value in self.tensor_to_source:\n+            return VariableBuilder(tx, self.tensor_to_source[tensor_value])(\n+                tensor_value\n+            )\n+        elif tensor_value in self.grad_to_source:\n             return VariableBuilder(tx, self.grad_to_source[tensor_value])(tensor_value)\n         else:\n             tx.store_dict_key(global_key_name(tensor_value), tensor_value)\n\nFrom afdc4841297c1736821af8b392bf3be7622cfe84 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Mon, 17 Jul 2023 18:41:53 -0700\nSubject: [PATCH 5/7] Revert optim enablement for merge\n\n---\n torch/_dynamo/eval_frame.py | 37 +++++++++++++++++++++++++++++++++++++\n 1 file changed, 37 insertions(+)\n\ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex 720a090cfef094..0e2c4b80649554 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -651,6 +651,43 @@ def patch():\n         ]\n \n         # Note: this excludes the optimizers that are unsupported in excluded_opts below\n+        from ..optim import (\n+            adadelta,\n+            adagrad,\n+            adamax,\n+            adamw,\n+            asgd,\n+            nadam,\n+            rmsprop,\n+            rprop,\n+            sgd,\n+        )\n+\n+        for opt_mod in (\n+            adadelta,\n+            adagrad,\n+            adamax,\n+            adamw,\n+            asgd,\n+            nadam,\n+            rmsprop,\n+            rprop,\n+            sgd,\n+        ):\n+            opt_name = opt_mod.__name__.split(\".\")[-1]\n+            multi_tensor_fn_name = f\"_multi_tensor_{opt_name}\"\n+            fused_fn_name = f\"_fused_{opt_name}\"\n+            if hasattr(opt_mod, multi_tensor_fn_name):\n+                setattr(\n+                    opt_mod,\n+                    multi_tensor_fn_name,\n+                    disable(getattr(opt_mod, multi_tensor_fn_name)),\n+                )\n+\n+            if hasattr(opt_mod, fused_fn_name):\n+                setattr(\n+                    opt_mod, fused_fn_name, disable(getattr(opt_mod, fused_fn_name))\n+                )\n \n         # Note: we don't support sparsity, data-dependent control, or tracing through backwards\n         excluded_opts = {torch.optim.SparseAdam, torch.optim.RAdam, torch.optim.LBFGS}\n\nFrom c9e33b69a108ae7667280a7c5124dcfadd27ad02 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Mon, 17 Jul 2023 18:44:13 -0700\nSubject: [PATCH 6/7] Enable foreach optims except nadam and adamax\n\n---\n torch/_dynamo/eval_frame.py | 7 -------\n 1 file changed, 7 deletions(-)\n\ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..c8fb414fcbadb6 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -1210,15 +1210,8 @@ def patch():\n         }\n \n         disabled_multi_tensor_opts = {\n-            adadelta,\n-            adagrad,\n             adamax,\n-            adamw,\n-            asgd,\n             nadam,\n-            rmsprop,\n-            rprop,\n-            sgd,\n         }\n \n         for opt_mod in all_opts:\n\nFrom d3fa3fdabe91e6c9bd8fed9112a690abcd6d2633 Mon Sep 17 00:00:00 2001\nFrom: Michael Lazos <mlazos922@gmail.com>\nDate: Tue, 18 Jul 2023 01:10:02 -0700\nSubject: [PATCH 7/7] Update tests to reflect lower number of graph breaks\n\n---\n .../inductor_huggingface_dynamic_training.csv | 72 +++++++-------\n .../inductor_huggingface_training.csv         | 72 +++++++-------\n .../inductor_timm_dynamic_training.csv        | 94 +++++++++---------\n .../inductor_timm_training.csv                | 98 +++++++++----------\n .../inductor_torchbench_dynamic_training.csv  | 94 +++++++++---------\n .../inductor_torchbench_training.csv          | 98 +++++++++----------\n test/dynamo/test_optimizers.py                |  2 +-\n 7 files changed, 265 insertions(+), 265 deletions(-)\n\ndiff --git a/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_dynamic_training.csv b/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_dynamic_training.csv\nindex f6c8e40d5ff0af..1307ee88cb001e 100644\n--- a/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_dynamic_training.csv\n+++ b/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_dynamic_training.csv\n@@ -1,43 +1,43 @@\n name,accuracy,graph_breaks\n-AlbertForMaskedLM,pass,6\n-AllenaiLongformerBase,pass,10\n-BartForCausalLM,pass,6\n-BertForMaskedLM,pass,6\n-BertForQuestionAnswering,pass,6\n-BlenderbotSmallForCausalLM,pass,6\n-BlenderbotSmallForConditionalGeneration,pass,6\n-CamemBert,pass,6\n-DebertaForMaskedLM,pass,6\n-DebertaForQuestionAnswering,pass,6\n+AlbertForMaskedLM,pass,5\n+AllenaiLongformerBase,pass,9\n+BartForCausalLM,pass,5\n+BertForMaskedLM,pass,5\n+BertForQuestionAnswering,pass,5\n+BlenderbotSmallForCausalLM,pass,5\n+BlenderbotSmallForConditionalGeneration,pass,5\n+CamemBert,pass,5\n+DebertaForMaskedLM,pass,5\n+DebertaForQuestionAnswering,pass,5\n DebertaV2ForMaskedLM,pass_due_to_skip,0\n-DistilBertForMaskedLM,pass,6\n-DistilBertForQuestionAnswering,pass,6\n-DistillGPT2,pass,6\n-ElectraForCausalLM,pass,6\n-ElectraForQuestionAnswering,pass,6\n-GPT2ForSequenceClassification,pass,8\n-GoogleFnet,pass,6\n-LayoutLMForMaskedLM,pass,6\n-LayoutLMForSequenceClassification,pass,8\n-M2M100ForConditionalGeneration,fail_accuracy,6\n-MBartForCausalLM,pass,6\n+DistilBertForMaskedLM,pass,5\n+DistilBertForQuestionAnswering,pass,5\n+DistillGPT2,pass,5\n+ElectraForCausalLM,pass,5\n+ElectraForQuestionAnswering,pass,5\n+GPT2ForSequenceClassification,pass,7\n+GoogleFnet,pass,5\n+LayoutLMForMaskedLM,pass,5\n+LayoutLMForSequenceClassification,pass,7\n+M2M100ForConditionalGeneration,fail_accuracy,5\n+MBartForCausalLM,pass,5\n MBartForConditionalGeneration,OOM,4\n-MT5ForConditionalGeneration,pass,6\n-MegatronBertForCausalLM,pass,6\n-MegatronBertForQuestionAnswering,pass,6\n+MT5ForConditionalGeneration,pass,5\n+MegatronBertForCausalLM,pass,5\n+MegatronBertForQuestionAnswering,pass,5\n MobileBertForMaskedLM,pass,4\n MobileBertForQuestionAnswering,pass,4\n-OPTForCausalLM,pass,6\n-PLBartForCausalLM,pass,6\n-PLBartForConditionalGeneration,pass,6\n-PegasusForCausalLM,pass,6\n+OPTForCausalLM,pass,5\n+PLBartForCausalLM,pass,5\n+PLBartForConditionalGeneration,pass,5\n+PegasusForCausalLM,pass,5\n PegasusForConditionalGeneration,pass,4\n-RobertaForCausalLM,pass,6\n-RobertaForQuestionAnswering,pass,6\n-Speech2Text2ForCausalLM,pass,6\n-T5ForConditionalGeneration,pass,6\n-T5Small,pass,6\n-TrOCRForCausalLM,pass,6\n+RobertaForCausalLM,pass,5\n+RobertaForQuestionAnswering,pass,5\n+Speech2Text2ForCausalLM,pass,5\n+T5ForConditionalGeneration,pass,5\n+T5Small,pass,5\n+TrOCRForCausalLM,pass,5\n XGLMForCausalLM,eager_2nd_run_OOM,0\n-XLNetLMHeadModel,pass,6\n-YituTechConvBert,pass,6\n+XLNetLMHeadModel,pass,5\n+YituTechConvBert,pass,5\ndiff --git a/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_training.csv b/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_training.csv\nindex 6c47151186a8ba..7cba3655865dd7 100644\n--- a/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_training.csv\n+++ b/benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_training.csv\n@@ -1,45 +1,45 @@\n name,accuracy,graph_breaks\n-AlbertForMaskedLM,pass,6\n-AllenaiLongformerBase,pass,10\n-BartForCausalLM,pass,6\n+AlbertForMaskedLM,pass,5\n+AllenaiLongformerBase,pass,9\n+BartForCausalLM,pass,5\n BartForConditionalGeneration,OOM,4\n-BertForMaskedLM,pass,6\n-BertForQuestionAnswering,pass,6\n-BlenderbotSmallForCausalLM,pass,6\n-BlenderbotSmallForConditionalGeneration,pass,6\n-CamemBert,pass,6\n-DebertaForMaskedLM,pass,6\n-DebertaForQuestionAnswering,pass,6\n+BertForMaskedLM,pass,5\n+BertForQuestionAnswering,pass,5\n+BlenderbotSmallForCausalLM,pass,5\n+BlenderbotSmallForConditionalGeneration,pass,5\n+CamemBert,pass,5\n+DebertaForMaskedLM,pass,5\n+DebertaForQuestionAnswering,pass,5\n DebertaV2ForMaskedLM,pass_due_to_skip,0\n DebertaV2ForQuestionAnswering,eager_1st_run_OOM,0\n-DistilBertForMaskedLM,pass,6\n-DistilBertForQuestionAnswering,pass,6\n-DistillGPT2,pass,6\n-ElectraForCausalLM,pass,6\n-ElectraForQuestionAnswering,pass,6\n-GPT2ForSequenceClassification,pass,8\n-GoogleFnet,pass,6\n-LayoutLMForMaskedLM,pass,6\n-LayoutLMForSequenceClassification,pass,8\n-M2M100ForConditionalGeneration,fail_accuracy,6\n-MBartForCausalLM,pass,6\n+DistilBertForMaskedLM,pass,5\n+DistilBertForQuestionAnswering,pass,5\n+DistillGPT2,pass,5\n+ElectraForCausalLM,pass,5\n+ElectraForQuestionAnswering,pass,5\n+GPT2ForSequenceClassification,pass,7\n+GoogleFnet,pass,5\n+LayoutLMForMaskedLM,pass,5\n+LayoutLMForSequenceClassification,pass,7\n+M2M100ForConditionalGeneration,fail_accuracy,5\n+MBartForCausalLM,pass,5\n MBartForConditionalGeneration,OOM,4\n-MT5ForConditionalGeneration,pass,6\n-MegatronBertForCausalLM,pass,6\n-MegatronBertForQuestionAnswering,pass,6\n+MT5ForConditionalGeneration,pass,5\n+MegatronBertForCausalLM,pass,5\n+MegatronBertForQuestionAnswering,pass,5\n MobileBertForMaskedLM,pass,4\n MobileBertForQuestionAnswering,pass,4\n-OPTForCausalLM,pass,6\n-PLBartForCausalLM,pass,6\n-PLBartForConditionalGeneration,pass,6\n-PegasusForCausalLM,pass,6\n+OPTForCausalLM,pass,5\n+PLBartForCausalLM,pass,5\n+PLBartForConditionalGeneration,pass,5\n+PegasusForCausalLM,pass,5\n PegasusForConditionalGeneration,pass,4\n-RobertaForCausalLM,pass,6\n-RobertaForQuestionAnswering,pass,6\n-Speech2Text2ForCausalLM,pass,6\n-T5ForConditionalGeneration,pass,6\n-T5Small,pass,6\n-TrOCRForCausalLM,pass,6\n+RobertaForCausalLM,pass,5\n+RobertaForQuestionAnswering,pass,5\n+Speech2Text2ForCausalLM,pass,5\n+T5ForConditionalGeneration,pass,5\n+T5Small,pass,5\n+TrOCRForCausalLM,pass,5\n XGLMForCausalLM,eager_2nd_run_OOM,0\n-XLNetLMHeadModel,pass,6\n-YituTechConvBert,pass,6\n+XLNetLMHeadModel,pass,5\n+YituTechConvBert,pass,5\ndiff --git a/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_dynamic_training.csv b/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_dynamic_training.csv\nindex 87915815e83a16..8287cb1e67f6f3 100644\n--- a/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_dynamic_training.csv\n+++ b/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_dynamic_training.csv\n@@ -1,51 +1,51 @@\n name,accuracy,graph_breaks\n-SelecSls42b,pass,8\n-adv_inception_v3,pass,8\n-beit_base_patch16_224,pass,8\n-botnet26t_256,pass,8\n-coat_lite_mini,pass,8\n-convit_base,pass,9\n+SelecSls42b,pass,7\n+adv_inception_v3,pass,7\n+beit_base_patch16_224,pass,7\n+botnet26t_256,pass,7\n+coat_lite_mini,pass,7\n+convit_base,pass,8\n convmixer_768_32,pass,6\n-convnext_base,pass,8\n-cspdarknet53,pass,8\n-dla102,pass,8\n-dm_nfnet_f0,pass,8\n-dpn107,pass,8\n-eca_botnext26ts_256,pass,8\n-eca_halonext26ts,pass,8\n-ese_vovnet19b_dw,pass,8\n-fbnetc_100,pass,8\n-fbnetv3_b,pass,8\n-gernet_l,pass,8\n-ghostnet_100,pass,8\n-gluon_inception_v3,pass,8\n-gmixer_24_224,pass,8\n-gmlp_s16_224,pass,8\n+convnext_base,pass,7\n+cspdarknet53,pass,7\n+dla102,pass,7\n+dm_nfnet_f0,pass,7\n+dpn107,pass,7\n+eca_botnext26ts_256,pass,7\n+eca_halonext26ts,pass,7\n+ese_vovnet19b_dw,pass,7\n+fbnetc_100,pass,7\n+fbnetv3_b,pass,7\n+gernet_l,pass,7\n+ghostnet_100,pass,7\n+gluon_inception_v3,pass,7\n+gmixer_24_224,pass,7\n+gmlp_s16_224,pass,7\n hrnet_w18,pass,6\n-inception_v3,pass,8\n-jx_nest_base,pass,8\n-lcnet_050,pass,8\n-mixer_b16_224,pass,8\n-mixnet_l,pass,8\n-mnasnet_100,pass,8\n-mobilenetv2_100,pass,8\n-mobilenetv3_large_100,pass,8\n-nfnet_l0,pass,8\n+inception_v3,pass,7\n+jx_nest_base,pass,7\n+lcnet_050,pass,7\n+mixer_b16_224,pass,7\n+mixnet_l,pass,7\n+mnasnet_100,pass,7\n+mobilenetv2_100,pass,7\n+mobilenetv3_large_100,pass,7\n+nfnet_l0,pass,7\n pnasnet5large,pass,6\n-poolformer_m36,pass,8\n-regnety_002,pass,8\n-repvgg_a2,pass,8\n-res2net101_26w_4s,pass,8\n-res2net50_14w_8s,pass,8\n-res2next50,pass,8\n-resmlp_12_224,pass,8\n-resnest101e,pass,8\n-rexnet_100,pass,8\n-spnasnet_100,pass,8\n-swin_base_patch4_window7_224,pass,8\n-swsl_resnext101_32x16d,pass,8\n-tf_efficientnet_b0,pass,8\n-tf_mixnet_l,pass,8\n-tinynet_a,pass,8\n-tnt_s_patch16_224,pass,8\n-volo_d1_224,pass,8\n+poolformer_m36,pass,7\n+regnety_002,pass,7\n+repvgg_a2,pass,7\n+res2net101_26w_4s,pass,7\n+res2net50_14w_8s,pass,7\n+res2next50,pass,7\n+resmlp_12_224,pass,7\n+resnest101e,pass,7\n+rexnet_100,pass,7\n+spnasnet_100,pass,7\n+swin_base_patch4_window7_224,pass,7\n+swsl_resnext101_32x16d,pass,7\n+tf_efficientnet_b0,pass,7\n+tf_mixnet_l,pass,7\n+tinynet_a,pass,7\n+tnt_s_patch16_224,pass,7\n+volo_d1_224,pass,7\ndiff --git a/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv b/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv\nindex ef470f7744e2a7..4b732db1e74701 100644\n--- a/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv\n+++ b/benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv\n@@ -1,53 +1,53 @@\n name,accuracy,graph_breaks\n-SelecSls42b,pass,8\n-adv_inception_v3,pass,8\n-beit_base_patch16_224,pass,8\n-botnet26t_256,pass,8\n-coat_lite_mini,pass,8\n-convit_base,pass,9\n+SelecSls42b,pass,7\n+adv_inception_v3,pass,7\n+beit_base_patch16_224,pass,7\n+botnet26t_256,pass,7\n+coat_lite_mini,pass,7\n+convit_base,pass,8\n convmixer_768_32,pass,6\n-convnext_base,pass,8\n-cspdarknet53,pass,8\n-dla102,pass,8\n-dm_nfnet_f0,pass,8\n-dpn107,pass,8\n-eca_botnext26ts_256,pass,8\n-eca_halonext26ts,pass,8\n-ese_vovnet19b_dw,pass,8\n-fbnetc_100,pass,8\n-fbnetv3_b,pass,8\n-gernet_l,pass,8\n-ghostnet_100,pass,8\n-gluon_inception_v3,pass,8\n-gmixer_24_224,pass,8\n-gmlp_s16_224,pass,8\n+convnext_base,pass,7\n+cspdarknet53,pass,7\n+dla102,pass,7\n+dm_nfnet_f0,pass,7\n+dpn107,pass,7\n+eca_botnext26ts_256,pass,7\n+eca_halonext26ts,pass,7\n+ese_vovnet19b_dw,pass,7\n+fbnetc_100,pass,7\n+fbnetv3_b,pass,7\n+gernet_l,pass,7\n+ghostnet_100,pass,7\n+gluon_inception_v3,pass,7\n+gmixer_24_224,pass,7\n+gmlp_s16_224,pass,7\n hrnet_w18,pass,6\n-inception_v3,pass,8\n-jx_nest_base,pass,8\n-lcnet_050,pass,8\n-levit_128,pass,8\n-mixer_b16_224,pass,8\n-mixnet_l,pass,8\n-mnasnet_100,pass,8\n-mobilenetv2_100,pass,8\n-mobilenetv3_large_100,pass,8\n-nfnet_l0,pass,8\n+inception_v3,pass,7\n+jx_nest_base,pass,7\n+lcnet_050,pass,7\n+levit_128,pass,7\n+mixer_b16_224,pass,7\n+mixnet_l,pass,7\n+mnasnet_100,pass,7\n+mobilenetv2_100,pass,7\n+mobilenetv3_large_100,pass,7\n+nfnet_l0,pass,7\n pnasnet5large,pass,6\n-poolformer_m36,pass,8\n-regnety_002,pass,8\n-repvgg_a2,pass,8\n-res2net101_26w_4s,pass,8\n-res2net50_14w_8s,pass,8\n-res2next50,pass,8\n-resmlp_12_224,pass,8\n-resnest101e,pass,8\n-rexnet_100,pass,8\n-sebotnet33ts_256,pass,8\n-spnasnet_100,pass,8\n-swin_base_patch4_window7_224,pass,8\n-swsl_resnext101_32x16d,pass,8\n-tf_efficientnet_b0,pass,8\n-tf_mixnet_l,pass,8\n-tinynet_a,pass,8\n-tnt_s_patch16_224,pass,8\n-volo_d1_224,pass,8\n+poolformer_m36,pass,7\n+regnety_002,pass,7\n+repvgg_a2,pass,7\n+res2net101_26w_4s,pass,7\n+res2net50_14w_8s,pass,7\n+res2next50,pass,7\n+resmlp_12_224,pass,7\n+resnest101e,pass,7\n+rexnet_100,pass,7\n+sebotnet33ts_256,pass,7\n+spnasnet_100,pass,7\n+swin_base_patch4_window7_224,pass,7\n+swsl_resnext101_32x16d,pass,7\n+tf_efficientnet_b0,pass,7\n+tf_mixnet_l,pass,7\n+tinynet_a,pass,7\n+tnt_s_patch16_224,pass,7\n+volo_d1_224,pass,7\ndiff --git a/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_dynamic_training.csv b/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_dynamic_training.csv\nindex 172e67c507a73f..314cf7cc86359f 100644\n--- a/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_dynamic_training.csv\n+++ b/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_dynamic_training.csv\n@@ -1,50 +1,50 @@\n name,accuracy,graph_breaks\n-BERT_pytorch,pass,8\n-LearningToPaint,pass,8\n-Super_SloMo,pass,8\n-alexnet,pass,8\n-attention_is_all_you_need_pytorch,pass,8\n-basic_gnn_edgecnn,pass,23\n-basic_gnn_gcn,pass,14\n-basic_gnn_gin,pass,8\n-basic_gnn_sage,pass,8\n-dcgan,pass,8\n-drq,pass,7\n-fastNLP_Bert,pass,13\n-functorch_dp_cifar10,pass,8\n-functorch_maml_omniglot,pass,8\n-hf_Albert,pass,7\n-hf_Bart,pass,7\n-hf_Bert,pass,7\n-hf_Bert_large,pass,7\n-hf_DistilBert,pass,7\n-hf_GPT2,pass,7\n-hf_Reformer,pass,43\n+BERT_pytorch,pass,7\n+LearningToPaint,pass,7\n+Super_SloMo,pass,7\n+alexnet,pass,7\n+attention_is_all_you_need_pytorch,pass,7\n+basic_gnn_edgecnn,pass,22\n+basic_gnn_gcn,pass,13\n+basic_gnn_gin,pass,7\n+basic_gnn_sage,pass,7\n+dcgan,pass,7\n+drq,pass,6\n+fastNLP_Bert,pass,12\n+functorch_dp_cifar10,pass,7\n+functorch_maml_omniglot,pass,7\n+hf_Albert,pass,6\n+hf_Bart,pass,6\n+hf_Bert,pass,6\n+hf_Bert_large,pass,6\n+hf_DistilBert,pass,6\n+hf_GPT2,pass,6\n+hf_Reformer,pass,42\n hf_T5_large,pass_due_to_skip,0\n-lennard_jones,pass,8\n-maml_omniglot,pass,8\n-mnasnet1_0,pass,8\n-mobilenet_v2,pass,8\n-moco,pass,19\n-nvidia_deeprecommender,pass,8\n-phlippe_densenet,pass,8\n-phlippe_resnet,pass,8\n-pytorch_CycleGAN_and_pix2pix,pass,8\n-pytorch_stargan,pass,8\n-pytorch_unet,pass,8\n-resnet152,pass,8\n-resnet18,pass,8\n-resnet50,pass,8\n-resnext50_32x4d,pass,8\n-shufflenet_v2_x1_0,pass,8\n-soft_actor_critic,pass,7\n-squeezenet1_1,pass,8\n-timm_efficientnet,pass,8\n-timm_regnet,pass,8\n-timm_resnest,pass,8\n-timm_vision_transformer,pass,8\n+lennard_jones,pass,7\n+maml_omniglot,pass,7\n+mnasnet1_0,pass,7\n+mobilenet_v2,pass,7\n+moco,pass,18\n+nvidia_deeprecommender,pass,7\n+phlippe_densenet,pass,7\n+phlippe_resnet,pass,7\n+pytorch_CycleGAN_and_pix2pix,pass,7\n+pytorch_stargan,pass,7\n+pytorch_unet,pass,7\n+resnet152,pass,7\n+resnet18,pass,7\n+resnet50,pass,7\n+resnext50_32x4d,pass,7\n+shufflenet_v2_x1_0,pass,7\n+soft_actor_critic,pass,6\n+squeezenet1_1,pass,7\n+timm_efficientnet,pass,7\n+timm_regnet,pass,7\n+timm_resnest,pass,7\n+timm_vision_transformer,pass,7\n timm_vision_transformer_large,pass_due_to_skip,0\n-timm_vovnet,pass,8\n-tts_angular,pass,10\n-vgg16,pass,8\n-yolov3,pass,10\n+timm_vovnet,pass,7\n+tts_angular,pass,9\n+vgg16,pass,7\n+yolov3,pass,9\ndiff --git a/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv b/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv\nindex 1c961ad251716d..9da2d89b454a38 100644\n--- a/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv\n+++ b/benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv\n@@ -1,52 +1,52 @@\n name,accuracy,graph_breaks\n-BERT_pytorch,pass,8\n-LearningToPaint,pass,8\n-Super_SloMo,pass,8\n-alexnet,pass,8\n-attention_is_all_you_need_pytorch,pass,8\n-basic_gnn_edgecnn,pass,23\n-basic_gnn_gcn,pass,14\n-basic_gnn_gin,pass,8\n-basic_gnn_sage,pass,8\n-dcgan,pass,8\n-drq,pass,7\n-fastNLP_Bert,pass,13\n-functorch_dp_cifar10,pass,8\n-functorch_maml_omniglot,pass,8\n-hf_Albert,pass,7\n-hf_Bart,pass,7\n-hf_Bert,pass,7\n-hf_Bert_large,pass,7\n-hf_DistilBert,pass,7\n-hf_GPT2,pass,7\n-hf_Reformer,pass,43\n+BERT_pytorch,pass,7\n+LearningToPaint,pass,7\n+Super_SloMo,pass,7\n+alexnet,pass,7\n+attention_is_all_you_need_pytorch,pass,7\n+basic_gnn_edgecnn,pass,22\n+basic_gnn_gcn,pass,13\n+basic_gnn_gin,pass,7\n+basic_gnn_sage,pass,7\n+dcgan,pass,7\n+drq,pass,6\n+fastNLP_Bert,pass,12\n+functorch_dp_cifar10,pass,7\n+functorch_maml_omniglot,pass,7\n+hf_Albert,pass,6\n+hf_Bart,pass,6\n+hf_Bert,pass,6\n+hf_Bert_large,pass,6\n+hf_DistilBert,pass,6\n+hf_GPT2,pass,6\n+hf_Reformer,pass,42\n hf_T5_large,pass_due_to_skip,0\n-lennard_jones,pass,8\n-maml_omniglot,pass,8\n-mnasnet1_0,pass,8\n-mobilenet_v2,pass,8\n-moco,pass,19\n-nvidia_deeprecommender,pass,8\n-phlippe_densenet,pass,8\n-phlippe_resnet,pass,8\n-pytorch_CycleGAN_and_pix2pix,pass,8\n-pytorch_stargan,pass,8\n-pytorch_unet,pass,8\n-resnet152,pass,8\n-resnet18,pass,8\n-resnet50,pass,8\n-resnext50_32x4d,pass,8\n-shufflenet_v2_x1_0,pass,8\n-soft_actor_critic,pass,7\n-speech_transformer,pass,18\n-squeezenet1_1,pass,8\n-timm_efficientnet,pass,8\n-timm_regnet,pass,8\n-timm_resnest,pass,8\n-timm_vision_transformer,pass,8\n+lennard_jones,pass,7\n+maml_omniglot,pass,7\n+mnasnet1_0,pass,7\n+mobilenet_v2,pass,7\n+moco,pass,18\n+nvidia_deeprecommender,pass,7\n+phlippe_densenet,pass,7\n+phlippe_resnet,pass,7\n+pytorch_CycleGAN_and_pix2pix,pass,7\n+pytorch_stargan,pass,7\n+pytorch_unet,pass,7\n+resnet152,pass,7\n+resnet18,pass,7\n+resnet50,pass,7\n+resnext50_32x4d,pass,7\n+shufflenet_v2_x1_0,pass,7\n+soft_actor_critic,pass,6\n+speech_transformer,pass,17\n+squeezenet1_1,pass,7\n+timm_efficientnet,pass,7\n+timm_regnet,pass,7\n+timm_resnest,pass,7\n+timm_vision_transformer,pass,7\n timm_vision_transformer_large,pass_due_to_skip,0\n-timm_vovnet,pass,8\n-tts_angular,pass,10\n-vgg16,pass,8\n-vision_maskrcnn,fail_accuracy,40\n-yolov3,pass,10\n+timm_vovnet,pass,7\n+tts_angular,pass,9\n+vgg16,pass,7\n+vision_maskrcnn,fail_accuracy,39\n+yolov3,pass,9\ndiff --git a/test/dynamo/test_optimizers.py b/test/dynamo/test_optimizers.py\nindex cb7292497648d0..8a47df6655049a 100644\n--- a/test/dynamo/test_optimizers.py\n+++ b/test/dynamo/test_optimizers.py\n@@ -121,7 +121,7 @@ def training_iter_fn(batch, model, optimizer):\n         batch = {\"x\": input1, \"y\": input2}\n         for _ in range(2):\n             opt_training_iter_fn(batch, net, optimizer)\n-        self.assertEqual(cnts.frame_count, 3)\n+        self.assertEqual(cnts.frame_count, 2)\n \n     def test_state_dict(self):\n         @torch.compile(backend=\"eager\")\n"
  },
  {
    "number": 105437,
    "title": "[BE] Enable ruff's UP rules in pyproject.toml",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #105437\n* #105436\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\nSigned-off-by: Justin Chu <justinchu@microsoft.com>",
    "merge_commit_sha": "6cf85f8217fa7af7201f74b96f95f273a16053e5",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105437",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105437/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105437.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105437.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105437/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105437/comments",
    "labels": [
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:23:34.970666Z",
    "state": "open",
    "patch": "From 0f0b6c13eb5628ed8cdee1f84e5d5387634f1844 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:23:28 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules in pyproject.toml\n\nSigned-off-by: Justin Chu <justinchu@microsoft.com>\n\n[ghstack-poisoned]\n---\n pyproject.toml | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 69afc67e2b8696..95dc1427f14095 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -55,6 +55,8 @@ ignore = [\n     \"SIM116\", # Disable Use a dictionary instead of consecutive `if` statements\n     \"SIM117\",\n     \"SIM118\",\n+    \"UP006\", # keep-runtime-typing\n+    \"UP007\", # keep-runtime-typing\n ]\n line-length = 120\n select = [\n@@ -66,6 +68,7 @@ select = [\n     \"SIM1\",\n     \"W\",\n     # Not included in flake8\n+    \"UP\",\n     \"PERF\",\n     \"PLE\",\n     \"TRY302\",\n@@ -81,3 +84,6 @@ select = [\n     \"F401\",\n     \"F403\",\n ]\n+\"torch/utils/collect_env.py\" = [\n+    \"UP\", # collect_env.py needs to work with older versions of Python\n+]\n"
  },
  {
    "number": 105436,
    "title": "[BE] Enable ruff's UP rules and autoformat nn/ mps/ and torch/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* __->__ #105436\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "fa0844bc2efbcd5e8359f39ab22e94cd4b9deaa1",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105436",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105436/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105436.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105436.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105436/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105436/comments",
    "labels": [
      "release notes: onnx",
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:23:30.269074Z",
    "state": "open",
    "patch": "From 69de828bcb47c60413699275e57395e9dc4a5255 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:23:24 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat nn/ mps/ and\n torch/\n\n[ghstack-poisoned]\n---\n test/nn/test_convolution.py                |  8 ++---\n test/nn/test_module_hooks.py               |  2 +-\n test/nn/test_multihead_attention.py        |  2 +-\n test/nn/test_pooling.py                    | 20 ++++++------\n test/onnx/dynamo/test_exporter_api.py      | 18 +++++------\n test/onnx/test_pytorch_onnx_onnxruntime.py |  6 ++--\n torch/__init__.py                          |  4 +--\n torch/_custom_op/impl.py                   |  2 +-\n torch/_linalg_utils.py                     |  2 +-\n torch/_lobpcg.py                           | 28 ++++++++--------\n torch/_logging/_internal.py                |  2 +-\n torch/_lowrank.py                          |  2 +-\n torch/_ops.py                              |  2 +-\n torch/_python_dispatcher.py                |  6 ++--\n torch/_tensor.py                           |  4 +--\n torch/_tensor_str.py                       | 28 +++++++---------\n torch/_torch_docs.py                       |  1 -\n torch/_utils.py                            |  6 ++--\n torch/autograd/function.py                 |  2 +-\n torch/autograd/functional.py               |  6 ++--\n torch/autograd/gradcheck.py                | 16 +++++-----\n torch/autograd/graph.py                    |  6 ++--\n torch/autograd/profiler_util.py            | 37 +++++++++++-----------\n torch/backends/_nnapi/serializer.py        |  7 ++--\n torch/backends/cudnn/rnn.py                |  2 +-\n torch/backends/mps/__init__.py             |  4 +--\n torch/backends/opt_einsum/__init__.py      |  2 +-\n torch/backends/quantized/__init__.py       |  2 +-\n torch/backends/xeon/run_cpu.py             |  4 +--\n torch/contrib/_tensorboard_vis.py          |  2 +-\n torch/cuda/__init__.py                     |  4 +--\n torch/cuda/_memory_viz.py                  |  2 +-\n torch/cuda/_utils.py                       |  6 ++--\n torch/cuda/amp/grad_scaler.py              |  4 +--\n torch/cuda/graphs.py                       |  2 +-\n torch/cuda/memory.py                       |  4 +--\n torch/cuda/streams.py                      | 14 ++++----\n torch/hub.py                               | 14 ++++----\n torch/jit/_decompositions.py               |  6 ++--\n torch/jit/_recursive.py                    | 10 +++---\n torch/jit/_script.py                       |  6 ++--\n torch/jit/_serialization.py                |  4 +--\n torch/jit/_state.py                        |  2 +-\n torch/jit/_trace.py                        |  4 +--\n torch/jit/annotations.py                   |  2 +-\n torch/jit/frontend.py                      |  4 +--\n torch/jit/mobile/__init__.py               |  4 +--\n torch/jit/quantized.py                     | 14 ++++----\n torch/jit/supported_ops.py                 | 16 +++++-----\n torch/library.py                           |  6 ++--\n torch/linalg/__init__.py                   |  1 -\n torch/masked/_docs.py                      |  1 -\n torch/masked/_ops.py                       |  1 -\n torch/masked/maskedtensor/core.py          |  2 +-\n torch/mps/__init__.py                      |  2 +-\n torch/nn/_reduction.py                     |  2 +-\n torch/nn/functional.py                     | 18 +++++------\n torch/nn/init.py                           |  6 ++--\n torch/nn/modules/_functions.py             |  4 +--\n torch/nn/modules/activation.py             | 30 +++++++++---------\n torch/nn/modules/adaptive.py               |  1 -\n torch/nn/modules/batchnorm.py              | 14 ++++----\n torch/nn/modules/channelshuffle.py         |  2 +-\n torch/nn/modules/container.py              | 16 +++++-----\n torch/nn/modules/conv.py                   |  3 +-\n torch/nn/modules/dropout.py                |  2 +-\n torch/nn/modules/flatten.py                | 10 +++---\n torch/nn/modules/fold.py                   |  1 -\n torch/nn/modules/instancenorm.py           |  2 +-\n torch/nn/modules/lazy.py                   |  4 +--\n torch/nn/modules/module.py                 | 16 +++++-----\n torch/nn/modules/padding.py                | 12 +++----\n torch/nn/modules/pixelshuffle.py           |  4 +--\n torch/nn/modules/pooling.py                |  4 +--\n torch/nn/modules/rnn.py                    | 16 +++++-----\n torch/nn/modules/transformer.py            |  2 +-\n torch/nn/modules/utils.py                  |  2 +-\n torch/nn/parallel/distributed.py           |  4 +--\n torch/nn/parallel/parallel_apply.py        |  2 +-\n torch/nn/utils/init.py                     |  2 +-\n torch/nn/utils/parametrizations.py         |  4 +--\n torch/nn/utils/prune.py                    | 20 ++++++------\n torch/nn/utils/rnn.py                      |  2 +-\n torch/nn/utils/spectral_norm.py            |  2 +-\n torch/overrides.py                         |  2 +-\n torch/serialization.py                     | 20 ++++++------\n torch/storage.py                           | 12 +++----\n torch/torch_version.py                     |  2 +-\n 88 files changed, 295 insertions(+), 316 deletions(-)\n\ndiff --git a/test/nn/test_convolution.py b/test/nn/test_convolution.py\nindex 38a060cb72e906..3e4f481fd94a93 100644\n--- a/test/nn/test_convolution.py\n+++ b/test/nn/test_convolution.py\n@@ -1539,13 +1539,13 @@ def test_conv3d_valid_padding_backward(self, device, dtype):\n         gradcheck(lambda x, y: F.conv3d(x, y, padding='valid'), (x, y), check_forward_ad=check_forward_ad)\n         gradgradcheck(lambda x, y: F.conv3d(x, y, padding='valid'), (x, y), check_fwd_over_rev=check_forward_ad)\n \n-    @parametrize_test(\"N\", range(2, 4), name_fn=lambda N: 'ConvTranspose{}d'.format(N))\n+    @parametrize_test(\"N\", range(2, 4), name_fn=lambda N: f'ConvTranspose{N}d')\n     def test_conv_transpose_with_output_size_and_no_batch_dim(self, device, N):\n         # For inputs with no batch dim, verify output is the correct shape when output_size is set.\n         # See https://github.com/pytorch/pytorch/issues/75889\n         inp = torch.randn((1, 15, 13) if N == 2 else (1, 15, 13, 13), device=device)\n         output_size = (1, 240, 200) if N == 2 else (1, 240, 200, 200)\n-        ConvTransposeNd = getattr(nn, 'ConvTranspose{}d'.format(N))\n+        ConvTransposeNd = getattr(nn, f'ConvTranspose{N}d')\n         m = ConvTransposeNd(1, 1, kernel_size=16, stride=16, padding=7, bias=False, device=device)\n         output = m(inp, output_size=output_size)\n         self.assertEqual(output.shape, output_size)\n@@ -1892,9 +1892,9 @@ def test_conv_noncontig_weights(self, device):\n                 w = w.expand([nc, int(nc / groups)] + list(w.shape))\n                 w = w.detach().requires_grad_()\n                 x = torch.randn([1, nc] + ([5] * dim), device=device, requires_grad=True)\n-                y = getattr(F, 'conv{}d'.format(dim))(x, w, groups=groups)\n+                y = getattr(F, f'conv{dim}d')(x, w, groups=groups)\n                 y.sum().backward()\n-                y = getattr(F, 'conv_transpose{}d'.format(dim))(x, w, groups=groups)\n+                y = getattr(F, f'conv_transpose{dim}d')(x, w, groups=groups)\n                 y.sum().backward()\n \n     def test_conv_noncontig_weights_and_bias(self, device):\ndiff --git a/test/nn/test_module_hooks.py b/test/nn/test_module_hooks.py\nindex 6b186e856f0f62..18d631d8b6fba8 100644\n--- a/test/nn/test_module_hooks.py\n+++ b/test/nn/test_module_hooks.py\n@@ -165,7 +165,7 @@ def kwarg_forward_hook(\n     return out\n \n \n-class DummyContextManager():\n+class DummyContextManager:\n     def __init__(self, inp):\n         self.input = inp\n \ndiff --git a/test/nn/test_multihead_attention.py b/test/nn/test_multihead_attention.py\nindex fcbf3c179b455f..38aadf6cdd1ea3 100644\n--- a/test/nn/test_multihead_attention.py\n+++ b/test/nn/test_multihead_attention.py\n@@ -118,7 +118,7 @@ def _multihead_attn_test_helper(add_key_padding_mask=False, add_bias_kv=False, a\n                                         saved_kv=False, same_embed_dim=False,\n                                         average_attn_weights=average_attn_weights):\n             for _ in range(100):\n-                batch_sz, seq_len = [random.randint(2, 10) for r in range(2)]\n+                batch_sz, seq_len = (random.randint(2, 10) for r in range(2))\n                 d_head = random.randint(3, 10)\n                 nheads = random.randint(2, 5) * 2\n                 d_model = d_head * nheads\ndiff --git a/test/nn/test_pooling.py b/test/nn/test_pooling.py\nindex 388e73f5c79857..93487e4962de1d 100644\n--- a/test/nn/test_pooling.py\n+++ b/test/nn/test_pooling.py\n@@ -138,7 +138,7 @@ class TestPoolingNN(NNTestCase):\n     def test_adaptive_pooling_size_none(self):\n         for numel in (2, 3):\n             for pool_type in ('Max', 'Avg'):\n-                cls_name = 'Adaptive{}Pool{}d'.format(pool_type, numel)\n+                cls_name = f'Adaptive{pool_type}Pool{numel}d'\n                 module_cls = getattr(nn, cls_name)\n                 output_size = (2,) * (numel - 1) + (None,)\n                 module = module_cls(output_size)\n@@ -757,7 +757,7 @@ def test_adaptive_pooling_no_suppot_input(self, device, dtype):\n                 # adapative_avg_pool2d for int is implemented\n                 if numel == 2 and pool_type == 'Avg':\n                     continue\n-                cls_name = 'Adaptive{}Pool{}d'.format(pool_type, numel)\n+                cls_name = f'Adaptive{pool_type}Pool{numel}d'\n                 module_cls = getattr(nn, cls_name)\n                 output_size = (2,) * numel\n                 module = module_cls(output_size)\n@@ -1209,9 +1209,9 @@ def expected_output(dim, dtype):\n                 return torch.stack([col, col + 2], 1).view(2, 2, 2, 2)\n \n         if adaptive:\n-            cls_name = 'AdaptiveMaxPool{}d'.format(num_dim)\n+            cls_name = f'AdaptiveMaxPool{num_dim}d'\n         else:\n-            cls_name = 'MaxPool{}d'.format(num_dim)\n+            cls_name = f'MaxPool{num_dim}d'\n         module_cls = getattr(nn, cls_name)\n         module = module_cls(2, return_indices=True).to(device, dtype=dtype)\n         numel = 4 ** (num_dim + 1)\n@@ -1323,7 +1323,7 @@ def test_maxpool_indices_no_batch_dim(self, device, dtype):\n     def test_max_pool_nan_inf(self, device, dtype):\n         for adaptive in ['', 'adaptive_']:\n             for num_dim in [1, 2, 3]:\n-                fn_name = '{}max_pool{}d'.format(adaptive, num_dim)\n+                fn_name = f'{adaptive}max_pool{num_dim}d'\n                 fn = getattr(F, fn_name)\n \n                 x = torch.full([1, 1] + num_dim * [3], nan, device=device, dtype=dtype, requires_grad=True)\n@@ -1423,7 +1423,7 @@ def func(x):\n     @onlyNativeDeviceTypes  # TODO: Fails on XLA\n     def test_fractional_max_pool_nan_inf(self, device, dtype):\n         for num_dim in [2, 3]:\n-            fn_name = 'FractionalMaxPool{}d'.format(num_dim)\n+            fn_name = f'FractionalMaxPool{num_dim}d'\n             fn = getattr(nn, fn_name)(kernel_size=2, output_size=1)\n             x = torch.full([1, 1] + num_dim * [3], nan, device=device, dtype=dtype, requires_grad=True)\n             res = fn(x)\n@@ -1439,13 +1439,13 @@ def test_fractional_max_pool_nan_inf(self, device, dtype):\n     def test_pooling_zero_stride(self, device):\n         for op in ('max', 'avg'):\n             for num_dim in [1, 2, 3]:\n-                fn_name = '{}_pool{}d'.format(op, num_dim)\n+                fn_name = f'{op}_pool{num_dim}d'\n                 fn = getattr(F, fn_name)\n                 x = torch.ones([1, 2] + num_dim * [4], device=device, dtype=torch.float)\n                 self.assertRaisesRegex(RuntimeError, r\"stride should not be zero|stride must be greater than zero\",\n                                        lambda: fn(x, kernel_size=2, stride=0))\n \n-                fn_module_name = '{}Pool{}d'.format(op.title(), num_dim)\n+                fn_module_name = f'{op.title()}Pool{num_dim}d'\n                 fn_module = getattr(nn, fn_module_name)(kernel_size=2, stride=0)\n                 self.assertRaisesRegex(RuntimeError, r\"stride should not be zero|stride must be greater than zero\",\n                                        lambda: fn_module(x))\n@@ -1456,7 +1456,7 @@ def test_pooling_zero_stride(self, device):\n     def test_pool_large_size(self, device, dtype):\n         for op in ('max', 'avg'):\n             for num_dim in [1, 2, 3]:\n-                fn_name = '{}_pool{}d'.format(op, num_dim)\n+                fn_name = f'{op}_pool{num_dim}d'\n                 fn = getattr(F, fn_name)\n                 # 16777217 is the smallest integer not expressible in float32\n                 x = torch.ones([1, 1, 16777217] + (num_dim - 1) * [1],\n@@ -1486,7 +1486,7 @@ def helper(pool):\n     def test_pool_invalid_size(self, device, dtype):\n         for op in ('max', 'avg'):\n             for num_dim in [1, 2, 3]:\n-                fn_name = '{}_pool{}d'.format(op, num_dim)\n+                fn_name = f'{op}_pool{num_dim}d'\n                 if op == 'max':\n                     # New implementation without indices supports empty tensors\n                     # TODO(Heitor) change once with_indices code is updated\ndiff --git a/test/onnx/dynamo/test_exporter_api.py b/test/onnx/dynamo/test_exporter_api.py\nindex ef0ee639495bcd..6a1fff67c61fc9 100644\n--- a/test/onnx/dynamo/test_exporter_api.py\n+++ b/test/onnx/dynamo/test_exporter_api.py\n@@ -29,11 +29,11 @@ def forward(self, x):\n class TestExportOptionsAPI(common_utils.TestCase):\n     def test_opset_version_default(self):\n         options = ResolvedExportOptions(None)\n-        self.assertEquals(options.opset_version, _DEFAULT_OPSET_VERSION)\n+        self.assertEqual(options.opset_version, _DEFAULT_OPSET_VERSION)\n \n     def test_opset_version_explicit(self):\n         options = ResolvedExportOptions(ExportOptions(opset_version=3000))\n-        self.assertEquals(options.opset_version, 3000)\n+        self.assertEqual(options.opset_version, 3000)\n \n     def test_raise_on_invalid_argument_type(self):\n         expected_exception_type = roar.BeartypeException\n@@ -60,12 +60,12 @@ def test_dynamic_shapes_explicit(self):\n \n     def test_logger_default(self):\n         options = ResolvedExportOptions(None)\n-        self.assertEquals(options.logger, logging.getLogger().getChild(\"torch.onnx\"))\n+        self.assertEqual(options.logger, logging.getLogger().getChild(\"torch.onnx\"))\n \n     def test_logger_explicit(self):\n         options = ResolvedExportOptions(ExportOptions(logger=logging.getLogger()))\n-        self.assertEquals(options.logger, logging.getLogger())\n-        self.assertNotEquals(options.logger, logging.getLogger().getChild(\"torch.onnx\"))\n+        self.assertEqual(options.logger, logging.getLogger())\n+        self.assertNotEqual(options.logger, logging.getLogger().getChild(\"torch.onnx\"))\n \n \n class TestDynamoExportAPI(common_utils.TestCase):\n@@ -111,8 +111,8 @@ def serialize(\n             dynamo_export(SampleModel(), torch.randn(1, 1, 2)).save(\n                 path, serializer=CustomSerializer()\n             )\n-            with open(path, \"r\") as fp:\n-                self.assertEquals(fp.read(), expected_buffer)\n+            with open(path) as fp:\n+                self.assertEqual(fp.read(), expected_buffer)\n \n     def test_save_to_file_using_specified_serializer_without_inheritance(self):\n         expected_buffer = \"I am not actually ONNX\"\n@@ -130,8 +130,8 @@ def serialize(\n             dynamo_export(SampleModel(), torch.randn(1, 1, 2)).save(\n                 path, serializer=CustomSerializer()\n             )\n-            with open(path, \"r\") as fp:\n-                self.assertEquals(fp.read(), expected_buffer)\n+            with open(path) as fp:\n+                self.assertEqual(fp.read(), expected_buffer)\n \n     def test_save_sarif_log_to_file_with_successful_export(self):\n         with common_utils.TemporaryFileName() as path:\ndiff --git a/test/onnx/test_pytorch_onnx_onnxruntime.py b/test/onnx/test_pytorch_onnx_onnxruntime.py\nindex 50c4842ac67e23..0d5ec9a3e74d41 100644\n--- a/test/onnx/test_pytorch_onnx_onnxruntime.py\n+++ b/test/onnx/test_pytorch_onnx_onnxruntime.py\n@@ -749,7 +749,7 @@ def forward(\n     def test_logit(self):\n         class Logit(torch.nn.Module):\n             def __init__(self, eps):\n-                super(Logit, self).__init__()\n+                super().__init__()\n                 self.eps = eps\n \n             def forward(self, x):\n@@ -4126,7 +4126,7 @@ def forward(self, input, indices, src):\n     def test_scatter_reduce(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x, index, input):\n                 y_max = input.scatter_reduce(0, index, x, reduce=\"amax\")\n@@ -4148,7 +4148,7 @@ def forward(self, x, index, input):\n     def test_scatter_reduce_self_rank_zero(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x, index, input):\n                 y_max = input.scatter_reduce(0, index, x, reduce=\"amax\")\ndiff --git a/torch/__init__.py b/torch/__init__.py\nindex 236e6db366c0ab..a9aa12ac5d194c 100644\n--- a/torch/__init__.py\n+++ b/torch/__init__.py\n@@ -16,8 +16,6 @@\n import textwrap\n import ctypes\n import inspect\n-if sys.version_info < (3,):\n-    raise Exception(\"Python 2 has reached end-of-life and is no longer supported by PyTorch.\")\n \n # multipy/deploy is setting this import before importing torch, this is the most\n # reliable way we have to detect if we're running within deploy.\n@@ -1731,7 +1729,7 @@ def _sparse_coo_tensor_unsafe(*args, **kwargs):\n if not _running_with_deploy():\n     from torch import compiler as compiler\n \n-    class _TritonLibrary(object):\n+    class _TritonLibrary:\n         lib = torch.library.Library(\"triton\", \"DEF\")\n         ops_table: Dict[Tuple[str, str], Callable] = {}\n \ndiff --git a/torch/_custom_op/impl.py b/torch/_custom_op/impl.py\nindex bb10e7da20f16a..0bb3c6567159a2 100644\n--- a/torch/_custom_op/impl.py\n+++ b/torch/_custom_op/impl.py\n@@ -183,7 +183,7 @@ class CustomOp:\n     \"\"\"\n \n     def __init__(self, lib, cpp_ns, schema, operator_name, ophandle, *, _private_access=False):\n-        super(CustomOp, self).__init__()\n+        super().__init__()\n         if not _private_access:\n             raise RuntimeError(\n                 \"The CustomOp constructor is private and we do not guarantee \"\ndiff --git a/torch/_linalg_utils.py b/torch/_linalg_utils.py\nindex 3a81fc6c27adce..6da8cefc296c09 100644\n--- a/torch/_linalg_utils.py\n+++ b/torch/_linalg_utils.py\n@@ -15,7 +15,7 @@ def is_sparse(A):\n \n     error_str = \"expected Tensor\"\n     if not torch.jit.is_scripting():\n-        error_str += \" but got {}\".format(type(A))\n+        error_str += f\" but got {type(A)}\"\n     raise TypeError(error_str)\n \n \ndiff --git a/torch/_lobpcg.py b/torch/_lobpcg.py\nindex 4efdcb105c8960..aaed2d951b2ff9 100644\n--- a/torch/_lobpcg.py\n+++ b/torch/_lobpcg.py\n@@ -728,18 +728,18 @@ def __init__(\n \n     def __str__(self):\n         lines = [\"LOPBCG:\"]\n-        lines += [\"  iparams={}\".format(self.iparams)]\n-        lines += [\"  fparams={}\".format(self.fparams)]\n-        lines += [\"  bparams={}\".format(self.bparams)]\n-        lines += [\"  ivars={}\".format(self.ivars)]\n-        lines += [\"  fvars={}\".format(self.fvars)]\n-        lines += [\"  bvars={}\".format(self.bvars)]\n-        lines += [\"  tvars={}\".format(self.tvars)]\n-        lines += [\"  A={}\".format(self.A)]\n-        lines += [\"  B={}\".format(self.B)]\n-        lines += [\"  iK={}\".format(self.iK)]\n-        lines += [\"  X={}\".format(self.X)]\n-        lines += [\"  E={}\".format(self.E)]\n+        lines += [f\"  iparams={self.iparams}\"]\n+        lines += [f\"  fparams={self.fparams}\"]\n+        lines += [f\"  bparams={self.bparams}\"]\n+        lines += [f\"  ivars={self.ivars}\"]\n+        lines += [f\"  fvars={self.fvars}\"]\n+        lines += [f\"  bvars={self.bvars}\"]\n+        lines += [f\"  tvars={self.tvars}\"]\n+        lines += [f\"  A={self.A}\"]\n+        lines += [f\"  B={self.B}\"]\n+        lines += [f\"  iK={self.iK}\"]\n+        lines += [f\"  X={self.X}\"]\n+        lines += [f\"  E={self.E}\"]\n         r = \"\"\n         for line in lines:\n             r += line + \"\\n\"\n@@ -1133,7 +1133,7 @@ def _get_ortho(self, U, V):\n                 R_norm = torch.norm(R)\n                 # https://github.com/pytorch/pytorch/issues/33810 workaround:\n                 rerr = float(R_norm) * float(BU_norm * U_norm) ** -1\n-                vkey = \"ortho_UBUmI_rerr[{}, {}]\".format(i, j)\n+                vkey = f\"ortho_UBUmI_rerr[{i}, {j}]\"\n                 self.fvars[vkey] = rerr\n                 if rerr < tau_ortho:\n                     break\n@@ -1141,7 +1141,7 @@ def _get_ortho(self, U, V):\n             VBU_norm = torch.norm(VBU)\n             U_norm = torch.norm(U)\n             rerr = float(VBU_norm) * float(BV_norm * U_norm) ** -1\n-            vkey = \"ortho_VBU_rerr[{}]\".format(i)\n+            vkey = f\"ortho_VBU_rerr[{i}]\"\n             self.fvars[vkey] = rerr\n             if rerr < tau_ortho:\n                 break\ndiff --git a/torch/_logging/_internal.py b/torch/_logging/_internal.py\nindex c4e7fcd735a0fd..36aff78b8df428 100644\n--- a/torch/_logging/_internal.py\n+++ b/torch/_logging/_internal.py\n@@ -442,7 +442,7 @@ def _invalid_settings_err_msg(settings):\n     return msg\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def _parse_log_settings(settings):\n     if settings == \"\":\n         return dict()\ndiff --git a/torch/_lowrank.py b/torch/_lowrank.py\nindex 0c55a566ba86de..c6dedc00a1f8a2 100644\n--- a/torch/_lowrank.py\n+++ b/torch/_lowrank.py\n@@ -267,7 +267,7 @@ def pca_lowrank(\n             \" and not greater than min(m, n)={}\".format(q, min(m, n))\n         )\n     if not (niter >= 0):\n-        raise ValueError(\"niter(={}) must be non-negative integer\".format(niter))\n+        raise ValueError(f\"niter(={niter}) must be non-negative integer\")\n \n     dtype = _utils.get_floating_dtype(A)\n \ndiff --git a/torch/_ops.py b/torch/_ops.py\nindex a7138fbf143283..5903477b9103d1 100644\n--- a/torch/_ops.py\n+++ b/torch/_ops.py\n@@ -745,7 +745,7 @@ def __getattr__(self, op_name):\n         # Get the op `my_namespace::my_op` if available. This will also check\n         # for overloads and raise an exception if there are more than one.\n         namespace_name = self.name\n-        qualified_op_name = \"{}::{}\".format(namespace_name, op_name)\n+        qualified_op_name = f\"{namespace_name}::{op_name}\"\n         try:\n             op, overload_names = torch._C._jit_get_operation(qualified_op_name)\n         except RuntimeError as e:\ndiff --git a/torch/_python_dispatcher.py b/torch/_python_dispatcher.py\nindex c420ad044f716f..bfd208eddb9e8c 100644\n--- a/torch/_python_dispatcher.py\n+++ b/torch/_python_dispatcher.py\n@@ -116,7 +116,7 @@ def register(self, dispatchKeys):\n     \"\"\"\n \n     def _format_line(self, key, kernel):\n-        return \"{:<15} {}\\n\".format(key, kernel)\n+        return f\"{key:<15} {kernel}\\n\"\n \n     \"\"\"\n     Helper function to print a table header.\n@@ -136,7 +136,7 @@ def _format_header(self, header):\n     \"\"\"\n \n     def rawRegistrations(self):\n-        return C._dispatch_dump(\"{}::{}\".format(self.namespace, self.name))  # type: ignore[attr-defined]\n+        return C._dispatch_dump(f\"{self.namespace}::{self.name}\")  # type: ignore[attr-defined]\n \n     \"\"\"\n     Returns raw output of computed dispatch table for debugging only.\n@@ -144,7 +144,7 @@ def rawRegistrations(self):\n     \"\"\"\n \n     def rawDispatchTable(self):\n-        return C._dispatch_dump_table(\"{}::{}\".format(self.namespace, self.name))  # type: ignore[attr-defined]\n+        return C._dispatch_dump_table(f\"{self.namespace}::{self.name}\")  # type: ignore[attr-defined]\n \n     \"\"\"\n     Returns a table(str) including all the registrations from users.\ndiff --git a/torch/_tensor.py b/torch/_tensor.py\nindex 0179df79860a0f..b78c1a4ecb3a72 100644\n--- a/torch/_tensor.py\n+++ b/torch/_tensor.py\n@@ -1380,9 +1380,7 @@ def __dlpack_device__(self) -> Tuple[enum.IntEnum, int]:\n         elif self.device.type == \"xpu\":\n             device_type = DLDeviceType.kDLOneAPI\n         else:\n-            raise ValueError(\n-                \"Unknown device type {} for Dlpack\".format(torch_device_type)\n-            )\n+            raise ValueError(f\"Unknown device type {torch_device_type} for Dlpack\")\n         return (device_type, idx)\n \n     __module__ = \"torch\"\ndiff --git a/torch/_tensor_str.py b/torch/_tensor_str.py\nindex 7b519ad8ae36dd..fabaaa4f3347e2 100644\n--- a/torch/_tensor_str.py\n+++ b/torch/_tensor_str.py\n@@ -130,7 +130,7 @@ def __init__(self, tensor):\n \n         if not self.floating_dtype:\n             for value in tensor_view:\n-                value_str = \"{}\".format(value)\n+                value_str = f\"{value}\"\n                 self.max_width = max(self.max_width, len(value_str))\n \n         else:\n@@ -161,13 +161,11 @@ def __init__(self, tensor):\n                 ):\n                     self.sci_mode = True\n                     for value in nonzero_finite_vals:\n-                        value_str = (\n-                            (\"{{:.{}e}}\").format(PRINT_OPTS.precision).format(value)\n-                        )\n+                        value_str = f\"{{:.{PRINT_OPTS.precision}e}}\".format(value)\n                         self.max_width = max(self.max_width, len(value_str))\n                 else:\n                     for value in nonzero_finite_vals:\n-                        value_str = (\"{:.0f}\").format(value)\n+                        value_str = f\"{value:.0f}\"\n                         self.max_width = max(self.max_width, len(value_str) + 1)\n             else:\n                 # Check if scientific representation should be used.\n@@ -178,15 +176,11 @@ def __init__(self, tensor):\n                 ):\n                     self.sci_mode = True\n                     for value in nonzero_finite_vals:\n-                        value_str = (\n-                            (\"{{:.{}e}}\").format(PRINT_OPTS.precision).format(value)\n-                        )\n+                        value_str = f\"{{:.{PRINT_OPTS.precision}e}}\".format(value)\n                         self.max_width = max(self.max_width, len(value_str))\n                 else:\n                     for value in nonzero_finite_vals:\n-                        value_str = (\n-                            (\"{{:.{}f}}\").format(PRINT_OPTS.precision).format(value)\n-                        )\n+                        value_str = f\"{{:.{PRINT_OPTS.precision}f}}\".format(value)\n                         self.max_width = max(self.max_width, len(value_str))\n \n         if PRINT_OPTS.sci_mode is not None:\n@@ -204,13 +198,13 @@ def format(self, value):\n                     .format(value)\n                 )\n             elif self.int_mode:\n-                ret = \"{:.0f}\".format(value)\n+                ret = f\"{value:.0f}\"\n                 if not (math.isinf(value) or math.isnan(value)):\n                     ret += \".\"\n             else:\n-                ret = (\"{{:.{}f}}\").format(PRINT_OPTS.precision).format(value)\n+                ret = f\"{{:.{PRINT_OPTS.precision}f}}\".format(value)\n         else:\n-            ret = \"{}\".format(value)\n+            ret = f\"{value}\"\n         return (self.max_width - len(ret)) * \" \" + ret\n \n \n@@ -608,15 +602,15 @@ def indented_str(s, indent):\n         name = type(inp.grad_fn).__name__\n         if name == \"CppFunction\":\n             name = inp.grad_fn.name().rsplit(\"::\", 1)[-1]\n-        suffixes.append(\"grad_fn=<{}>\".format(name))\n+        suffixes.append(f\"grad_fn=<{name}>\")\n     elif inp.requires_grad:\n         suffixes.append(\"requires_grad=True\")\n \n     if self.has_names():\n-        suffixes.append(\"names={}\".format(self.names))\n+        suffixes.append(f\"names={self.names}\")\n \n     if tangent is not None:\n-        suffixes.append(\"tangent={}\".format(tangent))\n+        suffixes.append(f\"tangent={tangent}\")\n \n     string_repr = _add_suffixes(\n         prefix + tensor_str, suffixes, indent, force_newline=self.is_sparse\ndiff --git a/torch/_torch_docs.py b/torch/_torch_docs.py\nindex bafe20da6d63ef..d56ea422fe3d83 100644\n--- a/torch/_torch_docs.py\n+++ b/torch/_torch_docs.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"Adds docstrings to functions defined in the torch._C\"\"\"\n \n import re\ndiff --git a/torch/_utils.py b/torch/_utils.py\nindex 3c1ec83de67999..deaee30764ea4e 100644\n--- a/torch/_utils.py\n+++ b/torch/_utils.py\n@@ -375,9 +375,7 @@ def _rebuild_qtensor(\n             device=storage.device,\n         )\n     else:\n-        raise RuntimeError(\n-            \"Can't deserialize quantized tensor with qscheme {}\".format(qscheme)\n-        )\n+        raise RuntimeError(f\"Can't deserialize quantized tensor with qscheme {qscheme}\")\n     tensor.set_(storage, storage_offset, size, stride)\n     tensor.requires_grad = requires_grad\n     # NB: This line exists only for backwards compatibility; the\n@@ -782,7 +780,7 @@ def _get_device_index(\n     device_idx: Optional[int] = None\n     if isinstance(device, torch.device):\n         if not allow_cpu and device.type == \"cpu\":\n-            raise ValueError(\"Expected a non cpu device, but got: {}\".format(device))\n+            raise ValueError(f\"Expected a non cpu device, but got: {device}\")\n         device_idx = -1 if device.type == \"cpu\" else device.index\n     if isinstance(device, int):\n         device_idx = device\ndiff --git a/torch/autograd/function.py b/torch/autograd/function.py\nindex 292f112e476c0f..0947207fde510a 100644\n--- a/torch/autograd/function.py\n+++ b/torch/autograd/function.py\n@@ -290,7 +290,7 @@ def __init__(cls, name, bases, attrs):\n         backward_fn = type(name + 'Backward', (BackwardCFunction,), {'_forward_cls': cls})\n         cls._backward_cls = backward_fn\n \n-        super(FunctionMeta, cls).__init__(name, bases, attrs)\n+        super().__init__(name, bases, attrs)\n \n \n class _SingleLevelFunction(_C._FunctionBase, FunctionCtx, _HookMixin, metaclass=FunctionMeta):\ndiff --git a/torch/autograd/functional.py b/torch/autograd/functional.py\nindex efddb5fde0eda3..c69ad4aaf69d84 100644\n--- a/torch/autograd/functional.py\n+++ b/torch/autograd/functional.py\n@@ -98,7 +98,7 @@ def _validate_v(v, other, is_other_tuple):\n     # Both are assumed to be tuples of Tensors\n     if len(other) != len(v):\n         if is_other_tuple:\n-            raise RuntimeError(\"v is a tuple of invalid length: should be {} but got {}.\".format(len(other), len(v)))\n+            raise RuntimeError(f\"v is a tuple of invalid length: should be {len(other)} but got {len(v)}.\")\n         else:\n             raise RuntimeError(\"The given v should contain a single Tensor.\")\n \n@@ -106,7 +106,7 @@ def _validate_v(v, other, is_other_tuple):\n         if el_v.size() != el_other.size():\n             prepend = \"\"\n             if is_other_tuple:\n-                prepend = \"Entry {} in \".format(idx)\n+                prepend = f\"Entry {idx} in \"\n             raise RuntimeError(\"{}v has invalid size: should be {} but got {}.\".format(\n                                prepend, el_other.size(), el_v.size()))\n \n@@ -175,7 +175,7 @@ def _fill_in_zeros(grads, refs, strict, create_graph, stage):\n     # strict and create graph allow us to detect when it is appropriate to raise an error\n     # stage gives us information of which backward call we consider to give good error message\n     if stage not in [\"back\", \"back_trick\", \"double_back\", \"double_back_trick\"]:\n-        raise RuntimeError(\"Invalid stage argument '{}' to _fill_in_zeros\".format(stage))\n+        raise RuntimeError(f\"Invalid stage argument '{stage}' to _fill_in_zeros\")\n \n     res: Tuple[torch.Tensor, ...] = tuple()\n     for i, grads_i in enumerate(grads):\ndiff --git a/torch/autograd/gradcheck.py b/torch/autograd/gradcheck.py\nindex 18382806c0d4df..bf605a561e34b6 100644\n--- a/torch/autograd/gradcheck.py\n+++ b/torch/autograd/gradcheck.py\n@@ -1035,11 +1035,11 @@ def _test_undefined_backward_mode(func, outputs, inputs) -> bool:\n         raise GradcheckError(\"no Tensors requiring grad found in input\")\n \n     def warn_bc_breaking():\n-        warnings.warn((\n+        warnings.warn(\n             'Backwards compatibility: New undefined gradient support checking '\n             'feature is enabled by default, but it may break existing callers '\n             'of this function. If this is true for you, you can call this '\n-            'function with \"check_undefined_grad=False\" to disable the feature'))\n+            'function with \"check_undefined_grad=False\" to disable the feature')\n \n     def check_undefined_grad_support(output_to_check):\n         grads_output = [torch.zeros_like(o, memory_format=torch.legacy_contiguous_format) for o in output_to_check]\n@@ -1048,18 +1048,18 @@ def check_undefined_grad_support(output_to_check):\n                                               grads_output, allow_unused=True)\n         except RuntimeError as e:\n             warn_bc_breaking()\n-            raise GradcheckError((\n+            raise GradcheckError(\n                 'Expected backward function to handle undefined output grads. '\n                 'Please look at \"Notes about undefined output gradients\" in '\n-                '\"tools/autograd/derivatives.yaml\"')) from e\n+                '\"tools/autograd/derivatives.yaml\"') from e\n \n         for gi, i in zip(grads_input, diff_input_list):\n             if (gi is not None) and (not gi.eq(0).all()):\n                 warn_bc_breaking()\n-                raise GradcheckError((\n+                raise GradcheckError(\n                     'Expected all input grads to be undefined or zero when all output grads are undefined '\n                     'or zero. Please look at \"Notes about undefined output gradients\" in '\n-                    '\"tools/autograd/derivatives.yaml\"'))\n+                    '\"tools/autograd/derivatives.yaml\"')\n         return True\n \n     # All backward functions must work properly if all output grads are undefined\n@@ -1525,9 +1525,9 @@ def gradcheck(\n         else:\n             check_sparse_nnz = masked\n     else:\n-        warnings.warn((\n+        warnings.warn(\n             'Backwards compatibility: check_sparse_nnz is deprecated, it will be removed in a future version of PyTorch.'\n-            f' Use masked={check_sparse_nnz} instead.'))\n+            f' Use masked={check_sparse_nnz} instead.')\n         if masked is None:\n             masked = check_sparse_nnz\n         elif check_sparse_nnz != masked:\ndiff --git a/torch/autograd/graph.py b/torch/autograd/graph.py\nindex 182a5e8f56fe0a..1b6b04c8226c91 100644\n--- a/torch/autograd/graph.py\n+++ b/torch/autograd/graph.py\n@@ -144,7 +144,7 @@ def increment_version(tensor):\n     \"\"\"\n     torch._C._increment_version(tensor)\n \n-class saved_tensors_hooks():\n+class saved_tensors_hooks:\n     \"\"\"Context-manager that sets a pair of pack / unpack hooks for saved tensors.\n \n     Use this context-manager to define how intermediary results of an operation\n@@ -434,7 +434,7 @@ def _get_tid(t) -> Tuple[int, int, int]:\n def _get_sid(t) -> Tuple[int, int]:\n     return (t.data_ptr(), t._version)\n \n-class _Handle():\n+class _Handle:\n     pass\n \n class _swap_with_cloned(saved_tensors_hooks):\n@@ -509,7 +509,7 @@ def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n         rs = func(*args, **kwargs)\n         return rs\n \n-class _AllowMutationOnSavedContext():\n+class _AllowMutationOnSavedContext:\n     def __init__(self):\n         self.cloned: weakref.WeakKeyDictionary = weakref.WeakKeyDictionary()\n         self.original: weakref.WeakKeyDictionary = weakref.WeakKeyDictionary()\ndiff --git a/torch/autograd/profiler_util.py b/torch/autograd/profiler_util.py\nindex 6adba82dbea0f7..a76ff4b103e7be 100644\n--- a/torch/autograd/profiler_util.py\n+++ b/torch/autograd/profiler_util.py\n@@ -206,14 +206,13 @@ def export_chrome_trace(self, path):\n                 if evt.trace_name is None:\n                     continue\n                 f.write(\n-                    '{\"name\": \"%s\", '\n+                    '{{\"name\": \"{}\", '\n                     '\"ph\": \"X\", '\n-                    '\"ts\": %s, '\n-                    '\"dur\": %s, '\n-                    '\"tid\": %s, '\n+                    '\"ts\": {}, '\n+                    '\"dur\": {}, '\n+                    '\"tid\": {}, '\n                     '\"pid\": \"CPU functions\", '\n-                    '\"args\": {}}, '\n-                    % (\n+                    '\"args\": {{}}}}, '.format(\n                         evt.trace_name,\n                         evt.time_range.start,\n                         evt.time_range.elapsed_us(),\n@@ -225,14 +224,14 @@ def export_chrome_trace(self, path):\n                 for k in evt.kernels:\n                     # 's' and 'f' draw Flow arrows from\n                     # the CPU launch to the GPU kernel\n-                    f.write('{\"name\": \"%s\", '\n+                    f.write('{{\"name\": \"{}\", '\n                             '\"ph\": \"s\", '\n-                            '\"ts\": %s, '\n-                            '\"tid\": %s, '\n+                            '\"ts\": {}, '\n+                            '\"tid\": {}, '\n                             '\"pid\": \"CPU functions\", '\n-                            '\"id\": %s, '\n+                            '\"id\": {}, '\n                             '\"cat\": \"cpu_to_cuda\", '\n-                            '\"args\": {}}, ' % (evt.trace_name, evt.time_range.start,\n+                            '\"args\": {{}}}}, '.format(evt.trace_name, evt.time_range.start,\n                                                evt.thread, next_id))\n                     # Note: use torch.profiler to get device kernel trace\n                     next_id += 1\n@@ -319,15 +318,15 @@ def _format_time(time_us):\n     US_IN_SECOND = 1000.0 * 1000.0\n     US_IN_MS = 1000.0\n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n def _format_time_share(time_us, total_time_us):\n     \"\"\"Defines how to format time in FunctionEvent\"\"\"\n     if total_time_us == 0:\n-        assert time_us == 0, \"Expected time_us == 0 but got {}\".format(time_us)\n+        assert time_us == 0, f\"Expected time_us == 0 but got {time_us}\"\n         return \"NaN\"\n     return '{:.2f}%'.format(time_us * 100.0 / total_time_us)\n \n@@ -804,7 +803,7 @@ def auto_scale_flops(flops):\n                 raw_flops.append(evt.flops)\n         if len(raw_flops) != 0:\n             (flops_scale, flops_header) = auto_scale_flops(min(raw_flops))\n-            headers.append('Total {}'.format(flops_header))\n+            headers.append(f'Total {flops_header}')\n             add_column(flops_column_width)\n         else:\n             with_flops = False  # can't find any valid flops\n@@ -907,7 +906,7 @@ def trim_path(path, src_column_width):\n             if evt.flops <= 0:\n                 row_values.append(\"--\")\n             else:\n-                row_values.append('{0:8.3f}'.format(evt.flops * flops_scale))\n+                row_values.append('{:8.3f}'.format(evt.flops * flops_scale))\n         if has_stack:\n             src_field = \"\"\n             if len(evt.stack) > 0:\n@@ -923,7 +922,7 @@ def trim_path(path, src_column_width):\n             append(row_format.format(*empty_headers))\n \n     append(header_sep)\n-    append(\"Self CPU time total: {}\".format(_format_time(sum_self_cpu_time_total)))\n+    append(f\"Self CPU time total: {_format_time(sum_self_cpu_time_total)}\")\n     if has_cuda_time:\n-        append(\"Self CUDA time total: {}\".format(_format_time(sum_self_cuda_time_total)))\n+        append(f\"Self CUDA time total: {_format_time(sum_self_cuda_time_total)}\")\n     return ''.join(result)\ndiff --git a/torch/backends/_nnapi/serializer.py b/torch/backends/_nnapi/serializer.py\nindex 4340b9cdeee174..933b2a97144f31 100644\n--- a/torch/backends/_nnapi/serializer.py\n+++ b/torch/backends/_nnapi/serializer.py\n@@ -257,7 +257,7 @@ def broadcast_shapes(shape1, shape2):\n         elif d1 == d2:\n             ret.append(d1)\n         else:\n-            raise Exception(\"Cannot broadcast shapes: {} and {}\".format(shape1, shape2))\n+            raise Exception(f\"Cannot broadcast shapes: {shape1} and {shape2}\")\n     return tuple(ret)\n \n \n@@ -607,8 +607,7 @@ def transpose_for_broadcast(self, in0_id, in0_oper, in1_id, in1_oper):\n             return (in0_id, in0_oper) + self.transpose_to_nhwc(in1_id, in1_oper)\n \n         raise Exception(\n-            \"Automatic transpose not supported for dim_orders: %r, %r\" %\n-            (in0_oper.dim_order, in1_oper.dim_order))\n+            f\"Automatic transpose not supported for dim_orders: {in0_oper.dim_order!r}, {in1_oper.dim_order!r}\")\n \n     def get_size_arg(self, jitval):\n         ctype, value = self.get_constant_value(jitval)\n@@ -867,7 +866,7 @@ def serialize_ints(ints):\n     def add_node(self, node):\n         adder = self.ADDER_MAP.get(node.kind())\n         if not adder:\n-            raise Exception(\"Unsupported node kind (%r) in node %r\" % (node.kind(), node))\n+            raise Exception(f\"Unsupported node kind ({node.kind()!r}) in node {node!r}\")\n         adder(self, node)\n \n     def _identity(self, node):\ndiff --git a/torch/backends/cudnn/rnn.py b/torch/backends/cudnn/rnn.py\nindex 706244e2bc3e86..3c5740622f2840 100644\n--- a/torch/backends/cudnn/rnn.py\n+++ b/torch/backends/cudnn/rnn.py\n@@ -18,7 +18,7 @@ def get_cudnn_mode(mode):\n     elif mode == 'GRU':\n         return int(_cudnn.RNNMode.gru)\n     else:\n-        raise Exception(\"Unknown mode: {}\".format(mode))\n+        raise Exception(f\"Unknown mode: {mode}\")\n \n \n # NB: We don't actually need this class anymore (in fact, we could serialize the\ndiff --git a/torch/backends/mps/__init__.py b/torch/backends/mps/__init__.py\nindex 28fda2bfce87a6..97370e8ad65eef 100644\n--- a/torch/backends/mps/__init__.py\n+++ b/torch/backends/mps/__init__.py\n@@ -13,13 +13,13 @@ def is_built() -> bool:\n     return torch._C._has_mps\n \n \n-@_lru_cache()\n+@_lru_cache\n def is_available() -> bool:\n     r\"\"\"Returns a bool indicating if MPS is currently available.\"\"\"\n     return torch._C._mps_is_available()\n \n \n-@_lru_cache()\n+@_lru_cache\n def is_macos13_or_newer(minor: int = 0) -> bool:\n     r\"\"\"Returns a bool indicating whether MPS is running on MacOS 13 or newer.\"\"\"\n     return torch._C._mps_is_on_macos_13_or_newer(minor)\ndiff --git a/torch/backends/opt_einsum/__init__.py b/torch/backends/opt_einsum/__init__.py\nindex 5a280b08b4f911..ab8c4bc193b952 100644\n--- a/torch/backends/opt_einsum/__init__.py\n+++ b/torch/backends/opt_einsum/__init__.py\n@@ -11,7 +11,7 @@\n     _opt_einsum = None\n \n \n-@_lru_cache()\n+@_lru_cache\n def is_available() -> bool:\n     r\"\"\"Returns a bool indicating if opt_einsum is currently available.\"\"\"\n     return _opt_einsum is not None\ndiff --git a/torch/backends/quantized/__init__.py b/torch/backends/quantized/__init__.py\nindex c0d60916084f8f..70b7db458f5bbd 100644\n--- a/torch/backends/quantized/__init__.py\n+++ b/torch/backends/quantized/__init__.py\n@@ -17,7 +17,7 @@ def _get_qengine_id(qengine: str) -> int:\n         ret = 4\n     else:\n         ret = -1\n-        raise RuntimeError(\"{} is not a valid value for quantized engine\".format(qengine))\n+        raise RuntimeError(f\"{qengine} is not a valid value for quantized engine\")\n     return ret\n \n # This function should correspond to the enums present in c10/core/QEngine.h\ndiff --git a/torch/backends/xeon/run_cpu.py b/torch/backends/xeon/run_cpu.py\nindex c220d9e32ccebf..a0764cb8339a58 100644\n--- a/torch/backends/xeon/run_cpu.py\n+++ b/torch/backends/xeon/run_cpu.py\n@@ -138,7 +138,7 @@\n logging.basicConfig(level=logging.INFO, format=format_str)\n logger = logging.getLogger(__name__)\n \n-class _CPUinfo():\n+class _CPUinfo:\n     \"\"\"\n     Get CPU information, such as cores list and NUMA information.\n     \"\"\"\n@@ -239,7 +239,7 @@ def numa_aware_check(self, core_list):\n             raise RuntimeError(\"invalid number of NUMA nodes; please make sure numa_ids >= 1\")\n         return numa_ids\n \n-class _Launcher():\n+class _Launcher:\n     r\"\"\"\n      Class for launcher\n     \"\"\"\ndiff --git a/torch/contrib/_tensorboard_vis.py b/torch/contrib/_tensorboard_vis.py\nindex 94b2f64f789121..a15f865bf528cb 100644\n--- a/torch/contrib/_tensorboard_vis.py\n+++ b/torch/contrib/_tensorboard_vis.py\n@@ -75,7 +75,7 @@ def visualize_graph_executor(state, name_prefix, pb_graph, inline_graph):\n                   executors_it=iter(state.autograd_fallback.executors()))\n \n     for i, (arg_spec, plan) in enumerate(state.execution_plans.items()):\n-        subgraph_name = name_prefix + 'plan{}/'.format(i)\n+        subgraph_name = name_prefix + f'plan{i}/'\n \n         # Create a disconnected node that will keep information regarding the input\n         # types of this trace. This is unfortunately a bit too verbose to be included\ndiff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py\nindex 4c46e26698c4d8..98e0cb54e9f208 100644\n--- a/torch/cuda/__init__.py\n+++ b/torch/cuda/__init__.py\n@@ -298,7 +298,7 @@ class cudaStatus:\n class CudaError(RuntimeError):\n     def __init__(self, code: int) -> None:\n         msg = _cudart.cudaGetErrorString(_cudart.cudaError(code))\n-        super().__init__('{0} ({1})'.format(msg, code))\n+        super().__init__(f'{msg} ({code})')\n \n \n def check_error(res: int) -> None:\n@@ -1140,7 +1140,7 @@ def _dtype(self):\n torch._storage_classes.add(ComplexFloatStorage)\n \n \n-class _WrappedTritonKernel(object):\n+class _WrappedTritonKernel:\n     \"\"\" Just a simple wrapper to store some metadata for testing purposes.\n     \"\"\"\n \ndiff --git a/torch/cuda/_memory_viz.py b/torch/cuda/_memory_viz.py\nindex f32fc361c28f64..0af30d3ac61d83 100644\n--- a/torch/cuda/_memory_viz.py\n+++ b/torch/cuda/_memory_viz.py\n@@ -248,7 +248,7 @@ def segsum(data):\n             finish_ = (start_ + size)\n             start = start_ // PAGE_SIZE\n             finish = (finish_ - 1) // PAGE_SIZE + 1\n-            m = chr((ord('a' if active else 'A') + (i % 26)))\n+            m = chr(ord('a' if active else 'A') + (i % 26))\n             for j in range(start, finish):\n                 s = max(start_, j * PAGE_SIZE)\n                 e = min(finish_, (j + 1) * PAGE_SIZE)\ndiff --git a/torch/cuda/_utils.py b/torch/cuda/_utils.py\nindex 0dfc56cf7faa39..8a40767c662b63 100644\n--- a/torch/cuda/_utils.py\n+++ b/torch/cuda/_utils.py\n@@ -27,9 +27,9 @@ def _get_device_index(device: Any, optional: bool = False,\n     if isinstance(device, torch.device):\n         if allow_cpu:\n             if device.type not in ['cuda', 'cpu']:\n-                raise ValueError('Expected a cuda or cpu device, but got: {}'.format(device))\n+                raise ValueError(f'Expected a cuda or cpu device, but got: {device}')\n         elif device.type != 'cuda':\n-            raise ValueError('Expected a cuda device, but got: {}'.format(device))\n+            raise ValueError(f'Expected a cuda device, but got: {device}')\n     if not torch.jit.is_scripting():\n         if isinstance(device, torch.cuda.device):\n             return device.idx\n@@ -44,6 +44,6 @@ def err_fn(obj, *args, **kwargs):\n             else:\n                 class_name = obj.__name__\n             raise RuntimeError(\n-                \"Tried to instantiate dummy base class {}\".format(class_name))\n+                f\"Tried to instantiate dummy base class {class_name}\")\n         return err_fn\n     return type(name, (object,), {\"__init__\": get_err_fn(True), \"__new__\": get_err_fn(False)})\ndiff --git a/torch/cuda/amp/grad_scaler.py b/torch/cuda/amp/grad_scaler.py\nindex 1dcd919995e50f..c3a05db71d538f 100644\n--- a/torch/cuda/amp/grad_scaler.py\n+++ b/torch/cuda/amp/grad_scaler.py\n@@ -139,8 +139,8 @@ def __init__(self,\n \n     def _check_scale_growth_tracker(self, funcname) -> Tuple[torch.Tensor, torch.Tensor]:\n         fix = \"This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.\"\n-        assert self._scale is not None, \"Attempted {} but _scale is None.  \".format(funcname) + fix\n-        assert self._growth_tracker is not None, \"Attempted {} but _growth_tracker is None.  \".format(funcname) + fix\n+        assert self._scale is not None, f\"Attempted {funcname} but _scale is None.  \" + fix\n+        assert self._growth_tracker is not None, f\"Attempted {funcname} but _growth_tracker is None.  \" + fix\n         return (self._scale, self._growth_tracker)\n \n     def _lazy_init_scale_growth_tracker(self, dev):\ndiff --git a/torch/cuda/graphs.py b/torch/cuda/graphs.py\nindex 1ce4b4754b8c95..865a07dc2709a7 100644\n--- a/torch/cuda/graphs.py\n+++ b/torch/cuda/graphs.py\n@@ -45,7 +45,7 @@ class CUDAGraph(torch._C._CUDAGraph):\n         This API is in beta and may change in future releases.\n     \"\"\"\n     def __new__(cls):\n-        return super(CUDAGraph, cls).__new__(cls)\n+        return super().__new__(cls)\n \n     def capture_begin(self, pool=None):\n         r\"\"\"\ndiff --git a/torch/cuda/memory.py b/torch/cuda/memory.py\nindex 8a34531e14d4b1..61b03deb83ef60 100644\n--- a/torch/cuda/memory.py\n+++ b/torch/cuda/memory.py\n@@ -474,7 +474,7 @@ def _format_size(sz, pref_sz):\n             prefix = new_prefix\n             sz //= 1024\n             pref_sz /= 1024\n-        return \"{:6d} {}\".format(sz, prefix)\n+        return f\"{sz:6d} {prefix}\"\n \n     def _format_count(cnt, pref_cnt):\n         prefixes = [\" \", \"K\", \"M\"]\n@@ -485,7 +485,7 @@ def _format_count(cnt, pref_cnt):\n             prefix = new_prefix\n             cnt //= 1000\n             pref_cnt /= 1000\n-        return \"{:7d} {} \".format(cnt, prefix)\n+        return f\"{cnt:7d} {prefix} \"\n \n     metrics_to_display = [\n         (\"allocated_bytes\", \"Allocated memory\", _format_size),\ndiff --git a/torch/cuda/streams.py b/torch/cuda/streams.py\nindex 52eab59a2fdb2f..8e5406a10a02d0 100644\n--- a/torch/cuda/streams.py\n+++ b/torch/cuda/streams.py\n@@ -29,10 +29,10 @@ class Stream(torch._C._CudaStreamBase):\n     def __new__(cls, device=None, priority=0, **kwargs):\n         # setting device manager is expensive, so we avoid it unless necessary\n         if device is None or (\"stream_id\" in kwargs and \"device_index\" in kwargs):\n-            return super(Stream, cls).__new__(cls, priority=priority, **kwargs)\n+            return super().__new__(cls, priority=priority, **kwargs)\n         else:\n             with torch.cuda.device(device):\n-                return super(Stream, cls).__new__(cls, priority=priority, **kwargs)\n+                return super().__new__(cls, priority=priority, **kwargs)\n \n     def wait_event(self, event):\n         r\"\"\"Makes all future work submitted to the stream wait for an event.\n@@ -108,7 +108,7 @@ def __hash__(self):\n         return hash((self.cuda_stream, self.device))\n \n     def __repr__(self):\n-        return ('<torch.cuda.Stream device={0} cuda_stream={1:#x}>'\n+        return ('<torch.cuda.Stream device={} cuda_stream={:#x}>'\n                 .format(self.device, self.cuda_stream))\n \n \n@@ -132,7 +132,7 @@ class ExternalStream(Stream):\n \n     def __new__(cls, stream_ptr, device=None, **kwargs):\n         with torch.cuda.device(device):\n-            return super(ExternalStream, cls).__new__(cls, stream_ptr=stream_ptr, **kwargs)\n+            return super().__new__(cls, stream_ptr=stream_ptr, **kwargs)\n \n \n class Event(torch._C._CudaEventBase):\n@@ -159,14 +159,14 @@ class Event(torch._C._CudaEventBase):\n     \"\"\"\n \n     def __new__(cls, enable_timing=False, blocking=False, interprocess=False):\n-        return super(Event, cls).__new__(\n+        return super().__new__(\n             cls,\n             enable_timing=enable_timing, blocking=blocking, interprocess=interprocess)\n \n     @classmethod\n     def from_ipc_handle(cls, device, handle):\n         r\"\"\"Reconstruct an event from an IPC handle on the given device.\"\"\"\n-        return super(Event, cls).from_ipc_handle(device, handle)\n+        return super().from_ipc_handle(device, handle)\n \n     def record(self, stream=None):\n         r\"\"\"Records the event in a given stream.\n@@ -227,6 +227,6 @@ def _as_parameter_(self):\n \n     def __repr__(self):\n         if self.cuda_event:\n-            return '<torch.cuda.Event {0:#x}>'.format(self._as_parameter_.value)\n+            return f'<torch.cuda.Event {self._as_parameter_.value:#x}>'\n         else:\n             return '<torch.cuda.Event uninitialized>'\ndiff --git a/torch/hub.py b/torch/hub.py\nindex d536a3ee712c08..2b4c20cfa04ff2 100644\n--- a/torch/hub.py\n+++ b/torch/hub.py\n@@ -32,9 +32,9 @@ def update(self, n):\n \n         self.n += n\n         if self.total is None:\n-            sys.stderr.write(\"\\r{0:.1f} bytes\".format(self.n))\n+            sys.stderr.write(f\"\\r{self.n:.1f} bytes\")\n         else:\n-            sys.stderr.write(\"\\r{0:.1f}%\".format(100 * self.n / float(self.total)))\n+            sys.stderr.write(\"\\r{:.1f}%\".format(100 * self.n / float(self.total)))\n         sys.stderr.flush()\n \n     # Don't bother implementing; use real tqdm if you want\n@@ -222,7 +222,7 @@ def _get_cache_or_reload(github, force_reload, trust_repo, calling_fn, verbose=T\n \n     if use_cache:\n         if verbose:\n-            sys.stderr.write('Using cache found in {}\\n'.format(repo_dir))\n+            sys.stderr.write(f'Using cache found in {repo_dir}\\n')\n     else:\n         # Validate the tag/branch is from the original repo instead of a forked repo\n         if not skip_validation:\n@@ -233,7 +233,7 @@ def _get_cache_or_reload(github, force_reload, trust_repo, calling_fn, verbose=T\n \n         try:\n             url = _git_archive_link(repo_owner, repo_name, ref)\n-            sys.stderr.write('Downloading: \\\"{}\\\" to {}\\n'.format(url, cached_file))\n+            sys.stderr.write(f'Downloading: \\\"{url}\\\" to {cached_file}\\n')\n             download_url_to_file(url, cached_file, progress=False)\n         except HTTPError as err:\n             if err.code == 300:\n@@ -273,7 +273,7 @@ def _check_repo_is_trusted(repo_owner, repo_name, owner_name_branch, trust_repo,\n \n     if not os.path.exists(filepath):\n         Path(filepath).touch()\n-    with open(filepath, 'r') as file:\n+    with open(filepath) as file:\n         trusted_repos = tuple(line.strip() for line in file)\n \n     # To minimize friction of introducing the new trust_repo mechanism, we consider that\n@@ -344,7 +344,7 @@ def _load_entry_from_hubconf(m, model):\n     func = _load_attr_from_module(m, model)\n \n     if func is None or not callable(func):\n-        raise RuntimeError('Cannot find callable {} in hubconf'.format(model))\n+        raise RuntimeError(f'Cannot find callable {model} in hubconf')\n \n     return func\n \n@@ -749,7 +749,7 @@ def load_state_dict_from_url(\n         filename = file_name\n     cached_file = os.path.join(model_dir, filename)\n     if not os.path.exists(cached_file):\n-        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n+        sys.stderr.write(f'Downloading: \"{url}\" to {cached_file}\\n')\n         hash_prefix = None\n         if check_hash:\n             r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\ndiff --git a/torch/jit/_decompositions.py b/torch/jit/_decompositions.py\nindex 6d50d534c957c8..beeca5df4e9552 100644\n--- a/torch/jit/_decompositions.py\n+++ b/torch/jit/_decompositions.py\n@@ -17,9 +17,9 @@ def check_decomposition_has_type_annotations(f):\n     sig = inspect.signature(f)\n     for param in sig.parameters.values():\n         assert param.annotation != inspect_empty, \\\n-            \"No signature on param {name} for function {func}\".format(name=param.name, func=f.name)\n+            f\"No signature on param {param.name} for function {f.name}\"\n \n-    assert sig.return_annotation != inspect_empty, \"No return annotation for function {func}\".format(func=f.name)\n+    assert sig.return_annotation != inspect_empty, f\"No return annotation for function {f.name}\"\n \n def signatures_match(decomposition_sig, torch_op_sig):\n     decomp_params = decomposition_sig.parameters\n@@ -61,7 +61,7 @@ def decomposition_decorator(f):\n         assert isinstance(aten_op, torch._ops.OpOverload)\n \n         # Need unique name for jit function serialization\n-        assert f.__name__ not in function_name_set, \"Duplicated function name {}\".format(f.__name__)\n+        assert f.__name__ not in function_name_set, f\"Duplicated function name {f.__name__}\"\n         function_name_set.add(f.__name__)\n \n         scripted_func = torch.jit.script(f)\ndiff --git a/torch/jit/_recursive.py b/torch/jit/_recursive.py\nindex 3f72fab43c9841..d40254fc45d170 100644\n--- a/torch/jit/_recursive.py\n+++ b/torch/jit/_recursive.py\n@@ -195,7 +195,7 @@ def infer_type(name, item):\n                 inferred = True\n         except RuntimeError as re:\n             raise RuntimeError(\n-                \"Error inferring type for {name}: {item}: {re}\".format(name=name, item=item, re=re)\n+                f\"Error inferring type for {name}: {item}: {re}\"\n             ) from re\n \n         return attr_type, inferred\n@@ -518,7 +518,7 @@ def init_fn(script_module):\n         #    recursively scripting them.\n         for name, sub_concrete_type in concrete_type.get_modules():\n             orig_value = getattr(nn_module, name)\n-            assert isinstance(orig_value, Module), \"Expected Module but got {}\".format(type(orig_value))\n+            assert isinstance(orig_value, Module), f\"Expected Module but got {type(orig_value)}\"\n             module_type = sub_concrete_type.jit_type\n             if isinstance(module_type, torch._C.InterfaceType):\n                 # use the interface inference rule to compile the module\n@@ -571,12 +571,12 @@ def init_fn(script_module):\n     # Special handling so methods like __len__ work in script methods on classes derived from containers\n     if isinstance(nn_module, (torch.nn.ModuleList, torch.nn.Sequential, torch.nn.ModuleDict)) and \\\n             '__len__' not in cpp_module._method_names():\n-        script_module.define(\"def __len__(self):\\n   return {}\\n\".format(len(nn_module)))\n+        script_module.define(f\"def __len__(self):\\n   return {len(nn_module)}\\n\")\n     if isinstance(nn_module, torch.nn.ModuleDict) and \\\n             '__contains__' not in cpp_module._method_names():\n         if len(nn_module.keys()):\n             keys = repr(list(nn_module.keys()))\n-            script_module.define(\"def __contains__(self, key: str):\\n   return key in {}\\n\".format(keys))\n+            script_module.define(f\"def __contains__(self, key: str):\\n   return key in {keys}\\n\")\n         else:\n             script_module.define(\"def __contains__(self, key: str):\\n   return False\\n\")\n \n@@ -686,7 +686,7 @@ def _check_no_signature(func):\n     signature = torch.jit.annotations.get_signature(func, None, fake_range(), inspect.ismethod(func))\n     if signature is None:\n         qual_name = _jit_internal._qualified_name(func)\n-        raise RuntimeError(\"Must explicitly add type annotations to overloaded functions: {}\".format(qual_name))\n+        raise RuntimeError(f\"Must explicitly add type annotations to overloaded functions: {qual_name}\")\n \n def make_stubs_for_overloads(overload_info):\n     overload_stubs = []\ndiff --git a/torch/jit/_script.py b/torch/jit/_script.py\nindex a6b2cb9cea7d73..63f01c3fb29e91 100644\n--- a/torch/jit/_script.py\n+++ b/torch/jit/_script.py\n@@ -282,7 +282,7 @@ def __init__(cls, name, bases, attrs):  # noqa: B902\n             # We leave built-in ScriptModule types alone, since this metaclass\n             # is only for compiling user classes that inherit from\n             # ScriptModule.\n-            return super(ScriptMeta, cls).__init__(name, bases, attrs)\n+            return super().__init__(name, bases, attrs)\n \n         original_init = getattr(cls, \"__init__\", lambda self: None)\n \n@@ -317,7 +317,7 @@ def make_stubs(module):\n                     delattr(self, name)\n \n         cls.__init__ = init_then_script  # type: ignore[misc]\n-        super(ScriptMeta, cls).__init__(name, bases, attrs)\n+        super().__init__(name, bases, attrs)\n \n \n class _CachedForward:\n@@ -736,7 +736,7 @@ def get_debug_state(self, *args, **kwargs):\n             return self._c.get_debug_state()\n \n         def extra_repr(self):\n-            return \"original_name={}\".format(self.original_name)\n+            return f\"original_name={self.original_name}\"\n \n         def graph_for(self, *args, **kwargs):\n             return self.forward.graph_for(self, *args, **kwargs)  # type: ignore[attr-defined]\ndiff --git a/torch/jit/_serialization.py b/torch/jit/_serialization.py\nindex 936cb61ce1c8be..b301e35a502ddf 100644\n--- a/torch/jit/_serialization.py\n+++ b/torch/jit/_serialization.py\n@@ -149,9 +149,9 @@ def load(f, map_location=None, _extra_files=None, _restore_shapes=False):\n \n     if isinstance(f, str):\n         if not os.path.exists(f):  # type: ignore[type-var]\n-            raise ValueError(\"The provided filename {} does not exist\".format(f))  # type: ignore[str-bytes-safe]\n+            raise ValueError(f\"The provided filename {f} does not exist\")  # type: ignore[str-bytes-safe]\n         if os.path.isdir(f):\n-            raise ValueError(\"The provided filename {} is a directory\".format(f))  # type: ignore[str-bytes-safe]\n+            raise ValueError(f\"The provided filename {f} is a directory\")  # type: ignore[str-bytes-safe]\n \n     map_location = validate_map_location(map_location)\n     if _extra_files is None:\ndiff --git a/torch/jit/_state.py b/torch/jit/_state.py\nindex b87ba5dd60d2f3..3980a1e7440522 100644\n--- a/torch/jit/_state.py\n+++ b/torch/jit/_state.py\n@@ -34,7 +34,7 @@ def parse_env(self, name, default, true_message, false_message):\n         elif value == \"0v\":\n             print(false_message)\n             return False\n-        raise ValueError(\"Unknown setting of {}. Try using 0 or 1.\".format(name))\n+        raise ValueError(f\"Unknown setting of {name}. Try using 0 or 1.\")\n \n     def __bool__(self):\n         return self.enabled\ndiff --git a/torch/jit/_trace.py b/torch/jit/_trace.py\nindex c8250017bd9556..27332f00c3b1b6 100644\n--- a/torch/jit/_trace.py\n+++ b/torch/jit/_trace.py\n@@ -183,7 +183,7 @@ def _time(trace_name, name, time=True):\n     finally:\n         stream.record_event(end)\n         end.synchronize()\n-        print(\"{} {} time: {} ms\".format(trace_name, name, start.elapsed_time(end)))\n+        print(f\"{trace_name} {name} time: {start.elapsed_time(end)} ms\")\n \n \n def verify(model, args, loss_fn=torch.sum, devices=None):\n@@ -1197,7 +1197,7 @@ def _get_name(self):\n         return self._name\n \n     def extra_repr(self):\n-        return \"original_name={}\".format(self._name)\n+        return f\"original_name={self._name}\"\n \n \n class TopLevelTracedModule(TracedModule):\ndiff --git a/torch/jit/annotations.py b/torch/jit/annotations.py\nindex 8a37969d40e90b..e0734ba85ba914 100644\n--- a/torch/jit/annotations.py\n+++ b/torch/jit/annotations.py\n@@ -133,7 +133,7 @@ def check_fn(fn, loc):\n     # Make sure the function definition is not a class instantiation\n     try:\n         source = dedent(''.join(get_source_lines_and_file(fn)[0]))\n-    except (TypeError, IOError):\n+    except (OSError, TypeError):\n         return\n     if source is None:\n         return\ndiff --git a/torch/jit/frontend.py b/torch/jit/frontend.py\nindex c3d5ce10aa25c6..582b5753788d5c 100644\n--- a/torch/jit/frontend.py\n+++ b/torch/jit/frontend.py\n@@ -461,14 +461,14 @@ def build_args(args):\n \n     # registers the custom function in the global context\n     ignore_func_str = \"@torch.jit.ignore\\n\" + astunparse.unparse(ignore_function)\n-    ignore_func_str += \"\\nglobals()[\\\"{}\\\"] = {}\".format(ignore_function_name, ignore_function_name)\n+    ignore_func_str += f\"\\nglobals()[\\\"{ignore_function_name}\\\"] = {ignore_function_name}\"\n     exec(ignore_func_str)  # noqa: P204\n \n     # build the statements as:\n     # <out_1>, <out_2>, ... = torch.jit.frontend.<func>(<in_1>, <in_2>)\n     assign_str_lhs = build_args(outputs)\n     # this function will be registered in torch.jit.frontend module by default\n-    assign_str_rhs = \"torch.jit.frontend.{}(\".format(ignore_function_name) + build_args(inputs) + \")\"\n+    assign_str_rhs = f\"torch.jit.frontend.{ignore_function_name}(\" + build_args(inputs) + \")\"\n \n     if len(outputs) > 0:\n         assign_str = assign_str_lhs + \" = \" + assign_str_rhs\ndiff --git a/torch/jit/mobile/__init__.py b/torch/jit/mobile/__init__.py\nindex 01a7495e9922e7..f58dfd04d59fcf 100644\n--- a/torch/jit/mobile/__init__.py\n+++ b/torch/jit/mobile/__init__.py\n@@ -38,9 +38,9 @@ def _load_for_lite_interpreter(f, map_location=None):\n     \"\"\"\n     if isinstance(f, str):\n         if not os.path.exists(f):\n-            raise ValueError(\"The provided filename {} does not exist\".format(f))\n+            raise ValueError(f\"The provided filename {f} does not exist\")\n         if os.path.isdir(f):\n-            raise ValueError(\"The provided filename {} is a directory\".format(f))\n+            raise ValueError(f\"The provided filename {f} is a directory\")\n \n     map_location = validate_map_location(map_location)\n \ndiff --git a/torch/jit/quantized.py b/torch/jit/quantized.py\nindex cb4c5f04df2dfb..a5cf772b28e289 100644\n--- a/torch/jit/quantized.py\n+++ b/torch/jit/quantized.py\n@@ -203,7 +203,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n         return ret\n \n \n@@ -284,7 +284,7 @@ def __init__(self, other, dtype=torch.int8):\n             raise RuntimeError('Only LSTM or GRU is supported for QuantizedRNN')\n \n         if dtype != torch.int8 and dtype != torch.float16:\n-            raise RuntimeError('Unsupported dtype: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype: {dtype}')\n \n         self.all_weights = []\n         for layer in range(self.num_layers):\n@@ -294,8 +294,8 @@ def __init__(self, other, dtype=torch.int8):\n                 suffix = '_reverse' if direction == 1 else ''\n \n                 def get_weight_bias(ihhh):\n-                    weight_name = 'weight_{}_l{}{}'.format(ihhh, layer, suffix)\n-                    bias_name = 'bias_{}_l{}{}'.format(ihhh, layer, suffix)\n+                    weight_name = f'weight_{ihhh}_l{layer}{suffix}'\n+                    bias_name = f'bias_{ihhh}_l{layer}{suffix}'\n \n                     weight = getattr(other, weight_name)\n                     bias = getattr(other, bias_name)\n@@ -316,7 +316,7 @@ def get_weight_bias(ihhh):\n                     cell_params = torch.ops.quantized.make_quantized_cell_params_fp16(\n                         packed_ih, packed_hh)\n \n-                setattr(self, 'cell_params_{}_{}'.format(layer, suffix), cell_params)\n+                setattr(self, f'cell_params_{layer}_{suffix}', cell_params)\n                 self.all_weights.append(cell_params)\n \n     @torch.jit.script_method\n@@ -551,7 +551,7 @@ def quantize_linear_modules(module, dtype=torch.int8):\n             return QuantizedLinearFP16(module)\n         else:\n             raise RuntimeError(\n-                \"Unsupported dtype: {}\".format(dtype))\n+                f\"Unsupported dtype: {dtype}\")\n     return module\n \n \n@@ -570,7 +570,7 @@ def quantize_rnn_modules(module, dtype=torch.int8):\n         setattr(module, name, mod)\n     if isinstance(module, torch.nn.LSTM):\n         if dtype != torch.int8 and dtype != torch.float16:\n-            raise RuntimeError(\"Unsupported dtype: {}\".format(dtype))\n+            raise RuntimeError(f\"Unsupported dtype: {dtype}\")\n         return QuantizedLSTM(module, dtype)\n     if isinstance(module, torch.nn.GRU):\n         return QuantizedGRU(module)\ndiff --git a/torch/jit/supported_ops.py b/torch/jit/supported_ops.py\nindex ca508e3008d9a9..6b7a7e5f96b5be 100644\n--- a/torch/jit/supported_ops.py\n+++ b/torch/jit/supported_ops.py\n@@ -13,10 +13,10 @@ def _emit_type(type):\n     return str(type)\n \n def _emit_arg(indent, i, arg):\n-    v = \"{} : {}\".format(arg.name, _emit_type(arg.type))\n+    v = f\"{arg.name} : {_emit_type(arg.type)}\"\n     default = arg.default_value\n     if default is not None:\n-        v = \"{}={}\".format(v, str(default))\n+        v = f\"{v}={str(default)}\"\n     if i > 0:\n         v = \"\\n{}{}\".format(\" \" * indent, v)\n     return v\n@@ -36,7 +36,7 @@ def _emit_schema(mod, name, schema, arg_start=0, padding=4):\n     if mod is None:\n         qualified_name = name\n     else:\n-        qualified_name = \"{}.{}\".format(mod, name)\n+        qualified_name = f\"{mod}.{name}\"\n     schema_str = \"{}({}) -> {}\".format(qualified_name,\n                                        _emit_args(len(qualified_name) + 1 + padding, schema.arguments[arg_start:]),\n                                        _emit_rets(schema.returns))\n@@ -246,13 +246,13 @@ def _get_global_builtins():\n \n     magic_methods_rows = []\n     for fn, magic_method in magic_methods:\n-        magic_methods_rows.append('\"{}\", \"``{}``\"'.format(fn, magic_method))\n+        magic_methods_rows.append(f'\"{fn}\", \"``{magic_method}``\"')\n \n     schematized_ops = []\n     schemaless_ops = []\n \n     for fn in supported_builtins:\n-        op_name = 'aten::{}'.format(fn)\n+        op_name = f'aten::{fn}'\n         if fn in op_renames:\n             op_name = op_renames[fn]\n         schemas = torch._C._jit_get_schemas_for_operator(op_name)\n@@ -261,7 +261,7 @@ def _get_global_builtins():\n         if len(schemas) > 0:\n             schematized_ops.append('')\n         else:\n-            table_row = '\":any:`{}`\", \"{}\"'.format(fn, schemaless_op_explanations[fn])\n+            table_row = f'\":any:`{fn}`\", \"{schemaless_op_explanations[fn]}\"'\n             schemaless_ops.append(table_row)\n \n     schematized_ops_str = '\\n'.join(schematized_ops)\n@@ -299,7 +299,7 @@ def _get_global_builtins():\n \n def _list_supported_ops():\n     def emit_block(decls):\n-        return '\\n.. rst-class:: codeblock-height-limiter\\n\\n::\\n\\n{}\\n'.format(''.join('    {}\\n\\n'.format(d) for d in decls))\n+        return '\\n.. rst-class:: codeblock-height-limiter\\n\\n::\\n\\n{}\\n'.format(''.join(f'    {d}\\n\\n' for d in decls))\n \n     body = ''\n     op_gathering_fns = (\n@@ -316,7 +316,7 @@ def emit_block(decls):\n             section = \"{}\\n{}\\n{}\\n\".format(header, '~' * len(header), items)\n         else:\n             section = \"{}\\n{}\\n{}\".format(header, '~' * len(header), emit_block(items))\n-        section = '.. _{}:'.format(link_target) + '\\n\\n' + section\n+        section = f'.. _{link_target}:' + '\\n\\n' + section\n         body += section\n \n     return body\ndiff --git a/torch/library.py b/torch/library.py\nindex a5ea7967add328..def51fda92a480 100644\n--- a/torch/library.py\n+++ b/torch/library.py\n@@ -55,7 +55,7 @@ def __init__(self, ns, kind, dispatch_key=\"\"):\n         weakref.finalize(self, _del_library, _impls, self._op_impls)\n \n     def __repr__(self):\n-        return \"Library(kind={}, ns={}, dispatch_key={})>\".format(self.kind, self.ns, self.dispatch_key)\n+        return f\"Library(kind={self.kind}, ns={self.ns}, dispatch_key={self.dispatch_key})>\"\n \n     def define(self, schema, alias_analysis=\"\"):\n         r'''Defines a new operator and its semantics in the ns namespace.\n@@ -75,7 +75,7 @@ def define(self, schema, alias_analysis=\"\"):\n         # This is added because we also want to disallow PURE_FUNCTION alias analysis which is a valid\n         # AliasAnalysis type in C++\n         if alias_analysis not in [\"\", \"FROM_SCHEMA\", \"CONSERVATIVE\"]:\n-            raise RuntimeError(\"Invalid alias_analysis type {}\".format(alias_analysis))\n+            raise RuntimeError(f\"Invalid alias_analysis type {alias_analysis}\")\n         return self.m.define(schema, alias_analysis)\n \n     def impl(self, op_name, fn, dispatch_key=''):\n@@ -94,7 +94,7 @@ def impl(self, op_name, fn, dispatch_key=''):\n             >>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n         '''\n         if not callable(fn):\n-            raise TypeError(\"Input function is required to be a callable but found type {}\".format(type(fn)))\n+            raise TypeError(f\"Input function is required to be a callable but found type {type(fn)}\")\n         if dispatch_key == '':\n             dispatch_key = self.dispatch_key\n \ndiff --git a/torch/linalg/__init__.py b/torch/linalg/__init__.py\nindex 3beb7769126578..3808752ab930cd 100644\n--- a/torch/linalg/__init__.py\n+++ b/torch/linalg/__init__.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import sys\n \n import torch\ndiff --git a/torch/masked/_docs.py b/torch/masked/_docs.py\nindex cfd8f397bafa86..bf96b49e3e8271 100644\n--- a/torch/masked/_docs.py\n+++ b/torch/masked/_docs.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # This file is generated, do not modify it!\n #\n # To update this file, run the update masked docs script as follows:\ndiff --git a/torch/masked/_ops.py b/torch/masked/_ops.py\nindex 21434d95c77ae9..50e464d4c1cce0 100644\n--- a/torch/masked/_ops.py\n+++ b/torch/masked/_ops.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \n import warnings\n \ndiff --git a/torch/masked/maskedtensor/core.py b/torch/masked/maskedtensor/core.py\nindex 3b15fa9f7802a5..42321bb959c4f5 100644\n--- a/torch/masked/maskedtensor/core.py\n+++ b/torch/masked/maskedtensor/core.py\n@@ -109,7 +109,7 @@ def _masked_tensor_str(data, mask, formatter):\n             for d in data\n         ]\n         max_len = max(\n-            (8 if x[1] else len(x[0]) for x in zip(formatted_elements, ~mask))\n+            8 if x[1] else len(x[0]) for x in zip(formatted_elements, ~mask)\n         )\n         return (\n             \"[\"\ndiff --git a/torch/mps/__init__.py b/torch/mps/__init__.py\nindex cab48f92f858f2..e455e7de0557c3 100644\n--- a/torch/mps/__init__.py\n+++ b/torch/mps/__init__.py\n@@ -79,7 +79,7 @@ def set_per_process_memory_fraction(fraction) -> None:\n     if not isinstance(fraction, float):\n         raise TypeError('Invalid type for fraction argument, must be `float`')\n     if fraction < 0 or fraction > 2:\n-        raise ValueError('Invalid fraction value: {}. Allowed range: 0~2'.format(fraction))\n+        raise ValueError(f'Invalid fraction value: {fraction}. Allowed range: 0~2')\n \n     torch._C._mps_setMemoryFraction(fraction)\n \ndiff --git a/torch/nn/_reduction.py b/torch/nn/_reduction.py\nindex 2fc9c58033a268..ac2a8bb0a0e9ed 100644\n--- a/torch/nn/_reduction.py\n+++ b/torch/nn/_reduction.py\n@@ -16,7 +16,7 @@ def get_enum(reduction: str) -> int:\n         ret = 2\n     else:\n         ret = -1  # TODO: remove once JIT exceptions support control flow\n-        raise ValueError(\"{} is not a valid value for reduction\".format(reduction))\n+        raise ValueError(f\"{reduction} is not a valid value for reduction\")\n     return ret\n \n # In order to support previous versions, accept boolean size_average and reduce\ndiff --git a/torch/nn/functional.py b/torch/nn/functional.py\nindex e1b1227d235d6c..cdce0f6475f1d2 100644\n--- a/torch/nn/functional.py\n+++ b/torch/nn/functional.py\n@@ -2446,7 +2446,7 @@ def _verify_batch_size(size: List[int]) -> None:\n     for i in range(len(size) - 2):\n         size_prods *= size[i + 2]\n     if size_prods == 1:\n-        raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n+        raise ValueError(f\"Expected more than 1 value per channel when training, got input size {size}\")\n \n \n def batch_norm(\n@@ -2491,7 +2491,7 @@ def _verify_spatial_size(size: List[int]) -> None:\n     for i in range(2, len(size)):\n         size_prods *= size[i]\n     if size_prods == 1:\n-        raise ValueError(\"Expected more than 1 spatial element when training, got input size {}\".format(size))\n+        raise ValueError(f\"Expected more than 1 spatial element when training, got input size {size}\")\n \n \n def instance_norm(\n@@ -3198,7 +3198,7 @@ def binary_cross_entropy_with_logits(\n         reduction_enum = _Reduction.get_enum(reduction)\n \n     if not (target.size() == input.size()):\n-        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n+        raise ValueError(f\"Target size ({target.size()}) must be the same as input size ({input.size()})\")\n \n     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n \n@@ -3368,10 +3368,10 @@ def margin_ranking_loss(\n         reduction_enum = _Reduction.get_enum(reduction)\n     if (input1.dim() != input2.dim() or input1.dim() != target.dim()):\n         raise RuntimeError(\n-            (\n+\n                 \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n                 \"input1: {}, input2: {}, target: {} \".format(input1.size(), input2.size(), target.size())\n-            )\n+\n         )\n     return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)\n \n@@ -4376,7 +4376,7 @@ def affine_grid(theta: Tensor, size: List[int], align_corners: Optional[bool] =\n \n     # enforce floating point dtype on theta\n     if not theta.is_floating_point():\n-        raise ValueError(\"Expected theta to have floating point type, but got {}\".format(theta.dtype))\n+        raise ValueError(f\"Expected theta to have floating point type, but got {theta.dtype}\")\n     # check that shapes and sizes match\n     if len(size) == 4:\n         if theta.dim() != 3 or theta.shape[-2] != 2 or theta.shape[-1] != 3:\n@@ -4407,7 +4407,7 @@ def affine_grid(theta: Tensor, size: List[int], align_corners: Optional[bool] =\n             \"See the documentation of affine_grid for details.\"\n         )\n     elif min(size) <= 0:\n-        raise ValueError(\"Expected non-zero, positive output size. Got {}\".format(size))\n+        raise ValueError(f\"Expected non-zero, positive output size. Got {size}\")\n \n     return torch.affine_grid_generator(theta, size, align_corners)\n \n@@ -4677,9 +4677,9 @@ def triplet_margin_with_distance_loss(\n     n_dim = negative.ndim\n     if not (a_dim == p_dim and p_dim == n_dim):\n         raise RuntimeError(\n-            (f\"The anchor, positive, and negative tensors are expected to have \"\n+            f\"The anchor, positive, and negative tensors are expected to have \"\n              f\"the same number of dimensions, but got: anchor {a_dim}D, \"\n-             f\"positive {p_dim}D, and negative {n_dim}D inputs\"))\n+             f\"positive {p_dim}D, and negative {n_dim}D inputs\")\n \n     # Calculate loss\n     if distance_function is None:\ndiff --git a/torch/nn/init.py b/torch/nn/init.py\nindex 830aae235dc413..dc7d121f7aca2f 100644\n--- a/torch/nn/init.py\n+++ b/torch/nn/init.py\n@@ -111,12 +111,12 @@ def calculate_gain(nonlinearity, param=None):\n             # True/False are instances of int, hence check above\n             negative_slope = param\n         else:\n-            raise ValueError(\"negative_slope {} not a valid number\".format(param))\n+            raise ValueError(f\"negative_slope {param} not a valid number\")\n         return math.sqrt(2.0 / (1 + negative_slope ** 2))\n     elif nonlinearity == 'selu':\n         return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n     else:\n-        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n+        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n \n \n def uniform_(tensor: Tensor, a: float = 0., b: float = 1.) -> Tensor:\n@@ -364,7 +364,7 @@ def _calculate_correct_fan(tensor, mode):\n     mode = mode.lower()\n     valid_modes = ['fan_in', 'fan_out']\n     if mode not in valid_modes:\n-        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n+        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n \n     fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n     return fan_in if mode == 'fan_in' else fan_out\ndiff --git a/torch/nn/modules/_functions.py b/torch/nn/modules/_functions.py\nindex 4b5842d881f552..770609a9ec9cad 100644\n--- a/torch/nn/modules/_functions.py\n+++ b/torch/nn/modules/_functions.py\n@@ -17,7 +17,7 @@ def forward(self, input, weight, bias, running_mean, running_var, eps, momentum,\n \n         size = int(input.numel() // input.size(1))\n         if size == 1 and world_size < 2:\n-            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))\n+            raise ValueError(f'Expected more than 1 value per channel when training, got input size {size}')\n \n         num_channels = input.shape[1]\n         if input.numel() > 0:\n@@ -194,7 +194,7 @@ def forward(ctx, input, size, alpha=1e-4, beta=0.75, k=1):\n         ctx.scale = None\n \n         if input.dim() != 4:\n-            raise ValueError(\"CrossMapLRN2d: Expected input to be 4D, got {}D instead.\".format(input.dim()))\n+            raise ValueError(f\"CrossMapLRN2d: Expected input to be 4D, got {input.dim()}D instead.\")\n \n         ctx.scale = ctx.scale or input.new()\n         output = input.new()\ndiff --git a/torch/nn/modules/activation.py b/torch/nn/modules/activation.py\nindex d4f349dcb0070b..28021a61a520e0 100644\n--- a/torch/nn/modules/activation.py\n+++ b/torch/nn/modules/activation.py\n@@ -170,7 +170,7 @@ def forward(self, input: Tensor) -> Tensor:\n \n     def extra_repr(self):\n         inplace_str = ', inplace=True' if self.inplace else ''\n-        return 'lower={}, upper={}{}'.format(self.lower, self.upper, inplace_str)\n+        return f'lower={self.lower}, upper={self.upper}{inplace_str}'\n \n \n class Hardtanh(Module):\n@@ -519,7 +519,7 @@ def forward(self, input: Tensor) -> Tensor:\n \n     def extra_repr(self) -> str:\n         inplace_str = ', inplace=True' if self.inplace else ''\n-        return 'alpha={}{}'.format(self.alpha, inplace_str)\n+        return f'alpha={self.alpha}{inplace_str}'\n \n \n class CELU(Module):\n@@ -563,7 +563,7 @@ def forward(self, input: Tensor) -> Tensor:\n \n     def extra_repr(self) -> str:\n         inplace_str = ', inplace=True' if self.inplace else ''\n-        return 'alpha={}{}'.format(self.alpha, inplace_str)\n+        return f'alpha={self.alpha}{inplace_str}'\n \n \n class SELU(Module):\n@@ -645,7 +645,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.glu(input, self.dim)\n \n     def extra_repr(self) -> str:\n-        return 'dim={}'.format(self.dim)\n+        return f'dim={self.dim}'\n \n \n class GELU(Module):\n@@ -686,7 +686,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.gelu(input, approximate=self.approximate)\n \n     def extra_repr(self) -> str:\n-        return 'approximate={}'.format(repr(self.approximate))\n+        return f'approximate={repr(self.approximate)}'\n \n \n class Hardshrink(Module):\n@@ -728,7 +728,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.hardshrink(input, self.lambd)\n \n     def extra_repr(self) -> str:\n-        return '{}'.format(self.lambd)\n+        return f'{self.lambd}'\n \n \n class LeakyReLU(Module):\n@@ -779,7 +779,7 @@ def forward(self, input: Tensor) -> Tensor:\n \n     def extra_repr(self) -> str:\n         inplace_str = ', inplace=True' if self.inplace else ''\n-        return 'negative_slope={}{}'.format(self.negative_slope, inplace_str)\n+        return f'negative_slope={self.negative_slope}{inplace_str}'\n \n \n class LogSigmoid(Module):\n@@ -844,7 +844,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.softplus(input, self.beta, self.threshold)\n \n     def extra_repr(self) -> str:\n-        return 'beta={}, threshold={}'.format(self.beta, self.threshold)\n+        return f'beta={self.beta}, threshold={self.threshold}'\n \n \n class Softshrink(Module):\n@@ -1209,10 +1209,10 @@ def forward(\n                 if query is key:\n                     query = key = value = query.transpose(1, 0)\n                 else:\n-                    query, key = [x.transpose(1, 0) for x in (query, key)]\n+                    query, key = (x.transpose(1, 0) for x in (query, key))\n                     value = key\n             else:\n-                query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n+                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n \n         if not self._qkv_same_embed_dim:\n             attn_output, attn_output_weights = F.multi_head_attention_forward(\n@@ -1350,7 +1350,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.prelu(input, self.weight)\n \n     def extra_repr(self) -> str:\n-        return 'num_parameters={}'.format(self.num_parameters)\n+        return f'num_parameters={self.num_parameters}'\n \n \n class Softsign(Module):\n@@ -1444,7 +1444,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.softmin(input, self.dim, _stacklevel=5)\n \n     def extra_repr(self):\n-        return 'dim={dim}'.format(dim=self.dim)\n+        return f'dim={self.dim}'\n \n class Softmax(Module):\n     r\"\"\"Applies the Softmax function to an n-dimensional input Tensor\n@@ -1500,7 +1500,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.softmax(input, self.dim, _stacklevel=5)\n \n     def extra_repr(self) -> str:\n-        return 'dim={dim}'.format(dim=self.dim)\n+        return f'dim={self.dim}'\n \n \n class Softmax2d(Module):\n@@ -1528,7 +1528,7 @@ class Softmax2d(Module):\n     def forward(self, input: Tensor) -> Tensor:\n         if input.dim() not in (3, 4):\n             raise ValueError(\n-                \"Softmax2d: expected input to be 3D or 4D, got {}D instead\".format(input.dim())\n+                f\"Softmax2d: expected input to be 3D or 4D, got {input.dim()}D instead\"\n             )\n         return F.softmax(input, -3, _stacklevel=5)\n \n@@ -1574,4 +1574,4 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.log_softmax(input, self.dim, _stacklevel=5)\n \n     def extra_repr(self):\n-        return 'dim={dim}'.format(dim=self.dim)\n+        return f'dim={self.dim}'\ndiff --git a/torch/nn/modules/adaptive.py b/torch/nn/modules/adaptive.py\nindex 2e677963938f8e..cf2f56efa8178d 100644\n--- a/torch/nn/modules/adaptive.py\n+++ b/torch/nn/modules/adaptive.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \n from collections import namedtuple\n \ndiff --git a/torch/nn/modules/batchnorm.py b/torch/nn/modules/batchnorm.py\nindex 921e4ba1b33dfe..f79f90aeb825b4 100644\n--- a/torch/nn/modules/batchnorm.py\n+++ b/torch/nn/modules/batchnorm.py\n@@ -299,7 +299,7 @@ class BatchNorm1d(_BatchNorm):\n     def _check_input_dim(self, input):\n         if input.dim() != 2 and input.dim() != 3:\n             raise ValueError(\n-                \"expected 2D or 3D input (got {}D input)\".format(input.dim())\n+                f\"expected 2D or 3D input (got {input.dim()}D input)\"\n             )\n \n \n@@ -334,7 +334,7 @@ class LazyBatchNorm1d(_LazyNormBase, _BatchNorm):\n     def _check_input_dim(self, input):\n         if input.dim() != 2 and input.dim() != 3:\n             raise ValueError(\n-                \"expected 2D or 3D input (got {}D input)\".format(input.dim())\n+                f\"expected 2D or 3D input (got {input.dim()}D input)\"\n             )\n \n \n@@ -410,7 +410,7 @@ class BatchNorm2d(_BatchNorm):\n \n     def _check_input_dim(self, input):\n         if input.dim() != 4:\n-            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n+            raise ValueError(f\"expected 4D input (got {input.dim()}D input)\")\n \n \n class LazyBatchNorm2d(_LazyNormBase, _BatchNorm):\n@@ -443,7 +443,7 @@ class LazyBatchNorm2d(_LazyNormBase, _BatchNorm):\n \n     def _check_input_dim(self, input):\n         if input.dim() != 4:\n-            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n+            raise ValueError(f\"expected 4D input (got {input.dim()}D input)\")\n \n \n class BatchNorm3d(_BatchNorm):\n@@ -516,7 +516,7 @@ class BatchNorm3d(_BatchNorm):\n \n     def _check_input_dim(self, input):\n         if input.dim() != 5:\n-            raise ValueError(\"expected 5D input (got {}D input)\".format(input.dim()))\n+            raise ValueError(f\"expected 5D input (got {input.dim()}D input)\")\n \n \n class LazyBatchNorm3d(_LazyNormBase, _BatchNorm):\n@@ -549,7 +549,7 @@ class LazyBatchNorm3d(_LazyNormBase, _BatchNorm):\n \n     def _check_input_dim(self, input):\n         if input.dim() != 5:\n-            raise ValueError(\"expected 5D input (got {}D input)\".format(input.dim()))\n+            raise ValueError(f\"expected 5D input (got {input.dim()}D input)\")\n \n \n class SyncBatchNorm(_BatchNorm):\n@@ -674,7 +674,7 @@ def __init__(\n     def _check_input_dim(self, input):\n         if input.dim() < 2:\n             raise ValueError(\n-                \"expected at least 2D input (got {}D input)\".format(input.dim())\n+                f\"expected at least 2D input (got {input.dim()}D input)\"\n             )\n \n     def _check_non_zero_input_channels(self, input):\ndiff --git a/torch/nn/modules/channelshuffle.py b/torch/nn/modules/channelshuffle.py\nindex ffb235713c7143..b30391ffc0da1a 100644\n--- a/torch/nn/modules/channelshuffle.py\n+++ b/torch/nn/modules/channelshuffle.py\n@@ -51,4 +51,4 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.channel_shuffle(input, self.groups)\n \n     def extra_repr(self) -> str:\n-        return 'groups={}'.format(self.groups)\n+        return f'groups={self.groups}'\ndiff --git a/torch/nn/modules/container.py b/torch/nn/modules/container.py\nindex 03ce028b7256cf..ea6f1399c35564 100644\n--- a/torch/nn/modules/container.py\n+++ b/torch/nn/modules/container.py\n@@ -108,7 +108,7 @@ def _get_item_by_idx(self, iterator, idx) -> T:  # type: ignore[misc, type-var]\n         size = len(self)\n         idx = operator.index(idx)\n         if not -size <= idx < size:\n-            raise IndexError('index {} is out of range'.format(idx))\n+            raise IndexError(f'index {idx} is out of range')\n         idx %= size\n         return next(islice(iterator, idx, None))\n \n@@ -229,11 +229,11 @@ def append(self, module: Module) -> 'Sequential':\n     def insert(self, index: int, module: Module) -> 'Sequential':\n         if not isinstance(module, Module):\n             raise AssertionError(\n-                'module should be of type: {}'.format(Module))\n+                f'module should be of type: {Module}')\n         n = len(self._modules)\n         if not (-n <= index <= n):\n             raise IndexError(\n-                'Index out of range: {}'.format(index))\n+                f'Index out of range: {index}')\n         if index < 0:\n             index += n\n         for i in range(n, index, -1):\n@@ -282,7 +282,7 @@ def _get_abs_string_index(self, idx):\n         \"\"\"Get the absolute index for the list of modules\"\"\"\n         idx = operator.index(idx)\n         if not (-len(self) <= idx < len(self)):\n-            raise IndexError('index {} is out of range'.format(idx))\n+            raise IndexError(f'index {idx} is out of range')\n         if idx < 0:\n             idx += len(self)\n         return str(idx)\n@@ -587,7 +587,7 @@ def _get_abs_string_index(self, idx):\n         \"\"\"Get the absolute index for the list of modules\"\"\"\n         idx = operator.index(idx)\n         if not (-len(self) <= idx < len(self)):\n-            raise IndexError('index {} is out of range'.format(idx))\n+            raise IndexError(f'index {idx} is out of range')\n         if idx < 0:\n             idx += len(self)\n         return str(idx)\n@@ -667,7 +667,7 @@ def extra_repr(self) -> str:\n             if isinstance(p, torch.Tensor):\n                 size_str = 'x'.join(str(size) for size in p.size())\n                 if p.device.type in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n-                    device_str = ' ({})'.format(p.device)\n+                    device_str = f' ({p.device})'\n                 else:\n                     device_str = ''\n                 parastr = '{} containing: [{} of size {}{}]'.format(\n@@ -834,7 +834,7 @@ def fromkeys(self, keys: Iterable[str], default: Optional[Any] = None) -> 'Param\n             keys (iterable, string): keys to make the new ParameterDict from\n             default (Parameter, optional): value to set for all keys\n         \"\"\"\n-        return ParameterDict(((k, default) for k in keys))\n+        return ParameterDict((k, default) for k in keys)\n \n     def keys(self) -> Iterable[str]:\n         r\"\"\"Return an iterable of the ParameterDict keys.\n@@ -894,7 +894,7 @@ def extra_repr(self) -> str:\n             if isinstance(p, torch.Tensor):\n                 size_str = 'x'.join(str(size) for size in p.size())\n                 if p.device.type in [\"cuda\", torch._C._get_privateuse1_backend_name()]:\n-                    device_str = ' ({})'.format(p.device)\n+                    device_str = f' ({p.device})'\n                 else:\n                     device_str = ''\n                 parastr = '{} containing: [{} of size {}{}]'.format(\ndiff --git a/torch/nn/modules/conv.py b/torch/nn/modules/conv.py\nindex bb5931b76b442e..04d9e6a649fd5c 100644\n--- a/torch/nn/modules/conv.py\n+++ b/torch/nn/modules/conv.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import math\n import warnings\n \n@@ -619,7 +618,7 @@ def __init__(self, in_channels, out_channels, kernel_size, stride,\n                  padding, dilation, transposed, output_padding,\n                  groups, bias, padding_mode, device=None, dtype=None) -> None:\n         if padding_mode != 'zeros':\n-            raise ValueError('Only \"zeros\" padding mode is supported for {}'.format(self.__class__.__name__))\n+            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n \n         factory_kwargs = {'device': device, 'dtype': dtype}\n         super().__init__(\ndiff --git a/torch/nn/modules/dropout.py b/torch/nn/modules/dropout.py\nindex a92a58c0f88231..afe11e0859c44b 100644\n--- a/torch/nn/modules/dropout.py\n+++ b/torch/nn/modules/dropout.py\n@@ -19,7 +19,7 @@ def __init__(self, p: float = 0.5, inplace: bool = False) -> None:\n         self.inplace = inplace\n \n     def extra_repr(self) -> str:\n-        return 'p={}, inplace={}'.format(self.p, self.inplace)\n+        return f'p={self.p}, inplace={self.inplace}'\n \n \n class Dropout(_DropoutNd):\ndiff --git a/torch/nn/modules/flatten.py b/torch/nn/modules/flatten.py\nindex c55ababd5ebb06..1ab2dd187b0e4b 100644\n--- a/torch/nn/modules/flatten.py\n+++ b/torch/nn/modules/flatten.py\n@@ -122,22 +122,22 @@ def _require_tuple_tuple(self, input):\n             for idx, elem in enumerate(input):\n                 if not isinstance(elem, tuple):\n                     raise TypeError(\"unflattened_size must be tuple of tuples, \" +\n-                                    \"but found element of type {} at pos {}\".format(type(elem).__name__, idx))\n+                                    f\"but found element of type {type(elem).__name__} at pos {idx}\")\n             return\n         raise TypeError(\"unflattened_size must be a tuple of tuples, \" +\n-                        \"but found type {}\".format(type(input).__name__))\n+                        f\"but found type {type(input).__name__}\")\n \n     def _require_tuple_int(self, input):\n         if (isinstance(input, (tuple, list))):\n             for idx, elem in enumerate(input):\n                 if not isinstance(elem, int):\n                     raise TypeError(\"unflattened_size must be tuple of ints, \" +\n-                                    \"but found element of type {} at pos {}\".format(type(elem).__name__, idx))\n+                                    f\"but found element of type {type(elem).__name__} at pos {idx}\")\n             return\n-        raise TypeError(\"unflattened_size must be a tuple of ints, but found type {}\".format(type(input).__name__))\n+        raise TypeError(f\"unflattened_size must be a tuple of ints, but found type {type(input).__name__}\")\n \n     def forward(self, input: Tensor) -> Tensor:\n         return input.unflatten(self.dim, self.unflattened_size)\n \n     def extra_repr(self) -> str:\n-        return 'dim={}, unflattened_size={}'.format(self.dim, self.unflattened_size)\n+        return f'dim={self.dim}, unflattened_size={self.unflattened_size}'\ndiff --git a/torch/nn/modules/fold.py b/torch/nn/modules/fold.py\nindex 770ba429bd76d5..2d14a9b7f7aadf 100644\n--- a/torch/nn/modules/fold.py\n+++ b/torch/nn/modules/fold.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from .module import Module\n from .. import functional as F\n \ndiff --git a/torch/nn/modules/instancenorm.py b/torch/nn/modules/instancenorm.py\nindex 97a70cde16e503..8e0a54ed0d7476 100644\n--- a/torch/nn/modules/instancenorm.py\n+++ b/torch/nn/modules/instancenorm.py\n@@ -58,7 +58,7 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                     'the running stats are actually needed, instead set '\n                     'track_running_stats=True in {klass} to enable them. See '\n                     'the documentation of {klass} for details.'\n-                    .format(names=\" and \".join('\"{}\"'.format(k) for k in running_stats_keys),\n+                    .format(names=\" and \".join(f'\"{k}\"' for k in running_stats_keys),\n                             klass=self.__class__.__name__))\n                 for key in running_stats_keys:\n                     state_dict.pop(key)\ndiff --git a/torch/nn/modules/lazy.py b/torch/nn/modules/lazy.py\nindex 0c77c3550d15b0..386585af924ece 100644\n--- a/torch/nn/modules/lazy.py\n+++ b/torch/nn/modules/lazy.py\n@@ -223,7 +223,7 @@ def initialize_parameters(self: _LazyProtocol, *args, **kwargs):\n         This adds an interface to isolate parameter initialization from the\n         forward pass when doing parameter shape inference.\n         \"\"\"\n-        raise NotImplementedError('initialize_parameters is not implemented for {}'.format(self.__class__.__name__))\n+        raise NotImplementedError(f'initialize_parameters is not implemented for {self.__class__.__name__}')\n \n     def has_uninitialized_params(self: _LazyProtocol):\n         r\"\"\"Check if a module has parameters that are not initialized\n@@ -249,7 +249,7 @@ def _infer_parameters(self: _LazyProtocol, module, input):\n         \"\"\"\n         module.initialize_parameters(*input)\n         if module.has_uninitialized_params():\n-            raise RuntimeError('module {} has not been fully initialized'.format(self._get_name()))\n+            raise RuntimeError(f'module {self._get_name()} has not been fully initialized')\n         module._initialize_hook.remove()\n         module._load_hook.remove()\n         delattr(module, '_initialize_hook')\ndiff --git a/torch/nn/modules/module.py b/torch/nn/modules/module.py\nindex 10122433b57eba..f94cec4eb78aa6 100644\n--- a/torch/nn/modules/module.py\n+++ b/torch/nn/modules/module.py\n@@ -536,7 +536,7 @@ def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool\n         elif name == '':\n             raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n         elif hasattr(self, name) and name not in self._buffers:\n-            raise KeyError(\"attribute '{}' already exists\".format(name))\n+            raise KeyError(f\"attribute '{name}' already exists\")\n         elif tensor is not None and not isinstance(tensor, torch.Tensor):\n             raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n                             \"(torch Tensor or None required)\"\n@@ -577,7 +577,7 @@ def register_parameter(self, name: str, param: Optional[Parameter]) -> None:\n         elif name == '':\n             raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n         elif hasattr(self, name) and name not in self._parameters:\n-            raise KeyError(\"attribute '{}' already exists\".format(name))\n+            raise KeyError(f\"attribute '{name}' already exists\")\n \n         if param is None:\n             self._parameters[name] = None\n@@ -615,9 +615,9 @@ def add_module(self, name: str, module: Optional['Module']) -> None:\n             raise TypeError(\"module name should be a string. Got {}\".format(\n                 torch.typename(name)))\n         elif hasattr(self, name) and name not in self._modules:\n-            raise KeyError(\"attribute '{}' already exists\".format(name))\n+            raise KeyError(f\"attribute '{name}' already exists\")\n         elif '.' in name:\n-            raise KeyError(\"module name can't contain \\\".\\\", got: {}\".format(name))\n+            raise KeyError(f\"module name can't contain \\\".\\\", got: {name}\")\n         elif name == '':\n             raise KeyError(\"module name can't be empty string \\\"\\\"\")\n         for hook in _global_module_registration_hooks.values():\n@@ -1599,7 +1599,7 @@ def _call_impl(self, *args, **kwargs):\n                 var = result\n                 while not isinstance(var, torch.Tensor):\n                     if isinstance(var, dict):\n-                        var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n+                        var = next(v for v in var.values() if isinstance(v, torch.Tensor))\n                     else:\n                         var = var[0]\n                 grad_fn = var.grad_fn\n@@ -2098,7 +2098,7 @@ def load_state_dict(self, state_dict: Mapping[str, Any],\n             ``RuntimeError``.\n         \"\"\"\n         if not isinstance(state_dict, Mapping):\n-            raise TypeError(\"Expected state_dict to be dict-like, got {}.\".format(type(state_dict)))\n+            raise TypeError(f\"Expected state_dict to be dict-like, got {type(state_dict)}.\")\n \n         missing_keys: List[str] = []\n         unexpected_keys: List[str] = []\n@@ -2140,11 +2140,11 @@ def load(module, local_state_dict, prefix=''):\n             if len(unexpected_keys) > 0:\n                 error_msgs.insert(\n                     0, 'Unexpected key(s) in state_dict: {}. '.format(\n-                        ', '.join('\"{}\"'.format(k) for k in unexpected_keys)))\n+                        ', '.join(f'\"{k}\"' for k in unexpected_keys)))\n             if len(missing_keys) > 0:\n                 error_msgs.insert(\n                     0, 'Missing key(s) in state_dict: {}. '.format(\n-                        ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n+                        ', '.join(f'\"{k}\"' for k in missing_keys)))\n \n         if len(error_msgs) > 0:\n             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\ndiff --git a/torch/nn/modules/padding.py b/torch/nn/modules/padding.py\nindex e688b9d64d9307..62b2dec23e394d 100644\n--- a/torch/nn/modules/padding.py\n+++ b/torch/nn/modules/padding.py\n@@ -25,7 +25,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.pad(input, self.padding, 'constant', self.value)\n \n     def extra_repr(self) -> str:\n-        return 'padding={}, value={}'.format(self.padding, self.value)\n+        return f'padding={self.padding}, value={self.value}'\n \n \n class ConstantPad1d(_ConstantPadNd):\n@@ -178,7 +178,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.pad(input, self.padding, 'reflect')\n \n     def extra_repr(self) -> str:\n-        return '{}'.format(self.padding)\n+        return f'{self.padding}'\n \n \n class ReflectionPad1d(_ReflectionPadNd):\n@@ -335,7 +335,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.pad(input, self.padding, 'replicate')\n \n     def extra_repr(self) -> str:\n-        return '{}'.format(self.padding)\n+        return f'{self.padding}'\n \n \n class ReplicationPad1d(_ReplicationPadNd):\n@@ -522,7 +522,7 @@ def __init__(self, padding: _size_2_t) -> None:\n         super().__init__(padding, 0.)\n \n     def extra_repr(self) -> str:\n-        return '{}'.format(self.padding)\n+        return f'{self.padding}'\n \n class ZeroPad2d(ConstantPad2d):\n     r\"\"\"Pads the input tensor boundaries with zero.\n@@ -575,7 +575,7 @@ def __init__(self, padding: _size_4_t) -> None:\n         super().__init__(padding, 0.)\n \n     def extra_repr(self) -> str:\n-        return '{}'.format(self.padding)\n+        return f'{self.padding}'\n \n class ZeroPad3d(ConstantPad3d):\n     r\"\"\"Pads the input tensor boundaries with zero.\n@@ -617,4 +617,4 @@ def __init__(self, padding: _size_6_t) -> None:\n         super().__init__(padding, 0.)\n \n     def extra_repr(self) -> str:\n-        return '{}'.format(self.padding)\n+        return f'{self.padding}'\ndiff --git a/torch/nn/modules/pixelshuffle.py b/torch/nn/modules/pixelshuffle.py\nindex 5120a21eed10fa..4e537ef7c2f9a0 100644\n--- a/torch/nn/modules/pixelshuffle.py\n+++ b/torch/nn/modules/pixelshuffle.py\n@@ -54,7 +54,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.pixel_shuffle(input, self.upscale_factor)\n \n     def extra_repr(self) -> str:\n-        return 'upscale_factor={}'.format(self.upscale_factor)\n+        return f'upscale_factor={self.upscale_factor}'\n \n \n class PixelUnshuffle(Module):\n@@ -104,4 +104,4 @@ def forward(self, input: Tensor) -> Tensor:\n         return F.pixel_unshuffle(input, self.downscale_factor)\n \n     def extra_repr(self) -> str:\n-        return 'downscale_factor={}'.format(self.downscale_factor)\n+        return f'downscale_factor={self.downscale_factor}'\ndiff --git a/torch/nn/modules/pooling.py b/torch/nn/modules/pooling.py\nindex 59ee19c529c5a4..17836f339f4adb 100644\n--- a/torch/nn/modules/pooling.py\n+++ b/torch/nn/modules/pooling.py\n@@ -1001,7 +1001,7 @@ def __init__(self, output_size: _size_any_opt_t, return_indices: bool = False) -\n         self.return_indices = return_indices\n \n     def extra_repr(self) -> str:\n-        return 'output_size={}'.format(self.output_size)\n+        return f'output_size={self.output_size}'\n \n # FIXME (by @ssnl): Improve adaptive pooling docs: specify what the input and\n #   output shapes are, and how the operation computes output.\n@@ -1130,7 +1130,7 @@ def __init__(self, output_size: _size_any_opt_t) -> None:\n         self.output_size = output_size\n \n     def extra_repr(self) -> str:\n-        return 'output_size={}'.format(self.output_size)\n+        return f'output_size={self.output_size}'\n \n \n class AdaptiveAvgPool1d(_AdaptiveAvgPoolNd):\ndiff --git a/torch/nn/modules/rnn.py b/torch/nn/modules/rnn.py\nindex 13f9acb234d87f..89aa7083dc6a89 100644\n--- a/torch/nn/modules/rnn.py\n+++ b/torch/nn/modules/rnn.py\n@@ -453,7 +453,7 @@ def __init__(self, *args, **kwargs):\n         elif self.nonlinearity == 'relu':\n             mode = 'RNN_RELU'\n         else:\n-            raise ValueError(\"Unknown nonlinearity '{}'\".format(self.nonlinearity))\n+            raise ValueError(f\"Unknown nonlinearity '{self.nonlinearity}'\")\n         super().__init__(mode, *args, **kwargs)\n \n     @overload\n@@ -489,7 +489,7 @@ def forward(self, input, hx=None):  # noqa: F811\n         else:\n             batch_sizes = None\n             if input.dim() not in (2, 3):\n-                raise ValueError(\"RNN: Expected input to be 2D or 3D, got {}D tensor instead\".format(input.dim()))\n+                raise ValueError(f\"RNN: Expected input to be 2D or 3D, got {input.dim()}D tensor instead\")\n             is_batched = input.dim() == 3\n             batch_dim = 0 if self.batch_first else 1\n             if not is_batched:\n@@ -798,7 +798,7 @@ def forward(self, input, hx=None):  # noqa: F811\n                 hx = self.permute_hidden(hx, sorted_indices)\n         else:\n             if input.dim() not in (2, 3):\n-                raise ValueError(\"LSTM: Expected input to be 2D or 3D, got {}D instead\".format(input.dim()))\n+                raise ValueError(f\"LSTM: Expected input to be 2D or 3D, got {input.dim()}D instead\")\n             is_batched = input.dim() == 3\n             batch_dim = 0 if self.batch_first else 1\n             if not is_batched:\n@@ -1017,7 +1017,7 @@ def forward(self, input, hx=None):  # noqa: F811\n         else:\n             batch_sizes = None\n             if input.dim() not in (2, 3):\n-                raise ValueError(\"GRU: Expected input to be 2D or 3D, got {}D instead\".format(input.dim()))\n+                raise ValueError(f\"GRU: Expected input to be 2D or 3D, got {input.dim()}D instead\")\n             is_batched = input.dim() == 3\n             batch_dim = 0 if self.batch_first else 1\n             if not is_batched:\n@@ -1174,7 +1174,7 @@ def __init__(self, input_size: int, hidden_size: int, bias: bool = True, nonline\n \n     def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         if input.dim() not in (1, 2):\n-            raise ValueError(\"RNNCell: Expected input to be 1D or 2D, got {}D instead\".format(input.dim()))\n+            raise ValueError(f\"RNNCell: Expected input to be 1D or 2D, got {input.dim()}D instead\")\n         is_batched = input.dim() == 2\n         if not is_batched:\n             input = input.unsqueeze(0)\n@@ -1199,7 +1199,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n \n         if not is_batched:\n             ret = ret.squeeze(0)\n@@ -1274,7 +1274,7 @@ def __init__(self, input_size: int, hidden_size: int, bias: bool = True,\n \n     def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) -> Tuple[Tensor, Tensor]:\n         if input.dim() not in (1, 2):\n-            raise ValueError(\"LSTMCell: Expected input to be 1D or 2D, got {}D instead\".format(input.dim()))\n+            raise ValueError(f\"LSTMCell: Expected input to be 1D or 2D, got {input.dim()}D instead\")\n         is_batched = input.dim() == 2\n         if not is_batched:\n             input = input.unsqueeze(0)\n@@ -1365,7 +1365,7 @@ def __init__(self, input_size: int, hidden_size: int, bias: bool = True,\n \n     def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         if input.dim() not in (1, 2):\n-            raise ValueError(\"GRUCell: Expected input to be 1D or 2D, got {}D instead\".format(input.dim()))\n+            raise ValueError(f\"GRUCell: Expected input to be 1D or 2D, got {input.dim()}D instead\")\n         is_batched = input.dim() == 2\n         if not is_batched:\n             input = input.unsqueeze(0)\ndiff --git a/torch/nn/modules/transformer.py b/torch/nn/modules/transformer.py\nindex 78918aee3b88d1..d95819a40f6643 100644\n--- a/torch/nn/modules/transformer.py\n+++ b/torch/nn/modules/transformer.py\n@@ -824,4 +824,4 @@ def _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n     elif activation == \"gelu\":\n         return F.gelu\n \n-    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n+    raise RuntimeError(f\"activation should be relu/gelu, not {activation}\")\ndiff --git a/torch/nn/modules/utils.py b/torch/nn/modules/utils.py\nindex a0b8428e62f527..1ec89d61790257 100644\n--- a/torch/nn/modules/utils.py\n+++ b/torch/nn/modules/utils.py\n@@ -35,7 +35,7 @@ def _list_with_default(out_size: List[int], defaults: List[int]) -> List[int]:\n         return out_size\n     if len(defaults) <= len(out_size):\n         raise ValueError(\n-            \"Input dimension should be at least {}\".format(len(out_size) + 1)\n+            f\"Input dimension should be at least {len(out_size) + 1}\"\n         )\n     return [\n         v if v is not None else d for v, d in zip(out_size, defaults[-len(out_size) :])\ndiff --git a/torch/nn/parallel/distributed.py b/torch/nn/parallel/distributed.py\nindex 6f9fdcca018ac5..ff9d80eb7d8f6d 100644\n--- a/torch/nn/parallel/distributed.py\n+++ b/torch/nn/parallel/distributed.py\n@@ -217,7 +217,7 @@ def _dump_DDP_relevant_env_vars():\n     formatted_output = \"\"\n     for var in relevant_env_vars:\n         value = os.environ[var] if var in os.environ else \"N/A\"\n-        formatted_output += \"env:%s=%s\\n\" % (var, value)\n+        formatted_output += f\"env:{var}={value}\\n\"\n     print(formatted_output)\n \n \n@@ -671,7 +671,7 @@ def __init__(\n             for n, p in module.named_parameters()\n             if n not in self.parameters_to_ignore\n         ]\n-        if not any((p.requires_grad for p in self._module_parameters)):\n+        if not any(p.requires_grad for p in self._module_parameters):\n             if len(self._delay_all_reduce_params):\n                 logger.info(\"Delay the AllReduce of all parameters.\")\n             else:\ndiff --git a/torch/nn/parallel/parallel_apply.py b/torch/nn/parallel/parallel_apply.py\nindex c37ac25be1c979..fc14a968620dca 100644\n--- a/torch/nn/parallel/parallel_apply.py\n+++ b/torch/nn/parallel/parallel_apply.py\n@@ -88,7 +88,7 @@ def _worker(\n         except Exception:\n             with lock:\n                 results[i] = ExceptionWrapper(\n-                    where=\"in replica {} on device {}\".format(i, device))\n+                    where=f\"in replica {i} on device {device}\")\n \n     if len(modules) > 1:\n         threads = [threading.Thread(target=_worker,\ndiff --git a/torch/nn/utils/init.py b/torch/nn/utils/init.py\nindex 5be10555d9eb55..4649751bc57ca7 100644\n--- a/torch/nn/utils/init.py\n+++ b/torch/nn/utils/init.py\n@@ -43,7 +43,7 @@ def skip_init(module_cls, *args, **kwargs):\n \n     \"\"\"\n     if not issubclass(module_cls, torch.nn.Module):\n-        raise RuntimeError('Expected a Module; got {}'.format(module_cls))\n+        raise RuntimeError(f'Expected a Module; got {module_cls}')\n     if 'device' not in inspect.signature(module_cls).parameters:\n         raise RuntimeError('Module must support a \\'device\\' arg to skip initialization')\n \ndiff --git a/torch/nn/utils/parametrizations.py b/torch/nn/utils/parametrizations.py\nindex 9c32876b0eb075..c451be6dd7920c 100644\n--- a/torch/nn/utils/parametrizations.py\n+++ b/torch/nn/utils/parametrizations.py\n@@ -262,7 +262,7 @@ def orthogonal(module: Module,\n     weight = getattr(module, name, None)\n     if not isinstance(weight, Tensor):\n         raise ValueError(\n-            \"Module '{}' has no parameter or buffer with name '{}'\".format(module, name)\n+            f\"Module '{module}' has no parameter or buffer with name '{name}'\"\n         )\n \n     # We could implement this for 1-dim tensors as the maps on the sphere\n@@ -556,7 +556,7 @@ def spectral_norm(module: Module,\n     weight = getattr(module, name, None)\n     if not isinstance(weight, Tensor):\n         raise ValueError(\n-            \"Module '{}' has no parameter or buffer with name '{}'\".format(module, name)\n+            f\"Module '{module}' has no parameter or buffer with name '{name}'\"\n         )\n \n     if dim is None:\ndiff --git a/torch/nn/utils/prune.py b/torch/nn/utils/prune.py\nindex caa946dc152db8..9c81618d7646a9 100644\n--- a/torch/nn/utils/prune.py\n+++ b/torch/nn/utils/prune.py\n@@ -293,7 +293,7 @@ def add_pruning_method(self, method):\n         # check that we're adding a pruning method to the container\n         if not isinstance(method, BasePruningMethod) and method is not None:\n             raise TypeError(\n-                \"{} is not a BasePruningMethod subclass\".format(type(method))\n+                f\"{type(method)} is not a BasePruningMethod subclass\"\n             )\n         elif method is not None and self._tensor_name != method._tensor_name:\n             raise ValueError(\n@@ -301,7 +301,7 @@ def add_pruning_method(self, method):\n                 \"the parameter named '{}' to PruningContainer {}.\".format(\n                     self._tensor_name, self\n                 )\n-                + \" Found '{}'\".format(method._tensor_name)\n+                + f\" Found '{method._tensor_name}'\"\n             )\n         # if all checks passed, add to _pruning_methods tuple\n         self._pruning_methods += (method,)  # type: ignore[operator]\n@@ -400,7 +400,7 @@ def _combine_masks(method, t, mask):\n \n             else:\n                 raise ValueError(\n-                    \"Unrecognized PRUNING_TYPE {}\".format(method.PRUNING_TYPE)\n+                    f\"Unrecognized PRUNING_TYPE {method.PRUNING_TYPE}\"\n                 )\n \n             # compute the new mask on the unpruned slice of the tensor t\n@@ -436,7 +436,7 @@ def apply(cls, module, name):\n             name (str): parameter name within ``module`` on which pruning\n                 will act.\n         \"\"\"\n-        return super(Identity, cls).apply(module, name)\n+        return super().apply(module, name)\n \n \n class RandomUnstructured(BasePruningMethod):\n@@ -493,7 +493,7 @@ def apply(cls, module, name, amount):\n                 fraction of parameters to prune. If ``int``, it represents the\n                 absolute number of parameters to prune.\n         \"\"\"\n-        return super(RandomUnstructured, cls).apply(module, name, amount=amount)\n+        return super().apply(module, name, amount=amount)\n \n \n class L1Unstructured(BasePruningMethod):\n@@ -556,7 +556,7 @@ def apply(cls, module, name, amount, importance_scores=None):\n                 elements in the parameter being pruned.\n                 If unspecified or None, the module parameter will be used in its place.\n         \"\"\"\n-        return super(L1Unstructured, cls).apply(\n+        return super().apply(\n             module, name, amount=amount, importance_scores=importance_scores\n         )\n \n@@ -660,7 +660,7 @@ def apply(cls, module, name, amount, dim=-1):\n             dim (int, optional): index of the dim along which we define\n                 channels to prune. Default: -1.\n         \"\"\"\n-        return super(RandomStructured, cls).apply(module, name, amount=amount, dim=dim)\n+        return super().apply(module, name, amount=amount, dim=dim)\n \n \n class LnStructured(BasePruningMethod):\n@@ -780,7 +780,7 @@ def apply(cls, module, name, amount, n, dim, importance_scores=None):\n                 elements in the parameter being pruned.\n                 If unspecified or None, the module parameter will be used in its place.\n         \"\"\"\n-        return super(LnStructured, cls).apply(\n+        return super().apply(\n             module,\n             name,\n             amount=amount,\n@@ -813,7 +813,7 @@ def apply(cls, module, name, mask):\n             name (str): parameter name within ``module`` on which pruning\n                 will act.\n         \"\"\"\n-        return super(CustomFromMask, cls).apply(module, name, mask=mask)\n+        return super().apply(module, name, mask=mask)\n \n \n def identity(module, name):\n@@ -1331,7 +1331,7 @@ def _validate_pruning_dim(t, dim):\n         dim (int): index of the dim along which we define channels to prune\n     \"\"\"\n     if dim >= t.dim():\n-        raise IndexError(\"Invalid index {} for tensor of size {}\".format(dim, t.shape))\n+        raise IndexError(f\"Invalid index {dim} for tensor of size {t.shape}\")\n \n \n def _compute_norm(t, n, dim):\ndiff --git a/torch/nn/utils/rnn.py b/torch/nn/utils/rnn.py\nindex 4f6eaa53a30e75..4f84cfb9d5a9a3 100644\n--- a/torch/nn/utils/rnn.py\n+++ b/torch/nn/utils/rnn.py\n@@ -61,7 +61,7 @@ class PackedSequence(PackedSequence_):\n \n     \"\"\"\n     def __new__(cls, data, batch_sizes=None, sorted_indices=None, unsorted_indices=None):\n-        return super(PackedSequence, cls).__new__(\n+        return super().__new__(\n             cls,\n             *_packed_sequence_init_args(data, batch_sizes, sorted_indices,\n                                         unsorted_indices))\ndiff --git a/torch/nn/utils/spectral_norm.py b/torch/nn/utils/spectral_norm.py\nindex 974b8c6f882a4b..a6d45342a37b0b 100644\n--- a/torch/nn/utils/spectral_norm.py\n+++ b/torch/nn/utils/spectral_norm.py\n@@ -212,7 +212,7 @@ def __call__(self, module, state_dict, prefix, local_metadata) -> None:\n             local_metadata['spectral_norm'] = {}\n         key = self.fn.name + '.version'\n         if key in local_metadata['spectral_norm']:\n-            raise RuntimeError(\"Unexpected key in metadata['spectral_norm']: {}\".format(key))\n+            raise RuntimeError(f\"Unexpected key in metadata['spectral_norm']: {key}\")\n         local_metadata['spectral_norm'][key] = self.fn._version\n \n \ndiff --git a/torch/overrides.py b/torch/overrides.py\nindex 64320dd7ef1dbe..7eee2f4f184a44 100644\n--- a/torch/overrides.py\n+++ b/torch/overrides.py\n@@ -1570,7 +1570,7 @@ def handle_torch_function(\n         if result is not NotImplemented:\n             return result\n \n-    func_name = '{}.{}'.format(public_api.__module__, public_api.__name__)\n+    func_name = f'{public_api.__module__}.{public_api.__name__}'\n     msg = (\n         \"no implementation found for '{}' on types that implement \"\n         '__torch_function__: {}'\ndiff --git a/torch/serialization.py b/torch/serialization.py\nindex ca362851687147..5c2e4ecd685513 100644\n--- a/torch/serialization.py\n+++ b/torch/serialization.py\n@@ -202,9 +202,9 @@ def check_module_version_greater_or_equal(module, req_version_tuple, error_if_ma\n \n     except Exception as e:\n         message = (\n-            \"'%s' module version string is malformed '%s' and cannot be compared\"\n-            \" with tuple %s\"\n-        ) % (\n+            \"'{}' module version string is malformed '{}' and cannot be compared\"\n+            \" with tuple {}\"\n+        ).format(\n             module.__name__, module.__version__, str(req_version_tuple)\n         )\n         if error_if_malformed:\n@@ -549,9 +549,9 @@ def _check_dill_version(pickle_module) -> None:\n         required_dill_version = (0, 3, 1)\n         if not check_module_version_greater_or_equal(pickle_module, required_dill_version, False):\n             raise ValueError((\n-                \"'torch' supports dill >= %s, but you have dill %s.\"\n+                \"'torch' supports dill >= {}, but you have dill {}.\"\n                 \" Please upgrade dill or switch to 'pickle'\"\n-            ) % (\n+            ).format(\n                 '.'.join([str(num) for num in required_dill_version]),\n                 pickle_module.__version__\n             ))\n@@ -559,9 +559,9 @@ def _check_dill_version(pickle_module) -> None:\n \n def _check_save_filelike(f):\n     if not isinstance(f, (str, os.PathLike)) and not hasattr(f, 'write'):\n-        raise AttributeError((\n+        raise AttributeError(\n             \"expected 'f' to be string, path, or a file-like object with \"\n-            \"a 'write' attribute\"))\n+            \"a 'write' attribute\")\n \n \n def save(\n@@ -1085,11 +1085,11 @@ def _check_container_source(container_type, source_file, original_source):\n                         if file_size == 0:\n                             f.write(lines)\n                         elif file_size != len(lines) or f.read() != lines:\n-                            raise IOError\n+                            raise OSError\n                     msg = (\"Saved a reverse patch to \" + file_name + \". \"\n                            \"Run `patch -p0 < \" + file_name + \"` to revert your \"\n                            \"changes.\")\n-                except IOError:\n+                except OSError:\n                     msg = (\"Tried to save a patch, but couldn't create a \"\n                            \"writable file \" + file_name + \". Make sure it \"\n                            \"doesn't exist and your working directory is \"\n@@ -1308,7 +1308,7 @@ def restore_location(storage, location):\n     return restore_location\n \n \n-class StorageType():\n+class StorageType:\n     def __init__(self, name):\n         self.dtype = _get_dtype_from_pickle_storage_type(name)\n \ndiff --git a/torch/storage.py b/torch/storage.py\nindex 43c4b89a2279de..5c9dd90f5bd6c9 100644\n--- a/torch/storage.py\n+++ b/torch/storage.py\n@@ -99,7 +99,7 @@ def __repr__(self):\n         return str(self)\n \n     def __iter__(self):\n-        return iter((self[i] for i in range(self.size())))\n+        return iter(self[i] for i in range(self.size()))\n \n     def __copy__(self):\n         return self.clone()\n@@ -784,7 +784,7 @@ def __repr__(self):\n \n     def __iter__(self):\n         _warn_typed_storage_removal()\n-        return iter((self[i] for i in range(self.size())))\n+        return iter(self[i] for i in range(self.size()))\n \n     def __copy__(self):\n         _warn_typed_storage_removal()\n@@ -943,13 +943,13 @@ def _from_buffer(cls, *args, dtype=None, device=None, **kwargs):\n \n         else:\n             if dtype is not None or len(args) == 5:\n-                raise RuntimeError((\n+                raise RuntimeError(\n                     \"from_buffer: 'dtype' can only be specified in \"\n-                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\"))\n+                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\")\n             if device is not None:\n-                raise RuntimeError((\n+                raise RuntimeError(\n                     \"from_buffer: 'device' can only be specified in \"\n-                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\"))\n+                    \"UntypedStorage.from_buffer and TypedStorage.from_buffer\")\n \n             dtype = cls._dtype\n             untyped_storage = torch.UntypedStorage.from_buffer(*args, dtype=dtype, **kwargs)\ndiff --git a/torch/torch_version.py b/torch/torch_version.py\nindex 745595f1df15bd..f9445ce82c413c 100644\n--- a/torch/torch_version.py\n+++ b/torch/torch_version.py\n@@ -68,7 +68,7 @@ def _convert_to_version(self, inp: Any) -> Any:\n             #   * (1)         -> Version(\"1\")\n             #   * (1, 20)     -> Version(\"1.20\")\n             #   * (1, 20, 1)  -> Version(\"1.20.1\")\n-            return Version('.'.join((str(item) for item in inp)))\n+            return Version('.'.join(str(item) for item in inp))\n         else:\n             raise InvalidVersion(inp)\n \n"
  },
  {
    "number": 105435,
    "title": "[BE] Enable ruff's UP rules and autoformat tools/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* __->__ #105435\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "bd6b7dee6eded0154c5c89efd8dfc6eb8c782c7c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105435",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105435/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105435.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105435.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105435/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105435/comments",
    "labels": [
      "release notes: onnx",
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:23:25.735153Z",
    "state": "closed",
    "patch": "From 25a0f446d45251d49f7b540e6479453a718329a9 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:23:19 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat tools/\n\n[ghstack-poisoned]\n---\n tools/autograd/gen_python_functions.py     |  5 +----\n tools/onnx/update_default_opset_version.py |  2 +-\n tools/pyi/gen_pyi.py                       |  4 +---\n tools/setup_helpers/cmake.py               | 18 +++++-------------\n tools/setup_helpers/cmake_utils.py         |  4 +---\n tools/stats/upload_stats_lib.py            |  6 ++----\n 6 files changed, 11 insertions(+), 28 deletions(-)\n\ndiff --git a/tools/autograd/gen_python_functions.py b/tools/autograd/gen_python_functions.py\nindex d1e9d60737defc..a55a40935d5ae7 100644\n--- a/tools/autograd/gen_python_functions.py\n+++ b/tools/autograd/gen_python_functions.py\n@@ -1255,10 +1255,7 @@ def go(f: NativeFunction) -> str:\n         # dispatch lambda signature\n         name = cpp.name(f.func)\n         lambda_formals = \", \".join(\n-\n-                f\"{a.type_str} {a.name}\"\n-                for a in dispatch_lambda_args(ps, f, symint=symint)\n-\n+            f\"{a.type_str} {a.name}\" for a in dispatch_lambda_args(ps, f, symint=symint)\n         )\n         lambda_return = dispatch_lambda_return_str(f)\n \ndiff --git a/tools/onnx/update_default_opset_version.py b/tools/onnx/update_default_opset_version.py\nindex 6463c6271b6ee6..a6446b1ca8a919 100755\n--- a/tools/onnx/update_default_opset_version.py\n+++ b/tools/onnx/update_default_opset_version.py\n@@ -23,7 +23,7 @@\n def read_sub_write(path: str, prefix_pat: str, new_default: int) -> None:\n     with open(path, encoding=\"utf-8\") as f:\n         content_str = f.read()\n-    content_str = re.sub(prefix_pat, fr\"\\g<1>{new_default}\", content_str)\n+    content_str = re.sub(prefix_pat, rf\"\\g<1>{new_default}\", content_str)\n     with open(path, \"w\", encoding=\"utf-8\") as f:\n         f.write(content_str)\n     print(\"modified\", path)\ndiff --git a/tools/pyi/gen_pyi.py b/tools/pyi/gen_pyi.py\nindex c74d737416870a..3f48686c5a51c3 100644\n--- a/tools/pyi/gen_pyi.py\n+++ b/tools/pyi/gen_pyi.py\n@@ -1119,9 +1119,7 @@ def replace_special_case(hint: str) -> str:\n         \"bfloat16\",\n     ]\n     for name in simple_conversions:\n-        unsorted_tensor_method_hints[name].append(\n-            f\"def {name}(self) -> Tensor: ...\"\n-        )\n+        unsorted_tensor_method_hints[name].append(f\"def {name}(self) -> Tensor: ...\")\n \n     # pyi tensor methods don't currently include deprecated signatures for some reason\n     # TODO: we should probably add them in\ndiff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py\nindex cf80bd3eb1e62c..7a58b3a03d4148 100644\n--- a/tools/setup_helpers/cmake.py\n+++ b/tools/setup_helpers/cmake.py\n@@ -61,10 +61,8 @@ def _get_cmake_command() -> str:\n \n         _cmake_min_version = LooseVersion(\"3.13.0\")\n         if all(\n-\n-                ver is None or ver < _cmake_min_version\n-                for ver in [cmake_version, cmake3_version]\n-\n+            ver is None or ver < _cmake_min_version\n+            for ver in [cmake_version, cmake3_version]\n         ):\n             raise RuntimeError(\"no cmake or cmake3 with version >= 3.13.0 found\")\n \n@@ -172,9 +170,7 @@ def generate(\n                     args.append(\"-Ax64\")\n                     toolset_dict[\"host\"] = \"x64\"\n             if toolset_dict:\n-                toolset_expr = \",\".join(\n-                    [f\"{k}={v}\" for k, v in toolset_dict.items()]\n-                )\n+                toolset_expr = \",\".join([f\"{k}={v}\" for k, v in toolset_dict.items()])\n                 args.append(\"-T\" + toolset_expr)\n \n         base_dir = os.path.dirname(\n@@ -324,9 +320,7 @@ def generate(\n             if \"CMAKE_C_COMPILER\" not in build_options and \"CC\" not in os.environ:\n                 CMake.defines(args, CMAKE_C_COMPILER=f\"{expected_wrapper}/gcc\")\n             if \"CMAKE_CXX_COMPILER\" not in build_options and \"CXX\" not in os.environ:\n-                CMake.defines(\n-                    args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\"\n-                )\n+                CMake.defines(args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\")\n \n         for env_var_name in my_env:\n             if env_var_name.startswith(\"gh\"):\n@@ -335,9 +329,7 @@ def generate(\n                 try:\n                     my_env[env_var_name] = str(my_env[env_var_name].encode(\"utf-8\"))\n                 except UnicodeDecodeError as e:\n-                    shex = \":\".join(\n-                        f\"{ord(c):02x}\" for c in my_env[env_var_name]\n-                    )\n+                    shex = \":\".join(f\"{ord(c):02x}\" for c in my_env[env_var_name])\n                     print(\n                         f\"Invalid ENV[{env_var_name}] = {shex}\",\n                         file=sys.stderr,\ndiff --git a/tools/setup_helpers/cmake_utils.py b/tools/setup_helpers/cmake_utils.py\nindex c15b6f7592c015..bbef56de175bbc 100644\n--- a/tools/setup_helpers/cmake_utils.py\n+++ b/tools/setup_helpers/cmake_utils.py\n@@ -71,9 +71,7 @@ def get_cmake_cache_variables_from_file(\n             r'(\"?)(.+?)\\1(?::\\s*([a-zA-Z_-][a-zA-Z0-9_-]*)?)?\\s*=\\s*(.*)', line\n         )\n         if matched is None:  # Illegal line\n-            raise ValueError(\n-                f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\"\n-            )\n+            raise ValueError(f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\")\n         _, variable, type_, value = matched.groups()\n         if type_ is None:\n             type_ = \"\"\ndiff --git a/tools/stats/upload_stats_lib.py b/tools/stats/upload_stats_lib.py\nindex c7e90286d00a2b..126d78233d924b 100644\n--- a/tools/stats/upload_stats_lib.py\n+++ b/tools/stats/upload_stats_lib.py\n@@ -249,10 +249,8 @@ def value(self) -> Any:\n         value = os.environ.get(self.env_var)\n         if value is None and self.required:\n             raise ValueError(\n-\n-                    f\"Missing {self.name}. Please set the {self.env_var}\"\n-                    \"environment variable to pass in this value.\"\n-\n+                f\"Missing {self.name}. Please set the {self.env_var}\"\n+                \"environment variable to pass in this value.\"\n             )\n         if self.type_conversion_fn:\n             return self.type_conversion_fn(value)\n"
  },
  {
    "number": 105434,
    "title": "[BE] Enable ruff's UP rules and autoformat test/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* __->__ #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @mcarilli @ptrblck @leslie-fang-intel",
    "merge_commit_sha": "7d8efda71d518dbfc2abb2a399f3a997e62814de",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105434",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105434/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105434.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105434.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105434/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105434/comments",
    "labels": [
      "ciflow/mps",
      "open source",
      "module: mkldnn",
      "release notes: quantization",
      "module: amp (automated mixed precision)",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:23:22.051077Z",
    "state": "open",
    "patch": "From 8c0bd716928917e40aa65e4e9b72ea0feb375814 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:23:14 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat test/\n\n[ghstack-poisoned]\n---\n test/benchmark_utils/test_benchmark_utils.py  |  4 +-\n test/cpp/api/init_baseline.py                 |  4 +-\n test/cpp/api/optim_baseline.py                |  4 +-\n test/cpp_api_parity/functional_impl_check.py  |  6 +-\n test/cpp_api_parity/module_impl_check.py      | 14 ++--\n test/cpp_api_parity/parity_table_parser.py    |  8 +-\n test/cpp_api_parity/utils.py                  | 16 ++--\n test/custom_backend/backend.py                |  2 +-\n test/custom_operator/model.py                 |  2 +-\n test/jit/test_class_type.py                   |  2 +-\n test/jit/test_enum.py                         |  4 +-\n test/jit/test_list_dict.py                    |  2 +-\n test/jit/test_module_containers.py            |  2 +-\n ...optimize_for_mobile_preserve_debug_info.py |  5 +-\n test/jit/test_save_load.py                    |  6 +-\n test/jit/test_string_formatting.py            |  4 +-\n test/jit/test_tracer.py                       |  6 +-\n test/mobile/model_test/nn_ops.py              |  2 +-\n test/mobile/test_lite_script_module.py        |  8 +-\n test/mobile/test_upgrader_codegen.py          |  4 +-\n test/package/test_directory_reader.py         |  7 +-\n test/package/test_misc.py                     |  1 -\n test/package/test_resources.py                |  7 +-\n test/profiler/test_profiler.py                | 31 ++++---\n .../bc/test_backward_compatibility.py         |  3 +-\n .../experimental/apot_fx_graph_mode_ptq.py    | 16 ++--\n .../experimental/apot_fx_graph_mode_qat.py    | 12 +--\n .../core/test_quantized_module.py             |  8 +-\n test/quantization/core/test_quantized_op.py   | 80 +++++++++----------\n .../quantization/core/test_workflow_module.py |  2 +-\n test/quantization/fx/test_model_report_fx.py  |  1 -\n test/quantization/fx/test_quantize_fx.py      | 12 +--\n test/quantization/jit/test_fusion_passes.py   |  1 -\n .../jit/test_ondevice_quantization.py         |  1 -\n test/quantization/jit/test_quantize_jit.py    | 15 ++--\n test/run_test.py                              |  4 +-\n test/test_ao_sparsity.py                      |  1 -\n test/test_autocast.py                         |  2 +-\n test/test_autograd.py                         | 12 +--\n test/test_binary_ufuncs.py                    | 18 ++---\n test/test_compile_benchmark_util.py           |  2 +-\n test/test_cpp_extensions_jit.py               |  2 +-\n ...cpp_extensions_open_device_registration.py |  4 +-\n test/test_cuda.py                             | 16 ++--\n test/test_cuda_expandable_segments.py         |  2 +-\n test/test_dataloader.py                       | 28 +++----\n test/test_datapipe.py                         |  4 +-\n test/test_dispatch.py                         | 10 +--\n test/test_dlpack.py                           |  1 -\n test/test_dynamic_shapes.py                   |  1 -\n test/test_fake_tensor.py                      |  2 +-\n test/test_foreach.py                          | 12 +--\n test/test_functional_optim.py                 | 10 +--\n test/test_jit.py                              | 49 ++++++------\n test/test_jit_fuser.py                        |  3 +-\n test/test_jit_fuser_te.py                     |  2 +-\n test/test_jit_llga_fuser.py                   |  2 +-\n test/test_linalg.py                           | 23 +++---\n test/test_logging.py                          |  2 +-\n test/test_matmul_cuda.py                      |  3 +-\n test/test_meta.py                             |  4 +-\n test/test_mobile_optimizer.py                 | 12 +--\n test/test_mps.py                              | 17 ++--\n test/test_namedtensor.py                      |  4 +-\n test/test_namedtuple_return_api.py            |  2 +-\n test/test_nestedtensor.py                     |  2 +-\n test/test_nn.py                               | 20 ++---\n test/test_nnapi.py                            |  2 +-\n test/test_openmp.py                           |  2 +-\n test/test_ops.py                              | 32 ++++----\n test/test_ops_jit.py                          |  2 +-\n test/test_overrides.py                        | 12 +--\n test/test_public_bindings.py                  |  1 -\n test/test_python_dispatch.py                  |  2 +-\n test/test_quantization.py                     |  1 -\n test/test_reductions.py                       | 14 ++--\n test/test_scatter_gather_ops.py               |  1 -\n test/test_schema_check.py                     |  2 +-\n test/test_segment_reductions.py               |  2 +-\n test/test_serialization.py                    |  5 +-\n test/test_sort_and_select.py                  |  6 +-\n test/test_sparse.py                           | 15 ++--\n test/test_sparse_csr.py                       |  8 +-\n test/test_subclass.py                         |  2 +-\n test/test_sympy_utils.py                      | 13 ++-\n test/test_tensor_creation_ops.py              | 14 ++--\n test/test_tensorboard.py                      |  2 +-\n test/test_tensorexpr.py                       |  8 +-\n test/test_tensorexpr_pybind.py                |  6 +-\n test/test_testing.py                          | 32 ++++----\n test/test_torch.py                            | 35 ++++----\n test/test_type_promotion.py                   | 20 ++---\n test/test_unary_ufuncs.py                     | 16 ++--\n test/test_utils.py                            | 14 ++--\n test/test_xnnpack_integration.py              | 36 ++++-----\n 95 files changed, 428 insertions(+), 455 deletions(-)\n\ndiff --git a/test/benchmark_utils/test_benchmark_utils.py b/test/benchmark_utils/test_benchmark_utils.py\nindex 933ff877b816db..2267262551b43f 100644\n--- a/test/benchmark_utils/test_benchmark_utils.py\n+++ b/test/benchmark_utils/test_benchmark_utils.py\n@@ -55,7 +55,7 @@ def to_entry(fn_counts):\n         \"ones_with_data_exclusive\": to_entry(stats_with_data.stmt_exclusive_stats),\n     }\n \n-    with open(CALLGRIND_ARTIFACTS, \"wt\") as f:\n+    with open(CALLGRIND_ARTIFACTS, \"w\") as f:\n         json.dump(artifacts, f, indent=4)\n \n \n@@ -70,7 +70,7 @@ def load_callgrind_artifacts() -> Tuple[benchmark_utils.CallgrindStats, benchmar\n     testing are stored in raw string form for easier inspection and to avoid\n     baking any implementation details into the artifact itself.\n     \"\"\"\n-    with open(CALLGRIND_ARTIFACTS, \"rt\") as f:\n+    with open(CALLGRIND_ARTIFACTS) as f:\n         artifacts = json.load(f)\n \n     pattern = re.compile(r\"^\\s*([0-9]+)\\s(.+)$\")\ndiff --git a/test/cpp/api/init_baseline.py b/test/cpp/api/init_baseline.py\nindex 018a0aa824c241..9ed88f7c226c6e 100644\n--- a/test/cpp/api/init_baseline.py\n+++ b/test/cpp/api/init_baseline.py\n@@ -34,7 +34,7 @@ def emit(initializer_parameter_map):\n             print(\"    {\")\n             for parameter in sample:\n                 parameter_values = \"{{{}}}\".format(\", \".join(map(str, parameter)))\n-                print(\"      torch::tensor({}),\".format(parameter_values))\n+                print(f\"      torch::tensor({parameter_values}),\")\n             print(\"    },\")\n         print(\"  };\")\n         print(\"}\\n\")\n@@ -63,7 +63,7 @@ def run(initializer):\n def main():\n     initializer_parameter_map = {}\n     for initializer in INITIALIZERS.keys():\n-        sys.stderr.write('Evaluating {} ...\\n'.format(initializer))\n+        sys.stderr.write(f'Evaluating {initializer} ...\\n')\n         initializer_parameter_map[initializer] = run(initializer)\n \n     emit(initializer_parameter_map)\ndiff --git a/test/cpp/api/optim_baseline.py b/test/cpp/api/optim_baseline.py\nindex e2e8f9e61e807b..16d2508ab41ce6 100644\n--- a/test/cpp/api/optim_baseline.py\n+++ b/test/cpp/api/optim_baseline.py\n@@ -98,7 +98,7 @@ def emit(optimizer_parameter_map):\n             print(\"    {\")\n             for parameter in sample:\n                 parameter_values = \"{{{}}}\".format(\", \".join(map(str, parameter)))\n-                print(\"      torch::tensor({}),\".format(parameter_values))\n+                print(f\"      torch::tensor({parameter_values}),\")\n             print(\"    },\")\n         print(\"  };\")\n         print(\"}\\n\")\n@@ -115,7 +115,7 @@ def main():\n \n     optimizer_parameter_map = {}\n     for optimizer in OPTIMIZERS.keys():\n-        sys.stderr.write('Evaluating {} ...\\n'.format(optimizer))\n+        sys.stderr.write(f'Evaluating {optimizer} ...\\n')\n         optimizer_parameter_map[optimizer] = run(\n             optimizer, options.iterations, options.sample_every\n         )\ndiff --git a/test/cpp_api_parity/functional_impl_check.py b/test/cpp_api_parity/functional_impl_check.py\nindex 55cfcb5f626172..c09aaccd8f76b6 100644\n--- a/test/cpp_api_parity/functional_impl_check.py\n+++ b/test/cpp_api_parity/functional_impl_check.py\n@@ -90,7 +90,7 @@ def test_forward(unit_test_class, test_params):\n     arg_dict_file_path = compute_temp_file_path(cpp_tmp_folder, functional_variant_name, 'arg_dict')\n     serialize_arg_dict_as_script_module(test_params.arg_dict).save(arg_dict_file_path)\n \n-    cpp_test_name = '{}_test_forward'.format(test_params.functional_variant_name)\n+    cpp_test_name = f'{test_params.functional_variant_name}_test_forward'\n     cpp_test_fn = getattr(unit_test_class.functional_impl_check_cpp_module, cpp_test_name)\n \n     def run_cpp_test_fn_and_check_output():\n@@ -194,7 +194,7 @@ def write_test_to_test_class(\n             test_instance_class=test_instance_class,\n         )\n         try_remove_folder(test_params.cpp_tmp_folder)\n-        unit_test_name = 'test_torch_nn_functional_{}'.format(test_params.functional_variant_name)\n+        unit_test_name = f'test_torch_nn_functional_{test_params.functional_variant_name}'\n         unit_test_class.functional_test_params_map[unit_test_name] = test_params\n \n         def test_fn(self):\n@@ -227,7 +227,7 @@ def build_cpp_tests(unit_test_class, print_cpp_source=False):\n     functions = []\n     for test_name, test_params in unit_test_class.functional_test_params_map.items():\n         cpp_sources += generate_test_cpp_sources(test_params=test_params, template=TORCH_NN_FUNCTIONAL_TEST_FORWARD)\n-        functions.append('{}_test_forward'.format(test_params.functional_variant_name))\n+        functions.append(f'{test_params.functional_variant_name}_test_forward')\n     if print_cpp_source:\n         print(cpp_sources)\n \ndiff --git a/test/cpp_api_parity/module_impl_check.py b/test/cpp_api_parity/module_impl_check.py\nindex f2c86e9eb91b45..8ede07b59034a4 100644\n--- a/test/cpp_api_parity/module_impl_check.py\n+++ b/test/cpp_api_parity/module_impl_check.py\n@@ -146,7 +146,7 @@ def test_forward_backward(unit_test_class, test_params):\n     script_module.save(module_file_path)\n     serialize_arg_dict_as_script_module(test_params.arg_dict).save(arg_dict_file_path)\n \n-    cpp_test_name = '{}_test_forward_backward'.format(test_params.module_variant_name)\n+    cpp_test_name = f'{test_params.module_variant_name}_test_forward_backward'\n     cpp_test_fn = getattr(unit_test_class.module_impl_check_cpp_module, cpp_test_name)\n \n     def run_cpp_test_fn_and_check_output():\n@@ -177,12 +177,12 @@ def run_cpp_test_fn_and_check_output():\n             unit_test_class.assertTrue(\n                 key in cpp_grad_dict,\n                 msg=generate_error_msg(\n-                    \"\\\"Does module have a parameter named `{}` with {} gradient?\\\"\".format(param_name, sparsity_str),\n+                    f\"\\\"Does module have a parameter named `{param_name}` with {sparsity_str} gradient?\\\"\",\n                     False, True))\n             unit_test_class.assertEqual(\n                 python_grad_dict[key], cpp_grad_dict[key],\n                 msg=generate_error_msg(\n-                    \"`{}`'s {} gradient (`{}`)\".format(param_name, sparsity_str, key),\n+                    f\"`{param_name}`'s {sparsity_str} gradient (`{key}`)\",\n                     cpp_grad_dict[key], python_grad_dict[key]))\n \n     run_cpp_test_fn_and_check_output()\n@@ -251,7 +251,7 @@ def write_test_to_test_class(\n             test_instance_class=test_instance_class,\n         )\n         try_remove_folder(test_params.cpp_tmp_folder)\n-        unit_test_name = 'test_torch_nn_{}'.format(test_params.module_variant_name)\n+        unit_test_name = f'test_torch_nn_{test_params.module_variant_name}'\n         unit_test_class.module_test_params_map[unit_test_name] = test_params\n \n         def test_fn(self):\n@@ -272,14 +272,14 @@ def generate_test_cpp_sources(test_params, template):\n \n     cpp_constructor_args = test_params.cpp_constructor_args\n     if cpp_constructor_args != '':\n-        cpp_constructor_args = '({})'.format(cpp_constructor_args)\n+        cpp_constructor_args = f'({cpp_constructor_args})'\n \n     cpp_args_construction_stmts, cpp_forward_args_symbols = \\\n         compute_cpp_args_construction_stmts_and_forward_arg_symbols(test_params)\n \n     test_cpp_sources = template.substitute(\n         module_variant_name=test_params.module_variant_name,\n-        module_qualified_name='torch::nn::{}'.format(test_params.module_name),\n+        module_qualified_name=f'torch::nn::{test_params.module_name}',\n         cpp_args_construction_stmts=\";\\n  \".join(cpp_args_construction_stmts),\n         cpp_constructor_args=cpp_constructor_args,\n         cpp_forward_args_symbols=\", \".join(cpp_forward_args_symbols),\n@@ -295,7 +295,7 @@ def build_cpp_tests(unit_test_class, print_cpp_source=False):\n     for test_name, test_params in unit_test_class.module_test_params_map.items():\n         cpp_sources += generate_test_cpp_sources(\n             test_params=test_params, template=TORCH_NN_MODULE_TEST_FORWARD_BACKWARD)\n-        functions.append('{}_test_forward_backward'.format(test_params.module_variant_name))\n+        functions.append(f'{test_params.module_variant_name}_test_forward_backward')\n     if print_cpp_source:\n         print(cpp_sources)\n \ndiff --git a/test/cpp_api_parity/parity_table_parser.py b/test/cpp_api_parity/parity_table_parser.py\nindex b4f0248c4aa2d7..f9f5497d968d3c 100644\n--- a/test/cpp_api_parity/parity_table_parser.py\n+++ b/test/cpp_api_parity/parity_table_parser.py\n@@ -35,22 +35,22 @@ def parse_parity_choice(str):\n             return str == 'Yes'\n         else:\n             raise RuntimeError(\n-                '{} is not a supported parity choice. The valid choices are \"Yes\" and \"No\".'.format(str))\n+                f'{str} is not a supported parity choice. The valid choices are \"Yes\" and \"No\".')\n \n     parity_tracker_dict = {}\n \n-    with open(file_path, 'r') as f:\n+    with open(file_path) as f:\n         all_text = f.read()\n         packages = all_text.split('##')\n         for package in packages[1:]:\n             lines = [line.strip() for line in package.split('\\n') if line.strip() != '']\n             package_name = lines[0]\n             if package_name in parity_tracker_dict:\n-                raise RuntimeError(\"Duplicated package name `{}` found in {}\".format(package_name, file_path))\n+                raise RuntimeError(f\"Duplicated package name `{package_name}` found in {file_path}\")\n             else:\n                 parity_tracker_dict[package_name] = {}\n             for api_status in lines[3:]:\n-                api_name, has_impl_parity_str, has_doc_parity_str = [x.strip() for x in api_status.split('|')]\n+                api_name, has_impl_parity_str, has_doc_parity_str = (x.strip() for x in api_status.split('|'))\n                 parity_tracker_dict[package_name][api_name] = ParityStatus(\n                     has_impl_parity=parse_parity_choice(has_impl_parity_str),\n                     has_doc_parity=parse_parity_choice(has_doc_parity_str))\ndiff --git a/test/cpp_api_parity/utils.py b/test/cpp_api_parity/utils.py\nindex 62a85b0f695971..8ac4700cb3c948 100644\n--- a/test/cpp_api_parity/utils.py\n+++ b/test/cpp_api_parity/utils.py\n@@ -150,7 +150,7 @@ def compile_cpp_code_inline(name, cpp_sources, functions):\n     return cpp_module\n \n def compute_temp_file_path(cpp_tmp_folder, variant_name, file_suffix):\n-    return os.path.join(cpp_tmp_folder, '{}_{}.pt'.format(variant_name, file_suffix))\n+    return os.path.join(cpp_tmp_folder, f'{variant_name}_{file_suffix}.pt')\n \n def is_torch_nn_functional_test(test_params_dict):\n     return 'wrap_functional' in str(test_params_dict.get('constructor', ''))\n@@ -177,11 +177,11 @@ def add_test(unit_test_class, test_name, test_fn):\n \n def set_cpp_tensors_requires_grad(cpp_tensor_stmts, python_tensors):\n     assert len(cpp_tensor_stmts) == len(python_tensors)\n-    return ['{}.requires_grad_(true)'.format(tensor_stmt) if tensor.dtype != torch.long else tensor_stmt\n+    return [f'{tensor_stmt}.requires_grad_(true)' if tensor.dtype != torch.long else tensor_stmt\n             for tensor_stmt, (_, tensor) in zip(cpp_tensor_stmts, python_tensors)]\n \n def move_cpp_tensors_to_device(cpp_tensor_stmts, device):\n-    return ['{}.to(\"{}\")'.format(tensor_stmt, device) for tensor_stmt in cpp_tensor_stmts]\n+    return [f'{tensor_stmt}.to(\"{device}\")' for tensor_stmt in cpp_tensor_stmts]\n \n def is_criterion_test(test_instance):\n     return isinstance(test_instance, common_nn.CriterionTest)\n@@ -209,7 +209,7 @@ def compute_cpp_args_construction_stmts_and_forward_arg_symbols(test_params):\n     def add_cpp_forward_args(args):\n         args_stmts = []\n         for arg_name, _ in args:\n-            args_stmts.append('auto {} = arg_dict.at(\"{}\")'.format(arg_name, arg_name))\n+            args_stmts.append(f'auto {arg_name} = arg_dict.at(\"{arg_name}\")')\n             cpp_forward_args_symbols.append(arg_name)\n         return args_stmts\n \n@@ -223,7 +223,7 @@ def add_cpp_forward_args(args):\n     # Build the list of other arguments needed\n     cpp_other_args_stmts = []\n     for arg_name, _ in test_params.arg_dict['other']:\n-        cpp_other_args_stmts.append('auto {} = arg_dict.at(\"{}\")'.format(arg_name, arg_name))\n+        cpp_other_args_stmts.append(f'auto {arg_name} = arg_dict.at(\"{arg_name}\")')\n     cpp_other_args_stmts = move_cpp_tensors_to_device(cpp_other_args_stmts, device)\n \n     cpp_args_construction_stmts = cpp_forward_input_args_stmts + cpp_forward_target_args_stmts + \\\n@@ -292,11 +292,11 @@ def put_args_into_arg_dict(arg_type, arg_type_prefix, args):\n             if arg_value == '_get_input()':\n                 arg_dict['other'].append(CppArg(name=arg_name, value=test_instance._get_input()))\n             else:\n-                raise RuntimeError(\"`{}` has unsupported string value: {}\".format(arg_name, arg_value))\n+                raise RuntimeError(f\"`{arg_name}` has unsupported string value: {arg_value}\")\n         elif isinstance(arg_value, torch.Tensor):\n             arg_dict['other'].append(CppArg(name=arg_name, value=arg_value))\n         else:\n-            raise RuntimeError(\"`{}` has unsupported value: {}\".format(arg_name, arg_value))\n+            raise RuntimeError(f\"`{arg_name}` has unsupported value: {arg_value}\")\n \n     return arg_dict\n \n@@ -351,4 +351,4 @@ def try_remove_folder(folder_path):\n         try:\n             shutil.rmtree(folder_path)\n         except Exception as e:\n-            warnings.warn(\"Non-blocking folder removal fails with the following error:\\n{}\".format(str(e)))\n+            warnings.warn(f\"Non-blocking folder removal fails with the following error:\\n{str(e)}\")\ndiff --git a/test/custom_backend/backend.py b/test/custom_backend/backend.py\nindex 7c811424765546..477dd37b57d6f3 100644\n--- a/test/custom_backend/backend.py\n+++ b/test/custom_backend/backend.py\n@@ -17,7 +17,7 @@ def get_custom_backend_library_path():\n         library_filename = \"libcustom_backend.dylib\"\n     else:\n         library_filename = \"libcustom_backend.so\"\n-    path = os.path.abspath(\"build/{}\".format(library_filename))\n+    path = os.path.abspath(f\"build/{library_filename}\")\n     assert os.path.exists(path), path\n     return path\n \ndiff --git a/test/custom_operator/model.py b/test/custom_operator/model.py\nindex ff9e310b556d34..ce763bdcce5d40 100644\n--- a/test/custom_operator/model.py\n+++ b/test/custom_operator/model.py\n@@ -12,7 +12,7 @@ def get_custom_op_library_path():\n         library_filename = \"libcustom_ops.dylib\"\n     else:\n         library_filename = \"libcustom_ops.so\"\n-    path = os.path.abspath(\"build/{}\".format(library_filename))\n+    path = os.path.abspath(f\"build/{library_filename}\")\n     assert os.path.exists(path), path\n     return path\n \ndiff --git a/test/jit/test_class_type.py b/test/jit/test_class_type.py\nindex 80829795d0abb4..cf98d485dd62c2 100644\n--- a/test/jit/test_class_type.py\n+++ b/test/jit/test_class_type.py\n@@ -1404,7 +1404,7 @@ def test_recursive_script_module_builtin_type_resolution(self):\n         Test resolution of built-in torch types(e.g. torch.Tensor, torch.device) when a class is recursively compiled\n         when compiling a module.\n         \"\"\"\n-        class Wrapper():\n+        class Wrapper:\n             def __init__(self, t):\n                 self.t = t\n \ndiff --git a/test/jit/test_enum.py b/test/jit/test_enum.py\nindex 5198688c08dfe2..8428ab9769765d 100644\n--- a/test/jit/test_enum.py\n+++ b/test/jit/test_enum.py\n@@ -218,7 +218,7 @@ def closed_over_aliased_type():\n             return a.RED.value\n \n         FileCheck() \\\n-            .check(\"prim::Constant[value={}]\".format(a.RED.value)) \\\n+            .check(f\"prim::Constant[value={a.RED.value}]\") \\\n             .check_next(\"return\") \\\n             .run(str(closed_over_aliased_type.graph))\n \n@@ -231,7 +231,7 @@ def closed_over_aliased_value():\n             return b.value\n \n         FileCheck() \\\n-            .check(\"prim::Constant[value={}]\".format(b.value)) \\\n+            .check(f\"prim::Constant[value={b.value}]\") \\\n             .check_next(\"return\") \\\n             .run(str(closed_over_aliased_value.graph))\n \ndiff --git a/test/jit/test_list_dict.py b/test/jit/test_list_dict.py\nindex 57b2281070b3f1..7e0afd7b31ac06 100644\n--- a/test/jit/test_list_dict.py\n+++ b/test/jit/test_list_dict.py\n@@ -1391,7 +1391,7 @@ def x():\n \n class TestDict(JitTestCase):\n     def dict(self):\n-        return {u'a': torch.ones(1), u'b': torch.ones(1) + 1, u'c': torch.ones(1) + 2}\n+        return {'a': torch.ones(1), 'b': torch.ones(1) + 1, 'c': torch.ones(1) + 2}\n \n     def dict2(self):\n         return {'x': torch.ones(1) + 100, 'y': torch.ones(1) + 101, 'z': torch.ones(1) + 102}\ndiff --git a/test/jit/test_module_containers.py b/test/jit/test_module_containers.py\nindex 62699e0958dabd..7500bff82479ab 100644\n--- a/test/jit/test_module_containers.py\n+++ b/test/jit/test_module_containers.py\n@@ -643,7 +643,7 @@ def forward(self, x):\n \n         n._reconstruct(m._c)\n \n-        inp = torch.rand((3))\n+        inp = torch.rand(3)\n \n         # Check that both modules produce the same output.\n         with torch.no_grad():\ndiff --git a/test/jit/test_optimize_for_mobile_preserve_debug_info.py b/test/jit/test_optimize_for_mobile_preserve_debug_info.py\nindex 78d3fae5937169..26642a9ac4cb5f 100644\n--- a/test/jit/test_optimize_for_mobile_preserve_debug_info.py\n+++ b/test/jit/test_optimize_for_mobile_preserve_debug_info.py\n@@ -76,10 +76,7 @@ def __init__(\n                 conv_transpose2d_weight,\n                 conv_transpose2d_bias,\n             ):\n-                super(\n-                    TestPrepackedLinearBeforeInlineAndConv2dOp,\n-                    self,\n-                ).__init__()\n+                super().__init__()\n                 self.linear_weight = linear_weight.float()\n                 self.linear_bias = linear_bias.float()\n                 self.conv2d_weight = conv2d_weight.float()\ndiff --git a/test/jit/test_save_load.py b/test/jit/test_save_load.py\nindex 079247200cb171..296515fe3eef55 100644\n--- a/test/jit/test_save_load.py\n+++ b/test/jit/test_save_load.py\n@@ -638,7 +638,7 @@ def get_loaded_inputs(inputs):\n         # Validate that with no input specified the traced inputs are stored\n         traced_module = torch.jit.trace(module, input_tensor)\n         traced_inputs = list(traced_module.graph.inputs())\n-        self.assertEquals(traced_module._c._retrieve_traced_inputs()['forward'], [input_tensor])\n+        self.assertEqual(traced_module._c._retrieve_traced_inputs()['forward'], [input_tensor])\n         with TemporaryFileName() as fname:\n             path = pathlib.Path(fname)\n             traced_module.save(path)\n@@ -654,7 +654,7 @@ def get_loaded_inputs(inputs):\n         # Validate that inputs aren't saved when requested not to\n         traced_module = torch.jit.trace(module, input_tensor, _store_inputs=False)\n         traced_inputs = list(traced_module.graph.inputs())\n-        self.assertEquals(len(traced_module._c._retrieve_traced_inputs()), 0)\n+        self.assertEqual(len(traced_module._c._retrieve_traced_inputs()), 0)\n \n         with TemporaryFileName() as fname:\n             path = pathlib.Path(fname)\n@@ -721,7 +721,7 @@ def test_save_load_large_string_attribute(self):\n \n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.x = \"x\" * (2 ** 32 + 1)\n \n             def forward(self, i) -> int:\ndiff --git a/test/jit/test_string_formatting.py b/test/jit/test_string_formatting.py\nindex e739de3be20d69..4f4605ec804659 100644\n--- a/test/jit/test_string_formatting.py\n+++ b/test/jit/test_string_formatting.py\n@@ -136,7 +136,7 @@ def fn(arg1: List[str]) -> str:\n     def test_string_interpolation_with_too_few_arguments(self):\n         @torch.jit.script\n         def fn(arg1: str) -> str:\n-            return \"%s %s in template\" % arg1\n+            return \"{} {} in template\".format(*arg1)\n \n         with self.assertRaisesRegexWithHighlight(RuntimeError,\n                                                  \"Too few arguments for format string\",\n@@ -146,7 +146,7 @@ def fn(arg1: str) -> str:\n     def test_string_interpolation_with_too_many_arguments(self):\n         @torch.jit.script\n         def fn(arg1: str, arg2: str) -> str:\n-            return \"%s in template\" % (arg1, arg2)    # noqa: F507\n+            return f\"{arg1} in template\"    # noqa: F507\n \n         with self.assertRaisesRegexWithHighlight(RuntimeError,\n                                                  \"Too many arguments for format string\",\ndiff --git a/test/jit/test_tracer.py b/test/jit/test_tracer.py\nindex f7fd1f22a68a7a..b9fc5c451f4bb4 100644\n--- a/test/jit/test_tracer.py\n+++ b/test/jit/test_tracer.py\n@@ -1973,7 +1973,7 @@ def forward(self, first_arg: torch.Tensor, second_arg: torch.Tensor):\n     def test_trace_checking_with_deprecated_name(self):\n         class MyClass(torch.nn.Module):\n             def __init__(self):\n-                super(MyClass, self).__init__()\n+                super().__init__()\n \n             def forward(self, x, y, **deprecated_arguments):\n                 if len(deprecated_arguments) > 0:\n@@ -1987,7 +1987,7 @@ def forward(self, x, y, **deprecated_arguments):\n     def test_trace_with_tuple_tensor(self):\n         class MyClass(torch.nn.Module):\n             def __init__(self):\n-                super(MyClass, self).__init__()\n+                super().__init__()\n \n             def forward(self, x, y):\n                 return x + y[0] + y[1]\n@@ -2593,7 +2593,7 @@ def inner_fn(x):\n         def outer_fn(x, y):\n             return inner_fn(x + y).relu()\n \n-        x, y = [torch.rand((2, 2), dtype=torch.float) for _ in range(2)]\n+        x, y = (torch.rand((2, 2), dtype=torch.float) for _ in range(2))\n         fn_t = torch.jit.trace(outer_fn, (x, y))\n \n         # expect that the CallFunction node return type has shape information on it.\ndiff --git a/test/mobile/model_test/nn_ops.py b/test/mobile/model_test/nn_ops.py\nindex 6389a00812680c..890d524d2138b4 100644\n--- a/test/mobile/model_test/nn_ops.py\n+++ b/test/mobile/model_test/nn_ops.py\n@@ -60,7 +60,7 @@ def __init__(self):\n                 nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)),\n                 nn.LPPool2d(2, 3, stride=(2, 1)),\n                 nn.AdaptiveMaxPool2d((5, 7)),\n-                nn.AdaptiveAvgPool2d((7)),\n+                nn.AdaptiveAvgPool2d(7),\n             ]\n         )\n \ndiff --git a/test/mobile/test_lite_script_module.py b/test/mobile/test_lite_script_module.py\nindex f75a02b28c2a8d..a695f3a62279bb 100644\n--- a/test/mobile/test_lite_script_module.py\n+++ b/test/mobile/test_lite_script_module.py\n@@ -197,7 +197,7 @@ def forward(self, x, one: int = 1):\n         )\n \n     def test_unsupported_classtype(self):\n-        class Foo():\n+        class Foo:\n             def __init__(self):\n                 return\n \n@@ -313,7 +313,7 @@ def forward(self, x, w):\n         loaded = self.getScriptExportImportCopy(ft)\n         _, lineno = inspect.getsourcelines(FooTest)\n \n-        with self.assertRaisesRegex(RuntimeError, 'test_lite_script_module.py\\\", line {}'.format(lineno + 3)):\n+        with self.assertRaisesRegex(RuntimeError, f'test_lite_script_module.py\\\", line {lineno + 3}'):\n             loaded(torch.rand(3, 4), torch.rand(30, 40))\n \n     def test_source_range_raise_exception(self):\n@@ -357,8 +357,8 @@ def forward(self, x, y, w):\n             loaded(torch.rand(3, 4), torch.rand(3, 4), torch.rand(30, 40))\n         except RuntimeError as e:\n             error_message = f\"{e}\"\n-        self.assertTrue('test_lite_script_module.py\\\", line {}'.format(lineno + 3) in error_message)\n-        self.assertTrue('test_lite_script_module.py\\\", line {}'.format(lineno + 9) in error_message)\n+        self.assertTrue(f'test_lite_script_module.py\\\", line {lineno + 3}' in error_message)\n+        self.assertTrue(f'test_lite_script_module.py\\\", line {lineno + 9}' in error_message)\n         self.assertTrue('top(FooTest3)' in error_message)\n \n     def test_source_range_no_debug_info(self):\ndiff --git a/test/mobile/test_upgrader_codegen.py b/test/mobile/test_upgrader_codegen.py\nindex 5ccf9a020a5b54..3357507f2e7244 100644\n--- a/test/mobile/test_upgrader_codegen.py\n+++ b/test/mobile/test_upgrader_codegen.py\n@@ -21,9 +21,9 @@ def test_generate_bytecode(self):\n         upgrader_mobile_cpp_path = pytorch_caffe2_dir / \"torch\" / \"csrc\" / \"jit\" / \"mobile\" / \"upgrader_mobile.cpp\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             write_cpp(tmpdirname, sorted_upgrader_list)\n-            with open(os.path.join(tmpdirname, 'upgrader_mobile.cpp'), 'r') as file_name:\n+            with open(os.path.join(tmpdirname, 'upgrader_mobile.cpp')) as file_name:\n                 actual_output = [line.strip() for line in file_name.readlines() if line]\n-            with open(str(upgrader_mobile_cpp_path), 'r') as file_name:\n+            with open(str(upgrader_mobile_cpp_path)) as file_name:\n                 expect_output = [line.strip() for line in file_name.readlines() if line]\n             actual_output_filtered = list(filter(lambda token: len(token) != 0, actual_output))\n             expect_output_filtered = list(filter(lambda token: len(token) != 0, expect_output))\ndiff --git a/test/package/test_directory_reader.py b/test/package/test_directory_reader.py\nindex d4bf4ae99057ef..0f19b4789510e5 100644\n--- a/test/package/test_directory_reader.py\n+++ b/test/package/test_directory_reader.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: package/deploy\"]\n \n import os\n@@ -207,7 +206,7 @@ def test_importer_access(self):\n         filename = self.temp()\n         with PackageExporter(filename) as he:\n             he.save_text(\"main\", \"main\", \"my string\")\n-            he.save_binary(\"main\", \"main_binary\", \"my string\".encode(\"utf-8\"))\n+            he.save_binary(\"main\", \"main_binary\", b\"my string\")\n             src = dedent(\n                 \"\"\"\\\n                 import importlib\n@@ -226,7 +225,7 @@ def test_importer_access(self):\n             dir_importer = PackageImporter(Path(temp_dir) / Path(filename).name)\n             m = dir_importer.import_module(\"main\")\n             self.assertEqual(m.t, \"my string\")\n-            self.assertEqual(m.b, \"my string\".encode(\"utf-8\"))\n+            self.assertEqual(m.b, b\"my string\")\n \n     @skipIf(version_info < (3, 7), \"ResourceReader API introduced in Python 3.7\")\n     def test_resource_access_by_path(self):\n@@ -235,7 +234,7 @@ def test_resource_access_by_path(self):\n         \"\"\"\n         filename = self.temp()\n         with PackageExporter(filename) as e:\n-            e.save_binary(\"string_module\", \"my_string\", \"my string\".encode(\"utf-8\"))\n+            e.save_binary(\"string_module\", \"my_string\", b\"my string\")\n             src = dedent(\n                 \"\"\"\\\n                 import importlib.resources\ndiff --git a/test/package/test_misc.py b/test/package/test_misc.py\nindex 72e1a069eb3ac1..5bc53f8b4959a2 100644\n--- a/test/package/test_misc.py\n+++ b/test/package/test_misc.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: package/deploy\"]\n \n import inspect\ndiff --git a/test/package/test_resources.py b/test/package/test_resources.py\nindex 3425825bb097d4..208917be771acd 100644\n--- a/test/package/test_resources.py\n+++ b/test/package/test_resources.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: package/deploy\"]\n \n from io import BytesIO\n@@ -105,7 +104,7 @@ def test_importer_access(self):\n         buffer = BytesIO()\n         with PackageExporter(buffer) as he:\n             he.save_text(\"main\", \"main\", \"my string\")\n-            he.save_binary(\"main\", \"main_binary\", \"my string\".encode(\"utf-8\"))\n+            he.save_binary(\"main\", \"main_binary\", b\"my string\")\n             src = dedent(\n                 \"\"\"\\\n                 import importlib\n@@ -120,7 +119,7 @@ def test_importer_access(self):\n         hi = PackageImporter(buffer)\n         m = hi.import_module(\"main\")\n         self.assertEqual(m.t, \"my string\")\n-        self.assertEqual(m.b, \"my string\".encode(\"utf-8\"))\n+        self.assertEqual(m.b, b\"my string\")\n \n     def test_resource_access_by_path(self):\n         \"\"\"\n@@ -128,7 +127,7 @@ def test_resource_access_by_path(self):\n         \"\"\"\n         buffer = BytesIO()\n         with PackageExporter(buffer) as he:\n-            he.save_binary(\"string_module\", \"my_string\", \"my string\".encode(\"utf-8\"))\n+            he.save_binary(\"string_module\", \"my_string\", b\"my string\")\n             src = dedent(\n                 \"\"\"\\\n                 import importlib.resources\ndiff --git a/test/profiler/test_profiler.py b/test/profiler/test_profiler.py\nindex e141e96c7b88bd..1a2071bbf28a36 100644\n--- a/test/profiler/test_profiler.py\n+++ b/test/profiler/test_profiler.py\n@@ -1,7 +1,6 @@\n # Owner(s): [\"oncall: profiler\"]\n import collections\n import gc\n-import io\n import json\n import os\n import re\n@@ -111,7 +110,7 @@ def test_mem_leak(self):\n         for idx in range(1, len(last_rss)):\n             max_diff = max(max_diff, last_rss[idx] - last_rss[idx - 1])\n         self.assertTrue(not (is_increasing and max_diff > 100 * 1024),\n-                        msg='memory usage is increasing, {}'.format(str(last_rss)))\n+                        msg=f'memory usage is increasing, {str(last_rss)}')\n \n     def test_custom_module_input_op_ids(self):\n         class MyFunc(torch.autograd.Function):\n@@ -340,7 +339,7 @@ def payload(self, use_cuda=False):\n \n     def get_execution_trace_root(self, output_file_name):\n         nodes = []\n-        with open(output_file_name, 'r') as f:\n+        with open(output_file_name) as f:\n             et_graph = json.load(f)\n             assert \"nodes\" in et_graph\n             nodes = et_graph[\"nodes\"]\n@@ -575,7 +574,7 @@ def call_module(x):\n         if kineto_available() and not IS_WINDOWS:\n             with TemporaryFileName(mode=\"w+\") as fname:\n                 p.export_chrome_trace(fname)\n-                with io.open(fname, 'r') as f:\n+                with open(fname) as f:\n                     events = json.load(f)[\"traceEvents\"]\n \n                 def extract(pattern: str):\n@@ -870,7 +869,7 @@ def create_mkldnn_tensor():\n                     with record_function(\"test_user_scope_dealloc\"):\n                         del x\n                 prof.export_chrome_trace(fname)\n-                with io.open(fname, 'r') as f:\n+                with open(fname) as f:\n                     trace = json.load(f)\n                     assert \"traceEvents\" in trace\n                     events = trace[\"traceEvents\"]\n@@ -974,7 +973,7 @@ def create_cuda_tensor_oom():\n \n         def check_trace(fname):\n             prof.export_chrome_trace(fname)\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 trace = json.load(f)\n                 self.assertTrue(\"traceEvents\" in trace)\n                 events = trace[\"traceEvents\"]\n@@ -1045,7 +1044,7 @@ def forward(self, x, y):\n             with profile(activities=[torch.profiler.ProfilerActivity.CPU], with_modules=True,) as prof:\n                 model(input_a, input_b)\n             prof.export_chrome_trace(fname)\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 trace = json.load(f)\n                 assert \"traceEvents\" in trace\n                 events = trace[\"traceEvents\"]\n@@ -1296,7 +1295,7 @@ def test_export_stacks(self):\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             p.export_stacks(fname)\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 lines = f.readlines()\n             assert len(lines) > 0, \"Empty stacks file\"\n             for line in lines:\n@@ -1383,7 +1382,7 @@ def test_profiler_metadata(self):\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 trace = json.load(f)\n                 assert \"test_key1\" in trace\n                 assert trace[\"test_key1\"] == \"test_value1\"\n@@ -1399,7 +1398,7 @@ def _test_profiler_tracing(self, use_kineto):\n             prof.export_chrome_trace(fname)\n             # read the trace and expect valid json\n             # if the JSON generated by export_chrome_trace is not valid, this will throw and fail the test.\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 json.load(f)\n \n         # test empty trace\n@@ -1422,7 +1421,7 @@ def _test_profiler_tracing(self, use_kineto):\n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n             # Now validate the json\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 json.load(f)\n \n     def test_profiler_tracing(self):\n@@ -1439,7 +1438,7 @@ def test_profiler_fwd_bwd_link(self):\n             loss.backward()\n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with io.open(fname, 'r') as f:\n+            with open(fname) as f:\n                 j = json.load(f)\n                 events = j[\"traceEvents\"]\n                 ts_to_name = {}\n@@ -1481,7 +1480,7 @@ def test_profiler_disable_fwd_bwd_link(self):\n \n             with TemporaryFileName(mode=\"w+\") as fname:\n                 prof.export_chrome_trace(fname)\n-                with io.open(fname, 'r') as f:\n+                with open(fname) as f:\n                     j = json.load(f)\n                     events = j[\"traceEvents\"]\n \n@@ -2452,7 +2451,7 @@ def inner():\n \n \n @dataclass(frozen=True)\n-class MockKinetoEvent():\n+class MockKinetoEvent:\n     _name: str\n     _start_us: int\n     _duration_us: int\n@@ -2477,7 +2476,7 @@ def device_type(self) -> DeviceType:\n \n \n @dataclass(frozen=True)\n-class MockProfilerEvent():\n+class MockProfilerEvent:\n     _name: str\n     id: int\n     start_time_ns: int\n@@ -2646,7 +2645,7 @@ def EventTreeDFS(event_tree):\n                 json.dump([kineto_events, profiler_events], f)\n \n         assert (os.path.exists(json_file_path))\n-        with open(json_file_path, \"r\") as f:\n+        with open(json_file_path) as f:\n             kineto_events, profiler_events = json.load(f)\n \n         cuda_events = [\ndiff --git a/test/quantization/bc/test_backward_compatibility.py b/test/quantization/bc/test_backward_compatibility.py\nindex 0dbe60d93166c3..8387a5a40fb95c 100644\n--- a/test/quantization/bc/test_backward_compatibility.py\n+++ b/test/quantization/bc/test_backward_compatibility.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: quantization\"]\n \n import sys\n@@ -42,7 +41,7 @@ def get_filenames(self, subname):\n     subname_output = \"\"\n     if subname:\n         base_name += \"_\" + subname\n-        subname_output = \" ({})\".format(subname)\n+        subname_output = f\" ({subname})\"\n \n     input_file = base_name + \".input.pt\"\n     state_dict_file = base_name + \".state_dict.pt\"\ndiff --git a/test/quantization/core/experimental/apot_fx_graph_mode_ptq.py b/test/quantization/core/experimental/apot_fx_graph_mode_ptq.py\nindex cbf3cb6756296e..f56c1938ccaab0 100644\n--- a/test/quantization/core/experimental/apot_fx_graph_mode_ptq.py\n+++ b/test/quantization/core/experimental/apot_fx_graph_mode_ptq.py\n@@ -49,7 +49,7 @@ def calibrate(model, data_loader):\n full_precision_model = float_model\n \n top1, top5 = evaluate(full_precision_model, criterion, data_loader_test)\n-print(\"Model #0 Evaluation accuracy on test dataset: %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #0 Evaluation accuracy on test dataset: {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model PTQ for specified qconfig for torch.nn.Linear\n@@ -69,7 +69,7 @@ def prepare_ptq_linear(qconfig):\n quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model\n \n top1, top5 = evaluate(quantized_model, criterion, data_loader_test)\n-print(\"Model #1 Evaluation accuracy on test dataset (b=8, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #1 Evaluation accuracy on test dataset (b=8, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with uniform activation, uniform weight\n@@ -80,7 +80,7 @@ def prepare_ptq_linear(qconfig):\n quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model\n \n top1, top5 = evaluate(quantized_model1, criterion, data_loader_test)\n-print(\"Model #1 Evaluation accuracy on test dataset (b=4, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #1 Evaluation accuracy on test dataset (b=4, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with uniform activation, APoT weight\n@@ -90,7 +90,7 @@ def prepare_ptq_linear(qconfig):\n prepared_model = prepare_ptq_linear(apot_weights_qconfig_8bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #2 Evaluation accuracy on test dataset (b=8, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #2 Evaluation accuracy on test dataset (b=8, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with uniform activation, APoT weight\n@@ -100,7 +100,7 @@ def prepare_ptq_linear(qconfig):\n prepared_model = prepare_ptq_linear(apot_weights_qconfig_4bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #2 Evaluation accuracy on test dataset (b=4, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #2 Evaluation accuracy on test dataset (b=4, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \n \"\"\"\n@@ -111,7 +111,7 @@ def prepare_ptq_linear(qconfig):\n prepared_model = prepare_ptq_linear(apot_qconfig_8bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #3 Evaluation accuracy on test dataset (b=8, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #3 Evaluation accuracy on test dataset (b=8, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with APoT activation and weight\n@@ -121,11 +121,11 @@ def prepare_ptq_linear(qconfig):\n prepared_model = prepare_ptq_linear(apot_qconfig_4bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #3 Evaluation accuracy on test dataset (b=4, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #3 Evaluation accuracy on test dataset (b=4, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare eager mode quantized model\n \"\"\"\n eager_quantized_model = resnet18(pretrained=True, quantize=True).eval()\n top1, top5 = evaluate(eager_quantized_model, criterion, data_loader_test)\n-print(\"Eager mode quantized model evaluation accuracy on test dataset: %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Eager mode quantized model evaluation accuracy on test dataset: {top1.avg:2.2f}, {top5.avg:2.2f}\")\ndiff --git a/test/quantization/core/experimental/apot_fx_graph_mode_qat.py b/test/quantization/core/experimental/apot_fx_graph_mode_qat.py\nindex a50b0df1bbca40..070e971c9a0f92 100644\n--- a/test/quantization/core/experimental/apot_fx_graph_mode_qat.py\n+++ b/test/quantization/core/experimental/apot_fx_graph_mode_qat.py\n@@ -40,7 +40,7 @@ def prepare_qat_linear(qconfig):\n prepared_model = prepare_qat_linear(uniform_qconfig_8bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #1 Evaluation accuracy on test dataset (b=8, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #1 Evaluation accuracy on test dataset (b=8, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with uniform activation, uniform weight\n@@ -50,7 +50,7 @@ def prepare_qat_linear(qconfig):\n prepared_model = prepare_qat_linear(uniform_qconfig_4bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #1 Evaluation accuracy on test dataset (b=4, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #1 Evaluation accuracy on test dataset (b=4, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with uniform activation, APoT weight\n@@ -60,7 +60,7 @@ def prepare_qat_linear(qconfig):\n prepared_model = prepare_qat_linear(apot_weights_qconfig_8bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #2 Evaluation accuracy on test dataset (b=8, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #2 Evaluation accuracy on test dataset (b=8, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with uniform activation, APoT weight\n@@ -70,7 +70,7 @@ def prepare_qat_linear(qconfig):\n prepared_model = prepare_qat_linear(apot_weights_qconfig_4bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #2 Evaluation accuracy on test dataset (b=4, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #2 Evaluation accuracy on test dataset (b=4, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \n \"\"\"\n@@ -81,7 +81,7 @@ def prepare_qat_linear(qconfig):\n prepared_model = prepare_qat_linear(apot_qconfig_8bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #3 Evaluation accuracy on test dataset (b=8, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #3 Evaluation accuracy on test dataset (b=8, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\n \n \"\"\"\n Prepare model with APoT activation and weight\n@@ -91,4 +91,4 @@ def prepare_qat_linear(qconfig):\n prepared_model = prepare_qat_linear(apot_qconfig_4bit)\n \n top1, top5 = evaluate(prepared_model, criterion, data_loader_test)\n-print(\"Model #3 Evaluation accuracy on test dataset (b=4, k=2): %2.2f, %2.2f\" % (top1.avg, top5.avg))\n+print(f\"Model #3 Evaluation accuracy on test dataset (b=4, k=2): {top1.avg:2.2f}, {top5.avg:2.2f}\")\ndiff --git a/test/quantization/core/test_quantized_module.py b/test/quantization/core/test_quantized_module.py\nindex c0ed9bf3a4c62d..ad1e391b1b6ad4 100644\n--- a/test/quantization/core/test_quantized_module.py\n+++ b/test/quantization/core/test_quantized_module.py\n@@ -1705,12 +1705,12 @@ def test_lstm_api(self, dtype, bidirectional):\n         for layer in range(num_layers):\n             for direction in range(num_directions):\n                 suffix = '_reverse' if direction == 1 else ''\n-                key_name1 = 'weight_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'weight_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'weight_ih_l{layer}{suffix}'\n+                key_name2 = f'weight_hh_l{layer}{suffix}'\n                 weight_keys.append(key_name1)\n                 weight_keys.append(key_name2)\n-                key_name1 = 'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'bias_ih_l{layer}{suffix}'\n+                key_name2 = f'bias_hh_l{layer}{suffix}'\n                 bias_keys.append(key_name1)\n                 bias_keys.append(key_name2)\n \ndiff --git a/test/quantization/core/test_quantized_op.py b/test/quantization/core/test_quantized_op.py\nindex 232150a0ba34a6..1e999349761481 100644\n--- a/test/quantization/core/test_quantized_op.py\n+++ b/test/quantization/core/test_quantized_op.py\n@@ -398,7 +398,7 @@ def test_leaky_relu(self):\n                                            dtype=torch_type)\n             qY_hat = op(qX, negative_slope=alpha)\n             self.assertEqual(qY.dequantize(), qY_hat.dequantize(),\n-                             msg=\"F.leaky_relu failed ({} vs {})\".format(qY, qY_hat))\n+                             msg=f\"F.leaky_relu failed ({qY} vs {qY_hat})\")\n \n     \"\"\"Tests the correctness of the quantized::elu op.\"\"\"\n     @given(X=hu.tensor(shapes=hu.array_shapes(1, 5, 1, 5),\n@@ -423,7 +423,7 @@ def test_qelu(self, X, alpha):\n \n         qY = torch.ao.nn.quantized.functional.elu(qX, output_scale, output_zero_point, alpha=alpha)\n         self.assertEqual(qY, qY_hat,\n-                         msg=\"F.elu failed ({} vs {})\".format(qY, qY_hat))\n+                         msg=f\"F.elu failed ({qY} vs {qY_hat})\")\n \n \n     \"\"\"Tests the correctness of the quantized::celu op.\"\"\"\n@@ -449,7 +449,7 @@ def test_qcelu(self, X, alpha):\n         # test regular\n         qY = torch.ops.quantized.celu(qX, output_scale, output_zero_point, alpha=alpha)\n         self.assertEqual(qY, qY_hat,\n-                         msg=\"F.celu failed ({} vs {})\".format(qY, qY_hat))\n+                         msg=f\"F.celu failed ({qY} vs {qY_hat})\")\n \n     \"\"\"Tests the correctness of the quantized::gelu op.\"\"\"\n     def test_qgelu(self):\n@@ -478,7 +478,7 @@ def test_qgelu(self):\n                                                dtype=torch_type)\n                 qY_hat = op(qX)\n                 self.assertEqual(qY.dequantize(), qY_hat.dequantize(),\n-                                 msg=\"F.gelu failed ({} vs {})\".format(qY, qY_hat))\n+                                 msg=f\"F.gelu failed ({qY} vs {qY_hat})\")\n \n     \"\"\"Tests the correctness of the quantized::prelu op.\"\"\"\n     def test_qprelu(self):\n@@ -512,7 +512,7 @@ def test_qprelu(self):\n                                            dtype=torch_type)\n             qY_hat = qop(qX, qW, scale, zero_point)\n             self.assertEqual(qY.dequantize(), qY_hat.dequantize(),\n-                             msg=\"F.prelu failed ({} vs {})\".format(qY, qY_hat))\n+                             msg=f\"F.prelu failed ({qY} vs {qY_hat})\")\n \n     \"\"\"Tests the correctness of the quantized::qlayer_norm op.\"\"\"\n     @skipIfNoFBGEMM\n@@ -636,7 +636,7 @@ def test_qtanh(self, X):\n                                        dtype=torch_type)\n         qY_hat = torch.tanh(qX)\n         self.assertEqual(qY, qY_hat,\n-                         msg=\"TanH failed: {} vs. {}\".format(qY, qY_hat))\n+                         msg=f\"TanH failed: {qY} vs. {qY_hat}\")\n \n     \"\"\"Tests the correctness of the quantized::threshold op.\"\"\"\n     @given(X=hu.tensor(shapes=hu.array_shapes(1, 5, 1, 5),\n@@ -666,7 +666,7 @@ def test_qthreshold(self, X, threshold, value):\n \n         for name, op in ops_under_test.items():\n             qY = op(qX, threshold, value)\n-            self.assertEqual(qY, qY_hat, msg=\"{} qthreshold failed\".format(name))\n+            self.assertEqual(qY, qY_hat, msg=f\"{name} qthreshold failed\")\n \n     \"\"\"Tests the correctness of the quantized::clamp op.\"\"\"\n     @given(X=hu.tensor(shapes=hu.array_shapes(1, 8, 1, 8, max_numel=10**5),\n@@ -691,7 +691,7 @@ def test_qclamp(self, X, min_val, max_val):\n \n         for name, op in ops_under_test.items():\n             qY_clamp_hat = op(qX, min=min_val, max=max_val)\n-            self.assertEqual(qY_clamp, qY_clamp_hat, msg=\"{} qclamp failed\".format(name))\n+            self.assertEqual(qY_clamp, qY_clamp_hat, msg=f\"{name} qclamp failed\")\n \n         if torch.backends.quantized.engine == 'fbgemm':\n             with override_quantized_engine('fbgemm'):\n@@ -706,9 +706,9 @@ def test_qclamp(self, X, min_val, max_val):\n \n                 for name, op in ops_under_test.items():\n                     qY_min_clamp_hat = op(qX, min=min_val)\n-                    self.assertEqual(qY_min_clamp, qY_min_clamp_hat, msg=\"{} qclamp failed\".format(name))\n+                    self.assertEqual(qY_min_clamp, qY_min_clamp_hat, msg=f\"{name} qclamp failed\")\n                     qY_max_clamp_hat = op(qX, max=max_val)\n-                    self.assertEqual(qY_max_clamp, qY_max_clamp_hat, msg=\"{} qclamp failed\".format(name))\n+                    self.assertEqual(qY_max_clamp, qY_max_clamp_hat, msg=f\"{name} qclamp failed\")\n \n     \"\"\"Tests the correctness of the quantized::hardtanh op.\"\"\"\n     @skipIfNoFBGEMM\n@@ -740,7 +740,7 @@ def test_hardtanh(self, X, min_val, max_val):\n \n             for name, op in ops_under_test.items():\n                 qY_hat = op(qX, min_val, max_val)\n-                self.assertEqual(qY, qY_hat, msg=\"{} hardtanh failed\".format(name))\n+                self.assertEqual(qY, qY_hat, msg=f\"{name} hardtanh failed\")\n \n             ops_under_test_inplace = {\n                 'inplace nn.quantized.functional.hardtanh':\n@@ -752,7 +752,7 @@ def test_hardtanh(self, X, min_val, max_val):\n             for name, op_ in ops_under_test_inplace.items():\n                 qY_hat = qX.clone()\n                 op_(qY_hat, min_val, max_val, inplace=True)\n-                self.assertEqual(qY, qY_hat, msg=\"{} hardtanh failed\".format(name))\n+                self.assertEqual(qY, qY_hat, msg=f\"{name} hardtanh failed\")\n \n     \"\"\"Tests the correctness of the quantized::hardswish op.\"\"\"\n     @override_qengines\n@@ -789,7 +789,7 @@ def test_hardswish(self):\n                     qX, scale=Y_scale, zero_point=Y_zero_point)\n                 self.assertEqual(\n                     qY, qY_hat,\n-                    msg=\"Hardswish failed: {} vs {}, {}\".format(qY, qY_hat, torch.backends.quantized.engine))\n+                    msg=f\"Hardswish failed: {qY} vs {qY_hat}, {torch.backends.quantized.engine}\")\n \n     \"\"\"Tests the correctness of the binary op + scalar.\"\"\"\n     def _test_binary_op_scalar_relu(self, A, b, binary_op_name, binary_op, quantized_op, quantized_op_relu):\n@@ -1353,7 +1353,7 @@ def test_max_pool1d(self, X, kernel, stride, dilation, padding, ceil_mode):\n             a_hat = op(qa, kernel_size=kernel, stride=stride, padding=padding,\n                        dilation=dilation, ceil_mode=ceil_mode)\n             self.assertEqual(a_ref, a_hat.dequantize(),\n-                             msg=\"{} results are off\".format(name))\n+                             msg=f\"{name} results are off\")\n         # Test the ops.quantized separately, because None is not treated.\n         a_hat = torch.ops.quantized.max_pool1d(\n             qa, kernel_size=_single(kernel),\n@@ -1450,7 +1450,7 @@ def test_max_pool2d(self, X, kernel, stride, dilation, padding, ceil_mode):\n             a_hat = op(qa, kernel_size=kernel, stride=stride, padding=padding,\n                        dilation=dilation, ceil_mode=ceil_mode)\n             self.assertEqual(a_ref, a_hat.dequantize(),\n-                             msg=\"{} results are off\".format(name))\n+                             msg=f\"{name} results are off\")\n         # Test the ops.quantized separately, because None is not treated.\n         a_hat = torch.ops.quantized.max_pool2d(\n             qa, kernel_size=_pair(kernel),\n@@ -1503,7 +1503,7 @@ def test_max_pool3d(self):\n                 a_hat = op(qa, kernel_size=kernel, stride=stride, padding=padding,\n                            dilation=dilation, ceil_mode=ceil_mode)\n                 self.assertEqual(a_ref, a_hat.dequantize(),\n-                                 msg=\"{} results are off\".format(name))\n+                                 msg=f\"{name} results are off\")\n \n     \"\"\"Tests max pool operation on NHWC quantized tensors.\"\"\"\n     @given(X=hu.tensor(shapes=hu.array_shapes(min_dims=4, max_dims=4,\n@@ -1556,7 +1556,7 @@ def test_max_pool2d_nhwc(self, X, kernel, stride, dilation, padding, ceil_mode):\n                        dilation=dilation, ceil_mode=ceil_mode)\n             self.assertTrue(a_hat.stride() != sorted(a_hat.stride()))\n             self.assertEqual(a_ref, a_hat.dequantize(),\n-                             msg=\"{} results are off\".format(name))\n+                             msg=f\"{name} results are off\")\n         # Test the ops.quantized separately, because None is not treated.\n         a_hat = torch.ops.quantized.max_pool2d(\n             qa, kernel_size=_pair(kernel),\n@@ -1612,7 +1612,7 @@ def test_max_pool3d_nhwc(self):\n                 a_hat = op(qa, kernel_size=kernel, stride=stride, padding=padding,\n                            dilation=dilation, ceil_mode=ceil_mode)\n                 self.assertEqual(a_ref, a_hat.dequantize(),\n-                                 msg=\"{} results are off\".format(name))\n+                                 msg=f\"{name} results are off\")\n \n     @given(X=hu.tensor(shapes=hu.array_shapes(min_dims=3, max_dims=4,\n                                               min_side=5, max_side=10),\n@@ -1960,16 +1960,16 @@ def test_adaptive_avg_pool(self):\n                         output_size = (output_size_d, output_size_h, output_size_w)\n \n                 # Run reference on int_repr + round to avoid double rounding error.\n-                ref_op = getattr(torch.nn.functional, 'adaptive_avg_pool{}d'.format(dim))\n+                ref_op = getattr(torch.nn.functional, f'adaptive_avg_pool{dim}d')\n                 X_ref = ref_op(qX.int_repr().to(torch.float), output_size).round()\n \n                 ops_under_test = {\n                     \"nn.functional\":\n-                        getattr(torch.nn.functional, 'adaptive_avg_pool{}d'.format(dim)),\n+                        getattr(torch.nn.functional, f'adaptive_avg_pool{dim}d'),\n                     \"nn.quantized.functional\":\n-                        getattr(torch.ao.nn.quantized.functional, 'adaptive_avg_pool{}d'.format(dim)),\n+                        getattr(torch.ao.nn.quantized.functional, f'adaptive_avg_pool{dim}d'),\n                     \"ao.nn.quantized.functional\":\n-                        getattr(torch.ao.nn.quantized.functional, 'adaptive_avg_pool{}d'.format(dim))\n+                        getattr(torch.ao.nn.quantized.functional, f'adaptive_avg_pool{dim}d')\n                 }\n \n                 error_message = r\"Results are off for {}:\\n\\tExpected:\\n{}\\n\\tGot:\\n{}\"\n@@ -2678,7 +2678,7 @@ def test_batch_norm_relu(self):\n                 self.assertEqual(\n                     qy.int_repr().numpy(),\n                     quantize_ref.int_repr().numpy(),\n-                    msg=\"{} vs {}\".format(qy, quantize_ref))\n+                    msg=f\"{qy} vs {quantize_ref}\")\n \n     @skipIfNoFBGEMM\n     def test_batch_norm(self):\n@@ -2723,7 +2723,7 @@ def test_batch_norm(self):\n                 quantize_ref = torch.quantize_per_tensor(float_ref, Y_scale, Y_zero_point, dtype_x)\n                 self.assertEqual(\n                     qy.int_repr().numpy(), quantize_ref.int_repr().numpy(),\n-                    msg=\"{} vs {}\".format(qy, quantize_ref))\n+                    msg=f\"{qy} vs {quantize_ref}\")\n \n     @override_qengines\n     def test_empty_batch(self):\n@@ -6477,10 +6477,10 @@ def test_qnnpack_tanh(self):\n                 qY_hat = torch.tanh(qX)\n                 self.assertEqual(\n                     qY, qY_hat,\n-                    msg=\"QNNPACK TanH failed (FP ref), memory_format {}\".format(memory_format))\n+                    msg=f\"QNNPACK TanH failed (FP ref), memory_format {memory_format}\")\n                 self.assertEqual(\n                     qYserver, qY_hat,\n-                    msg=\"QNNPACK TanH failed (FBGEMM ref), memory_format {}\".format(memory_format))\n+                    msg=f\"QNNPACK TanH failed (FBGEMM ref), memory_format {memory_format}\")\n \n     \"\"\"Tests the correctness of the quantized::qnnpack_sigmoid op.\"\"\"\n     @skipIfNoFBGEMM\n@@ -6509,10 +6509,10 @@ def test_qnnpack_sigmoid(self):\n                 qY_hat = torch.sigmoid(qX)\n                 self.assertEqual(\n                     qY, qY_hat,\n-                    msg=\"QNNPACK Sigmoid failed (FP ref), memory_format {}\".format(memory_format))\n+                    msg=f\"QNNPACK Sigmoid failed (FP ref), memory_format {memory_format}\")\n                 self.assertEqual(\n                     qYserver, qY_hat,\n-                    msg=\"QNNPACK Sigmoid failed (FBGEMM ref), memory_format {}\".format(memory_format))\n+                    msg=f\"QNNPACK Sigmoid failed (FBGEMM ref), memory_format {memory_format}\")\n \n     @skipIfNoFBGEMM\n     def test_qnnpack_sigmoid_sweep(self):\n@@ -6904,7 +6904,7 @@ def test_hardtanh(self):\n                 qY_hat = torch.ao.nn.quantized.functional.hardtanh(qX, min_val, max_val)\n                 self.assertEqual(\n                     qY, qY_hat,\n-                    msg=\"hardtanh failed:\\nactual {}\\nexpected {}\\nmemory_format {}\".format(qY_hat, qY, memory_format))\n+                    msg=f\"hardtanh failed:\\nactual {qY_hat}\\nexpected {qY}\\nmemory_format {memory_format}\")\n \n \"\"\"Tests the correctness of the tensor comparators.\"\"\"\n class TestComparatorOps(TestCase):\n@@ -6933,12 +6933,12 @@ def test_compare_tensor_tensor(self, A, B):\n             result_ref = getattr(dqA, op)(dqB)\n             result = getattr(qA, op)(qB)\n             self.assertEqual(result_ref, result,\n-                             msg=\"'tensor.{}(tensor)'' failed\".format(op))\n+                             msg=f\"'tensor.{op}(tensor)'' failed\")\n             # Reversed broadcasting.\n             result_ref = getattr(dqB, op)(dqA)\n             result = getattr(qB, op)(qA)\n             self.assertEqual(result_ref, result,\n-                             msg=\"'tensor.{}(tensor)'' failed\".format(op))\n+                             msg=f\"'tensor.{op}(tensor)'' failed\")\n \n     @given(A=hu.tensor(shapes=((3, 4, 5),),\n                        qparams=hu.qparams()),\n@@ -6958,22 +6958,22 @@ def test_compare_tensor_scalar(self, A, b):\n         for op in ops_under_test_reversible:\n             result_ref = getattr(dqA, op)(b)\n             result = getattr(qA, op)(b)\n-            note(\"result_ref 1: {}\".format(result_ref))\n-            note(\"result 1: {}\".format(result))\n+            note(f\"result_ref 1: {result_ref}\")\n+            note(f\"result 1: {result}\")\n             self.assertEqual(result_ref, result,\n-                             msg=\"'tensor.{}(scalar)'' failed\".format(op))\n+                             msg=f\"'tensor.{op}(scalar)'' failed\")\n             # Reversed broadcasting.\n             result_ref = getattr(b, op)(dqA)\n             result = getattr(b, op)(qA)\n-            note(\"result_ref 2: {}\".format(result_ref))\n-            note(\"result 2: {}\".format(result))\n+            note(f\"result_ref 2: {result_ref}\")\n+            note(f\"result 2: {result}\")\n             self.assertEqual(result_ref, result,\n-                             msg=\"'scalar.{}(tensor)'' failed\".format(op))\n+                             msg=f\"'scalar.{op}(tensor)'' failed\")\n \n         for op in ops_under_test_nonreversible:\n             result_ref = getattr(dqA, op)(b)\n             result = getattr(qA, op)(b)\n-            note(\"result_ref 3: {}\".format(result_ref))\n-            note(\"result 3: {}\".format(result))\n+            note(f\"result_ref 3: {result_ref}\")\n+            note(f\"result 3: {result}\")\n             self.assertEqual(result_ref, result,\n-                             msg=\"'tensor.{}(scalar)'' failed\".format(op))\n+                             msg=f\"'tensor.{op}(scalar)'' failed\")\ndiff --git a/test/quantization/core/test_workflow_module.py b/test/quantization/core/test_workflow_module.py\nindex 87a8c31c87c9e4..1dd7650d5706b4 100644\n--- a/test/quantization/core/test_workflow_module.py\n+++ b/test/quantization/core/test_workflow_module.py\n@@ -832,7 +832,7 @@ def test_observers_preserve_buffers(self):\n             self.assertEqual(\n                 buffer_ids_before,\n                 buffer_ids_after,\n-                msg=\"{}: Buffers must be modified in place\".format(str(observer)))\n+                msg=f\"{str(observer)}: Buffers must be modified in place\")\n \n     def test_fake_quant_preserves_buffers(self):\n         \"\"\"\ndiff --git a/test/quantization/fx/test_model_report_fx.py b/test/quantization/fx/test_model_report_fx.py\nindex cf7c7ec9949c5d..7077f1040ac24a 100644\n--- a/test/quantization/fx/test_model_report_fx.py\n+++ b/test/quantization/fx/test_model_report_fx.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: quantization\"]\n \n import torch\ndiff --git a/test/quantization/fx/test_quantize_fx.py b/test/quantization/fx/test_quantize_fx.py\nindex 77985257c06c2a..a4e9577b8f517e 100644\n--- a/test/quantization/fx/test_quantize_fx.py\n+++ b/test/quantization/fx/test_quantize_fx.py\n@@ -3916,7 +3916,7 @@ def _check_node_not_observed(model, arg_node, node):\n             elif arg_node.op == \"call_module\":\n                 self.assertTrue(\n                     not _is_activation_post_process(getattr(model, arg_node.target)),\n-                    \"Arg: {0} of node: {1} is observed but is not a float tensor\".format(\n+                    \"Arg: {} of node: {} is observed but is not a float tensor\".format(\n                         arg_node, node\n                     ),\n                 )\n@@ -5519,19 +5519,19 @@ def cleanUp():\n         self.addCleanup(cleanUp)\n \n         @_register_fusion_pattern(\"dummy_fusion\")\n-        class DummyFusion():\n+        class DummyFusion:\n             pass\n \n         @_register_quant_pattern(\"dummy_quant\")\n-        class DummyQuant():\n+        class DummyQuant:\n             pass\n \n         @_register_quant_pattern(\"dummy_quant2\", default_fixed_qparams_range_0to1_observer)\n-        class DummyQuant2():\n+        class DummyQuant2:\n             pass\n \n         @_register_quant_pattern(\"dummy_quant3\", default_fixed_qparams_range_neg1to1_observer)\n-        class DummyQuant3():\n+        class DummyQuant3:\n             pass\n \n         self.assertEqual(_DEFAULT_FUSION_PATTERNS[\"dummy_fusion\"], DummyFusion)\n@@ -8304,7 +8304,7 @@ def __init__(self):\n                 self.avg_pool1d = torch.nn.AvgPool1d(3)\n                 self.avg_pool2d = torch.nn.AvgPool2d(3)\n                 self.avg_pool3d = torch.nn.AvgPool3d(3)\n-                self.adaptive_avg_pool1d = torch.nn.AdaptiveAvgPool1d((1))\n+                self.adaptive_avg_pool1d = torch.nn.AdaptiveAvgPool1d(1)\n                 self.adaptive_avg_pool2d = torch.nn.AdaptiveAvgPool2d((1, 1))\n                 self.adaptive_avg_pool3d = torch.nn.AdaptiveAvgPool3d((1, 1, 1))\n \ndiff --git a/test/quantization/jit/test_fusion_passes.py b/test/quantization/jit/test_fusion_passes.py\nindex d35b341f05ad7e..1bb93d90518582 100644\n--- a/test/quantization/jit/test_fusion_passes.py\n+++ b/test/quantization/jit/test_fusion_passes.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: quantization\"]\n \n # torch\ndiff --git a/test/quantization/jit/test_ondevice_quantization.py b/test/quantization/jit/test_ondevice_quantization.py\nindex 2da8eea60a41d5..76dce10cf255a4 100644\n--- a/test/quantization/jit/test_ondevice_quantization.py\n+++ b/test/quantization/jit/test_ondevice_quantization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: quantization\"]\n \n import torch\ndiff --git a/test/quantization/jit/test_quantize_jit.py b/test/quantization/jit/test_quantize_jit.py\nindex 2787626d996722..1bbe492e237e3d 100644\n--- a/test/quantization/jit/test_quantize_jit.py\n+++ b/test/quantization/jit/test_quantize_jit.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: quantization\"]\n \n # torch\n@@ -1692,7 +1691,7 @@ def forward(self, x):\n             model = self.checkGraphModeOp(\n                 Conv(dim),\n                 self.img_data_dict[dim],\n-                \"quantized::conv{}d\".format(dim),\n+                f\"quantized::conv{dim}d\",\n                 tracing,\n             )\n             # make sure there is only one quantize_per_tensor for input\n@@ -1701,7 +1700,7 @@ def forward(self, x):\n                 model.graph\n             )\n \n-            FileCheck().check_not(\"quantized::conv{}d_prepack\".format(dim)).run(\n+            FileCheck().check_not(f\"quantized::conv{dim}d_prepack\").run(\n                 model.graph\n             )\n \n@@ -1743,17 +1742,17 @@ def forward(self, x):\n                 ConvNdFunctionalRelu(dim),\n                 ConvNdInplaceFunctionalRelu(dim),\n             ]:\n-                conv_name = \"conv{}d\".format(dim)\n+                conv_name = f\"conv{dim}d\"\n                 m = self.checkGraphModeOp(\n                     orig_m,\n                     self.img_data_dict[dim],\n-                    \"quantized::conv{}d_relu(\".format(dim),\n+                    f\"quantized::conv{dim}d_relu(\",\n                     tracing=tracing,\n                 )\n \n-                FileCheck().check_not(\"aten::conv{}d(\".format(dim)).check_not(\n+                FileCheck().check_not(f\"aten::conv{dim}d(\").check_not(\n                     \"aten::relu\"\n-                ).check_not(\"quantized::conv{}d(\".format(dim)).check_not(\n+                ).check_not(f\"quantized::conv{dim}d(\").check_not(\n                     \"quantized::relu(\"\n                 ).run(\n                     m.graph\n@@ -2896,7 +2895,7 @@ def __init__(self):\n                 self.avg_pool1d = torch.nn.AvgPool1d(3)\n                 self.avg_pool2d = torch.nn.AvgPool2d(3)\n                 self.avg_pool3d = torch.nn.AvgPool3d(3)\n-                self.adaptive_avg_pool1d = torch.nn.AdaptiveAvgPool1d((1))\n+                self.adaptive_avg_pool1d = torch.nn.AdaptiveAvgPool1d(1)\n                 self.adaptive_avg_pool2d = torch.nn.AdaptiveAvgPool2d((1, 1))\n                 self.adaptive_avg_pool3d = torch.nn.AdaptiveAvgPool3d((1, 1, 1))\n                 self.leaky_relu = torch.nn.LeakyReLU()\ndiff --git a/test/run_test.py b/test/run_test.py\nindex 2000cad75b98a1..fdcfc4a7033e76 100755\n--- a/test/run_test.py\n+++ b/test/run_test.py\n@@ -1272,7 +1272,7 @@ def exclude_tests(\n                 not exact_match and test.startswith(exclude_test)\n             ) or test == exclude_test:\n                 if exclude_message is not None:\n-                    print_to_stderr(\"Excluding {} {}\".format(test, exclude_message))\n+                    print_to_stderr(f\"Excluding {test} {exclude_message}\")\n                 selected_tests.remove(test)\n     return selected_tests\n \n@@ -1424,7 +1424,7 @@ def get_selected_tests(options) -> List[ShardedTest]:\n     # Download previous test times to make sharding decisions\n     path = os.path.join(str(REPO_ROOT), TEST_TIMES_FILE)\n     if os.path.exists(path):\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             test_file_times = cast(Dict[str, Any], json.load(f))\n     else:\n         test_file_times = {}\ndiff --git a/test/test_ao_sparsity.py b/test/test_ao_sparsity.py\nindex 89f44a53ebc02b..944b459b5bb55b 100644\n--- a/test/test_ao_sparsity.py\n+++ b/test/test_ao_sparsity.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.testing._internal.common_utils import run_tests, IS_ARM64\ndiff --git a/test/test_autocast.py b/test/test_autocast.py\nindex 93177ad6c71327..c64b3829a105c0 100644\n--- a/test/test_autocast.py\n+++ b/test/test_autocast.py\n@@ -83,7 +83,7 @@ def compare(first, second):\n                     control = getattr(args[0].to(run_as_type), op)(*cast(args[1:], run_as_type), **add_kwargs)\n                 self.assertTrue(type(output_to_compare) == type(control))\n                 comparison = compare(output_to_compare, control)\n-                self.assertTrue(comparison, \"torch.{} result did not match control\".format(op))\n+                self.assertTrue(comparison, f\"torch.{op} result did not match control\")\n             self.assertTrue(torch.is_autocast_cpu_enabled())\n         self.assertFalse(torch.is_autocast_cpu_enabled())\n \ndiff --git a/test/test_autograd.py b/test/test_autograd.py\nindex 767d849691e578..32e9c538467841 100644\n--- a/test/test_autograd.py\n+++ b/test/test_autograd.py\n@@ -121,7 +121,7 @@ def test_grad_mode_class_decoration(self):\n         # Decorating class is deprecated and should not be used\n         with self.assertWarnsRegex(UserWarning, \"Decorating classes is deprecated\"):\n             @torch.no_grad()\n-            class Foo():\n+            class Foo:\n                 def __init__(self):\n                     assert not torch.is_grad_enabled()\n \n@@ -141,7 +141,7 @@ def foo():\n \n             foo()\n \n-            class Foo2():\n+            class Foo2:\n                 @torch.no_grad()\n                 def __init__(self):\n                     assert not torch.is_grad_enabled()\n@@ -7493,7 +7493,7 @@ def test_create_graph_and_full_backward_hook_cycle(self):\n         #\n         #   grad_output -> grad_output.grad_fn -> graph -> hook -> grad_output\n         #\n-        class TestCls():\n+        class TestCls:\n             # Dummy class for the purpose of creating a weakref\n             pass\n \n@@ -7558,7 +7558,7 @@ def forward(ctx, x):\n             def backward(ctx, grad):\n                 return grad\n \n-        class Test():\n+        class Test:\n             pass\n \n         count = [0]\n@@ -9231,7 +9231,7 @@ def f3():\n             if dtype.is_floating_point:\n                 f()\n             else:\n-                with self.assertRaisesRegex(RuntimeError, 'floating point', msg=\"dt: {} device: {}\".format(a.dtype, a.device)):\n+                with self.assertRaisesRegex(RuntimeError, 'floating point', msg=f\"dt: {a.dtype} device: {a.device}\"):\n                     f()\n \n     @onlyCUDA\n@@ -10832,7 +10832,7 @@ def iter_graph(roots):\n \n                 yield node\n \n-        class Handle():\n+        class Handle:\n             __slot__ = [\"node_name\"]\n \n             def __init__(self, node_name):\ndiff --git a/test/test_binary_ufuncs.py b/test/test_binary_ufuncs.py\nindex 9cb5b75b86eeb0..054723b84551c6 100644\n--- a/test/test_binary_ufuncs.py\n+++ b/test/test_binary_ufuncs.py\n@@ -168,8 +168,8 @@ def _numel(x):\n             if _numel(l) <= 100 and _numel(r) <= 100:\n                 msg = (\n                     \"Failed to produce expected results! Input lhs tensor was\"\n-                    \" {0}, rhs tensor was {1}, torch result is {2}, and reference result is\"\n-                    \" {3}.\"\n+                    \" {}, rhs tensor was {}, torch result is {}, and reference result is\"\n+                    \" {}.\"\n                 ).format(l, r, actual, expected)\n             else:\n                 msg = None\n@@ -491,7 +491,7 @@ def test_type_promotion(self, device, op):\n         )\n \n         def _supported(dtypes):\n-            return all((x in supported_dtypes for x in dtypes))\n+            return all(x in supported_dtypes for x in dtypes)\n \n         # int x int type promotion\n         if _supported((torch.int16, torch.int32, torch.int64)):\n@@ -2424,21 +2424,21 @@ def test_min_max_binary_op_nan(self, device, dtype):\n         for i in range(750):\n             self.assertTrue(\n                 torch.isnan(ma[i]),\n-                \"max(a, b): {}, a: {}, b: {}\".format(ma[i], a[i], b[i]),\n+                f\"max(a, b): {ma[i]}, a: {a[i]}, b: {b[i]}\",\n             )\n             self.assertTrue(\n                 torch.isnan(mi[i]),\n-                \"min(a, b): {}, a: {}, b: {}\".format(mi[i], a[i], b[i]),\n+                f\"min(a, b): {mi[i]}, a: {a[i]}, b: {b[i]}\",\n             )\n \n         for i in range(750, 1000):\n             self.assertFalse(\n                 torch.isnan(ma[i]),\n-                \"max(a, b): {}, a: {}, b: {}\".format(ma[i], a[i], b[i]),\n+                f\"max(a, b): {ma[i]}, a: {a[i]}, b: {b[i]}\",\n             )\n             self.assertFalse(\n                 torch.isnan(mi[i]),\n-                \"min(a, b): {}, a: {}, b: {}\".format(mi[i], a[i], b[i]),\n+                f\"min(a, b): {mi[i]}, a: {a[i]}, b: {b[i]}\",\n             )\n \n     @dtypes(\n@@ -4448,8 +4448,8 @@ def test(self, device, dtype):\n         return test\n \n     for op in tensor_binary_ops:\n-        test_name = \"test_{}_not_implemented\".format(op)\n-        assert not hasattr(cls, test_name), \"{0} already in {1}\".format(\n+        test_name = f\"test_{op}_not_implemented\"\n+        assert not hasattr(cls, test_name), \"{} already in {}\".format(\n             test_name, cls.__name__\n         )\n \ndiff --git a/test/test_compile_benchmark_util.py b/test/test_compile_benchmark_util.py\nindex 0033cf2d644234..22d63a185630ae 100644\n--- a/test/test_compile_benchmark_util.py\n+++ b/test/test_compile_benchmark_util.py\n@@ -23,7 +23,7 @@ class TestCompileBenchmarkUtil(TestCase):\n     def test_training_and_inference(self):\n         class ToyModel(torch.nn.Module):\n             def __init__(self):\n-                super(ToyModel, self).__init__()\n+                super().__init__()\n                 self.weight = torch.nn.Parameter(torch.Tensor(2, 2))\n \n             def forward(self, x):\ndiff --git a/test/test_cpp_extensions_jit.py b/test/test_cpp_extensions_jit.py\nindex 91bc533917dd7c..16346acd69ac11 100644\n--- a/test/test_cpp_extensions_jit.py\n+++ b/test/test_cpp_extensions_jit.py\n@@ -230,7 +230,7 @@ def test_jit_cuda_archflags(self):\n         # expected values is length-2 tuple: (list of ELF, list of PTX)\n         # note: there should not be more than one PTX value\n         archflags = {\n-            '': (['{}{}'.format(capability[0], capability[1]) for capability in capabilities], None),\n+            '': ([f'{capability[0]}{capability[1]}' for capability in capabilities], None),\n             \"Maxwell+Tegra;6.1\": (['53', '61'], None),\n             \"Volta\": (['70'], ['70']),\n         }\ndiff --git a/test/test_cpp_extensions_open_device_registration.py b/test/test_cpp_extensions_open_device_registration.py\nindex 42fbc767a40268..9bcc3117290480 100644\n--- a/test/test_cpp_extensions_open_device_registration.py\n+++ b/test/test_cpp_extensions_open_device_registration.py\n@@ -33,7 +33,7 @@ def remove_build_path():\n         shutil.rmtree(default_build_root, ignore_errors=True)\n \n \n-class DummyModule(object):\n+class DummyModule:\n \n     @staticmethod\n     def device_count() -> int:\n@@ -381,7 +381,7 @@ def test_open_device_storage_type():\n             foo_storage = foo_tensor.storage()\n             self.assertEqual(foo_storage.type(), \"torch.storage.TypedStorage\")\n \n-            class CustomFloatStorage():\n+            class CustomFloatStorage:\n                 @property\n                 def __module__(self):\n                     return \"torch.\" + torch._C._get_privateuse1_backend_name()\ndiff --git a/test/test_cuda.py b/test/test_cuda.py\nindex 23a811b2a1b3fa..415e31a3cd0000 100644\n--- a/test/test_cuda.py\n+++ b/test/test_cuda.py\n@@ -1781,7 +1781,7 @@ def compare(first, second):\n                     control = getattr(args[0].to(run_as_type), op)(*cast(args[1:], run_as_type), **add_kwargs)\n                 self.assertTrue(type(output_to_compare) == type(control))\n                 comparison = compare(output_to_compare, control)\n-                self.assertTrue(comparison, \"torch.{} result did not match control\".format(op))\n+                self.assertTrue(comparison, f\"torch.{op} result did not match control\")\n             self.assertTrue(torch.is_autocast_enabled())\n         self.assertFalse(torch.is_autocast_enabled())\n \n@@ -2727,7 +2727,7 @@ def test_graph_memory_stats_and_use_result_after_destroy_graph(self):\n                     stat = stat + pool_string + \".current\"\n                     current = postcapture_stats[stat] - precapture_stats[stat]\n                     self.assertEqual(current, expected, \"Pre to post capture delta of \" +\n-                                     stat + \" = {}, expected = {}, numel = {}\".format(current, expected, numel))\n+                                     stat + f\" = {current}, expected = {expected}, numel = {numel}\")\n \n                 g.replay()\n                 self.assertEqual(b.sum().item(), 6 * numel)\n@@ -2748,7 +2748,7 @@ def test_graph_memory_stats_and_use_result_after_destroy_graph(self):\n                 stat = stat + pool_string + \".current\"\n                 current = postdel_stats[stat] - precapture_stats[stat]\n                 self.assertEqual(current, expected, \"Pre capture to post graph delete delta of \" +\n-                                 stat + \" = {}, expected = {}, numel = {}\".format(current, expected, numel))\n+                                 stat + f\" = {current}, expected = {expected}, numel = {numel}\")\n \n             # del a, b before the next case is essential, otherwise overwriting a and b in the next case\n             # can throw off its allocation/deallocation counts.\n@@ -3068,10 +3068,10 @@ def test_graph_adam_adamw_with_explicitly_capturable_param_groups(self):\n         # mimicking `_test_graphed_optimizer` maladroitly to pass two param_groups to optimizer.__init__\n         n_warmup, n_replay = 3, 2\n         for optimizer, second_param_group_capturable in product((torch.optim.Adam, torch.optim.AdamW), (True, False)):\n-            ref_p1, param1 = [torch.nn.Parameter(torch.ones(1, device=\"cuda\")) for _ in range(2)]\n-            ref_p2, param2 = [torch.nn.Parameter(torch.ones(1, device=\"cuda\")) for _ in range(2)]\n-            grads1, grads2 = [[torch.randn_like(param1) for _ in range(n_warmup + n_replay)] for _ in range(2)]\n-            ref_grads1, ref_grads2 = [[t.clone() for t in tensors] for tensors in (grads1, grads2)]\n+            ref_p1, param1 = (torch.nn.Parameter(torch.ones(1, device=\"cuda\")) for _ in range(2))\n+            ref_p2, param2 = (torch.nn.Parameter(torch.ones(1, device=\"cuda\")) for _ in range(2))\n+            grads1, grads2 = ([torch.randn_like(param1) for _ in range(n_warmup + n_replay)] for _ in range(2))\n+            ref_grads1, ref_grads2 = ([t.clone() for t in tensors] for tensors in (grads1, grads2))\n             params = [\n                 {\"params\": [param1], \"capturable\": True},\n                 {\"params\": [param2], \"capturable\": second_param_group_capturable},\n@@ -3313,7 +3313,7 @@ def test_memory_snapshot(self):\n             if not IS_WINDOWS:\n                 with tempfile.NamedTemporaryFile() as f:\n                     torch.cuda.memory._save_segment_usage(f.name)\n-                    with open(f.name, 'r') as f2:\n+                    with open(f.name) as f2:\n                         self.assertTrue('test_cuda.py' in f2.read())\n \n             del x\ndiff --git a/test/test_cuda_expandable_segments.py b/test/test_cuda_expandable_segments.py\nindex 31478cb6e05396..123d2d2fe8e1ba 100644\n--- a/test/test_cuda_expandable_segments.py\n+++ b/test/test_cuda_expandable_segments.py\n@@ -9,4 +9,4 @@\n \n current_dir = os.path.dirname(os.path.abspath(__file__))\n filepath = os.path.join(current_dir, 'test_cuda.py')\n-exec(compile(open(filepath, 'r').read(), filepath, mode='exec'))\n+exec(compile(open(filepath).read(), filepath, mode='exec'))\ndiff --git a/test/test_dataloader.py b/test/test_dataloader.py\nindex 8e2d08bea33047..f4a52150948291 100644\n--- a/test/test_dataloader.py\n+++ b/test/test_dataloader.py\n@@ -179,7 +179,7 @@ def test_splits_indexing_type(self):\n         r\"\"\"Indices generated by random_split\n           should be of integer type\n         \"\"\"\n-        class CustomDataset():\n+        class CustomDataset:\n             def __init__(self, test_object, custom_list):\n                 self.data = custom_list\n                 self.test_object = test_object\n@@ -874,7 +874,7 @@ def _test_worker_info_init_fn(worker_id):\n     except RuntimeError as e:\n         assert str(e) == \"Cannot assign attributes to WorkerInfo objects\"\n     for k in ['id', 'num_workers', 'seed', 'dataset']:\n-        assert \"{}=\".format(k) in repr(worker_info)\n+        assert f\"{k}=\" in repr(worker_info)\n     dataset.value = [worker_id, os.getpid()]\n \n \n@@ -1960,11 +1960,11 @@ def test_proper_exit(self):\n                     continue\n \n                 desc = []\n-                desc.append('is_iterable_dataset={}'.format(is_iterable_dataset))\n-                desc.append('use_workers={}'.format(use_workers))\n-                desc.append('pin_memory={}'.format(pin_memory))\n-                desc.append('hold_iter_reference={}'.format(hold_iter_reference))\n-                desc.append('exit_method={}'.format(exit_method))\n+                desc.append(f'is_iterable_dataset={is_iterable_dataset}')\n+                desc.append(f'use_workers={use_workers}')\n+                desc.append(f'pin_memory={pin_memory}')\n+                desc.append(f'hold_iter_reference={hold_iter_reference}')\n+                desc.append(f'exit_method={exit_method}')\n                 desc = 'test_proper_exit with ' + ', '.join(desc)\n \n                 # Event that the loader process uses to signal testing process\n@@ -1992,9 +1992,9 @@ def test_proper_exit(self):\n                 if not loader_setup_event.is_set():\n                     fail_msg = desc + ': loader process failed to setup within given time'\n                     if loader_p.exception is not None:\n-                        fail_msg += ', and had exception {}'.format(loader_p.exception)\n+                        fail_msg += f', and had exception {loader_p.exception}'\n                     elif not loader_p.is_alive():\n-                        fail_msg += ', and exited with code {} but had no exception'.format(loader_p.exitcode)\n+                        fail_msg += f', and exited with code {loader_p.exitcode} but had no exception'\n                     else:\n                         fail_msg += ', and is still alive.'\n                     if loader_p.is_alive():\n@@ -2013,18 +2013,18 @@ def fail(reason):\n                     if reason is None:\n                         err_msg = desc\n                     else:\n-                        err_msg = '{}: {}'.format(desc, reason)\n+                        err_msg = f'{desc}: {reason}'\n                     err_msg += '\\nLoader info:\\n\\t'\n                     if loader_psutil_p.is_running():\n                         err_msg += str(loader_psutil_p.as_dict(attrs=report_psutil_attrs))\n                         # this may kill the process, needs to run after the above line\n                         loader_p.print_traces_of_all_threads()\n                     else:\n-                        err_msg += 'exited with code {}'.format(loader_p.exitcode)\n+                        err_msg += f'exited with code {loader_p.exitcode}'\n                     if use_workers:\n                         err_msg += '\\nWorker(s) info:'\n                         for idx, worker_psutil_p in enumerate(worker_psutil_ps):\n-                            err_msg += '\\n\\tWorker {}:\\n\\t\\t'.format(idx)\n+                            err_msg += f'\\n\\tWorker {idx}:\\n\\t\\t'\n                             if worker_psutil_p.is_running():\n                                 err_msg += str(worker_psutil_p.as_dict(attrs=report_psutil_attrs))\n                                 # this may kill the process, needs to run after the above line\n@@ -2040,7 +2040,7 @@ def fail(reason):\n                     if loader_p.is_alive():\n                         fail_reason = 'loader process did not terminate'\n                         if loader_p.exception is not None:\n-                            fail(fail_reason + ', and had exception {}'.format(loader_p.exception))\n+                            fail(fail_reason + f', and had exception {loader_p.exception}')\n                         else:\n                             fail(fail_reason + ', and had no exception')\n                     _, alive = psutil.wait_procs(worker_psutil_ps, timeout=(MP_STATUS_CHECK_INTERVAL + JOIN_TIMEOUT))\n@@ -2049,7 +2049,7 @@ def fail(reason):\n                             ', '.join(str(p.pid) for p in alive)))\n                     if exit_method is None:\n                         if loader_p.exitcode != 0:\n-                            fail('loader process had nonzero exitcode {}'.format(loader_p.exitcode))\n+                            fail(f'loader process had nonzero exitcode {loader_p.exitcode}')\n                     else:\n                         if loader_p.exitcode == 0:\n                             fail('loader process had zero exitcode')\ndiff --git a/test/test_datapipe.py b/test/test_datapipe.py\nindex 67c14a1e24808c..5c2ebb996395c6 100644\n--- a/test/test_datapipe.py\n+++ b/test/test_datapipe.py\n@@ -206,7 +206,7 @@ def read(self):\n             if self.opened:\n                 return \"\".join(self)\n             else:\n-                raise IOError(\"Cannot read from un-opened file descriptor\")\n+                raise OSError(\"Cannot read from un-opened file descriptor\")\n \n         def __iter__(self):\n             for i in range(5):\n@@ -285,7 +285,7 @@ def tearDown(self):\n             self.temp_sub_dir.cleanup()\n             self.temp_dir.cleanup()\n         except Exception as e:\n-            warnings.warn(\"TestIterableDatasetBasic was not able to cleanup temp dir due to {}\".format(str(e)))\n+            warnings.warn(f\"TestIterableDatasetBasic was not able to cleanup temp dir due to {str(e)}\")\n \n     def test_listdirfiles_iterable_datapipe(self):\n         temp_dir = self.temp_dir.name\ndiff --git a/test/test_dispatch.py b/test/test_dispatch.py\nindex 0fdb92ba0dafd6..7a0da7e3b553d9 100644\n--- a/test/test_dispatch.py\n+++ b/test/test_dispatch.py\n@@ -101,15 +101,15 @@ def run_ops(self, name, ops, ctor_order=None, dtor_order=None,\n \n         # double underscore to make it less likely we conflict with something\n         # else\n-        test_namespace = \"__test{}__\".format(self.namespace_index)\n+        test_namespace = f\"__test{self.namespace_index}__\"\n \n         def check_invariants(actual_provenance):\n             C._dispatch_check_invariants(name)\n             # Normalize the test namespace so that expected outputs are stable\n             actual_state = C._dispatch_dump(\n-                \"{}::{}\".format(test_namespace, name)).replace(test_namespace, \"test\")\n+                f\"{test_namespace}::{name}\").replace(test_namespace, \"test\")\n             actual_table = C._dispatch_dump_table(\n-                \"{}::{}\".format(test_namespace, name)).replace(test_namespace, \"test\")\n+                f\"{test_namespace}::{name}\").replace(test_namespace, \"test\")\n             expected_state, expected_table, expected_provenance = results.setdefault(\n                 frozenset(active_ops),\n                 Result(actual_state, actual_table, actual_provenance)\n@@ -138,7 +138,7 @@ def check_invariants(actual_provenance):\n             active_ops.add(op_ix)\n             try:\n                 ops[op_ix](refs[op_ix])\n-                check_invariants(\"running ctors {}\".format(ctor_order[:i + 1]))\n+                check_invariants(f\"running ctors {ctor_order[:i + 1]}\")\n             except RuntimeError as e:\n                 if not expect_raises:\n                     raise\n@@ -146,7 +146,7 @@ def check_invariants(actual_provenance):\n                 actual = actual.split(\"\\nException raised from \")[0]\n                 expected, _, expected_provenance = results.setdefault(\n                     frozenset(active_ops),\n-                    Result(actual, \"\", \"error after running ctors {}\".format(ctor_order[:i + 1]))\n+                    Result(actual, \"\", f\"error after running ctors {ctor_order[:i + 1]}\")\n                 )\n                 self.assertMultiLineEqual(expected, actual, expected_provenance)\n                 set_to_report = frozenset(active_ops)\ndiff --git a/test/test_dlpack.py b/test/test_dlpack.py\nindex 5e3182e43685a5..023afcc9e7f165 100644\n--- a/test/test_dlpack.py\n+++ b/test/test_dlpack.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: tests\"]\n \n import torch\ndiff --git a/test/test_dynamic_shapes.py b/test/test_dynamic_shapes.py\nindex 506c05acb4b310..d5dd3e8ee8a908 100644\n--- a/test/test_dynamic_shapes.py\n+++ b/test/test_dynamic_shapes.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: jit\"]\n \n import contextlib\ndiff --git a/test/test_fake_tensor.py b/test/test_fake_tensor.py\nindex c841f80c185802..ec87cd8e13b36b 100644\n--- a/test/test_fake_tensor.py\n+++ b/test/test_fake_tensor.py\n@@ -851,7 +851,7 @@ def test_no_active_mode(self):\n         self.assertEqual(out.device.type, \"cpu\")\n \n     def test_multiple_modes(self):\n-        t = torch.rand(([4]))\n+        t = torch.rand([4])\n         t2 = torch.rand([4])\n         with FakeTensorMode() as m:\n             with FakeTensorMode() as m2:\ndiff --git a/test/test_foreach.py b/test/test_foreach.py\nindex e57d640c01738b..8ba692590f1c0e 100644\n--- a/test/test_foreach.py\n+++ b/test/test_foreach.py\n@@ -1002,16 +1002,16 @@ def test_tensors_grouping(self):\n         num_tensors_seen = 0\n         for (device, dtype), ([l1, l2, l3], indices) in grouped_tensors.items():\n             for t in itertools.chain(l1, l3):\n-                self.assertEquals(t.device, device)\n-                self.assertEquals(t.dtype, dtype)\n+                self.assertEqual(t.device, device)\n+                self.assertEqual(t.dtype, dtype)\n                 num_tensors_seen += 1\n             self.assertEqual(len(l1), len(l2))\n             self.assertTrue(all(p is None for p in l2))\n             for i, index in enumerate(indices):\n-                self.assertEquals(l1[i], list1[index])\n-                self.assertEquals(l2[i], list2[index])\n-                self.assertEquals(l3[i], list3[index])\n-        self.assertEquals(num_tensors_seen, 2 * num_tensors_per_list)\n+                self.assertEqual(l1[i], list1[index])\n+                self.assertEqual(l2[i], list2[index])\n+                self.assertEqual(l3[i], list3[index])\n+        self.assertEqual(num_tensors_seen, 2 * num_tensors_per_list)\n \n \n instantiate_device_type_tests(TestForeach, globals())\ndiff --git a/test/test_functional_optim.py b/test/test_functional_optim.py\nindex 98eb79f808c19d..ccb1e794279006 100644\n--- a/test/test_functional_optim.py\n+++ b/test/test_functional_optim.py\n@@ -34,15 +34,15 @@ def __init__(\n     ):\n \n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 < weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         self.defaults = {\n             \"lr\": lr,\ndiff --git a/test/test_jit.py b/test/test_jit.py\nindex 25e165f1e93161..9724962838af85 100644\n--- a/test/test_jit.py\n+++ b/test/test_jit.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: jit\"]\n \n import torch\n@@ -1186,7 +1185,7 @@ def test_torch_autograd_backward_with_grad_tensors(input):\n \n     def test_script_backward_twice(self):\n         def checkBackwardTwiceScript(fn, inputs, retain_graph_=False):\n-            class jit_profiling_executor_false():\n+            class jit_profiling_executor_false:\n                 def __enter__(self):\n                     torch._C._jit_set_profiling_executor(False)\n \n@@ -2897,7 +2896,7 @@ def fn(x):\n \n         with TemporaryFileName() as fname:\n             fn.save(fname)\n-            with io.open(fname, 'rb') as f:\n+            with open(fname, 'rb') as f:\n                 self.assertTrue(torch.serialization._is_zipfile(f))\n \n     def test_python_bindings(self):\n@@ -3954,7 +3953,7 @@ def select_expr_or_var():\n                 if idx < len(exprs):\n                     return get_expr(idx)\n                 else:\n-                    return 'v{}'.format(idx - len(exprs))\n+                    return f'v{idx - len(exprs)}'\n \n             for i in range(50):\n                 n = None\n@@ -3963,12 +3962,12 @@ def select_expr_or_var():\n                     n = template.count('{}')\n \n                 if 'VAR' in template:\n-                    src_lines.append('  v{} = {}'.format(n_variables, select_expr_or_var()))\n+                    src_lines.append(f'  v{n_variables} = {select_expr_or_var()}')\n                     n_variables += 1\n                 else:\n                     exprs.append(template.format(*(select_expr_or_var() for _ in range(n))))\n \n-            src_lines.append('  return ({})\\n'.format(''.join('v{},'.format(i) for i in range(n_variables))))\n+            src_lines.append('  return ({})\\n'.format(''.join(f'v{i},' for i in range(n_variables))))\n             return '\\n'.join(src_lines)\n \n         for i in range(100):\n@@ -4392,7 +4391,7 @@ def foobar(xyz):\n             return torch.blargh(xyz)\n \n         _, lineno = inspect.getsourcelines(foobar)\n-        with self.assertRaisesRegex(RuntimeError, \"test_jit.py\\\", line {}\".format(lineno + 1)):\n+        with self.assertRaisesRegex(RuntimeError, f\"test_jit.py\\\", line {lineno + 1}\"):\n             scripted = torch.jit.script(foobar)\n \n     def test_file_line_error_class_defn(self):\n@@ -4401,7 +4400,7 @@ def baz(self, xyz):\n                 return torch.blargh(xyz)\n \n         _, lineno = inspect.getsourcelines(FooBar)\n-        with self.assertRaisesRegex(RuntimeError, \"test_jit.py\\\", line {}\".format(lineno + 2)):\n+        with self.assertRaisesRegex(RuntimeError, f\"test_jit.py\\\", line {lineno + 2}\"):\n             torch.jit.script(FooBar)\n \n     def test_file_line_graph(self):\n@@ -4411,7 +4410,7 @@ def foobar(xyz):\n         scripted = torch.jit.script(foobar)\n \n         _, lineno = inspect.getsourcelines(foobar)\n-        fc = FileCheck().check('test_jit.py:{}:19'.format(lineno + 1))\n+        fc = FileCheck().check(f'test_jit.py:{lineno + 1}:19')\n         fc.run(scripted.graph)\n         fc.run(str(scripted.graph))\n \n@@ -4431,7 +4430,7 @@ def forward(self, xyz):\n         scripted = torch.jit.load(bytesio)\n \n         _, lineno = inspect.getsourcelines(Scripted)\n-        fc = FileCheck().check(':{}'.format(lineno + 3))\n+        fc = FileCheck().check(f':{lineno + 3}')\n         fc.run(scripted.graph)\n         fc.run(str(scripted.graph))\n \n@@ -4453,7 +4452,7 @@ def foobar(xyz):\n         scripted = torch.jit.trace(foobar, (torch.rand(3, 4)))\n \n         _, lineno = inspect.getsourcelines(foobar)\n-        fc = FileCheck().check('test_jit.py:{}:0'.format(lineno + 1))\n+        fc = FileCheck().check(f'test_jit.py:{lineno + 1}:0')\n         fc.run(scripted.graph)\n         fc.run(str(scripted.graph))\n \n@@ -4468,7 +4467,7 @@ def forward(self, x, w):\n         loaded = self.getExportImportCopy(ft)\n         _, lineno = inspect.getsourcelines(FooTest)\n \n-        with self.assertRaisesRegex(RuntimeError, 'test_jit.py\\\", line {}'.format(lineno + 3)):\n+        with self.assertRaisesRegex(RuntimeError, f'test_jit.py\\\", line {lineno + 3}'):\n             loaded(torch.rand(3, 4), torch.rand(30, 40))\n \n     def test_serialized_source_ranges_graph(self):\n@@ -4482,7 +4481,7 @@ def forward(self, x, w):\n         loaded = self.getExportImportCopy(ft)\n         _, lineno = inspect.getsourcelines(FooTest3)\n \n-        fc = FileCheck().check('test_jit.py:{}'.format(lineno + 3))\n+        fc = FileCheck().check(f'test_jit.py:{lineno + 3}')\n         fc.run(loaded.graph)\n \n     def test_serialized_source_ranges2(self):\n@@ -4494,7 +4493,7 @@ def forward(self):\n \n         _, lineno = inspect.getsourcelines(FooTest2)\n \n-        with self.assertRaisesRegex(torch.jit.Error, 'test_jit.py\\\", line {}'.format(lineno + 3)):\n+        with self.assertRaisesRegex(torch.jit.Error, f'test_jit.py\\\", line {lineno + 3}'):\n             ft = FooTest2()\n             loaded = self.getExportImportCopy(ft)\n             loaded()\n@@ -7240,7 +7239,7 @@ def test(op, tensor, const, swap_args, template=template):\n             scope = {}\n             execWrapper(code, globals(), scope)\n             cu = torch.jit.CompilationUnit(code)\n-            message = 'with code `{} {} {}` and t={}'.format(args[0], op, args[1], tensor)\n+            message = f'with code `{args[0]} {op} {args[1]}` and t={tensor}'\n             res1 = cu.func(tensor)\n             res2 = scope['func'](tensor)\n             self.assertEqual(res1, res2, msg=message + \"\\nres1=\" + str(res1) + \"\\nres2=\" + str(res2))\n@@ -7338,7 +7337,7 @@ def func():\n                 exec(code, globals(), scope)\n                 cu = torch.jit.CompilationUnit(code)\n                 torch._C._jit_pass_complete_shape_analysis(cu.func.graph, (), False)\n-                FileCheck().check(expect).check(\"aten::{tensor_op}\".format(tensor_op=op)).run(cu.func.graph)\n+                FileCheck().check(expect).check(f\"aten::{op}\").run(cu.func.graph)\n \n         @torch.jit.script\n         def test_dtype(inp_dtype: torch.dtype):\n@@ -7545,7 +7544,7 @@ def test_copy_behavior(t, non_blocking=False):\n             devices = [t.device]\n             if t.device.type == 'cuda':\n                 if t.device.index == -1:\n-                    devices.append('cuda:{}'.format(torch.cuda.current_device()))\n+                    devices.append(f'cuda:{torch.cuda.current_device()}')\n                 elif t.device.index == torch.cuda.current_device():\n                     devices.append('cuda')\n             for device in devices:\n@@ -8600,7 +8599,7 @@ def func():\n                     if(not tensor_type.is_floating_point or (dtype is not None and not dtype.is_floating_point)):\n                         if op in ['mean', 'softmax', 'log_softmax']:\n                             continue\n-                    return_line = \"torch.tensor({}, dtype={}).{}({}dtype={})\".format(tensor_data, tensor_type, op, str_args, dtype)\n+                    return_line = f\"torch.tensor({tensor_data}, dtype={tensor_type}).{op}({str_args}dtype={dtype})\"\n                     # uncomment for debugging a failed test:\n                     # print(\"testing {}\".format(return_line))\n                     code = template.format(return_line=return_line)\n@@ -8653,7 +8652,7 @@ def func():\n \n         args = []\n         for dtype in dtypes:\n-            args = args + [\"torch.tensor({}, dtype={})\".format(shape, dtype)]\n+            args = args + [f\"torch.tensor({shape}, dtype={dtype})\"]\n         args = args + [1, 1.5]\n \n         def isBool(arg):\n@@ -8670,7 +8669,7 @@ def isBool(arg):\n                        isinstance(first_arg, int) or\n                        (isinstance(first_arg, str) and 'int' in first_arg))):\n                         continue\n-                    return_line = \"torch.{}({}, {})\".format(op, first_arg, second_arg)\n+                    return_line = f\"torch.{op}({first_arg}, {second_arg})\"\n                     # uncomment for debugging a failed test:\n                     # print(\"testing {}\".format(return_line))\n                     code = template.format(first_arg, second_arg, op)\n@@ -10220,7 +10219,7 @@ def fn2(x, y, z):\n         def fn3(x, y, z):\n             return fn_varargs(x, y, z)\n \n-        x, y, z = [torch.randn(2, 2) for _ in range(3)]\n+        x, y, z = (torch.randn(2, 2) for _ in range(3))\n         self.checkScript(fn1, (x, y, z), optimize=True)\n         self.checkScript(fn2, (x, y, z), optimize=True)\n         self.checkScript(fn3, (x, y, z), optimize=True)\n@@ -12964,7 +12963,7 @@ def foo(x,  # type: {input}\n             returns = fn.schema.returns\n             self.assertEqual(str(args[0].type), pair[1])\n             self.assertEqual(str(args[1].type), \"Tuple[Tensor, Tensor]\")\n-            self.assertEqual(str(returns[0].type), \"Tuple[{}, {}]\".format(pair[1], pair[1]))\n+            self.assertEqual(str(returns[0].type), f\"Tuple[{pair[1]}, {pair[1]}]\")\n \n     def test_bad_multiline_annotations(self):\n         with self.assertRaisesRegex(RuntimeError, \"Return type line\"):\n@@ -13570,7 +13569,7 @@ def index_str_to_tensor(s):\n             # type: (str) -> Tensor\n             return torch.tensor(ord(s))  # noqa: T484\n \n-        s = u'\\u00a3'.encode('utf8')[:1]\n+        s = '\\u00a3'.encode()[:1]\n         self.checkScript(index_str_to_tensor, (s,))\n \n     def test_chr(self):\n@@ -14707,7 +14706,7 @@ def forward(self, x):\n \n         over = Over()\n         self.assertEqual(over((x, x)), x + 5)\n-        self.assertEqual(over((x)), x + 20)\n+        self.assertEqual(over(x), x + 20)\n \n         class Unannotated(torch.nn.Module):\n             @torch.jit._overload_method  # noqa: F811\n@@ -16130,7 +16129,7 @@ def create_script_module(*args, **kwargs):\n \n             method_args = ', '.join(['self'] + actuals)\n             call_args_str = ', '.join(actuals)\n-            call = \"self.submodule({})\".format(call_args_str)\n+            call = f\"self.submodule({call_args_str})\"\n             script = script_method_template.format(method_args, call)\n \n             submodule_constants = []\ndiff --git a/test/test_jit_fuser.py b/test/test_jit_fuser.py\nindex ef3843dc01c4dc..5f3dcf0dec4311 100644\n--- a/test/test_jit_fuser.py\n+++ b/test/test_jit_fuser.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: jit\"]\n \n import unittest\n@@ -51,7 +50,7 @@ def assertAllFused(self, graph, except_for=()):\n         allowed_nodes = {'prim::Constant', 'prim::FusionGroup', 'prim::BailoutTemplate',\n                          'prim::BailOut', 'prim::TupleConstruct'} | set(except_for)\n         self.assertTrue(all(node.kind() in allowed_nodes for node in graph.nodes()),\n-                        'got {}'.format(graph))\n+                        f'got {graph}')\n         self.assertTrue([node.kind() for node in graph.nodes()].count('prim::FusionGroup') == 1)\n \n     def _test_fused_abs(self, device='cpu'):\ndiff --git a/test/test_jit_fuser_te.py b/test/test_jit_fuser_te.py\nindex 706a37598e797f..dcd6c767ee9327 100644\n--- a/test/test_jit_fuser_te.py\n+++ b/test/test_jit_fuser_te.py\n@@ -1676,7 +1676,7 @@ def apply_with_scalar(fn, scalar):\n                 self.assertEqual(ref, t(x))\n             except Exception as e:\n                 raise RuntimeError(\n-                    \"Failed: {} {} {} {}\".format(dtype, op.__name__, device, scalar)\n+                    f\"Failed: {dtype} {op.__name__} {device} {scalar}\"\n                 ) from e\n \n     def test_binary_pow(self):\ndiff --git a/test/test_jit_llga_fuser.py b/test/test_jit_llga_fuser.py\nindex c9f9962601b49a..5aef08db328916 100644\n--- a/test/test_jit_llga_fuser.py\n+++ b/test/test_jit_llga_fuser.py\n@@ -847,7 +847,7 @@ def test(self, dtype=dtype):\n         return test\n \n     for dtype in [torch.bfloat16, torch.float32]:\n-        setattr(TestModel, 'test_vision_%s_%s' % (model_name, str(dtype).split(\"torch.\")[1]), _wrapper(model_name, dtype))\n+        setattr(TestModel, 'test_vision_{}_{}'.format(model_name, str(dtype).split(\"torch.\")[1]), _wrapper(model_name, dtype))\n \n \n instantiate_device_type_tests(TestFusionPattern, globals())\ndiff --git a/test/test_linalg.py b/test/test_linalg.py\nindex 3f2744630c11bd..dfefee9c00f8de 100644\n--- a/test/test_linalg.py\n+++ b/test/test_linalg.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: linear algebra\"]\n \n import torch\n@@ -1554,7 +1553,7 @@ def run_error_test_case(input, ord, dim, keepdim, error_type, error_regex):\n     @precisionOverride({torch.cfloat: 5e-4})\n     def test_norm_complex(self, device, dtype):\n         def gen_error_message(input_size, ord, keepdim, dim=None):\n-            return \"complex norm failed for input size %s, ord=%s, keepdim=%s, dim=%s\" % (\n+            return \"complex norm failed for input size {}, ord={}, keepdim={}, dim={}\".format(\n                 input_size, ord, keepdim, dim)\n \n         vector_ords = [None, 0, 1, 2, 3, inf, -1, -2, -3, -inf]\n@@ -2095,13 +2094,13 @@ def test_eigvals_errors_and_warnings(self, device, dtype):\n     @skipCPUIfNoLapack\n     def test_norm_old(self, device):\n         def gen_error_message(input_size, p, keepdim, dim=None):\n-            return \"norm failed for input size %s, p=%s, keepdim=%s, dim=%s\" % (\n+            return \"norm failed for input size {}, p={}, keepdim={}, dim={}\".format(\n                 input_size, p, keepdim, dim)\n \n         # 'nuc' norm uses SVD, and thus its precsion is much lower than other norms.\n         # test_svd takes @precisionOverride({torch.float: 1e-4, torch.cfloat: 2e-4}),\n         # and here we are doing the same thing for nuc norm.\n-        class PrecisionContext(object):\n+        class PrecisionContext:\n             def __init__(self, test, norm):\n                 self.norm = norm\n                 self.saved_overrides = getattr(test, 'precision_overrides', None)\n@@ -2193,7 +2192,7 @@ def test_norm_old_nan_propagation(self, device):\n     @skipCPUIfNoLapack\n     def test_norm_complex_old(self, device):\n         def gen_error_message(input_size, p, keepdim, dim=None):\n-            return \"complex norm failed for input size %s, p=%s, keepdim=%s, dim=%s\" % (\n+            return \"complex norm failed for input size {}, p={}, keepdim={}, dim={}\".format(\n                 input_size, p, keepdim, dim)\n \n         for keepdim in [False, True]:\n@@ -4740,7 +4739,7 @@ def test_renorm_ps(self, device):\n         for p in [1, 2, 3, 4, inf]:\n             res = x.renorm(p, 1, 1)\n             expected = x / x.norm(p, 0, keepdim=True).clamp(min=1)\n-            self.assertEqual(res, expected, msg=\"renorm failed for {}-norm\".format(p))\n+            self.assertEqual(res, expected, msg=f\"renorm failed for {p}-norm\")\n \n     @skipCPUIfNoLapack\n     @skipCUDAIfNoCusolver\n@@ -5608,7 +5607,7 @@ def test_addmm_baddbmm_overflow(self, device, dtype):\n     @dtypes(torch.float)\n     def test_baddbmm_nan_input_with_zero_beta(self, device, dtype):\n         for shape in [[3, 2, 2], [2, 20, 20]]:\n-            mat1, mat2 = [torch.randn(shape, dtype=dtype, device=device) for _ in range(2)]\n+            mat1, mat2 = (torch.randn(shape, dtype=dtype, device=device) for _ in range(2))\n             inputs = [torch.randn(shape, dtype=dtype, device=device),\n                       torch.randn(shape, dtype=dtype, device=device).fill_(torch.nan)]\n             outs = [None, torch.randn(shape, dtype=dtype, device=device),\n@@ -6670,15 +6669,15 @@ def test_single_det(M, target, desc):\n \n             # Test det\n             self.assertEqual(det, target_sdet * target_logabsdet.exp(),\n-                             atol=1e-6, rtol=0, msg='{} (det)'.format(desc))\n+                             atol=1e-6, rtol=0, msg=f'{desc} (det)')\n \n             # Test slogdet\n             # Compare the overall value rather than individual parts because of\n             # precision issues when det is near zero.\n             self.assertEqual(sdet * logabsdet.exp(), target_sdet * target_logabsdet.exp(),\n-                             atol=1e-6, rtol=0, msg='{} (slogdet)'.format(desc))\n+                             atol=1e-6, rtol=0, msg=f'{desc} (slogdet)')\n             self.assertEqual(linalg_sdet * linalg_logabsdet.exp(), target_sdet * target_logabsdet.exp(),\n-                             atol=1e-6, rtol=0, msg='{} (linalg_slogdet)'.format(desc))\n+                             atol=1e-6, rtol=0, msg=f'{desc} (linalg_slogdet)')\n \n             # Test logdet\n             # Compare logdet against our own pytorch slogdet because they should\n@@ -6686,10 +6685,10 @@ def test_single_det(M, target, desc):\n             # slogdet implementations when det is near zero due to precision\n             # issues.\n             if sdet.item() < 0:\n-                self.assertTrue(logdet.item() != logdet.item(), '{} (logdet negative case)'.format(desc))\n+                self.assertTrue(logdet.item() != logdet.item(), f'{desc} (logdet negative case)')\n             else:\n                 self.assertEqual(logdet.exp(), target_logabsdet.exp(),\n-                                 atol=1e-6, rtol=0, msg='{} (logdet non-negative case)'.format(desc))\n+                                 atol=1e-6, rtol=0, msg=f'{desc} (logdet non-negative case)')\n \n         eye = torch.eye(5, dtype=dtype, device=device)\n         test_single_det(eye, (torch.ones((), dtype=dtype, device=device), torch.zeros((), dtype=dtype, device=device)), 'identity')\ndiff --git a/test/test_logging.py b/test/test_logging.py\nindex 01fdd3f8edd838..ab9fc7299e26eb 100644\n--- a/test/test_logging.py\n+++ b/test/test_logging.py\n@@ -14,7 +14,7 @@ def testApiUsage(self):\n         s = TestCase.runWithPytorchAPIUsageStderr(\"import torch\")\n         self.assertRegex(s, \"PYTORCH_API_USAGE.*import\")\n         # import the shared library directly - it triggers static init but doesn't call anything\n-        s = TestCase.runWithPytorchAPIUsageStderr(\"from ctypes import CDLL; CDLL('{}')\".format(torch._C.__file__))\n+        s = TestCase.runWithPytorchAPIUsageStderr(f\"from ctypes import CDLL; CDLL('{torch._C.__file__}')\")\n         self.assertNotRegex(s, \"PYTORCH_API_USAGE\")\n \n \ndiff --git a/test/test_matmul_cuda.py b/test/test_matmul_cuda.py\nindex 982940c67b3842..7dea0e4c1d7be6 100644\n--- a/test/test_matmul_cuda.py\n+++ b/test/test_matmul_cuda.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: linear algebra\"]\n \n import unittest\n@@ -133,7 +132,7 @@ def test_cublas_addmm_alignment(self):\n          (2, 1000, 1000, 1000),\n          (1, 10000, 1000, 10000),\n          (1, 10000, 10000, 10000)],\n-        name_fn=lambda batch_size, N, M, P: \"{}_{}_{}_{}\".format(batch_size, N, M, P),\n+        name_fn=lambda batch_size, N, M, P: f\"{batch_size}_{N}_{M}_{P}\",\n     )\n     def test_cublas_baddbmm_large_input(self, device, batch_size, N, M, P, dtype):\n         cpu_dtype = dtype\ndiff --git a/test/test_meta.py b/test/test_meta.py\nindex b0879b930b5cf1..f71d15f1d2886a 100644\n--- a/test/test_meta.py\n+++ b/test/test_meta.py\n@@ -1343,7 +1343,7 @@ def print_op_str_if_not_supported(op_str):\n if __name__ == \"__main__\":\n     COMPARE_XLA = os.getenv('PYTORCH_COMPARE_XLA', None)\n     if COMPARE_XLA is not None:\n-        with open(COMPARE_XLA, \"r\") as f:\n+        with open(COMPARE_XLA) as f:\n             d = yaml.load(f, Loader=YamlLoader)\n             ops = d.get(\"full_codegen\", []) + d.get(\"supported\", []) + d.get(\"autograd\", [])\n             for op_str in ops:\n@@ -1352,7 +1352,7 @@ def print_op_str_if_not_supported(op_str):\n \n     COMPARE_TEXT = os.getenv('PYTORCH_COMPARE_TEXT', None)\n     if COMPARE_TEXT is not None:\n-        with open(COMPARE_TEXT, \"r\") as f:\n+        with open(COMPARE_TEXT) as f:\n             for op_str in f:\n                 print_op_str_if_not_supported(op_str.strip())\n         sys.exit(0)\ndiff --git a/test/test_mobile_optimizer.py b/test/test_mobile_optimizer.py\nindex a6c0a0692d4570..eeb62ecc4bc444 100644\n--- a/test/test_mobile_optimizer.py\n+++ b/test/test_mobile_optimizer.py\n@@ -46,7 +46,7 @@ def test_optimize_for_mobile(self):\n \n         input_data = torch.rand((batch_size, input_channels, height, width))\n         conv_weight = torch.rand((output_channels, input_channels_per_group, kernel_h, kernel_w))\n-        conv_bias = torch.rand((output_channels))\n+        conv_bias = torch.rand(output_channels)\n         result = F.conv2d(input_data, conv_weight, conv_bias, strides, paddings, dilations, groups)\n         weight_output_dim = 24\n         linear_input_shape = result.shape[1]\n@@ -56,9 +56,9 @@ class MyTestModule(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.conv_weight = torch.nn.Parameter(torch.rand(conv_weight_shape))\n-                self.conv_bias = torch.nn.Parameter(torch.rand((conv_bias_shape)))\n+                self.conv_bias = torch.nn.Parameter(torch.rand(conv_bias_shape))\n                 self.linear_weight = torch.nn.Parameter(torch.rand(linear_weight_shape))\n-                self.linear_bias = torch.nn.Parameter(torch.rand((weight_output_dim)))\n+                self.linear_bias = torch.nn.Parameter(torch.rand(weight_output_dim))\n                 self.strides = strides\n                 self.paddings = paddings\n                 self.dilations = dilations\n@@ -169,7 +169,7 @@ class MyMobileOptimizedTagTest(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.linear_weight = torch.nn.Parameter(torch.rand(linear_weight_shape))\n-                self.linear_bias = torch.nn.Parameter(torch.rand((weight_output_dim)))\n+                self.linear_bias = torch.nn.Parameter(torch.rand(weight_output_dim))\n \n             def forward(self, x):\n                 o = F.linear(x, self.linear_weight, self.linear_bias)\n@@ -186,7 +186,7 @@ class MyPreserveMethodsTest(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.linear_weight = torch.nn.Parameter(torch.rand(linear_weight_shape))\n-                self.linear_bias = torch.nn.Parameter(torch.rand((weight_output_dim)))\n+                self.linear_bias = torch.nn.Parameter(torch.rand(weight_output_dim))\n \n             def forward(self, x):\n                 o = F.linear(x, self.linear_weight, self.linear_bias)\n@@ -582,7 +582,7 @@ def dummy_method_ref_attr_pqr(self):\n         self.assertTrue(\n             cloned.qualified_name.startswith('__torch__.'),\n             (\"Expected the cloned module's name to start with the string \"\n-             \"'__torch__.', but got: {0}\").format(cloned.qualified_name),\n+             \"'__torch__.', but got: {}\").format(cloned.qualified_name),\n         )\n \n \ndiff --git a/test/test_mps.py b/test/test_mps.py\nindex 8f16af44d378be..d7bab2ceb9cfcf 100644\n--- a/test/test_mps.py\n+++ b/test/test_mps.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: mps\"]\n \n import io\n@@ -832,7 +831,7 @@ def dec(fn):\n         return fn\n     return dec\n \n-class MpsMemoryLeakCheck():\n+class MpsMemoryLeakCheck:\n     def __init__(self, testcase, name=None):\n         self.name = testcase.id() if name is None else name\n         self.testcase = testcase\n@@ -3745,7 +3744,7 @@ def helper(x, other):\n             result_cpu = torch.logical_and(cpu_x, cpu_other)\n             self.assertEqual(result, result_cpu)\n \n-        helper(self._wrap_tensor([1, 1, 0, 0]), self._wrap_tensor(([1, 0, 0, 1])))\n+        helper(self._wrap_tensor([1, 1, 0, 0]), self._wrap_tensor([1, 0, 0, 1]))\n         helper(\n             self._wrap_tensor([1, 1, 0, 0], dtype=torch.float, requires_grad=True),\n             self._wrap_tensor([1, 0, 0, 1], dtype=torch.float)\n@@ -3769,7 +3768,7 @@ def helper(x, other):\n \n             self.assertEqual(result, result_cpu)\n \n-        helper(self._wrap_tensor([1, 1, 0, 0]), self._wrap_tensor(([1, 0, 0, 1])))\n+        helper(self._wrap_tensor([1, 1, 0, 0]), self._wrap_tensor([1, 0, 0, 1]))\n         helper(\n             self._wrap_tensor([1, 1, 0, 0], dtype=torch.float, requires_grad=True),\n             self._wrap_tensor([1, 0, 0, 1], dtype=torch.float)\n@@ -3793,7 +3792,7 @@ def helper(x, other):\n \n             self.assertEqual(result, result_cpu)\n \n-        helper(self._wrap_tensor([1, 1, 0, 0]), self._wrap_tensor(([1, 0, 0, 1])))\n+        helper(self._wrap_tensor([1, 1, 0, 0]), self._wrap_tensor([1, 0, 0, 1]))\n         helper(\n             self._wrap_tensor([1, 1, 0, 0], dtype=torch.float, requires_grad=True),\n             self._wrap_tensor([1, 0, 0, 1], dtype=torch.float)\n@@ -9234,7 +9233,7 @@ def get_grid(device='cpu', data=None):\n                                     [[3.0000004768, 6.5000000000, 5.0000, 4.6675000191, 9.2500],\n                                      [1.0000000000, 7.1665000916, 5.0000, 5.0000000000, 9.2500]], device=\"mps\").view(1, 1, 2, 5)\n                         else:\n-                            raise AssertionError(\"missing groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing groundtruth test for padding mode '{padding_mode}'\")\n                     elif mode == 'nearest':\n                         if padding_mode == 'zeros':\n                             if align_corners:\n@@ -9264,7 +9263,7 @@ def get_grid(device='cpu', data=None):\n                                     [[1., 8., 5., 7., 9.],\n                                      [1., 8., 5., 8., 9.]], device=\"mps\").view(1, 1, 2, 5)\n                         else:\n-                            raise AssertionError(\"missing groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing groundtruth test for padding mode '{padding_mode}'\")\n                     elif mode == 'bicubic':\n                         if padding_mode == 'zeros':\n                             if align_corners:\n@@ -9294,10 +9293,10 @@ def get_grid(device='cpu', data=None):\n                                     [[2.7993753, 6.6050020, 4.25, 4.7138715, 10.269531],\n                                      [0.8125000, 7.2822485, 4.25, 5.0000052, 9.332031]], device=\"mps\").view(1, 1, 2, 5)\n                         else:\n-                            raise AssertionError(\"missing groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing groundtruth test for padding mode '{padding_mode}'\")\n \n                     else:\n-                        raise AssertionError(\"missing groundtruth test for interpolation mode '{}'\".format(mode))\n+                        raise AssertionError(f\"missing groundtruth test for interpolation mode '{mode}'\")\n                     output = F.grid_sample(input, grid, mode=mode, padding_mode=padding_mode,\n                                            align_corners=align_corners)\n                     self.assertEqual(output, groundtruth, atol=1e-5, rtol=0,\ndiff --git a/test/test_namedtensor.py b/test/test_namedtensor.py\nindex 751a56f168e78b..572a2c290c3a48 100644\n--- a/test/test_namedtensor.py\n+++ b/test/test_namedtensor.py\n@@ -328,7 +328,7 @@ def test_no_multiprocessing_support(self):\n     def test_big_tensor_repr_has_names(self):\n         def check_repr(named_tensor):\n             unnamed_tensor = named_tensor.rename(None)\n-            names_tag = 'names={}'.format(named_tensor.names)\n+            names_tag = f'names={named_tensor.names}'\n             self.assertIn(names_tag, repr(named_tensor))\n \n         check_repr(torch.randn(128, 3, 64, 64, names=('N', 'C', 'H', 'W')))\n@@ -854,7 +854,7 @@ def _test(testcase, names=('N', 'D'), device='cpu'):\n                 out = testcase.lambd(tensor)\n             except RuntimeError as err:\n                 # Get a better error message by catching the error and asserting.\n-                raise RuntimeError('{}: {}'.format(testcase.name, err)) from err\n+                raise RuntimeError(f'{testcase.name}: {err}') from err\n             self.assertEqual(out.names, tensor.names,\n                              msg=testcase.name)\n \ndiff --git a/test/test_namedtuple_return_api.py b/test/test_namedtuple_return_api.py\nindex 69e1a053268880..c4fee116d9c8fd 100644\n--- a/test/test_namedtuple_return_api.py\n+++ b/test/test_namedtuple_return_api.py\n@@ -39,7 +39,7 @@ def test_import_return_types(self):\n     def test_native_functions_yaml(self):\n         operators_found = set()\n         regex = re.compile(r\"^(\\w*)(\\(|\\.)\")\n-        with open(aten_native_yaml, 'r') as file:\n+        with open(aten_native_yaml) as file:\n             for f in yaml.safe_load(file.read()):\n                 f = f['func']\n                 ret = f.split('->')[1].strip()\ndiff --git a/test/test_nestedtensor.py b/test/test_nestedtensor.py\nindex 6aa8ba67f787b4..6582a49337876f 100644\n--- a/test/test_nestedtensor.py\n+++ b/test/test_nestedtensor.py\n@@ -403,7 +403,7 @@ def test_copy_behavior(t, non_blocking=False):\n             devices = [t.device]\n             if t.device.type == 'cuda':\n                 if t.device.index == -1:\n-                    devices.append('cuda:{}'.format(torch.cuda.current_device()))\n+                    devices.append(f'cuda:{torch.cuda.current_device()}')\n                 elif t.device.index == torch.cuda.current_device():\n                     devices.append('cuda')\n             for device in devices:\ndiff --git a/test/test_nn.py b/test/test_nn.py\nindex 91eea01c43a748..e913efe9e0ba48 100644\n--- a/test/test_nn.py\n+++ b/test/test_nn.py\n@@ -5913,7 +5913,7 @@ def get_grid(device='cpu', data=None):\n                                     [[3.0000004768, 6.5000000000, 5.0000, 4.6675000191, 9.2500],\n                                      [1.0000000000, 7.1665000916, 5.0000, 5.0000000000, 9.2500]]).view(1, 1, 2, 5)\n                         else:\n-                            raise AssertionError(\"missing groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing groundtruth test for padding mode '{padding_mode}'\")\n                     elif mode == 'nearest':\n                         if padding_mode == 'zeros':\n                             if align_corners:\n@@ -5943,7 +5943,7 @@ def get_grid(device='cpu', data=None):\n                                     [[1., 8., 5., 7., 9.],\n                                      [1., 8., 5., 8., 9.]]).view(1, 1, 2, 5)\n                         else:\n-                            raise AssertionError(\"missing groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing groundtruth test for padding mode '{padding_mode}'\")\n                     elif mode == 'bicubic':\n                         if padding_mode == 'zeros':\n                             if align_corners:\n@@ -5973,10 +5973,10 @@ def get_grid(device='cpu', data=None):\n                                     [[2.7993753, 6.6050020, 4.25, 4.7138715, 10.269531],\n                                      [0.8125000, 7.2822485, 4.25, 5.0000052, 9.332031]]).view(1, 1, 2, 5)\n                         else:\n-                            raise AssertionError(\"missing groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing groundtruth test for padding mode '{padding_mode}'\")\n \n                     else:\n-                        raise AssertionError(\"missing groundtruth test for interpolation mode '{}'\".format(mode))\n+                        raise AssertionError(f\"missing groundtruth test for interpolation mode '{mode}'\")\n                     output = F.grid_sample(input, grid, mode=mode, padding_mode=padding_mode,\n                                            align_corners=align_corners)\n                     self.assertEqual(output, groundtruth, atol=1e-5, rtol=0,\n@@ -6025,7 +6025,7 @@ def get_grid(device='cpu', data=None):\n                                     [[[[-0., -0.], [-0., 0.], [-0., -0.], [-0., 0.]],\n                                       [[0., 0.], [0., 0.], [0., 0.], [0., 0.]]]]).view(1, 2, 4, 2)\n                         else:\n-                            raise AssertionError(\"missing gradient groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing gradient groundtruth test for padding mode '{padding_mode}'\")\n                     elif mode == 'nearest':\n                         groundtruth = torch.tensor(\n                             [[[[-0., -0.], [-0., 0.], [-0., -0.], [-0., 0.]],\n@@ -6060,9 +6060,9 @@ def get_grid(device='cpu', data=None):\n                                     [[[[0., 0.], [0., 0.], [1.875, 0.], [1.875, 0.]],\n                                       [[0., 0.], [0., 0.], [1.875, 0.], [1.875, 0.]]]]).view(1, 2, 4, 2)\n                         else:\n-                            raise AssertionError(\"missing gradient groundtruth test for padding mode '{}'\".format(padding_mode))\n+                            raise AssertionError(f\"missing gradient groundtruth test for padding mode '{padding_mode}'\")\n                     else:\n-                        raise AssertionError(\"missing gradient groundtruth test for interpolation mode '{}'\".format(mode))\n+                        raise AssertionError(f\"missing gradient groundtruth test for interpolation mode '{mode}'\")\n                     for input_requires_grad in [False, True]:\n                         input = input.requires_grad_(input_requires_grad)\n                         F.grid_sample(input, grid, mode=mode, padding_mode=padding_mode,\n@@ -7839,7 +7839,7 @@ def _buildEquivalentAffineTransforms2d(device, input_size, output_size, angle_ra\n         outtrans_ary)\n     grid_ary = np.dot(np.dot(np.dot(reorder_ary, rotation_ary.T), outscale_ary), outtrans_ary)\n \n-    transform_tensor = torch.from_numpy((rotation_ary)).to(device, torch.float32)\n+    transform_tensor = torch.from_numpy(rotation_ary).to(device, torch.float32)\n     transform_tensor = transform_tensor[:2].unsqueeze(0)\n \n     return transform_tensor, transform_ary, grid_ary\n@@ -7912,7 +7912,7 @@ def _buildEquivalentAffineTransforms3d(device, input_size, output_size, angle_ra\n         outtrans_ary)\n     grid_ary = np.dot(np.dot(np.dot(reorder_ary, np.linalg.inv(scipyRotation_ary)), outscale_ary), outtrans_ary)\n \n-    transform_tensor = torch.from_numpy((torchRotation_ary)).to(device, torch.float32)\n+    transform_tensor = torch.from_numpy(torchRotation_ary).to(device, torch.float32)\n     transform_tensor = transform_tensor[:3].unsqueeze(0)\n \n     return transform_tensor, transform_ary, grid_ary\n@@ -10554,7 +10554,7 @@ def helper(shape_in, shape_out, align_corners):\n                 out_double = F.grid_sample(data.double(), grid.double(), mode=mode, padding_mode='zeros',\n                                            align_corners=align_corners)\n \n-                self.assertEqual(out_half, out_double.half(), msg=\"grid_sample with mode = {} doesn't match\".format(mode))\n+                self.assertEqual(out_half, out_double.half(), msg=f\"grid_sample with mode = {mode} doesn't match\")\n \n         helper((32, 64, 16, 16), (32, 8, 8, 2), True)\n         helper((32, 64, 16, 16, 16), (32, 8, 8, 8, 3), True)\ndiff --git a/test/test_nnapi.py b/test/test_nnapi.py\nindex ebc066dd8ebd11..df4bc06f7fb152 100644\n--- a/test/test_nnapi.py\n+++ b/test/test_nnapi.py\n@@ -508,7 +508,7 @@ def test_conv2d(self):\n         for kind in [\"float\", \"float-nhwc\", \"quant\", \"quant-nhwc\"]:\n             for case in cases:\n                 in_ch, out_ch, kernel, stride, padding, groups, bias, input_dim, name = case\n-                with self.subTest(\"{}-{}\".format(kind, name)):\n+                with self.subTest(f\"{kind}-{name}\"):\n                     inp = torch.randn(input_dim)\n                     model = torch.nn.Conv2d(in_ch, out_ch, kernel, stride, padding, groups=groups, bias=bool(bias))\n                     output_size = model(inp).numel()\ndiff --git a/test/test_openmp.py b/test/test_openmp.py\nindex f35ba4b1b09125..48c4d40205e7a7 100644\n--- a/test/test_openmp.py\n+++ b/test/test_openmp.py\n@@ -51,7 +51,7 @@ def func_rss(self, runs):\n                 continue\n             is_increasing = is_increasing and (last_rss[idx] > last_rss[idx - 1])\n         self.assertTrue(not is_increasing,\n-                        msg='memory usage is increasing, {}'.format(str(last_rss)))\n+                        msg=f'memory usage is increasing, {str(last_rss)}')\n \n     def test_one_thread(self):\n         \"\"\"Make sure there is no memory leak with one thread: issue gh-32284\ndiff --git a/test/test_ops.py b/test/test_ops.py\nindex 6daa9aa8b73855..440cb8f942cd02 100644\n--- a/test/test_ops.py\n+++ b/test/test_ops.py\n@@ -157,7 +157,7 @@ def test_multiple_devices(self, devices, dtype, op):\n             if isinstance(result, torch.Tensor):\n                 self.assertTrue(result.device == cuda_device)\n             elif is_iterable_of_tensors(result):\n-                self.assertTrue(all((t.device == cuda_device for t in result)))\n+                self.assertTrue(all(t.device == cuda_device for t in result))\n             else:\n                 self.skipTest(\n                     \"Skipped! Only supports single tensor or iterable of tensor outputs.\"\n@@ -219,7 +219,7 @@ def get_opoverloadpacket_from_dispatch(kernel):\n             self.assertTrue(False)\n \n         for file_name in files:\n-            with open(os.path.join(pytorch_dir, file_name), \"r\") as f:\n+            with open(os.path.join(pytorch_dir, file_name)) as f:\n                 lines = f.read()\n                 matches = regex.findall(lines)\n                 for match in matches:\n@@ -723,7 +723,7 @@ def _extract_strides(out):\n                     return (out.stride(),)\n \n                 # assumes (see above) that out is an iterable of tensors\n-                return tuple((t.stride() for t in out))\n+                return tuple(t.stride() for t in out)\n \n             # Extracts data pointers from a tensor or iterable of tensors into a tuple\n             # NOTE: only extracts on the CPU and CUDA device types since some\n@@ -736,7 +736,7 @@ def _extract_data_ptrs(out):\n                     return (out.data_ptr(),)\n \n                 # assumes (see above) that out is an iterable of tensors\n-                return tuple((t.data_ptr() for t in out))\n+                return tuple(t.data_ptr() for t in out)\n \n             @suppress_warnings\n             def _compare_out(transform, *, compare_strides_and_data_ptrs=True):\n@@ -751,7 +751,7 @@ def _compare_out(transform, *, compare_strides_and_data_ptrs=True):\n                 self.assertEqual(expected, out)\n \n                 if compare_strides_and_data_ptrs:\n-                    stride_msg = \"Strides are not the same! Original strides were {0} and strides are now {1}\".format(\n+                    stride_msg = \"Strides are not the same! Original strides were {} and strides are now {}\".format(\n                         original_strides, final_strides\n                     )\n                     self.assertEqual(original_strides, final_strides, msg=stride_msg)\n@@ -843,7 +843,7 @@ def _extract_strides(out):\n                     return (out.stride(),)\n \n                 # assumes (see above) that out is an iterable of tensors\n-                return tuple((t.stride() for t in out))\n+                return tuple(t.stride() for t in out)\n \n             # Extracts data pointers from a tensor or iterable of tensors into a tuple\n             # NOTE: only extracts on the CPU and CUDA device types since some\n@@ -856,7 +856,7 @@ def _extract_data_ptrs(out):\n                     return (out.data_ptr(),)\n \n                 # assumes (see above) that out is an iterable of tensors\n-                return tuple((t.data_ptr() for t in out))\n+                return tuple(t.data_ptr() for t in out)\n \n             def _compare_out(transform, *, compare_strides_and_data_ptrs=True):\n                 out = _apply_out_transform(transform, expected)\n@@ -869,7 +869,7 @@ def _compare_out(transform, *, compare_strides_and_data_ptrs=True):\n                 self.assertEqual(expected, out)\n \n                 if compare_strides_and_data_ptrs:\n-                    stride_msg = \"Strides are not the same! Original strides were {0} and strides are now {1}\".format(\n+                    stride_msg = \"Strides are not the same! Original strides were {} and strides are now {}\".format(\n                         original_strides, final_strides\n                     )\n                     self.assertEqual(original_strides, final_strides, msg=stride_msg)\n@@ -1390,20 +1390,20 @@ def _tensor_requires_grad(x):\n \n         # Partially supporting a dtype is not an error, but we print a warning\n         if (len(partially_supported_forward) + len(partially_supported_backward)) > 0:\n-            msg = \"Some dtypes for {0} on device type {1} are only partially supported!\\n\".format(\n+            msg = \"Some dtypes for {} on device type {} are only partially supported!\\n\".format(\n                 op.name, device_type\n             )\n             if len(partially_supported_forward) > 0:\n                 msg = (\n                     msg\n-                    + \"The following dtypes only worked on some samples during forward: {0}.\\n\".format(\n+                    + \"The following dtypes only worked on some samples during forward: {}.\\n\".format(\n                         partially_supported_forward\n                     )\n                 )\n             if len(partially_supported_backward) > 0:\n                 msg = (\n                     msg\n-                    + \"The following dtypes only worked on some samples during backward: {0}.\\n\".format(\n+                    + \"The following dtypes only worked on some samples during backward: {}.\\n\".format(\n                         partially_supported_backward\n                     )\n                 )\n@@ -1426,34 +1426,34 @@ def _tensor_requires_grad(x):\n                 return\n \n         # Generates error msg\n-        msg = \"The supported dtypes for {0} on device type {1} are incorrect!\\n\".format(\n+        msg = \"The supported dtypes for {} on device type {} are incorrect!\\n\".format(\n             op.name, device_type\n         )\n         if len(supported_but_unclaimed_forward) > 0:\n             msg = (\n                 msg\n-                + \"The following dtypes worked in forward but are not listed by the OpInfo: {0}.\\n\".format(\n+                + \"The following dtypes worked in forward but are not listed by the OpInfo: {}.\\n\".format(\n                     supported_but_unclaimed_forward\n                 )\n             )\n         if len(supported_but_unclaimed_backward) > 0:\n             msg = (\n                 msg\n-                + \"The following dtypes worked in backward but are not listed by the OpInfo: {0}.\\n\".format(\n+                + \"The following dtypes worked in backward but are not listed by the OpInfo: {}.\\n\".format(\n                     supported_but_unclaimed_backward\n                 )\n             )\n         if len(claimed_but_unsupported_forward) > 0:\n             msg = (\n                 msg\n-                + \"The following dtypes did not work in forward but are listed by the OpInfo: {0}.\\n\".format(\n+                + \"The following dtypes did not work in forward but are listed by the OpInfo: {}.\\n\".format(\n                     claimed_but_unsupported_forward\n                 )\n             )\n         if len(claimed_but_unsupported_backward) > 0:\n             msg = (\n                 msg\n-                + \"The following dtypes did not work in backward but are listed by the OpInfo: {0}.\\n\".format(\n+                + \"The following dtypes did not work in backward but are listed by the OpInfo: {}.\\n\".format(\n                     claimed_but_unsupported_backward\n                 )\n             )\ndiff --git a/test/test_ops_jit.py b/test/test_ops_jit.py\nindex 03996e95bb7f06..c01f6d36f3a501 100644\n--- a/test/test_ops_jit.py\n+++ b/test/test_ops_jit.py\n@@ -190,7 +190,7 @@ def get_sample():\n     _alias_ops = partial(ops, dtypes=OpDTypes.supported,\n                          allowed_dtypes=(torch.float,))\n \n-    @_alias_ops((op for op in op_db if op.aliases))\n+    @_alias_ops(op for op in op_db if op.aliases)\n     def test_jit_alias_remapping(self, device, dtype, op):\n         # NOTE: only tests on first sample\n         samples = op.sample_inputs(device, dtype, requires_grad=True)\ndiff --git a/test/test_overrides.py b/test/test_overrides.py\nindex 81add26faacee9..de60792a0db2f1 100644\n--- a/test/test_overrides.py\n+++ b/test/test_overrides.py\n@@ -129,7 +129,7 @@ def __init__(self, N, value):\n         self._i = value\n \n     def __repr__(self):\n-        return \"DiagonalTensor(N={}, value={})\".format(self._N, self._i)\n+        return f\"DiagonalTensor(N={self._N}, value={self._i})\"\n \n     def __array__(self):\n         return self._i * np.eye(self._N)\n@@ -271,7 +271,7 @@ class SubDiagonalTensor(DiagonalTensor):\n     handled_functions = HANDLED_FUNCTIONS_SUB_DIAGONAL\n \n     def __repr__(self):\n-        return \"SubDiagonalTensor(N={}, value={})\".format(self._N, self._i)\n+        return f\"SubDiagonalTensor(N={self._N}, value={self._i})\"\n \n \n @implements_sub_diagonal(torch.mean)\n@@ -336,7 +336,7 @@ def generate_tensor_like_torch_implementations():\n     for namespace, funcs in get_overridable_functions().items():\n         for func in funcs:\n             if func not in testing_overrides and func.__name__ not in testing_ignore:\n-                untested_funcs.append(\"{}.{}\".format(namespace, func.__name__))\n+                untested_funcs.append(f\"{namespace}.{func.__name__}\")\n     msg = (\n         \"The following functions are not tested for __torch_function__ \"\n         \"support, please ensure there is an entry in the dict returned by \"\n@@ -750,7 +750,7 @@ def test(self):\n         if module:\n             name = 'test_{}_{}'.format(module.replace('.', '_'), func.__name__)\n         else:\n-            name = 'test_{}'.format(func.__name__)\n+            name = f'test_{func.__name__}'\n         test_method.__name__ = name\n         setattr(cls, name, test_method)\n \n@@ -1094,7 +1094,7 @@ def test_rnn(self):\n class TestDisabledTorchFunction(TestCase):\n     # Regression test for gh-64687\n     def test_parameter_does_not_prevent_dispatch(self):\n-        class MyTensor():\n+        class MyTensor:\n             @classmethod\n             def __torch_function__(cls, func, types, args=(), kwargs=None):\n                 return \"called\"\n@@ -1119,7 +1119,7 @@ def test_resolve_name(self):\n \n class TestTorchFunctionWarning(TestCase):\n     def test_warn_on_invalid_torch_function(self):\n-        class Bad1():\n+        class Bad1:\n             def __torch_function__(self, *args, **kwargs):\n                 pass\n \ndiff --git a/test/test_public_bindings.py b/test/test_public_bindings.py\nindex e56927e6c10e7b..2a3e7ef9dc1bfc 100644\n--- a/test/test_public_bindings.py\n+++ b/test/test_public_bindings.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: autograd\"]\n \n from torch.testing._internal.common_utils import TestCase, run_tests, IS_JETSON, IS_WINDOWS\ndiff --git a/test/test_python_dispatch.py b/test/test_python_dispatch.py\nindex 77aa663bf09f11..a06a35a92f2fe1 100644\n--- a/test/test_python_dispatch.py\n+++ b/test/test_python_dispatch.py\n@@ -417,7 +417,7 @@ def sqsum(a: SymInt, b: SymInt):\n \n         out = getattr(torch.ops, self.test_ns).sqsum.default(s0, s1)\n         out_val = shape_env.evaluate_expr(out.node.expr)\n-        self.assertEquals(out_val, 13)\n+        self.assertEqual(out_val, 13)\n \n     def test_register_functional_op_error_cases(self):\n         lib = Library(self.test_ns, \"FRAGMENT\")\ndiff --git a/test/test_quantization.py b/test/test_quantization.py\nindex d9d0d68abdc4e3..70363667726ce4 100644\n--- a/test/test_quantization.py\n+++ b/test/test_quantization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: quantization\"]\n \n import logging\ndiff --git a/test/test_reductions.py b/test/test_reductions.py\nindex 0b196b674cd0a3..650cb02ac3a000 100644\n--- a/test/test_reductions.py\n+++ b/test/test_reductions.py\n@@ -1968,9 +1968,9 @@ def test_min_max_nan(self, device):\n             a[2, 2] = nan\n             actual = f(a.to(device)).cpu()\n             expected = f(a).cpu()\n-            self.assertEqual(torch.isnan(actual), torch.isnan(expected), msg='nans for {}'.format(name))\n+            self.assertEqual(torch.isnan(actual), torch.isnan(expected), msg=f'nans for {name}')\n             self.assertEqual(actual[~torch.isnan(actual)],\n-                             expected[~torch.isnan(expected)], msg='nans for {}'.format(name))\n+                             expected[~torch.isnan(expected)], msg=f'nans for {name}')\n \n     # TODO: make this test generic using OpInfos\n     @onlyCUDA\n@@ -2199,16 +2199,16 @@ def test_multidim(x, dim):\n                 fn_tuple(y, 1, keepdim=False, out=(values[:, 1], indices[:, 1]))\n                 values_expected, indices_expected = fn_tuple(y, 1, keepdim=False)\n                 self.assertEqual(values[:, 1], values_expected,\n-                                 msg='{} values with out= kwarg'.format(fn_name))\n+                                 msg=f'{fn_name} values with out= kwarg')\n                 self.assertEqual(indices[:, 1], indices_expected,\n-                                 msg='{} indices with out= kwarg'.format(fn_name))\n+                                 msg=f'{fn_name} indices with out= kwarg')\n                 continue\n \n             x = torch.randn(5, 3, device=device)\n             y = torch.randn(5, 3, device=device)\n             fn(y, 1, keepdim=False, out=x[:, 1])\n             expected = fn(y, 1, keepdim=False)\n-            self.assertEqual(x[:, 1], expected, msg='{} with out= kwarg'.format(fn_name))\n+            self.assertEqual(x[:, 1], expected, msg=f'{fn_name} with out= kwarg')\n \n     @onlyCUDA\n     @largeTensorTest('10GB')\n@@ -3498,8 +3498,8 @@ def to_numpy(input):\n             expected = np.asarray(expected)  # transform numpy scalars to numpy.ndarray instances\n \n             msg = (\"Failed to produce expected results! Input tensor was\"\n-                   \" {0}, torch result is {1}, and reference result is\"\n-                   \" {2}.\").format(t, actual, expected) if t.numel() < 10 else None\n+                   \" {}, torch result is {}, and reference result is\"\n+                   \" {}.\").format(t, actual, expected) if t.numel() < 10 else None\n \n             self.assertEqual(actual, expected, msg, exact_dtype=exact_dtype)\n \ndiff --git a/test/test_scatter_gather_ops.py b/test/test_scatter_gather_ops.py\nindex 8b0146b5a1aba4..3351b9d257cdf6 100644\n--- a/test/test_scatter_gather_ops.py\n+++ b/test/test_scatter_gather_ops.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: scatter & gather ops\"]\n \n import random\ndiff --git a/test/test_schema_check.py b/test/test_schema_check.py\nindex 35587963817132..177493d4a09ec5 100644\n--- a/test/test_schema_check.py\n+++ b/test/test_schema_check.py\n@@ -258,7 +258,7 @@ def test_schema_check_mode_functionality_wildcard_after(self):\n     @unittest.skipIf(not torch._C.has_spectral, \"ATen not built with FFT.\")\n     def test_schema_check_mode_functionality_kwarg_tensor(self):\n         x = torch.rand((3, 5))\n-        w = torch.rand((4))\n+        w = torch.rand(4)\n         expected = torch.stft(x, 4, win_length=4, window=w, return_complex=True)\n         with SchemaCheckMode():\n             actual = torch.stft(x, 4, win_length=4, window=w, return_complex=True)\ndiff --git a/test/test_segment_reductions.py b/test/test_segment_reductions.py\nindex c6ca62cdd4561b..cbf36ca3b25886 100644\n--- a/test/test_segment_reductions.py\n+++ b/test/test_segment_reductions.py\n@@ -183,7 +183,7 @@ def test_simple_1d(self, device, dtypes):\n     def test_simple_zero_length(self, device, dtypes):\n         val_dtype, length_type = dtypes\n         lengths = [0, 0]\n-        data = torch.ones((0))\n+        data = torch.ones(0)\n \n         for reduction in reductions:\n             for initial in [0, None]:\ndiff --git a/test/test_serialization.py b/test/test_serialization.py\nindex 8f927ad1a51704..76739ed5c62493 100644\n--- a/test/test_serialization.py\n+++ b/test/test_serialization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: serialization\"]\n \n import torch\n@@ -508,7 +507,7 @@ def load_bytes():\n             torch.device('cuda', 0)\n         ]\n         gpu_last_map_locations = [\n-            'cuda:{}'.format(torch.cuda.device_count() - 1),\n+            f'cuda:{torch.cuda.device_count() - 1}',\n         ]\n \n         def check_map_locations(map_locations, tensor_class, intended_device):\n@@ -815,7 +814,7 @@ class TestOldSerialization(TestCase, SerializationMixin):\n     # the warning module is the same, it is not raised again.\n     def _test_serialization_container(self, unique_key, filecontext_lambda):\n \n-        tmpmodule_name = 'tmpmodule{}'.format(unique_key)\n+        tmpmodule_name = f'tmpmodule{unique_key}'\n \n         def import_module(name, filename):\n             import importlib.util\ndiff --git a/test/test_sort_and_select.py b/test/test_sort_and_select.py\nindex a4ac2f1b70dd91..41f8ee9183077e 100644\n--- a/test/test_sort_and_select.py\n+++ b/test/test_sort_and_select.py\n@@ -33,12 +33,12 @@ def check_order(a, b):\n                 # see above\n                 return ((b != b) | (a <= b)).all().item()\n         else:\n-            error('unknown order \"{}\", must be \"ascending\" or \"descending\"'.format(order))\n+            error(f'unknown order \"{order}\", must be \"ascending\" or \"descending\"')\n \n         are_ordered = True\n         for k in range(1, SIZE):\n             self.assertTrue(check_order(mxx[:, k - 1], mxx[:, k]),\n-                            'torch.sort ({}) values unordered for {}'.format(order, task))\n+                            f'torch.sort ({order}) values unordered for {task}')\n \n         seen = set()\n         indicesCorrect = True\n@@ -51,7 +51,7 @@ def check_order(a, b):\n             seen.clear()\n             for j in range(size):\n                 self.assertEqual(x[k][ixx[k][j]], mxx[k][j],\n-                                 msg='torch.sort ({}) indices wrong for {}'.format(order, task))\n+                                 msg=f'torch.sort ({order}) indices wrong for {task}')\n                 seen.add(ixx[k][j])\n             self.assertEqual(len(seen), size)\n \ndiff --git a/test/test_sparse.py b/test/test_sparse.py\nindex 6525786883992f..3fcad7aa48784a 100644\n--- a/test/test_sparse.py\n+++ b/test/test_sparse.py\n@@ -285,11 +285,11 @@ def _test_print(self, device, dtype, coalesced):\n         for shape, sparse_dim, nnz in shape_sparse_dim_nnz:\n             indices_shape = torch.Size((sparse_dim, nnz))\n             values_shape = torch.Size((nnz,) + shape[sparse_dim:])\n-            printed.append(\"# shape: {}\".format(torch.Size(shape)))\n-            printed.append(\"# nnz: {}\".format(nnz))\n-            printed.append(\"# sparse_dim: {}\".format(sparse_dim))\n-            printed.append(\"# indices shape: {}\".format(indices_shape))\n-            printed.append(\"# values shape: {}\".format(values_shape))\n+            printed.append(f\"# shape: {torch.Size(shape)}\")\n+            printed.append(f\"# nnz: {nnz}\")\n+            printed.append(f\"# sparse_dim: {sparse_dim}\")\n+            printed.append(f\"# indices shape: {indices_shape}\")\n+            printed.append(f\"# values shape: {values_shape}\")\n \n             indices = torch.arange(indices_shape.numel(), dtype=self.index_tensor(0).dtype,\n                                    device=device).view(indices_shape)\n@@ -308,7 +308,7 @@ def _test_print(self, device, dtype, coalesced):\n             else:\n                 dtypes.append(torch.double)\n             for dtype in dtypes:\n-                printed.append(\"########## {} ##########\".format(dtype))\n+                printed.append(f\"########## {dtype} ##########\")\n                 x = sp_tensor.detach().to(dtype)\n                 printed.append(\"# sparse tensor\")\n                 printed.append(str(x))\n@@ -3382,8 +3382,7 @@ def sparse_softmax(sparse, dim):\n                                                dtype=dtype, device=device)\n             else:\n                 raise ValueError(\n-                    '`dim(=%s)` must be smaller than `sparse_dim(=%s) + dense_dim(=%s)`'\n-                    % (dim, sparse.sparse_dim(), sparse.dense_dim()))\n+                    f'`dim(={dim})` must be smaller than `sparse_dim(={sparse.sparse_dim()}) + dense_dim(={sparse.dense_dim()})`')\n \n         def softmax_jacobian_analytic(x, dim):\n             \"\"\"Return Jacobian of softmax using analytic formula\ndiff --git a/test/test_sparse_csr.py b/test/test_sparse_csr.py\nindex c937c5cd8b3b1c..473d5261578dcb 100644\n--- a/test/test_sparse_csr.py\n+++ b/test/test_sparse_csr.py\n@@ -904,10 +904,10 @@ def test_select_copy(self, device, dtype, index_dtype, layout):\n \n         def is_view_of(base, other):\n             # a shameless copy of TestViewOps.is_view_of\n-            if ((not other._is_view() or\n+            if (not other._is_view() or\n                  other is base or\n                  other._base is not base or\n-                 base.device != other.device)):\n+                 base.device != other.device):\n                 return False\n             if base.device.type == 'cpu' or base.device.type == 'cuda':\n                 if base.untyped_storage().data_ptr() != other.untyped_storage().data_ptr():\n@@ -1815,8 +1815,8 @@ def test_empty_inputs(lhs_layout, rhs_layout):\n             yd = xd.transpose(-2, -1)\n             zd = torch.rand(0, 0, device=device, dtype=dtype)\n \n-            xls, yls, zls = [t.to_sparse(layout=lhs_layout) for t in (xd, yd, zd)]\n-            xrs, yrs, zrs = [t.to_sparse(layout=rhs_layout) for t in (xd, yd, zd)]\n+            xls, yls, zls = (t.to_sparse(layout=lhs_layout) for t in (xd, yd, zd))\n+            xrs, yrs, zrs = (t.to_sparse(layout=rhs_layout) for t in (xd, yd, zd))\n \n             for ls, rs, ld, rd in [(xls, yrs, xd, yd), (xls, zrs, xd, zd), (zls, yrs, zd, yd), (zls, zrs, zd, zd)]:\n                 res_sparse = ls @ rs\ndiff --git a/test/test_subclass.py b/test/test_subclass.py\nindex c3cf1d51018a66..b9c1e4f622e522 100644\n--- a/test/test_subclass.py\n+++ b/test/test_subclass.py\n@@ -228,7 +228,7 @@ class NonRewrappingTensor(torch.Tensor):\n             def __new__(\n                 cls, t: torch.Tensor\n             ):\n-                r = super(NonRewrappingTensor, cls)._make_wrapper_subclass(\n+                r = super()._make_wrapper_subclass(\n                     cls, t.shape, dtype=t.dtype, requires_grad=t.requires_grad, device=t.device)\n                 return r\n \ndiff --git a/test/test_sympy_utils.py b/test/test_sympy_utils.py\nindex 0a3466d56660fa..b469b4c3e2537c 100644\n--- a/test/test_sympy_utils.py\n+++ b/test/test_sympy_utils.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"oncall: pt2\"]\n \n import itertools\n@@ -229,12 +228,12 @@ def test_rational_bounds(self):\n         from sympy import floor, Eq\n         shape_0 = sympy.Symbol('shape_0', positive=True, integer=True)\n         new_expr = (\n-            Eq(30 * floor(4 * (((shape_0 + 1) // 96)) *\n-                          (((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646))) / 647 +\n-                          2584 * (((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646))) / 647),\n-               2880 * floor((((shape_0 + 1) // 96)) *\n-                            (((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646))) / 15528 +\n-                            323 * (((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646))) / 7764)))\n+            Eq(30 * floor(4 * ((shape_0 + 1) // 96) *\n+                          ((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646)) / 647 +\n+                          2584 * ((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646)) / 647),\n+               2880 * floor(((shape_0 + 1) // 96) *\n+                            ((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646)) / 15528 +\n+                            323 * ((shape_0 + 62017) // (((shape_0 + 1) // 96) + 646)) / 7764)))\n         new_range_env = {shape_0: ValueRanges(lower=1, upper=190)}\n         self.assertTrue(new_expr.subs({shape_0: 95}))\n         self.assertIn(True, sympy_interp(ValueRangeAnalysis, new_range_env, new_expr))\ndiff --git a/test/test_tensor_creation_ops.py b/test/test_tensor_creation_ops.py\nindex 76bc2ef6ff59f9..0c8cbca27a15f6 100644\n--- a/test/test_tensor_creation_ops.py\n+++ b/test/test_tensor_creation_ops.py\n@@ -170,21 +170,21 @@ def test_roll(self, device):\n \n         single_roll = numbers.roll(1, 0)\n         expected = torch.tensor([8, 1, 2, 3, 4, 5, 6, 7], device=device)\n-        self.assertEqual(single_roll, expected, msg=\"{} did not equal expected result\".format(single_roll))\n+        self.assertEqual(single_roll, expected, msg=f\"{single_roll} did not equal expected result\")\n \n         roll_backwards = numbers.roll(-2, 0)\n         expected = torch.tensor([3, 4, 5, 6, 7, 8, 1, 2], device=device)\n-        self.assertEqual(roll_backwards, expected, msg=\"{} did not equal expected result\".format(roll_backwards))\n+        self.assertEqual(roll_backwards, expected, msg=f\"{roll_backwards} did not equal expected result\")\n \n         data = numbers.view(2, 2, 2)\n         rolled = data.roll(1, 0)\n         expected = torch.tensor([5, 6, 7, 8, 1, 2, 3, 4], device=device).view(2, 2, 2)\n-        self.assertEqual(expected, rolled, msg=\"{} did not equal expected result: {}\".format(rolled, expected))\n+        self.assertEqual(expected, rolled, msg=f\"{rolled} did not equal expected result: {expected}\")\n \n         data = data.view(2, 4)\n         # roll a loop until back where started\n         loop_rolled = data.roll(2, 0).roll(4, 1)\n-        self.assertEqual(data, loop_rolled, msg=\"{} did not equal the original: {}\".format(loop_rolled, data))\n+        self.assertEqual(data, loop_rolled, msg=f\"{loop_rolled} did not equal the original: {data}\")\n         # multiple inverse loops\n         self.assertEqual(data, data.roll(-20, 0).roll(-40, 1))\n         self.assertEqual(torch.tensor([8, 1, 2, 3, 4, 5, 6, 7], device=device), numbers.roll(1, 0))\n@@ -196,7 +196,7 @@ def test_roll(self, device):\n         expected = torch.tensor([4, 8, 1, 5, 2, 6, 3, 7]).view(4, 2)\n         rolled = strided.roll(1, 0)\n         self.assertEqual(expected, rolled,\n-                         msg=\"non contiguous tensor rolled to {} instead of {} \".format(rolled, expected))\n+                         msg=f\"non contiguous tensor rolled to {rolled} instead of {expected} \")\n \n         # test roll with no dimension specified\n         expected = numbers.roll(1, 0).view(2, 4)\n@@ -207,7 +207,7 @@ def test_roll(self, device):\n         expected = torch.tensor([[7, 8, 5, 6], [3, 4, 1, 2]], device=device)\n         double_rolled = data.roll(shifts=(2, -1), dims=(1, 0))\n         self.assertEqual(double_rolled, expected,\n-                         msg=\"should be able to roll over two dimensions, got {}\".format(double_rolled))\n+                         msg=f\"should be able to roll over two dimensions, got {double_rolled}\")\n \n         self.assertRaisesRegex(RuntimeError, \"required\", lambda: data.roll(shifts=(), dims=()))\n         self.assertRaisesRegex(RuntimeError, \"required\", lambda: data.roll(shifts=(), dims=1))\n@@ -812,7 +812,7 @@ def _test_special_stacks(self, dim, at_least_dim, torch_fn, np_fn, device, dtype\n             torch_fn(t)\n         # Test error for a single array\n         with self.assertRaisesRegex(TypeError, \"must be tuple of Tensors, not Tensor\"):\n-            torch_fn((t))\n+            torch_fn(t)\n \n         # Test 0-D\n         num_tensors = random.randint(1, 5)\ndiff --git a/test/test_tensorboard.py b/test/test_tensorboard.py\nindex 032cdae0b43c55..82192aab6ce1dc 100644\n--- a/test/test_tensorboard.py\n+++ b/test/test_tensorboard.py\n@@ -521,7 +521,7 @@ def get_expected_file(function_ptr):\n def read_expected_content(function_ptr):\n     expected_file = get_expected_file(function_ptr)\n     assert os.path.exists(expected_file), expected_file\n-    with open(expected_file, \"r\") as f:\n+    with open(expected_file) as f:\n         return f.read()\n \n def compare_image_proto(actual_proto, function_ptr):\ndiff --git a/test/test_tensorexpr.py b/test/test_tensorexpr.py\nindex 74868ae63a0120..a33c3afbf48d55 100644\n--- a/test/test_tensorexpr.py\n+++ b/test/test_tensorexpr.py\n@@ -591,7 +591,7 @@ def test(x, y, z):\n \n         xs = [(torch.rand(4) * 3 + 1).to(torch.int32) for i in range(3)]\n         x, y, z = xs\n-        xn, yn, zn = [t.numpy() for t in xs]\n+        xn, yn, zn = (t.numpy() for t in xs)\n         traced = torch.jit.trace(test, (x, y, z))\n         res = warmup_and_run_forward(traced, x, y, z)\n         self.assertLastGraphAllFused()\n@@ -1205,7 +1205,7 @@ def test_int(x: torch.Tensor, y: torch.Tensor, z: torch.Tensor, a: int, b: int)\n \n         for test in (test_float, test_int):\n             for data_type in self.dtypes:\n-                x, y, z = [torch.rand(4, dtype=data_type) for i in range(3)]\n+                x, y, z = (torch.rand(4, dtype=data_type) for i in range(3))\n                 a, b = 1, 2\n                 test(x, y, z, a, b)\n                 r = test(x, y, z, a, b)\n@@ -1379,7 +1379,7 @@ def test_dynamic_shape(self):\n             @torch.jit.script\n             def test(x, y, z):\n                 return x * y * z\n-            x, y, z = [torch.rand(4, 8).cuda() for _ in range(3)]\n+            x, y, z = (torch.rand(4, 8).cuda() for _ in range(3))\n             ref = test(x, y, z)\n             _ = test(*[torch.rand(6, 8).cuda() for _ in range(3)])\n             res = test(x, y, z)\n@@ -1390,7 +1390,7 @@ def test(x, y, z):\n             y = torch.rand(1, 8).cuda()\n             z = torch.rand(4, 1).cuda()\n             res = test(x, y, z)\n-            xn, yn, zn = [t.cpu().numpy() for t in (x, y, z)]\n+            xn, yn, zn = (t.cpu().numpy() for t in (x, y, z))\n             np.testing.assert_allclose(res.cpu().numpy(), xn * yn * zn)\n \n             # Mismatched shapes shouldn't reach codegen.\ndiff --git a/test/test_tensorexpr_pybind.py b/test/test_tensorexpr_pybind.py\nindex 486858d310a379..2c3ae7e78805c8 100644\n--- a/test/test_tensorexpr_pybind.py\n+++ b/test/test_tensorexpr_pybind.py\n@@ -392,9 +392,9 @@ def f(a):\n \n     @unittest.skipIf(not LLVM_ENABLED, \"LLVM backend not enabled\")\n     def test_alloc_in_loop(self):\n-        a, tmp, b = [\n+        a, tmp, b = (\n             te.BufHandle(name, [1], torch.float32) for name in [\"a\", \"tmp\", \"b\"]\n-        ]\n+        )\n         body = te.Block([tmp.store([0], a.load([0])), b.store([0], tmp.load([0]))])\n         for _ in range(4):\n             i = te.VarHandle(\"i\", torch.int32)\n@@ -402,7 +402,7 @@ def test_alloc_in_loop(self):\n         nest = te.LoopNest(body, [b])\n         nest.prepare_for_codegen()\n         f = te.construct_codegen(\"llvm\", nest.simplify(), [a, b])\n-        ta, tb = [torch.ones(1) for _ in range(2)]\n+        ta, tb = (torch.ones(1) for _ in range(2))\n         f.call([ta.data_ptr(), tb.data_ptr()])\n \n \ndiff --git a/test/test_testing.py b/test/test_testing.py\nindex bc6aca1d515733..451e31886c46a6 100644\n--- a/test/test_testing.py\n+++ b/test/test_testing.py\n@@ -1483,7 +1483,7 @@ def test_low_high_default_smoke(self, dtype, device):\n     @parametrize(\"value_types\", list(itertools.product([int, float], repeat=2)))\n     @supported_dtypes\n     def test_low_ge_high(self, dtype, device, low_high, value_types):\n-        low, high = [value_type(value) for value, value_type in zip(low_high, value_types)]\n+        low, high = (value_type(value) for value, value_type in zip(low_high, value_types))\n \n         if low == high and (dtype.is_floating_point or dtype.is_complex):\n             with self.assertWarnsRegex(\n@@ -1561,7 +1561,7 @@ def test_low_high_boolean_integral2(self, dtype, device):\n \n def _get_test_names_for_test_class(test_cls):\n     \"\"\" Convenience function to get all test names for a given test class. \"\"\"\n-    test_names = ['{}.{}'.format(test_cls.__name__, key) for key in test_cls.__dict__\n+    test_names = [f'{test_cls.__name__}.{key}' for key in test_cls.__dict__\n                   if key.startswith('test_')]\n     return sorted(test_names)\n \n@@ -1612,7 +1612,7 @@ def test_custom_names(self, bias):\n             def test_three_things_composition_custom_names(self, x, y, z):\n                 pass\n \n-            @parametrize(\"x,y\", [(1, 2), (1, 3), (1, 4)], name_fn=lambda x, y: '{}__{}'.format(x, y))\n+            @parametrize(\"x,y\", [(1, 2), (1, 3), (1, 4)], name_fn=lambda x, y: f'{x}__{y}')\n             def test_two_things_custom_names_alternate(self, x, y):\n                 pass\n \n@@ -1767,7 +1767,7 @@ def test_device_dtype_specific(self, device, dtype):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = [name.format(device_cls.__name__, device) for name in (\n             '{}.test_device_dtype_specific_{}_float32',\n             '{}.test_device_dtype_specific_{}_float64',\n@@ -1791,7 +1791,7 @@ def test_bar(self, device):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = [name.format(device_cls.__name__, device) for name in (\n             '{}.test_bar_{}',\n             '{}.test_foo_{}')\n@@ -1834,7 +1834,7 @@ def test_two_things_default_names(self, device, x, y):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = [name.format(device_cls.__name__, device) for name in (\n             '{}.test_default_names_x_0_{}',\n             '{}.test_default_names_x_1_{}',\n@@ -1862,13 +1862,13 @@ def test_custom_names(self, device, bias):\n             def test_three_things_composition_custom_names(self, device, x, y, z):\n                 pass\n \n-            @parametrize(\"x,y\", [(1, 2), (1, 3), (1, 4)], name_fn=lambda x, y: '{}__{}'.format(x, y))\n+            @parametrize(\"x,y\", [(1, 2), (1, 3), (1, 4)], name_fn=lambda x, y: f'{x}__{y}')\n             def test_two_things_custom_names_alternate(self, device, x, y):\n                 pass\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = [name.format(device_cls.__name__, device) for name in (\n             '{}.test_custom_names_bias_{}',\n             '{}.test_custom_names_no_bias_{}',\n@@ -1904,7 +1904,7 @@ def test_two_things_custom_names(self, device, x, y):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = [name.format(device_cls.__name__, device) for name in (\n             '{}.test_custom_names_bias_{}',\n             '{}.test_custom_names_no_bias_{}',\n@@ -1926,7 +1926,7 @@ def test_op_parametrized(self, device, dtype, op, flag):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = []\n         for op in op_db:\n             for dtype in op.supported_dtypes(torch.device(device).type):\n@@ -1949,7 +1949,7 @@ def test_module_parametrized(self, device, dtype, module_info, training, flag):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = []\n         for module_info in module_db:\n             for dtype in module_info.dtypes:\n@@ -2003,7 +2003,7 @@ def test_other(self, device, dtype, op, y):\n \n         device = self.device_type\n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n \n         for test_func, name in _get_test_funcs_for_test_class(device_cls):\n             should_apply = (name == 'test_op_param_test_op_x_2_cpu_float64' or\n@@ -2050,7 +2050,7 @@ def test_other(self, device, dtype, module_info, training, y):\n \n         device = self.device_type\n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n \n         for test_func, name in _get_test_funcs_for_test_class(device_cls):\n             should_apply = (name == 'test_module_param_TestModule_x_2_cpu_float64' or\n@@ -2071,7 +2071,7 @@ def test_parametrized(self, x, dtype):\n \n         instantiate_device_type_tests(TestParametrized, locals(), only_for=device)\n \n-        device_cls = locals()['TestParametrized{}'.format(device.upper())]\n+        device_cls = locals()[f'TestParametrized{device.upper()}']\n         expected_test_names = [name.format(device_cls.__name__, device) for name in (\n             '{}.test_parametrized_x_0_{}_float32',\n             '{}.test_parametrized_x_0_{}_float64',\n@@ -2226,7 +2226,7 @@ def test_no_mutate_global_logging_on_import(self, path) -> None:\n \n class TestOpInfos(TestCase):\n     def test_sample_input(self) -> None:\n-        a, b, c, d, e = [object() for _ in range(5)]\n+        a, b, c, d, e = (object() for _ in range(5))\n \n         # Construction with natural syntax\n         s = SampleInput(a, b, c, d=d, e=e)\n@@ -2270,7 +2270,7 @@ def test_sample_input(self) -> None:\n         assert s.broadcasts_input\n \n     def test_sample_input_metadata(self) -> None:\n-        a, b = [object() for _ in range(2)]\n+        a, b = (object() for _ in range(2))\n         s1 = SampleInput(a, b=b)\n         self.assertIs(s1.output_process_fn_grad(None), None)\n         self.assertFalse(s1.broadcasts_input)\ndiff --git a/test/test_torch.py b/test/test_torch.py\nindex 7fbd358dc80917..73efc1de466459 100644\n--- a/test/test_torch.py\n+++ b/test/test_torch.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: tests\"]\n \n import torch\n@@ -4873,26 +4872,26 @@ def _test_helper(x, y, bias, memory_format):\n                 x_c_clone = x_c.clone() if is_inplace else x_c\n                 result_c = fn(x_c_clone, y_c)\n                 result = fn(x_clone, y)\n-                self.assertEqual(result, result_c, \"Failed for '{}'\".format(inspect.getsource(fn).strip()))\n+                self.assertEqual(result, result_c, f\"Failed for '{inspect.getsource(fn).strip()}'\")\n                 self.assertTrue(\n                     result.is_contiguous(memory_format=memory_format),\n-                    \"result of the '{}' is not in '{}' format\".format(inspect.getsource(fn).strip(), memory_format))\n+                    f\"result of the '{inspect.getsource(fn).strip()}' is not in '{memory_format}' format\")\n \n             for fn in bias_fns:\n                 result_c = fn(x_c, b_c)\n                 result = fn(x, bias)\n-                self.assertEqual(result, result_c, \"Failed for '{}'\".format(inspect.getsource(fn).strip()))\n+                self.assertEqual(result, result_c, f\"Failed for '{inspect.getsource(fn).strip()}'\")\n                 self.assertTrue(\n                     result.is_contiguous(memory_format=memory_format),\n-                    \"result of the '{}' is not in '{}' format\".format(inspect.getsource(fn).strip(), memory_format))\n+                    f\"result of the '{inspect.getsource(fn).strip()}' is not in '{memory_format}' format\")\n \n             for fn in return_contig_fns:\n                 result_c = fn(x_c, y_c)\n                 result = fn(x, y)\n-                self.assertEqual(result, result_c, \"Failed for '{}'\".format(inspect.getsource(fn).strip()))\n+                self.assertEqual(result, result_c, f\"Failed for '{inspect.getsource(fn).strip()}'\")\n                 self.assertTrue(\n                     result.is_contiguous(memory_format=torch.contiguous_format),\n-                    \"result of the '{}' is not in '{}' format\".format(inspect.getsource(fn).strip(), torch.contiguous_format))\n+                    f\"result of the '{inspect.getsource(fn).strip()}' is not in '{torch.contiguous_format}' format\")\n \n         _test_helper(\n             torch.randn((4, 3, 8, 8), device=device).contiguous(memory_format=torch.channels_last),\n@@ -6406,11 +6405,11 @@ def test_contains(self):\n \n         self.assertRaisesRegex(\n             RuntimeError,\n-            \"Tensor.__contains__ only supports Tensor or scalar, but you passed in a {}.\".format(str),\n+            f\"Tensor.__contains__ only supports Tensor or scalar, but you passed in a {str}.\",\n             lambda: \"foo\" in x)\n         self.assertRaisesRegex(\n             RuntimeError,\n-            \"Tensor.__contains__ only supports Tensor or scalar, but you passed in a {}.\".format(type([1, 2])),\n+            f\"Tensor.__contains__ only supports Tensor or scalar, but you passed in a {type([1, 2])}.\",\n             lambda: [1, 2] in x)\n \n     @skipIfTorchDynamo(\"TorchDynamo fails with unknown reason\")\n@@ -6696,9 +6695,9 @@ def test_parsing_intlist(self):\n \n         # fail parse with > 1 element variables\n         self.assertRaises(TypeError, lambda: torch.ones(torch.tensor(3, 3)))\n-        self.assertRaises(TypeError, lambda: torch.ones((torch.tensor(3, 3))))\n+        self.assertRaises(TypeError, lambda: torch.ones(torch.tensor(3, 3)))\n+        self.assertRaises(TypeError, lambda: torch.ones(np.array(3, 3)))\n         self.assertRaises(TypeError, lambda: torch.ones(np.array(3, 3)))\n-        self.assertRaises(TypeError, lambda: torch.ones((np.array(3, 3))))\n \n         # fail parse with additional positional args after intlist arg\n         self.assertRaisesRegex(TypeError,\n@@ -8211,7 +8210,7 @@ def test_manual_seed(self):\n         for seed, expected_initial_seed in test_cases:\n             torch.manual_seed(seed)\n             actual_initial_seed = torch.initial_seed()\n-            msg = \"expected initial_seed() = %x after calling manual_seed(%x), but got %x instead\" % (\n+            msg = \"expected initial_seed() = {:x} after calling manual_seed({:x}), but got {:x} instead\".format(\n                 expected_initial_seed, seed, actual_initial_seed)\n             self.assertEqual(expected_initial_seed, actual_initial_seed, msg=msg)\n         for invalid_seed in [min_int64 - 1, max_uint64 + 1]:\n@@ -8319,7 +8318,7 @@ def test_copy_behavior(t, non_blocking=False):\n             devices = [t.device]\n             if t.device.type == 'cuda':\n                 if t.device.index == -1:\n-                    devices.append('cuda:{}'.format(torch.cuda.current_device()))\n+                    devices.append(f'cuda:{torch.cuda.current_device()}')\n                 elif t.device.index == torch.cuda.current_device():\n                     devices.append('cuda')\n             for device in devices:\n@@ -8436,7 +8435,7 @@ class SubTensor(torch.Tensor):\n         self.assertTrue(t.grad is not None)\n \n         # Make sure invalid subclasses raise nice errors\n-        class BadSubTensor():\n+        class BadSubTensor:\n             member_var = object()\n \n         err_msg = \"Creating a Tensor subclass from a class that does not inherit from Tensor\"\n@@ -8584,7 +8583,7 @@ def get_expected_device_repr(device):\n                 return \"device(type='{type}', index={index})\".format(\n                     type=device.type, index=device.index)\n \n-            return \"device(type='{type}')\".format(type=device.type)\n+            return f\"device(type='{device.type}')\"\n \n         for device in device_set:\n             dev = torch.device(device)\n@@ -8674,7 +8673,7 @@ def test_doc_template(self) -> None:\n         from torch._torch_docs import __file__ as doc_file\n         from torch._torch_docs import multi_dim_common, single_dim_common, factory_common_args, factory_like_common_args\n \n-        with open(doc_file, \"r\", encoding=\"utf-8\") as f:\n+        with open(doc_file, encoding=\"utf-8\") as f:\n             doc_strs = f.read()\n \n         matches = re.findall(\n@@ -8705,7 +8704,7 @@ def _test_namespace(ns, *skips):\n             skip_regexes = []\n             for r in skips:\n                 if isinstance(r, str):\n-                    skip_regexes.append(re.compile('^{}$'.format(re.escape(r))))\n+                    skip_regexes.append(re.compile(f'^{re.escape(r)}$'))\n                 else:\n                     skip_regexes.append(r)\n \n@@ -8730,7 +8729,7 @@ def _test_namespace(ns, *skips):\n                                      'New docs have been added for {}, please remove '\n                                      'it from the skipped list in TestTorch.test_doc'.format(full_name))\n                 else:\n-                    self.assertTrue(has_doc, '{} is missing documentation'.format(full_name))\n+                    self.assertTrue(has_doc, f'{full_name} is missing documentation')\n \n             # FIXME: All of the following should be marked as expected failures\n             # so that it is easier to tell when missing has been added.\ndiff --git a/test/test_type_promotion.py b/test/test_type_promotion.py\nindex c4c9e58719ef79..2f141221f3f1ca 100644\n--- a/test/test_type_promotion.py\n+++ b/test/test_type_promotion.py\n@@ -397,8 +397,8 @@ def test_many_promotions(self, device):\n                 self.assertEqual(not second.is_contiguous(), non_contiguous)\n                 result = op(first, second)\n                 expected = op(first.to(common_dtype), second.to(common_dtype))\n-                self.assertEqual(result.dtype, expected.dtype, msg='{} with {}, {}'.format(op.__name__, dt1, dt2))\n-                self.assertEqual(result, expected, msg='{} with {}, {}'.format(op.__name__, dt1, dt2))\n+                self.assertEqual(result.dtype, expected.dtype, msg=f'{op.__name__} with {dt1}, {dt2}')\n+                self.assertEqual(result, expected, msg=f'{op.__name__} with {dt1}, {dt2}')\n \n     @float_double_default_dtype\n     def test_non_promoting_ops(self, device):\n@@ -850,7 +850,7 @@ def op(t1, t2, suf=None):\n         # Test op(dense, sparse)\n         if add_sub or op_name == 'mul':\n             if inplace:\n-                e, d1, s1, d2, s2 = [x.clone() for x in test_tensors]\n+                e, d1, s1, d2, s2 = (x.clone() for x in test_tensors)\n             dense_sparse = op(d1, s2)\n             dense_sparse = dense_sparse.to_dense() if dense_sparse.is_sparse else dense_sparse\n             self.assertEqual(e, dense_sparse, atol=precision, rtol=rtol, msg=err)\n@@ -871,7 +871,7 @@ def op(t1, t2, suf=None):\n         # Test op(sparse, scalar)\n         if not add_sub and not (self.device_type == 'cpu' and dtype1 == torch.half):\n             if inplace:\n-                e, d1, s1, d2, s2 = [x.clone() for x in test_tensors]\n+                e, d1, s1, d2, s2 = (x.clone() for x in test_tensors)\n             scalar = d2.view(d2.numel())[0].item()\n \n             sparse = op(s1, scalar)\n@@ -984,18 +984,18 @@ def test_numpy_array_binary_ufunc_promotion(self, device, dtypes):\n                 # Note: These cases prettyprint the failing inputs to make\n                 # debugging test failures easier.\n                 if undesired_failure and same_result:\n-                    msg = (\"Failure: {0} == {1}. \"\n-                           \"torch type was {2}. NumPy type was {3}. np_first is {4} \"\n-                           \"default type is {5}.\").format(actual, expected,\n+                    msg = (\"Failure: {} == {}. \"\n+                           \"torch type was {}. NumPy type was {}. np_first is {} \"\n+                           \"default type is {}.\").format(actual, expected,\n                                                           torch_type, np_type,\n                                                           np_first,\n                                                           torch.get_default_dtype())\n                     self.fail(msg)\n \n                 if not undesired_failure and not same_result:\n-                    msg = (\"Failure: {0} != {1}. \"\n-                           \"torch type was {2}. NumPy type was {3}. np_first is {4} \"\n-                           \"default type is {5}.\").format(actual, expected,\n+                    msg = (\"Failure: {} != {}. \"\n+                           \"torch type was {}. NumPy type was {}. np_first is {} \"\n+                           \"default type is {}.\").format(actual, expected,\n                                                           torch_type, np_type,\n                                                           np_first,\n                                                           torch.get_default_dtype())\ndiff --git a/test/test_unary_ufuncs.py b/test/test_unary_ufuncs.py\nindex 4e375a44481c1b..2fe550a7917281 100644\n--- a/test/test_unary_ufuncs.py\n+++ b/test/test_unary_ufuncs.py\n@@ -102,8 +102,8 @@ def test_float_domains(self, device, dtype, op):\n                     result.item(),\n                     float(\"nan\"),\n                     msg=(\n-                        \"input of {0} outside lower domain boundary\"\n-                        \" {1} produced {2}, not nan!\"\n+                        \"input of {} outside lower domain boundary\"\n+                        \" {} produced {}, not nan!\"\n                     ).format(lower_tensor.item(), low, result.item()),\n                 )\n \n@@ -121,8 +121,8 @@ def test_float_domains(self, device, dtype, op):\n                     result.item(),\n                     float(\"nan\"),\n                     msg=(\n-                        \"input of {0} outside upper domain boundary\"\n-                        \" {1} produced {2}, not nan!\"\n+                        \"input of {} outside upper domain boundary\"\n+                        \" {} produced {}, not nan!\"\n                     ).format(higher_tensor.item(), high, result.item()),\n                 )\n \n@@ -162,7 +162,7 @@ def assertEqualHelper(\n                         )\n                     else:\n                         self.fail(\n-                            \"Expected dtype {0} but got {1}!\".format(\n+                            \"Expected dtype {} but got {}!\".format(\n                                 expected.dtype, actual.dtype\n                             )\n                         )\n@@ -248,8 +248,8 @@ def _helper_reference_numerics(\n             if t.numel() < 10:\n                 msg = (\n                     \"Failed to produce expected results! Input tensor was\"\n-                    \" {0}, torch result is {1}, and reference result is\"\n-                    \" {2}.\"\n+                    \" {}, torch result is {}, and reference result is\"\n+                    \" {}.\"\n                 ).format(t, actual, expected)\n             else:\n                 msg = None\n@@ -1146,7 +1146,7 @@ def test_log1p_complex(self, device, dtype):\n             self.assertEqual(res.imag, out.imag, atol=0.0, rtol=1e-6)\n \n         # test the log1p in tensor\n-        inp_lst, out_lst = [list(elmt) for elmt in zip(*inouts)]\n+        inp_lst, out_lst = (list(elmt) for elmt in zip(*inouts))\n         inp_tens = torch.tensor(inp_lst, dtype=dtype, device=device)\n         out_tens = torch.tensor(out_lst, dtype=dtype, device=device)\n         res_tens = torch.log1p(inp_tens)\ndiff --git a/test/test_utils.py b/test/test_utils.py\nindex 866aaca61b430c..aa3fd149e84eca 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -545,11 +545,11 @@ def _run(self, command, timeout=30):\n \n     def _run_bottleneck(self, test_file, scriptargs=''):\n         curdir = os.path.dirname(os.path.abspath(__file__))\n-        filepath = '{}/{}'.format(curdir, test_file)\n+        filepath = f'{curdir}/{test_file}'\n         if scriptargs != '':\n-            scriptargs = ' {}'.format(scriptargs)\n+            scriptargs = f' {scriptargs}'\n         rc, out, err = self._run(\n-            '{} -m torch.utils.bottleneck {}{}'.format(sys.executable, filepath, scriptargs))\n+            f'{sys.executable} -m torch.utils.bottleneck {filepath}{scriptargs}')\n         return rc, out, err\n \n     def _check_run_args(self):\n@@ -562,7 +562,7 @@ def _check_run_args(self):\n         self.assertEqual(rc, 0, atol=0, rtol=0, msg=self._fail_msg('Should pass args to script', out + err))\n \n     def _fail_msg(self, msg, output):\n-        return '{}, output was:\\n{}'.format(msg, output)\n+        return f'{msg}, output was:\\n{output}'\n \n     def _check_environment_summary(self, output):\n         results = re.search('Environment Summary', output)\n@@ -603,7 +603,7 @@ def _check_cuda(self, output):\n     @unittest.skipIf(HAS_CUDA, 'CPU-only test')\n     def test_bottleneck_cpu_only(self):\n         rc, out, err = self._run_bottleneck('bottleneck_test/test.py')\n-        self.assertEqual(rc, 0, msg='Run failed with\\n{}'.format(err))\n+        self.assertEqual(rc, 0, msg=f'Run failed with\\n{err}')\n \n         self._check_run_args()\n         self._check_environment_summary(out)\n@@ -614,7 +614,7 @@ def test_bottleneck_cpu_only(self):\n     @unittest.skipIf(not HAS_CUDA, 'No CUDA')\n     def test_bottleneck_cuda(self):\n         rc, out, err = self._run_bottleneck('bottleneck_test/test_cuda.py')\n-        self.assertEqual(rc, 0, msg='Run failed with\\n{}'.format(err))\n+        self.assertEqual(rc, 0, msg=f'Run failed with\\n{err}')\n \n         self._check_run_args()\n         self._check_environment_summary(out)\n@@ -740,7 +740,7 @@ def test_load_standalone(self):\n                     std::cout << x << std::endl;\n                 }\n             \"\"\")\n-            with open(src_path, \"wt\") as f:\n+            with open(src_path, \"w\") as f:\n                 f.write(src)\n \n             exec_path = torch.utils.cpp_extension.load(\ndiff --git a/test/test_xnnpack_integration.py b/test/test_xnnpack_integration.py\nindex ab764a61d8a95c..4170446c38f776 100644\n--- a/test/test_xnnpack_integration.py\n+++ b/test/test_xnnpack_integration.py\n@@ -31,7 +31,7 @@ def test_linear(self, batch_size, data_shape, weight_output_dim, use_bias):\n         input_data = torch.rand(data_shape)\n         weight = torch.rand((weight_output_dim, data_shape[-1]))\n         if use_bias:\n-            bias = torch.rand((weight_output_dim))\n+            bias = torch.rand(weight_output_dim)\n         else:\n             bias = None\n         ref_result = F.linear(input_data, weight, bias)\n@@ -46,7 +46,7 @@ def test_linear_1d_input(self, input_size, weight_output_dim, use_bias):\n         input_data = torch.rand(input_size)\n         weight = torch.rand((weight_output_dim, input_data.shape[-1]))\n         if use_bias:\n-            bias = torch.rand((weight_output_dim))\n+            bias = torch.rand(weight_output_dim)\n         else:\n             bias = None\n         ref_result = F.linear(input_data, weight, bias)\n@@ -102,7 +102,7 @@ def test_conv2d(self,\n         weight = torch.rand((output_channels, input_channels_per_group, kernel_h, kernel_w))\n         bias = None\n         if use_bias:\n-            bias = torch.rand((output_channels))\n+            bias = torch.rand(output_channels)\n \n         ref_result = F.conv2d(input_data, weight, bias,\n                               strides, paddings, dilations, groups)\n@@ -166,7 +166,7 @@ def test_conv2d_transpose(self,\n         weight = torch.rand((input_channels, output_channels_per_group, kernel_h, kernel_w))\n         bias = None\n         if use_bias:\n-            bias = torch.rand((output_channels))\n+            bias = torch.rand(output_channels)\n \n         # Note that groups/dilation is in reverse order from conv2d\n         ref_result = F.conv_transpose2d(input_data, weight, bias,\n@@ -209,7 +209,7 @@ def forward(self, x):\n         data_shape = [batch_size] + list(data_shape)\n         weight = torch.rand((weight_output_dim, data_shape[-1]))\n         if use_bias:\n-            bias = torch.rand((weight_output_dim))\n+            bias = torch.rand(weight_output_dim)\n         else:\n             bias = None\n         scripted_linear = torch.jit.script(Linear(weight, bias))\n@@ -304,7 +304,7 @@ def forward(self, x):\n         weight = torch.rand((output_channels, input_channels_per_group, kernel_h, kernel_w))\n         bias = None\n         if use_bias:\n-            bias = torch.rand((output_channels))\n+            bias = torch.rand(output_channels)\n \n         scripted_conv2d = torch.jit.script(Conv2D(weight, bias,\n                                                   strides, paddings, dilations, groups))\n@@ -411,7 +411,7 @@ def forward(self, x):\n         weight = torch.rand((input_channels, output_channels_per_group, kernel_h, kernel_w))\n         bias = None\n         if use_bias:\n-            bias = torch.rand((output_channels))\n+            bias = torch.rand(output_channels)\n \n         scripted_conv2d = torch.jit.script(Conv2DT(weight, bias,\n                                                    strides, paddings,\n@@ -525,7 +525,7 @@ def forward(self, x):\n         conv_weight = torch.rand((output_channels, input_channels_per_group, kernel_h, kernel_w))\n         conv_bias = None\n         if use_bias:\n-            conv_bias = torch.rand((output_channels))\n+            conv_bias = torch.rand(output_channels)\n \n         # This is done just to find the output shape of the result\n         # so that the shape of weight for the following linear layer\n@@ -537,7 +537,7 @@ def forward(self, x):\n         linear_weight = torch.rand((linear_weight_output_dim, linear_input_shape))\n         linear_bias = None\n         if use_bias:\n-            linear_bias = torch.rand((linear_weight_output_dim))\n+            linear_bias = torch.rand(linear_weight_output_dim)\n \n         scripted_m = torch.jit.script(M(conv_weight, conv_bias, linear_weight,\n                                         linear_bias, strides, paddings, dilations, groups))\n@@ -625,7 +625,7 @@ class Linear(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.weight = torch.nn.Parameter(torch.rand(weight_shape), requires_grad=False)\n-                self.bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n \n             def forward(self, x):\n                 return F.linear(x, self.weight, self.bias)\n@@ -712,7 +712,7 @@ def forward(self, x):\n \n         input_data = torch.rand((batch_size, input_channels, height, width))\n         conv_weight = torch.rand((output_channels, input_channels_per_group, kernel_h, kernel_w))\n-        conv_bias = torch.rand((output_channels))\n+        conv_bias = torch.rand(output_channels)\n         result = F.conv2d(input_data, conv_weight, conv_bias,\n                           strides, paddings, dilations, groups)\n         linear_input_shape = result.shape[1]\n@@ -722,9 +722,9 @@ class M(torch.nn.Module):\n             def __init__(self, activation_fn=F.relu):\n                 super().__init__()\n                 self.conv_weight = torch.nn.Parameter(torch.rand(conv_weight_shape), requires_grad=False)\n-                self.conv_bias = torch.nn.Parameter(torch.rand((conv_bias_shape)), requires_grad=False)\n+                self.conv_bias = torch.nn.Parameter(torch.rand(conv_bias_shape), requires_grad=False)\n                 self.linear_weight = torch.nn.Parameter(torch.rand(linear_weight_shape), requires_grad=False)\n-                self.linear_bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.linear_bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n                 self.strides = strides\n                 self.paddings = paddings\n                 self.dilations = dilations\n@@ -834,7 +834,7 @@ class MFusionAntiPattern(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.linear_weight = torch.nn.Parameter(torch.rand(linear_weight_shape), requires_grad=False)\n-                self.linear_bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.linear_bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n                 self.strides = strides\n                 self.paddings = paddings\n                 self.dilations = dilations\n@@ -862,7 +862,7 @@ class MFusionAntiPatternParamMinMax(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.linear_weight = torch.nn.Parameter(torch.rand(linear_weight_shape), requires_grad=False)\n-                self.linear_bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.linear_bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n                 self.strides = strides\n                 self.paddings = paddings\n                 self.dilations = dilations\n@@ -895,7 +895,7 @@ class DecomposedLinearAddmm(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.weight = torch.nn.Parameter(torch.rand(weight_shape), requires_grad=False)\n-                self.bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n \n             def forward(self, x):\n                 weight_t = self.weight.t()\n@@ -905,7 +905,7 @@ class DecomposedLinearMatmulAdd(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.weight = torch.nn.Parameter(torch.rand(weight_shape), requires_grad=False)\n-                self.bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n \n             def forward(self, x):\n                 weight_t = self.weight.t()\n@@ -917,7 +917,7 @@ class DecomposedLinearMatmul(torch.nn.Module):\n             def __init__(self):\n                 super().__init__()\n                 self.weight = torch.nn.Parameter(torch.rand(weight_shape), requires_grad=False)\n-                self.bias = torch.nn.Parameter(torch.rand((weight_output_dim)), requires_grad=False)\n+                self.bias = torch.nn.Parameter(torch.rand(weight_output_dim), requires_grad=False)\n \n             def forward(self, x):\n                 weight_t = self.weight.t()\n"
  },
  {
    "number": 105433,
    "title": "[BE] Enable ruff's UP rules and autoformat distributed/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* __->__ #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "0a40af833a3c1c9658bab5d791bd3ece759c9a11",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105433",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105433/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105433.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105433.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105433/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105433/comments",
    "labels": [
      "release notes: distributed (pipeline)",
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:23:16.375036Z",
    "state": "open",
    "patch": "From 2bd7da9e4929c1597c767182f8f2e79e8e673a72 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:23:09 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat distributed/\n\n[ghstack-poisoned]\n---\n test/distributed/_spmd/test_data_parallel.py     |  2 +-\n test/distributed/_spmd/test_transformation.py    |  2 +-\n .../_tensor/debug/test_op_coverage.py            |  2 +-\n .../server/test/local_elastic_agent_test.py      |  2 +-\n .../elastic/multiprocessing/api_test.py          |  2 +-\n .../elastic/multiprocessing/errors/api_test.py   |  8 ++++----\n .../multiprocessing/errors/error_handler_test.py |  2 +-\n .../elastic/multiprocessing/redirects_test.py    |  8 ++++----\n .../elastic/utils/distributed_test.py            |  2 +-\n test/distributed/elastic/utils/util_test.py      |  2 +-\n test/distributed/launcher/run_test.py            |  4 ++--\n test/distributed/pipeline/sync/conftest.py       |  2 +-\n test/distributed/test_c10d_common.py             |  6 +++---\n test/distributed/test_c10d_gloo.py               |  8 ++++----\n test/distributed/test_c10d_nccl.py               | 12 ++++++------\n test/distributed/test_c10d_ucc.py                | 16 ++++++++--------\n test/distributed/test_data_parallel.py           |  6 +++---\n test/distributed/test_store.py                   |  2 +-\n .../_shard/sharded_tensor/__init__.py            |  1 -\n torch/distributed/_shard/sharded_tensor/api.py   | 10 +++++-----\n .../chunk_sharding_spec_ops/_common.py           |  1 -\n .../chunk_sharding_spec_ops/embedding.py         |  1 -\n .../chunk_sharding_spec_ops/embedding_bag.py     |  1 -\n torch/distributed/_tensor/device_mesh.py         |  4 ++--\n .../_tensor/examples/checkpoint_example.py       |  2 +-\n torch/distributed/_tensor/op_schema.py           |  4 ++--\n torch/distributed/algorithms/join.py             |  4 ++--\n .../distributed/benchmarks/benchmark_ddp_rpc.py  | 16 ++++++++--------\n .../examples/fsdp_checkpoint_example.py          |  2 +-\n torch/distributed/elastic/metrics/api.py         |  6 +++---\n .../elastic/multiprocessing/errors/__init__.py   |  2 +-\n .../multiprocessing/errors/error_handler.py      |  4 ++--\n .../elastic/multiprocessing/tail_log.py          |  2 +-\n .../elastic/rendezvous/etcd_rendezvous.py        | 13 ++++++-------\n .../elastic/rendezvous/static_tcp_rendezvous.py  |  1 -\n .../elastic/timer/file_based_local_timer.py      |  2 +-\n torch/distributed/elastic/utils/store.py         |  2 +-\n torch/distributed/nn/api/remote_module.py        |  4 ++--\n torch/distributed/nn/jit/instantiator.py         |  6 +++---\n torch/distributed/optim/functional_adam.py       | 10 +++++-----\n torch/distributed/optim/functional_adamax.py     | 10 +++++-----\n torch/distributed/optim/functional_adamw.py      | 10 +++++-----\n .../pipeline/sync/_balance/blockpartition.py     |  1 -\n torch/distributed/pipeline/sync/pipeline.py      |  1 -\n torch/distributed/pipeline/sync/skip/portal.py   |  1 -\n .../distributed/pipeline/sync/skip/skippable.py  |  1 -\n torch/distributed/rendezvous.py                  | 12 ++++++------\n torch/distributed/rpc/__init__.py                |  2 +-\n .../_testing/faulty_agent_backend_registry.py    |  2 +-\n torch/distributed/rpc/api.py                     |  4 ++--\n torch/distributed/rpc/backend_registry.py        |  6 +++---\n 51 files changed, 113 insertions(+), 123 deletions(-)\n\ndiff --git a/test/distributed/_spmd/test_data_parallel.py b/test/distributed/_spmd/test_data_parallel.py\nindex e5915f185b2194..4940320c0724fa 100644\n--- a/test/distributed/_spmd/test_data_parallel.py\n+++ b/test/distributed/_spmd/test_data_parallel.py\n@@ -21,7 +21,7 @@\n \n class SimpleMLP(nn.Module):\n     def __init__(self):\n-        super(SimpleMLP, self).__init__()\n+        super().__init__()\n         self.net1 = nn.Linear(50, 32)\n         self.relu = nn.ReLU()\n         self.net2 = nn.Linear(32, 8)\ndiff --git a/test/distributed/_spmd/test_transformation.py b/test/distributed/_spmd/test_transformation.py\nindex 9e4196b0776206..e4cb13a9577d5c 100644\n--- a/test/distributed/_spmd/test_transformation.py\n+++ b/test/distributed/_spmd/test_transformation.py\n@@ -272,7 +272,7 @@ def my_transformation(gm):\n             split_fused_optimizer(gm, opt_block, gradients)\n             gm.graph.eliminate_dead_code()\n             gm.recompile()\n-            self.assertEquals(len(get_all_fused_optimizer_blocks(gm, \"_fused_adam\")), 2)\n+            self.assertEqual(len(get_all_fused_optimizer_blocks(gm, \"_fused_adam\")), 2)\n             gm.finalize_setup()\n             return gm\n \ndiff --git a/test/distributed/_tensor/debug/test_op_coverage.py b/test/distributed/_tensor/debug/test_op_coverage.py\nindex a72bb7a66f1397..1e6d3fb4356a0a 100644\n--- a/test/distributed/_tensor/debug/test_op_coverage.py\n+++ b/test/distributed/_tensor/debug/test_op_coverage.py\n@@ -10,7 +10,7 @@\n \n class SimpleMLP(nn.Module):\n     def __init__(self):\n-        super(SimpleMLP, self).__init__()\n+        super().__init__()\n         self.net1 = nn.Linear(50, 32)\n         self.relu = nn.ReLU()\n         self.net2 = nn.Linear(32, 8)\ndiff --git a/test/distributed/elastic/agent/server/test/local_elastic_agent_test.py b/test/distributed/elastic/agent/server/test/local_elastic_agent_test.py\nindex 174bcfdf7d584c..27ea0cc75b1ed7 100644\n--- a/test/distributed/elastic/agent/server/test/local_elastic_agent_test.py\n+++ b/test/distributed/elastic/agent/server/test/local_elastic_agent_test.py\n@@ -727,7 +727,7 @@ def run_sad_function(self):\n \n             rank, failure = cm.exception.get_first_failure()\n             failure_data = failure.error_file_data[\"message\"]\n-            with open(replyfile, \"r\") as fp:\n+            with open(replyfile) as fp:\n                 data = json.load(fp)[\"message\"]\n \n                 # ran two; both failed; first failure is either rank 0 or 1\ndiff --git a/test/distributed/elastic/multiprocessing/api_test.py b/test/distributed/elastic/multiprocessing/api_test.py\nindex 6a969a88d536dc..6240fbe41d6795 100644\n--- a/test/distributed/elastic/multiprocessing/api_test.py\n+++ b/test/distributed/elastic/multiprocessing/api_test.py\n@@ -244,7 +244,7 @@ def log_dir(self):\n \n         def assert_in_file(self, expected: List[str], filename: str) -> None:\n             expected = [f\"{line.rstrip()}\\n\" for line in expected]\n-            with open(filename, \"r\") as fp:\n+            with open(filename) as fp:\n                 actual = fp.readlines()\n                 for line in expected:\n                     self.assertIn(line, actual)\ndiff --git a/test/distributed/elastic/multiprocessing/errors/api_test.py b/test/distributed/elastic/multiprocessing/errors/api_test.py\nindex a9590bea313d2e..e49cbe85fe1591 100644\n--- a/test/distributed/elastic/multiprocessing/errors/api_test.py\n+++ b/test/distributed/elastic/multiprocessing/errors/api_test.py\n@@ -43,7 +43,7 @@ def raise_child_failure_error_fn(name, child_error_file=\"\"):\n \n \n def read_resource_file(resource_file: str) -> str:\n-    with open(os.path.join(os.path.dirname(__file__), resource_file), \"r\") as fp:\n+    with open(os.path.join(os.path.dirname(__file__), resource_file)) as fp:\n         return \"\".join(fp.readlines())\n \n \n@@ -169,7 +169,7 @@ def test_record(self):\n             with self.assertRaises(SentinelError):\n                 raise_exception_fn()\n \n-        with open(self.test_error_file, \"r\") as fp:\n+        with open(self.test_error_file) as fp:\n             err = json.load(fp)\n             self.assertIsNotNone(err[\"message\"][\"message\"])\n             self.assertIsNotNone(err[\"message\"][\"extraInfo\"][\"py_callstack\"])\n@@ -203,9 +203,9 @@ def test_record_child_failure(self):\n                 raise_child_failure_error_fn(\"trainer\", trainer_error_file)\n             pf = cm.exception.get_first_failure()[1]\n             # compare worker error file with reply file and overridden error code\n-            expect = json.load(open(pf.error_file, \"r\"))\n+            expect = json.load(open(pf.error_file))\n             expect[\"message\"][\"errorCode\"] = pf.exitcode\n-            actual = json.load(open(self.test_error_file, \"r\"))\n+            actual = json.load(open(self.test_error_file))\n             self.assertTrue(\n                 json.dumps(expect, sort_keys=True),\n                 json.dumps(actual, sort_keys=True),\ndiff --git a/test/distributed/elastic/multiprocessing/errors/error_handler_test.py b/test/distributed/elastic/multiprocessing/errors/error_handler_test.py\nindex 6adf97de9a275e..edc62aa8a72b96 100644\n--- a/test/distributed/elastic/multiprocessing/errors/error_handler_test.py\n+++ b/test/distributed/elastic/multiprocessing/errors/error_handler_test.py\n@@ -51,7 +51,7 @@ def test_record_exception(self):\n             except Exception as e:\n                 eh.record_exception(e)\n \n-            with open(self.test_error_file, \"r\") as fp:\n+            with open(self.test_error_file) as fp:\n                 err = json.load(fp)\n                 # error file content example:\n                 # {\ndiff --git a/test/distributed/elastic/multiprocessing/redirects_test.py b/test/distributed/elastic/multiprocessing/redirects_test.py\nindex 6a12ff06c7fab2..04e8b1c30dc7e0 100644\n--- a/test/distributed/elastic/multiprocessing/redirects_test.py\n+++ b/test/distributed/elastic/multiprocessing/redirects_test.py\n@@ -54,7 +54,7 @@ def test_redirect_stdout(self):\n         libc.printf(b\"foo again from c\\n\")\n         os.system(\"echo foo again from cmd\")\n \n-        with open(stdout_log, \"r\") as f:\n+        with open(stdout_log) as f:\n             # since we print from python, c, cmd -> the stream is not ordered\n             # do a set comparison\n             lines = set(f.readlines())\n@@ -78,7 +78,7 @@ def test_redirect_stderr(self):\n         libc.fprintf(c_stderr, b\"bar again from c\\n\")\n         os.system(\"echo bar again from cmd 1>&2\")\n \n-        with open(stderr_log, \"r\") as f:\n+        with open(stderr_log) as f:\n             lines = set(f.readlines())\n             self.assertEqual(\n                 {\"bar from python\\n\", \"bar from c\\n\", \"bar from cmd\\n\"}, lines\n@@ -103,13 +103,13 @@ def test_redirect_both(self):\n         print(\"again stdout from python\")\n         libc.fprintf(c_stderr, b\"again stderr from c\\n\")\n \n-        with open(stdout_log, \"r\") as f:\n+        with open(stdout_log) as f:\n             lines = set(f.readlines())\n             self.assertEqual(\n                 {\"redir stdout from python\\n\", \"redir stdout from c\\n\"}, lines\n             )\n \n-        with open(stderr_log, \"r\") as f:\n+        with open(stderr_log) as f:\n             lines = set(f.readlines())\n             self.assertEqual(\n                 {\"redir stderr from python\\n\", \"redir stderr from c\\n\"}, lines\ndiff --git a/test/distributed/elastic/utils/distributed_test.py b/test/distributed/elastic/utils/distributed_test.py\nindex 50696c146b8c4c..dc74e324cdae8a 100644\n--- a/test/distributed/elastic/utils/distributed_test.py\n+++ b/test/distributed/elastic/utils/distributed_test.py\n@@ -32,7 +32,7 @@ def _create_c10d_store_mp(is_server, server_addr, port, world_size, wait_for_wor\n     if store is None:\n         raise AssertionError()\n \n-    store.set(f\"test_key/{os.getpid()}\", \"test_value\".encode(\"UTF-8\"))\n+    store.set(f\"test_key/{os.getpid()}\", b\"test_value\")\n \n \n if IS_WINDOWS or IS_MACOS:\ndiff --git a/test/distributed/elastic/utils/util_test.py b/test/distributed/elastic/utils/util_test.py\nindex fefe40537a8fd5..60db327d5b95c6 100644\n--- a/test/distributed/elastic/utils/util_test.py\n+++ b/test/distributed/elastic/utils/util_test.py\n@@ -56,7 +56,7 @@ def test_get_all_rank_n(self):\n \n     def test_synchronize(self):\n         store_mock = mock.MagicMock()\n-        data = \"data0\".encode(encoding=\"UTF-8\")\n+        data = b\"data0\"\n         store_util.synchronize(store_mock, data, 0, 3, key_prefix=\"torchelastic/test\")\n         actual_set_call_args = store_mock.set.call_args_list\n         # omit empty kwargs\ndiff --git a/test/distributed/launcher/run_test.py b/test/distributed/launcher/run_test.py\nindex fb8f6d7968471b..7cdab9f02769af 100644\n--- a/test/distributed/launcher/run_test.py\n+++ b/test/distributed/launcher/run_test.py\n@@ -501,7 +501,7 @@ def test_is_torchelastic_launched(self):\n             ]\n         )\n \n-        with open(out_file, \"r\") as fp:\n+        with open(out_file) as fp:\n             is_torchelastic_launched = fp.readline()\n             self.assertEqual(\"True\", is_torchelastic_launched)\n \n@@ -523,7 +523,7 @@ def test_is_not_torchelastic_launched(self):\n             ],\n         ):\n             runpy.run_path(sys.argv[0], run_name=\"__main__\")\n-            with open(out_file, \"r\") as fp:\n+            with open(out_file) as fp:\n                 is_torchelastic_launched = fp.readline()\n                 self.assertEqual(\"False\", is_torchelastic_launched)\n \ndiff --git a/test/distributed/pipeline/sync/conftest.py b/test/distributed/pipeline/sync/conftest.py\nindex 04e9cbcb9a83ef..78f7d3a8f1bb4b 100644\n--- a/test/distributed/pipeline/sync/conftest.py\n+++ b/test/distributed/pipeline/sync/conftest.py\n@@ -46,7 +46,7 @@ def setup_rpc(scope=\"session\"):\n         rank=0,\n         world_size=1,\n         rpc_backend_options=dist.rpc.TensorPipeRpcBackendOptions(\n-            init_method=\"file://{}\".format(file.name),\n+            init_method=f\"file://{file.name}\",\n         )\n     )\n     yield\ndiff --git a/test/distributed/test_c10d_common.py b/test/distributed/test_c10d_common.py\nindex a1f4d22ca2835f..2d8a0966e5e01f 100644\n--- a/test/distributed/test_c10d_common.py\n+++ b/test/distributed/test_c10d_common.py\n@@ -127,7 +127,7 @@ def _test_default_store_timeout(self, backend):\n                 # let @retry_on_connect_failures handle the error\n                 raise c2p[0]\n             else:\n-                raise RuntimeError(\"Unexpected type {}\".format(type(c2p[0])))\n+                raise RuntimeError(f\"Unexpected type {type(c2p[0])}\")\n \n \n class TimeoutTest(TestCase):\n@@ -348,7 +348,7 @@ def _prepare_multi_device_module(\n     ):\n         self.assertTrue(\n             len(devices) == 2 or len(devices) == 4,\n-            \"unexpected devices for ddp tests {}\".format(devices),\n+            f\"unexpected devices for ddp tests {devices}\",\n         )\n         if len(devices) == 2:\n             model = DoubleGpuNet(devices)\n@@ -1339,7 +1339,7 @@ def _test_tensor_dtype_complex(self, backend):\n \n \n # Variant of AbstractCommTest that expects world size of 4\n-class AbstractLargeCommTest(object):\n+class AbstractLargeCommTest:\n     @property\n     def op_timeout_sec(self):\n         return 1\ndiff --git a/test/distributed/test_c10d_gloo.py b/test/distributed/test_c10d_gloo.py\nindex 63e437483c8bd3..19e4d0186fe5cb 100644\n--- a/test/distributed/test_c10d_gloo.py\n+++ b/test/distributed/test_c10d_gloo.py\n@@ -620,7 +620,7 @@ def _test_allreduce_coalesced_stress(self, inputs):\n             self.assertEqual(\n                 self._expected_output(i),\n                 result,\n-                msg=\"Mismatch in iteration {}\".format(i),\n+                msg=f\"Mismatch in iteration {i}\",\n             )\n \n     @requires_gloo()\n@@ -642,7 +642,7 @@ def test_allreduce_coalesced_async(self):\n             self.assertEqual(\n                 self._expected_output(i),\n                 fut.wait(),\n-                msg=\"Mismatch in iteration {}\".format(i),\n+                msg=f\"Mismatch in iteration {i}\",\n             )\n \n     @requires_gloo()\n@@ -2478,11 +2478,11 @@ def test_consecutive_comm_work_wait_gpu(self):\n \n class LargeCommTest(test_c10d_common.AbstractLargeCommTest, MultiProcessTestCase):\n     def setUp(self):\n-        super(LargeCommTest, self).setUp()\n+        super().setUp()\n         self._spawn_processes()\n \n     def tearDown(self):\n-        super(LargeCommTest, self).tearDown()\n+        super().tearDown()\n         try:\n             os.remove(self.file_name)\n         except OSError:\ndiff --git a/test/distributed/test_c10d_nccl.py b/test/distributed/test_c10d_nccl.py\nindex 2167f53b74d707..5791b993746536 100644\n--- a/test/distributed/test_c10d_nccl.py\n+++ b/test/distributed/test_c10d_nccl.py\n@@ -151,20 +151,20 @@ def withouts(d, keys):\n \n         with Env(without(vars, \"WORLD_SIZE\")):\n             self.assertEqual(None, os.environ.get(\"WORLD_SIZE\"))\n-            gen = c10d.rendezvous(\"env://?world_size={}\".format(1))\n+            gen = c10d.rendezvous(f\"env://?world_size={1}\")\n             _, _, size = next(gen)\n             self.assertEqual(size, 1)\n \n         with Env(without(vars, \"RANK\")):\n             self.assertEqual(None, os.environ.get(\"RANK\"))\n-            gen = c10d.rendezvous(\"env://?rank={}\".format(0))\n+            gen = c10d.rendezvous(f\"env://?rank={0}\")\n             _, rank, _ = next(gen)\n             self.assertEqual(rank, 0)\n \n         with Env(withouts(vars, [\"RANK\", \"WORLD_SIZE\"])):\n             self.assertEqual(None, os.environ.get(\"RANK\"))\n             self.assertEqual(None, os.environ.get(\"WORLD_SIZE\"))\n-            gen = c10d.rendezvous(\"env://?rank={}&world_size={}\".format(0, 1))\n+            gen = c10d.rendezvous(f\"env://?rank={0}&world_size={1}\")\n             _, rank, size = next(gen)\n             self.assertEqual(rank, 0)\n             self.assertEqual(size, 1)\n@@ -1932,7 +1932,7 @@ def first_bucket_size(ddp_bucket_mb):\n                     # 3 iters:  First iter creates grads, second iter retests after rebucketing,\n                     # third iter tries zeroed grads.\n                     for it in range(3):\n-                        iter_msg = \"iter = {} \".format(it) + model_msg\n+                        iter_msg = f\"iter = {it} \" + model_msg\n                         named_msg = iter_msg\n                         try:\n                             F.mse_loss(m(input).float(), target).backward()\n@@ -3059,14 +3059,14 @@ def test_reduce_scatter_tensor_coalesced(self):\n \n class LargeCommTest(test_c10d_common.AbstractLargeCommTest, MultiProcessTestCase):\n     def setUp(self):\n-        super(LargeCommTest, self).setUp()\n+        super().setUp()\n         # NCCL_BLOCKING_WAIT overrides NCCL_ASYNC_ERROR_HANDLING hence tests\n         # that use NCCL_BLOCKING_WAIT will test it as expected.\n         os.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n         self._spawn_processes()\n \n     def tearDown(self):\n-        super(LargeCommTest, self).tearDown()\n+        super().tearDown()\n         try:\n             os.remove(self.file_name)\n         except OSError:\ndiff --git a/test/distributed/test_c10d_ucc.py b/test/distributed/test_c10d_ucc.py\nindex 14f993b691fe60..0c35fea9b54aca 100644\n--- a/test/distributed/test_c10d_ucc.py\n+++ b/test/distributed/test_c10d_ucc.py\n@@ -341,7 +341,7 @@ class DistributedDataParallelTest(\n     test_c10d_common.CommonDistributedDataParallelTest, MultiProcessTestCase\n ):\n     def setUp(self):\n-        super(DistributedDataParallelTest, self).setUp()\n+        super().setUp()\n         self._spawn_processes()\n \n     def _get_process_group(self):\n@@ -410,7 +410,7 @@ def _test_global_local_unused_params_grad(\n \n         class GlobalLocalUnusedParamModule(nn.Module):\n             def __init__(self):\n-                super(GlobalLocalUnusedParamModule, self).__init__()\n+                super().__init__()\n                 self.t0 = Task()\n                 self.t1 = Task()\n                 self.task_unused = Task()\n@@ -500,7 +500,7 @@ def test_find_unused_parameters_when_unused_parameters_empty(self):\n \n         class FindUnusedParamModule(nn.Module):\n             def __init__(self):\n-                super(FindUnusedParamModule, self).__init__()\n+                super().__init__()\n                 self.t0 = Task()\n                 self.t1 = Task()\n \n@@ -553,7 +553,7 @@ def test_ignored_output(self):\n \n         class IgnoredOutput(nn.Module):\n             def __init__(self):\n-                super(IgnoredOutput, self).__init__()\n+                super().__init__()\n                 self.fc1 = nn.Linear(2, 10, bias=False)\n                 self.fc2 = nn.Linear(10, 4, bias=False)\n                 self.relu = nn.ReLU()\n@@ -595,7 +595,7 @@ def test_ignored_output_with_unused_parameters(self):\n \n         class IgnoredOutputWithUnusedParameters(nn.Module):\n             def __init__(self):\n-                super(IgnoredOutputWithUnusedParameters, self).__init__()\n+                super().__init__()\n                 self.fc1 = nn.Linear(2, 10, bias=False)\n                 self.fc2 = nn.Linear(10, 4, bias=False)\n                 self.fc3 = nn.Linear(4, 4, bias=False)\n@@ -660,7 +660,7 @@ def test_save_load_checkpoint(self):\n \n         class TestModel(nn.Module):\n             def __init__(self):\n-                super(TestModel, self).__init__()\n+                super().__init__()\n                 self.fc1 = nn.Linear(2, 10, bias=False)\n                 self.fc2 = nn.Linear(10, 4, bias=False)\n                 self.relu = nn.ReLU()\n@@ -969,11 +969,11 @@ def device(self):\n         return \"cpu\"\n \n     def setUp(self):\n-        super(CommTest, self).setUp()\n+        super().setUp()\n         self._spawn_processes()\n \n     def tearDown(self):\n-        super(CommTest, self).tearDown()\n+        super().tearDown()\n         try:\n             os.remove(self.file_name)\n         except OSError:\ndiff --git a/test/distributed/test_data_parallel.py b/test/distributed/test_data_parallel.py\nindex 43602d6b83cc09..e905fc7d81d0ef 100644\n--- a/test/distributed/test_data_parallel.py\n+++ b/test/distributed/test_data_parallel.py\n@@ -245,7 +245,7 @@ def test(inner_m, dp_device, inp, device_ids, should_fail):\n             if isinstance(device_ids[0], torch.device):\n                 expect_device = device_ids[0]\n             else:\n-                expect_device = torch.device(\"cuda:{}\".format(device_ids[0]))\n+                expect_device = torch.device(f\"cuda:{device_ids[0]}\")\n \n             if should_fail:\n                 def assert_correct():\n@@ -701,7 +701,7 @@ def forward(self, x):\n \n         with torch.backends.cudnn.flags(enabled=True, deterministic=True, benchmark=False):\n             for formats, dtype_list in product(layer_formats, layer_dtypes):\n-                model_msg = \"formats = {} dtypes = {}\".format(formats, dtypes)\n+                model_msg = f\"formats = {formats} dtypes = {dtypes}\"\n                 try:\n                     m = ConvNet(formats, dtype_list).cuda(device=\"cuda:0\")\n                     m_dp = dp.DataParallel(deepcopy(m), device_ids=device_ids)\n@@ -715,7 +715,7 @@ def forward(self, x):\n                     raise\n                 # 2 iters:  First iter creates grads, second iter tries zeroed grads.\n                 for it in range(2):\n-                    iter_msg = \"iter = {} \".format(it) + model_msg\n+                    iter_msg = f\"iter = {it} \" + model_msg\n                     named_msg = iter_msg\n                     try:\n                         F.mse_loss(m(input).float(), target).backward()\ndiff --git a/test/distributed/test_store.py b/test/distributed/test_store.py\nindex 67f0bc80ad0a65..b916f94ad64907 100644\n--- a/test/distributed/test_store.py\n+++ b/test/distributed/test_store.py\n@@ -354,7 +354,7 @@ def test_numkeys_delkeys(self):\n \n     def _create_client(self, index, addr, port, world_size):\n         client_store = dist.TCPStore(addr, port, world_size=world_size, timeout=timedelta(seconds=10))\n-        self.assertEqual(\"value\".encode(), client_store.get(\"key\"))\n+        self.assertEqual(b\"value\", client_store.get(\"key\"))\n         client_store.set(f\"new_key{index}\", f\"new_value{index}\")\n         self.assertEqual(f\"next_value{index}\".encode(),\n                          client_store.compare_set(f\"new_key{index}\", f\"new_value{index}\", f\"next_value{index}\"))\ndiff --git a/torch/distributed/_shard/sharded_tensor/__init__.py b/torch/distributed/_shard/sharded_tensor/__init__.py\nindex 386e53885a9fab..18d5d513202b05 100644\n--- a/torch/distributed/_shard/sharded_tensor/__init__.py\n+++ b/torch/distributed/_shard/sharded_tensor/__init__.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n \n import functools\n from typing import List\ndiff --git a/torch/distributed/_shard/sharded_tensor/api.py b/torch/distributed/_shard/sharded_tensor/api.py\nindex acab7081c91c25..2b6ed40f4d255b 100644\n--- a/torch/distributed/_shard/sharded_tensor/api.py\n+++ b/torch/distributed/_shard/sharded_tensor/api.py\n@@ -49,7 +49,7 @@\n # Tracking for sharded tensor objects.\n _sharded_tensor_lock = threading.Lock()\n _sharded_tensor_current_id = 0\n-_sharded_tensor_map: Dict[int, 'weakref.ReferenceType[ShardedTensor]'] = {}\n+_sharded_tensor_map: Dict[int, weakref.ReferenceType[ShardedTensor]] = {}\n \n # Default sharded ops\n _SHARDED_OPS: Dict[Callable, Callable] = {}\n@@ -136,7 +136,7 @@ def _init_from_local_shards_and_global_metadata(\n         local_shards: List[Shard],\n         sharded_tensor_metadata: ShardedTensorMetadata,\n         sharding_spec=None,\n-    ) -> \"ShardedTensorBase\":\n+    ) -> ShardedTensorBase:\n         \"\"\"\n         Initialize a ShardedTensorBase with local shards and a global\n         ShardedTensorMetadata built on each rank.\n@@ -234,7 +234,7 @@ class ShardedTensor(ShardedTensorBase):\n \n     \"\"\"\n     def __new__(cls, sharding_spec: shard_spec.ShardingSpec, *size, **kwargs):\n-        self = super(ShardedTensor, cls).__new__(cls, sharding_spec, *size, **kwargs)\n+        self = super().__new__(cls, sharding_spec, *size, **kwargs)\n         return self\n \n     def __init__(\n@@ -734,7 +734,7 @@ def _init_from_local_tensor(\n         *global_size: Sequence[int],\n         process_group: Optional[dist.ProcessGroup] = None,\n         init_rrefs=False,\n-    ) -> \"ShardedTensor\":\n+    ) -> ShardedTensor:\n         \"\"\"\n         Initialize a ShardedTensor given only one local tensor, global sharded tensor\n         size and sharding spec on each rank.\n@@ -839,7 +839,7 @@ def _init_from_local_shards_and_global_metadata(  # type: ignore[override]\n         process_group=None,\n         init_rrefs=False,\n         sharding_spec=None,\n-    ) -> \"ShardedTensor\":\n+    ) -> ShardedTensor:\n         \"\"\"\n         Initialize a ShardedTensor with local shards and a global\n         ShardedTensorMetadata built on each rank.\ndiff --git a/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/_common.py b/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/_common.py\nindex 22831dd233a3ee..24727005870de8 100644\n--- a/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/_common.py\n+++ b/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/_common.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n \n import torch\n import torch.distributed as dist\ndiff --git a/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding.py b/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding.py\nindex 4939d2c11e817d..e1c1cb6380439c 100644\n--- a/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding.py\n+++ b/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n \n import torch\n import torch.distributed as dist\ndiff --git a/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py b/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py\nindex 500694d6106dd1..1fa474e0342652 100644\n--- a/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py\n+++ b/torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n \n from typing import cast, List\n \ndiff --git a/torch/distributed/_tensor/device_mesh.py b/torch/distributed/_tensor/device_mesh.py\nindex 4e69e02054b201..4869ec03bdf591 100644\n--- a/torch/distributed/_tensor/device_mesh.py\n+++ b/torch/distributed/_tensor/device_mesh.py\n@@ -37,7 +37,7 @@\n         )\n \n \n-class _MeshEnv(object):\n+class _MeshEnv:\n     def __init__(self) -> None:\n         self.mesh_stack: List[DeviceMesh] = []\n \n@@ -60,7 +60,7 @@ def _get_device_handle(device_type: str = \"cuda\"):\n     return getattr(torch, device_type, None) if device_type != \"cpu\" else None\n \n \n-class DeviceMesh(object):\n+class DeviceMesh:\n     \"\"\"\n     DeviceMesh represents a mesh of devices, where layout of devices could be\n     represented as a n-d dimension array, and each value of the n-d dimensional\ndiff --git a/torch/distributed/_tensor/examples/checkpoint_example.py b/torch/distributed/_tensor/examples/checkpoint_example.py\nindex fb5141b17a027c..4ff51d7b88c2f3 100644\n--- a/torch/distributed/_tensor/examples/checkpoint_example.py\n+++ b/torch/distributed/_tensor/examples/checkpoint_example.py\n@@ -27,7 +27,7 @@\n \n class SimpleMLP(torch.nn.Module):\n     def __init__(self):\n-        super(SimpleMLP, self).__init__()\n+        super().__init__()\n         self.net1 = torch.nn.Linear(5, 128)\n         self.relu = torch.nn.ReLU()\n         self.net2 = torch.nn.Linear(128, 12)\ndiff --git a/torch/distributed/_tensor/op_schema.py b/torch/distributed/_tensor/op_schema.py\nindex e8bb45d36ffdc7..39146c02fdfcbe 100644\n--- a/torch/distributed/_tensor/op_schema.py\n+++ b/torch/distributed/_tensor/op_schema.py\n@@ -28,7 +28,7 @@ def _rebuild_tensor_from_dtensor_meta(arg) -> object:\n \n \n @dataclass\n-class PlacementStrategy(object):\n+class PlacementStrategy:\n     \"\"\"\n     A placement strategy describes an acceptable sharding placements of the output\n     and the tensor arguments of an operation.\n@@ -54,7 +54,7 @@ def __str__(self) -> str:\n         return f\"({input_specs_str}) -> ({output_spec_str}) @ mesh layout: {tuple(self.output_spec.mesh.mesh.shape)}\"\n \n \n-class StrategyType(object):\n+class StrategyType:\n     \"\"\"\n     Base class type for op strategy, We have two StrategyType:\n         OpStrategy and TupleStrategy\ndiff --git a/torch/distributed/algorithms/join.py b/torch/distributed/algorithms/join.py\nindex 1c00b2ca2ea435..d6b19450da39c5 100644\n--- a/torch/distributed/algorithms/join.py\n+++ b/torch/distributed/algorithms/join.py\n@@ -8,7 +8,7 @@\n \n __all__ = ['JoinHook', 'Joinable', 'Join']\n \n-class JoinHook():\n+class JoinHook:\n     r\"\"\"\n     This defines a join hook, which provides two entry points in the join\n     context manager: a main hook, which is called repeatedly while there exists\n@@ -109,7 +109,7 @@ def construct_disabled_join_config():\n \n \n \n-class Join():\n+class Join:\n     r\"\"\"\n     This class defines the generic join context manager, which allows custom\n     hooks to be called after a process joins. These hooks should shadow the\ndiff --git a/torch/distributed/benchmarks/benchmark_ddp_rpc.py b/torch/distributed/benchmarks/benchmark_ddp_rpc.py\nindex d8f5737d2d43d6..06b7f6ee0c08fa 100644\n--- a/torch/distributed/benchmarks/benchmark_ddp_rpc.py\n+++ b/torch/distributed/benchmarks/benchmark_ddp_rpc.py\n@@ -132,7 +132,7 @@ def _run_trainer(emb_rref_list, rank):\n     # Retrieve parameters from all embedding tables for the current trainer.\n     model_parameter_rrefs = []\n     for ind, emb_rref in enumerate(emb_rref_list):\n-        ps_name = \"ps{}\".format(ind)\n+        ps_name = f\"ps{ind}\"\n         model_parameter_rrefs.extend(\n             rpc.rpc_sync(ps_name, _retrieve_embedding_parameters, args=(emb_rref,))\n         )\n@@ -222,7 +222,7 @@ def run_worker(rank, world_size):\n         emb_rref_list = []\n         index = 0\n         while index < NUM_PS:\n-            ps_name = \"ps{}\".format(index)\n+            ps_name = f\"ps{index}\"\n             emb_rref = rpc.remote(\n                 ps_name,\n                 torch.nn.EmbeddingBag,\n@@ -235,7 +235,7 @@ def run_worker(rank, world_size):\n         # Run training loop on the trainers.\n         futs = []\n         for trainer_rank in range(NUM_TRAINERS):\n-            trainer_name = \"trainer{}\".format(trainer_rank)\n+            trainer_name = f\"trainer{trainer_rank}\"\n             fut = rpc.rpc_async(\n                 trainer_name, _run_trainer, args=(emb_rref_list, trainer_rank)\n             )\n@@ -248,7 +248,7 @@ def run_worker(rank, world_size):\n         # Wait for all training to finish.\n         for fut in futs:\n             rank, measurements, batch_size = fut.wait()\n-            _print_benchmark(\"Trainer{}\".format(rank), batch_size, measurements)\n+            _print_benchmark(f\"Trainer{rank}\", batch_size, measurements)\n             batch_size_all_trainers += batch_size\n             measurements_all_trainers.append(measurements)\n \n@@ -266,7 +266,7 @@ def run_worker(rank, world_size):\n         )\n \n         # Initialize RPC. Trainer just waits for RPCs from master.\n-        trainer_name = \"trainer{}\".format(rank)\n+        trainer_name = f\"trainer{rank}\"\n         rpc.init_rpc(\n             trainer_name,\n             rank=rank,\n@@ -276,7 +276,7 @@ def run_worker(rank, world_size):\n \n     # Rank 8-15. Parameter Servers\n     elif rank >= NUM_TRAINERS and rank < NUM_TRAINERS + NUM_PS:\n-        ps_name = \"ps{}\".format(rank - NUM_TRAINERS)\n+        ps_name = f\"ps{rank - NUM_TRAINERS}\"\n         rpc.init_rpc(\n             ps_name,\n             rank=rank,\n@@ -299,8 +299,8 @@ def run_worker(rank, world_size):\n     print(\"                  Info                     \")\n     print(\"-------------------------------------------\")\n     print(\"\")\n-    print(\"* PyTorch version: {}\".format(torch.__version__))\n-    print(\"* CUDA version: {}\".format(torch.version.cuda))\n+    print(f\"* PyTorch version: {torch.__version__}\")\n+    print(f\"* CUDA version: {torch.version.cuda}\")\n     print(\"\")\n     print(\"------------ nvidia-smi topo -m -----------\")\n     print(\"\")\ndiff --git a/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py b/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py\nindex b8553997f756f0..d363fdc1a107d3 100644\n--- a/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py\n+++ b/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py\n@@ -24,7 +24,7 @@\n \n \n def opt_at(opt, idx):\n-    return list((opt.state.values()))[idx]\n+    return list(opt.state.values())[idx]\n \n \n def init_model():\ndiff --git a/torch/distributed/elastic/metrics/api.py b/torch/distributed/elastic/metrics/api.py\nindex 2ad55683c33da9..c363a89a7859e7 100644\n--- a/torch/distributed/elastic/metrics/api.py\n+++ b/torch/distributed/elastic/metrics/api.py\n@@ -162,14 +162,14 @@ def wrapper(*args, **kwargs):\n             try:\n                 start_time = time.time()\n                 result = func(*args, **kwargs)\n-                publish_metric(group, \"{}.success\".format(func.__name__), 1)\n+                publish_metric(group, f\"{func.__name__}.success\", 1)\n             except Exception:\n-                publish_metric(group, \"{}.failure\".format(func.__name__), 1)\n+                publish_metric(group, f\"{func.__name__}.failure\", 1)\n                 raise\n             finally:\n                 publish_metric(\n                     group,\n-                    \"{}.duration.ms\".format(func.__name__),\n+                    f\"{func.__name__}.duration.ms\",\n                     get_elapsed_time_ms(start_time),\n                 )\n             return result\ndiff --git a/torch/distributed/elastic/multiprocessing/errors/__init__.py b/torch/distributed/elastic/multiprocessing/errors/__init__.py\nindex 70c66e01c6acfd..303bbbb5546226 100644\n--- a/torch/distributed/elastic/multiprocessing/errors/__init__.py\n+++ b/torch/distributed/elastic/multiprocessing/errors/__init__.py\n@@ -109,7 +109,7 @@ def __post_init__(self):\n         self.error_file_data = _EMPTY_ERROR_DATA\n         if os.path.isfile(self.error_file):\n             try:\n-                with open(self.error_file, \"r\") as fp:\n+                with open(self.error_file) as fp:\n                     self.error_file_data = json.load(fp)\n                     log.debug(\n                         \"User process failed with error data: %s\", json.dumps(self.error_file_data, indent=2)\ndiff --git a/torch/distributed/elastic/multiprocessing/errors/error_handler.py b/torch/distributed/elastic/multiprocessing/errors/error_handler.py\nindex e38e26a60752c4..82b5dc71680379 100644\n--- a/torch/distributed/elastic/multiprocessing/errors/error_handler.py\n+++ b/torch/distributed/elastic/multiprocessing/errors/error_handler.py\n@@ -109,7 +109,7 @@ def dump_error_file(self, rootcause_error_file: str, error_code: int = 0):\n         \"\"\"\n         Dumps parent error file from child process's root cause error and error code.\n         \"\"\"\n-        with open(rootcause_error_file, \"r\") as fp:\n+        with open(rootcause_error_file) as fp:\n             rootcause_error = json.load(fp)\n             # Override error code since the child process cannot capture the error code if it\n             # is terminated by signals like SIGSEGV.\n@@ -144,7 +144,7 @@ def dump_error_file(self, rootcause_error_file: str, error_code: int = 0):\n     def _rm(self, my_error_file):\n         if os.path.isfile(my_error_file):\n             # Log the contents of the original file.\n-            with open(my_error_file, \"r\") as fp:\n+            with open(my_error_file) as fp:\n                 try:\n                     original = json.dumps(json.load(fp), indent=2)\n                     log.warning(\ndiff --git a/torch/distributed/elastic/multiprocessing/tail_log.py b/torch/distributed/elastic/multiprocessing/tail_log.py\nindex 8f4dc1cfa41cb7..8feccd95515188 100644\n--- a/torch/distributed/elastic/multiprocessing/tail_log.py\n+++ b/torch/distributed/elastic/multiprocessing/tail_log.py\n@@ -28,7 +28,7 @@ def tail_logfile(\n             return\n         time.sleep(interval_sec)\n \n-    with open(file, \"r\") as fp:\n+    with open(file) as fp:\n         while True:\n             line = fp.readline()\n \ndiff --git a/torch/distributed/elastic/rendezvous/etcd_rendezvous.py b/torch/distributed/elastic/rendezvous/etcd_rendezvous.py\nindex 9af7ffee3a2ee8..4afad1f3388df6 100644\n--- a/torch/distributed/elastic/rendezvous/etcd_rendezvous.py\n+++ b/torch/distributed/elastic/rendezvous/etcd_rendezvous.py\n@@ -1,5 +1,4 @@\n #!/usr/bin/env python3\n-# -*- coding: utf-8 -*-\n \n # Copyright (c) Facebook, Inc. and its affiliates.\n # All rights reserved.\n@@ -472,7 +471,7 @@ def try_create_rendezvous(self):\n \n         # Create directory node for participant data\n         self.client.write(\n-            key=self.get_path(\"/rdzv/v_{}\".format(version_counter.value)),\n+            key=self.get_path(f\"/rdzv/v_{version_counter.value}\"),\n             value=None,\n             dir=True,\n             prevExist=False,\n@@ -588,7 +587,7 @@ def confirm_membership(self, expected_version, this_rank):\n                 )\n \n             this_lease_key = self.get_path(\n-                \"/rdzv/v_{}/rank_{}\".format(expected_version, this_rank)\n+                f\"/rdzv/v_{expected_version}/rank_{this_rank}\"\n             )\n             self.client.set(this_lease_key, value=None, ttl=CONST_WORKER_KEEPALIVE_TTL)\n \n@@ -687,7 +686,7 @@ def wait_for_rendezvous_to_free(self, expected_version):\n             # its members are alive (renewing their lease).\n             # If not, try destroy this rendezvous, so a new one can be created.\n             alive_members = self.client.get(\n-                self.get_path(\"/rdzv/v_{version}\".format(version=expected_version))\n+                self.get_path(f\"/rdzv/v_{expected_version}\")\n             )\n             keep_alive_keys = [ch.key for ch in alive_members.children]\n \n@@ -905,7 +904,7 @@ def lease_worker(client, path, ttl, stop_event):\n         return lease_stop_event\n \n     def store_extra_data(self, rdzv_version, key, value):\n-        node = self.get_path(\"/rdzv/v_{}/extra_data\".format(rdzv_version))\n+        node = self.get_path(f\"/rdzv/v_{rdzv_version}/extra_data\")\n         try:\n             # If first time we are storing anything:\n             extra_data = self.client.write(\n@@ -936,8 +935,8 @@ def store_extra_data(self, rdzv_version, key, value):\n \n     def load_extra_data(self, rdzv_version, key, timeout=None):\n         # 'extra_data' node itself, and the directory it is located in:\n-        node = self.get_path(\"/rdzv/v_{}/extra_data\".format(rdzv_version))\n-        node_dir = self.get_path(\"/rdzv/v_{}\".format(rdzv_version))\n+        node = self.get_path(f\"/rdzv/v_{rdzv_version}/extra_data\")\n+        node_dir = self.get_path(f\"/rdzv/v_{rdzv_version}\")\n \n         # TODO: implement timeout\n         # https://github.com/pytorch/elastic/issues/12\ndiff --git a/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py b/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\nindex 547d526c019451..480f2ed3065583 100644\n--- a/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\n+++ b/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\n@@ -1,5 +1,4 @@\n #!/usr/bin/env python3\n-# -*- coding: utf-8 -*-\n \n # Copyright (c) Facebook, Inc. and its affiliates.\n # All rights reserved.\ndiff --git a/torch/distributed/elastic/timer/file_based_local_timer.py b/torch/distributed/elastic/timer/file_based_local_timer.py\nindex deca745fbaad4f..b562aad33f972f 100644\n--- a/torch/distributed/elastic/timer/file_based_local_timer.py\n+++ b/torch/distributed/elastic/timer/file_based_local_timer.py\n@@ -218,7 +218,7 @@ def _watchdog_loop(self) -> None:\n         #  1. No client case usually does not happen.\n         #  2. We are running the watchdog loop in a separate daemon\n         #     thread, which will not block the process to stop.\n-        with open(self._file_path, \"rt\") as fd:\n+        with open(self._file_path) as fd:\n             while not self._stop_signaled:\n                 try:\n                     run_once = self._run_once\ndiff --git a/torch/distributed/elastic/utils/store.py b/torch/distributed/elastic/utils/store.py\nindex 80f9763f0294be..9c7abab9291c61 100644\n--- a/torch/distributed/elastic/utils/store.py\n+++ b/torch/distributed/elastic/utils/store.py\n@@ -74,5 +74,5 @@ def barrier(\n     Note: Since the data is not removed from the store, the barrier can be used\n         once per unique ``key_prefix``.\n     \"\"\"\n-    data = f\"{rank}\".encode(encoding=\"UTF-8\")\n+    data = f\"{rank}\".encode()\n     synchronize(store, data, rank, world_size, key_prefix, barrier_timeout)\ndiff --git a/torch/distributed/nn/api/remote_module.py b/torch/distributed/nn/api/remote_module.py\nindex 0f6c1fed68b0ed..75d7e6215d5620 100644\n--- a/torch/distributed/nn/api/remote_module.py\n+++ b/torch/distributed/nn/api/remote_module.py\n@@ -115,7 +115,7 @@ def _param_rrefs(module_rref, recurse) -> List[rpc.RRef[Parameter]]:\n \n \n def _raise_not_supported(name: str) -> None:\n-    raise ValueError(\"Method ``{}`` not supported for RemoteModule\".format(name))\n+    raise ValueError(f\"Method ``{name}`` not supported for RemoteModule\")\n \n \n class _RemoteModule(nn.Module):\n@@ -123,7 +123,7 @@ class _RemoteModule(nn.Module):\n     def __new__(cls, *args, **kwargs):\n         # Use __new__ for logging purposes.\n         torch._C._log_api_usage_once(\"torch.distributed.nn.api.remote_module\")\n-        return super(_RemoteModule, cls).__new__(cls)\n+        return super().__new__(cls)\n \n     def __init__(\n         self,\ndiff --git a/torch/distributed/nn/jit/instantiator.py b/torch/distributed/nn/jit/instantiator.py\nindex 77f79cd3a0dedd..1b44f8dfd72cd2 100644\n--- a/torch/distributed/nn/jit/instantiator.py\n+++ b/torch/distributed/nn/jit/instantiator.py\n@@ -43,7 +43,7 @@ def get_arg_return_types_from_interface(module_interface):\n         arg_str_list.append(argument.name)\n \n         if argument.has_default_value():\n-            default_value_str = \" = {}\".format(argument.default_value)\n+            default_value_str = f\" = {argument.default_value}\"\n         else:\n             default_value_str = \"\"\n         arg_type_str = \"{name}: {type}{default_value}\".format(\n@@ -67,9 +67,9 @@ def get_arg_return_types_from_interface(module_interface):\n def _write(out_path, text):\n     old_text: Optional[str]\n     try:\n-        with open(out_path, \"r\") as f:\n+        with open(out_path) as f:\n             old_text = f.read()\n-    except IOError:\n+    except OSError:\n         old_text = None\n     if old_text != text:\n         with open(out_path, \"w\") as f:\ndiff --git a/torch/distributed/optim/functional_adam.py b/torch/distributed/optim/functional_adam.py\nindex 7ef64f674fb578..5d9a2d47213383 100644\n--- a/torch/distributed/optim/functional_adam.py\n+++ b/torch/distributed/optim/functional_adam.py\n@@ -32,15 +32,15 @@ def __init__(\n         _allow_empty_param_list: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         self.defaults = {\n             \"lr\": lr,\ndiff --git a/torch/distributed/optim/functional_adamax.py b/torch/distributed/optim/functional_adamax.py\nindex 0b0ac03b674430..74d2a4a48e8537 100644\n--- a/torch/distributed/optim/functional_adamax.py\n+++ b/torch/distributed/optim/functional_adamax.py\n@@ -30,15 +30,15 @@ def __init__(\n         _allow_empty_param_list: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         self.defaults = {\n             \"lr\": lr,\ndiff --git a/torch/distributed/optim/functional_adamw.py b/torch/distributed/optim/functional_adamw.py\nindex d0b65eba3299f7..75ca5bb8b19164 100644\n--- a/torch/distributed/optim/functional_adamw.py\n+++ b/torch/distributed/optim/functional_adamw.py\n@@ -32,15 +32,15 @@ def __init__(\n         _allow_empty_param_list: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         self.defaults = {\n             \"lr\": lr,\ndiff --git a/torch/distributed/pipeline/sync/_balance/blockpartition.py b/torch/distributed/pipeline/sync/_balance/blockpartition.py\nindex 0e74eff33a2212..7afe782f6ac8c7 100644\n--- a/torch/distributed/pipeline/sync/_balance/blockpartition.py\n+++ b/torch/distributed/pipeline/sync/_balance/blockpartition.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Copyright 2019 Kakao Brain\n #\n # Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\ndiff --git a/torch/distributed/pipeline/sync/pipeline.py b/torch/distributed/pipeline/sync/pipeline.py\nindex 705fa16e45a337..fe247f12b5d847 100644\n--- a/torch/distributed/pipeline/sync/pipeline.py\n+++ b/torch/distributed/pipeline/sync/pipeline.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Copyright 2019 Kakao Brain\n #\n # Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\ndiff --git a/torch/distributed/pipeline/sync/skip/portal.py b/torch/distributed/pipeline/sync/skip/portal.py\nindex 058b6a543a5308..f3484a1b69d57b 100644\n--- a/torch/distributed/pipeline/sync/skip/portal.py\n+++ b/torch/distributed/pipeline/sync/skip/portal.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Copyright 2019 Kakao Brain\n #\n # Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\ndiff --git a/torch/distributed/pipeline/sync/skip/skippable.py b/torch/distributed/pipeline/sync/skip/skippable.py\nindex 14fcec08266e2e..81662f25116c74 100644\n--- a/torch/distributed/pipeline/sync/skip/skippable.py\n+++ b/torch/distributed/pipeline/sync/skip/skippable.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Copyright 2019 Kakao Brain\n #\n # Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\ndiff --git a/torch/distributed/rendezvous.py b/torch/distributed/rendezvous.py\nindex 016d712f78c06b..3665b2ffca6b7c 100644\n--- a/torch/distributed/rendezvous.py\n+++ b/torch/distributed/rendezvous.py\n@@ -45,7 +45,7 @@ def register_rendezvous_handler(scheme, handler):\n     global _rendezvous_handlers\n     if scheme in _rendezvous_handlers:\n         raise RuntimeError(\n-            \"Rendezvous handler for {}:// already registered\".format(scheme)\n+            f\"Rendezvous handler for {scheme}:// already registered\"\n         )\n     _rendezvous_handlers[scheme] = handler\n \n@@ -79,25 +79,25 @@ def _rendezvous_helper(url: str, rank: int, world_size_opt: Optional[int], **kwa\n             query_dict[\"world_size\"] = str(world_size)\n         result = result._replace(\n             query=\"{}\".format(\n-                \"&\".join([\"{}={}\".format(k, v) for k, v in query_dict.items()])\n+                \"&\".join([f\"{k}={v}\" for k, v in query_dict.items()])\n             )\n         )\n         url = urlunparse(result)\n \n     if result.scheme not in _rendezvous_handlers:\n-        raise RuntimeError(\"No rendezvous handler for {}://\".format(result.scheme))\n+        raise RuntimeError(f\"No rendezvous handler for {result.scheme}://\")\n     return _rendezvous_handlers[result.scheme](url, **kwargs)\n \n \n def rendezvous(url: str, rank: int = -1, world_size: int = -1, **kwargs):\n     if not isinstance(url, (str, bytes)):\n-        raise RuntimeError(\"`url` must be a string. {}: {}\".format(type(url), url))\n+        raise RuntimeError(f\"`url` must be a string. {type(url)}: {url}\")\n \n     if not isinstance(rank, numbers.Integral):\n-        raise RuntimeError(\"`rank` must be an integer. {}\".format(rank))\n+        raise RuntimeError(f\"`rank` must be an integer. {rank}\")\n \n     if not isinstance(world_size, numbers.Integral):\n-        raise RuntimeError(\"`world_size` must be an integer. {}\".format(world_size))\n+        raise RuntimeError(f\"`world_size` must be an integer. {world_size}\")\n \n     return _rendezvous_helper(url, rank, world_size, **kwargs)\n \ndiff --git a/torch/distributed/rpc/__init__.py b/torch/distributed/rpc/__init__.py\nindex 8c5ddbd7efcac7..00c74b590c7348 100644\n--- a/torch/distributed/rpc/__init__.py\n+++ b/torch/distributed/rpc/__init__.py\n@@ -184,7 +184,7 @@ def init_rpc(\n         # Use a PrefixStore to distinguish multiple invocations.\n         with _init_counter_lock:\n             global _init_counter\n-            store = dist.PrefixStore(str(\"rpc_prefix_{}\".format(_init_counter)), store)\n+            store = dist.PrefixStore(str(f\"rpc_prefix_{_init_counter}\"), store)\n             _init_counter += 1\n \n         # Initialize autograd before RPC since _init_rpc_backend guarantees all\ndiff --git a/torch/distributed/rpc/_testing/faulty_agent_backend_registry.py b/torch/distributed/rpc/_testing/faulty_agent_backend_registry.py\nindex 0b9b5eeee0230f..e5b9a8645b773e 100644\n--- a/torch/distributed/rpc/_testing/faulty_agent_backend_registry.py\n+++ b/torch/distributed/rpc/_testing/faulty_agent_backend_registry.py\n@@ -32,7 +32,7 @@ def _faulty_tensorpipe_init_backend_handler(\n     from torch.distributed.rpc import api\n \n     if not isinstance(store, dist.Store):\n-        raise TypeError(\"`store` must be a c10d::Store. {}\".format(store))\n+        raise TypeError(f\"`store` must be a c10d::Store. {store}\")\n \n     if not isinstance(\n         rpc_backend_options, FaultyTensorPipeRpcBackendOptions\ndiff --git a/torch/distributed/rpc/api.py b/torch/distributed/rpc/api.py\nindex 35e21632b8957c..cdc40ab94709ec 100644\n--- a/torch/distributed/rpc/api.py\n+++ b/torch/distributed/rpc/api.py\n@@ -141,7 +141,7 @@ def _broadcast_to_followers(sequence_id, objects_map):\n \n     assert (\n         not states.proceed_signal.is_set()\n-    ), \"Termination signal sequence id {} got set twice.\".format(sequence_id)\n+    ), f\"Termination signal sequence id {sequence_id} got set twice.\"\n     states.gathered_objects = objects_map\n     states.proceed_signal.set()\n \n@@ -428,7 +428,7 @@ def _to_worker_info(to):\n     elif isinstance(to, (str, int)):\n         return get_worker_info(to)\n     else:\n-        raise ValueError(\"Cannot get WorkerInfo from name {}\".format(to))\n+        raise ValueError(f\"Cannot get WorkerInfo from name {to}\")\n \n \n def _rref_typeof_on_owner(rref, blocking: bool = True):\ndiff --git a/torch/distributed/rpc/backend_registry.py b/torch/distributed/rpc/backend_registry.py\nindex 216401ecb41080..42a4af68c0fed2 100644\n--- a/torch/distributed/rpc/backend_registry.py\n+++ b/torch/distributed/rpc/backend_registry.py\n@@ -69,7 +69,7 @@ def register_backend(\n     \"\"\"\n     global BackendType\n     if backend_registered(backend_name):\n-        raise RuntimeError(\"RPC backend {}: already registered\".format(backend_name))\n+        raise RuntimeError(f\"RPC backend {backend_name}: already registered\")\n     # Create a new enum type, `BackendType`, with extended members.\n     existing_enum_dict = {member.name: member.value for member in BackendType}\n     extended_enum_dict = dict(\n@@ -115,7 +115,7 @@ def _init_process_group(store, rank, world_size):\n \n     if (rank != -1) and (rank != group.rank()):\n         raise RuntimeError(\n-            \"rank argument {} doesn't match pg rank {}\".format(rank, group.rank())\n+            f\"rank argument {rank} doesn't match pg rank {group.rank()}\"\n         )\n     if (world_size != -1) and (world_size != group.size()):\n         raise RuntimeError(\n@@ -306,7 +306,7 @@ def _tensorpipe_init_backend_handler(store, name, rank, world_size, rpc_backend_\n     from . import TensorPipeAgent\n     from . import TensorPipeRpcBackendOptions\n     if not isinstance(store, dist.Store):\n-        raise TypeError(\"`store` must be a c10d::Store. {}\".format(store))\n+        raise TypeError(f\"`store` must be a c10d::Store. {store}\")\n \n     if not isinstance(\n         rpc_backend_options, TensorPipeRpcBackendOptions\n"
  },
  {
    "number": 105432,
    "title": "[BE] Enable ruff's UP rules and autoformat dynamo / functorch and refs",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* #105433\n* __->__ #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "f3795dec2c8688af775608a8dd05a9f889e1cff2",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105432",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105432/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105432.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105432.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105432/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105432/comments",
    "labels": [
      "open source",
      "release notes: fx",
      "ciflow/inductor",
      "module: dynamo",
      "module: export",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:22:31.374394Z",
    "state": "open",
    "patch": "From 75f4e37bd807f5b6fe7a90a6c93655f33f95d1ac Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:22:24 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat dynamo / functorch\n and refs\n\n[ghstack-poisoned]\n---\n functorch/benchmarks/operator_authoring.py    |  2 +-\n functorch/einops/rearrange.py                 |  4 +-\n .../examples/compilation/linear_train.py      |  2 +-\n .../maml_omniglot/support/omniglot_loaders.py |  4 +-\n functorch/op_analysis/gen_data.py             | 10 +--\n test/dynamo/test_autograd_function.py         |  4 +-\n test/dynamo/test_compile.py                   |  2 +-\n test/dynamo/test_logging.py                   |  2 +-\n test/dynamo/test_misc.py                      | 34 +++++-----\n test/dynamo/test_modules.py                   | 12 ++--\n test/dynamo/test_profiler.py                  |  4 +-\n test/dynamo/test_repros.py                    |  2 +-\n test/error_messages/storage.py                |  2 +-\n test/export/test_db.py                        |  6 +-\n test/export/test_serialize.py                 |  2 +-\n .../check_forward_backward_compatibility.py   |  2 +-\n test/functorch/test_aotdispatch.py            |  4 +-\n test/functorch/test_vmap.py                   |  2 +-\n test/fx/test_future.py                        |  4 +-\n test/fx/test_gradual_type.py                  |  8 +--\n torch/_decomp/decompositions.py               | 14 ++---\n torch/_dynamo/debug_utils.py                  | 16 ++---\n torch/_dynamo/eval_frame.py                   |  4 +-\n torch/_dynamo/output_graph.py                 |  2 +-\n torch/_dynamo/resume_execution.py             |  6 +-\n torch/_dynamo/symbolic_convert.py             |  4 +-\n torch/_dynamo/test_minifier_common.py         |  6 +-\n torch/_dynamo/variables/builder.py            | 10 ++-\n torch/_dynamo/variables/misc.py               |  2 +-\n torch/_export/verifier.py                     |  8 +--\n torch/_functorch/eager_transforms.py          |  2 +-\n torch/_functorch/pytree_hacks.py              |  2 +-\n torch/_prims/__init__.py                      | 62 +++++++++----------\n torch/_prims/executor.py                      |  2 +-\n torch/_prims/nvfuser_executor.py              |  4 +-\n torch/_prims_common/__init__.py               | 28 ++++-----\n torch/_prims_common/wrappers.py               |  8 +--\n torch/_refs/__init__.py                       | 46 +++++++-------\n torch/_refs/nn/functional/__init__.py         | 16 ++---\n .../migrate_gradual_types/constraint.py       |  1 -\n .../constraint_transformation.py              |  4 +-\n .../migrate_gradual_types/operation.py        |  1 -\n torch/fx/experimental/symbolic_shapes.py      |  4 +-\n .../multipledispatch/dispatcher.py            | 15 ++---\n torch/fx/interpreter.py                       |  2 +-\n torch/fx/passes/utils/matcher_utils.py        |  2 +-\n torch/fx/passes/utils/source_matcher_utils.py |  2 +-\n 47 files changed, 181 insertions(+), 204 deletions(-)\n\ndiff --git a/functorch/benchmarks/operator_authoring.py b/functorch/benchmarks/operator_authoring.py\nindex cbd816e2ad1324..456f5040d759f2 100644\n--- a/functorch/benchmarks/operator_authoring.py\n+++ b/functorch/benchmarks/operator_authoring.py\n@@ -113,7 +113,7 @@ def out_setup(n):\n def test_backwards(make_args, nnc=nnc_add, aten=torch.add):\n     def backwards_setup(n):\n         args = make_args(n)\n-        (grad_var,) = [a for a in args if a.requires_grad]\n+        (grad_var,) = (a for a in args if a.requires_grad)\n         aten(*args).sum().backward()\n         correct = grad_var.grad.clone()\n         grad_var.grad.zero_()\ndiff --git a/functorch/einops/rearrange.py b/functorch/einops/rearrange.py\nindex c45d2063c7114a..f8f60c4917b766 100644\n--- a/functorch/einops/rearrange.py\n+++ b/functorch/einops/rearrange.py\n@@ -108,7 +108,7 @@ class dims.\"\"\"\n \n     custom_rearrange_callable_name = \"do_rearrange\"\n     custom_rearrange_callable_code = (\n-        (\n+\n             f\"def {custom_rearrange_callable_name}(tensor):\\n\"\n             f\"    {comma_separate(first_class_dims)} = dims({n_dims})\\n\"\n             + (\n@@ -120,7 +120,7 @@ class dims.\"\"\"\n                 f\"    return tensor.sum({comma_separate([anon_dims])}, keepdim=False)\\n\"\n                 if anon_dims else \"    return tensor\\n\"\n             )\n-        )\n+\n     )\n \n     exec(custom_rearrange_callable_code)\ndiff --git a/functorch/examples/compilation/linear_train.py b/functorch/examples/compilation/linear_train.py\nindex 2d5f9d7dd37b44..ee84347470835b 100644\n--- a/functorch/examples/compilation/linear_train.py\n+++ b/functorch/examples/compilation/linear_train.py\n@@ -18,7 +18,7 @@ def bench(f, iters=100, warmup=10):\n     begin = time.time()\n     for _ in range(iters):\n         f()\n-    print((time.time() - begin))\n+    print(time.time() - begin)\n \n \n class Foo(nn.Module):\ndiff --git a/functorch/examples/maml_omniglot/support/omniglot_loaders.py b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\nindex ce636ecca0b1b2..6a4369ba4b208f 100644\n--- a/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n+++ b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n@@ -276,10 +276,10 @@ def load_data_cache(self, data_pack):\n             x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n             y_qrys = np.array(y_qrys).astype(int).reshape(self.batchsz, querysz)\n \n-            x_spts, y_spts, x_qrys, y_qrys = [\n+            x_spts, y_spts, x_qrys, y_qrys = (\n                 torch.from_numpy(z).to(self.device) for z in\n                 [x_spts, y_spts, x_qrys, y_qrys]\n-            ]\n+            )\n \n             data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n \ndiff --git a/functorch/op_analysis/gen_data.py b/functorch/op_analysis/gen_data.py\nindex a9cc84e6f9362c..ab1f3a79125c20 100644\n--- a/functorch/op_analysis/gen_data.py\n+++ b/functorch/op_analysis/gen_data.py\n@@ -23,7 +23,7 @@ def gen_data(special_op_lists, analysis_name):\n     composite_ops = get_ops_for_key('CompositeImplicitAutograd')\n     noncomposite_ops = all_ops - composite_ops\n \n-    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)\n+    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml').read(), Loader=yaml.CLoader)\n \n     annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}\n     from collections import defaultdict\n@@ -132,19 +132,19 @@ def remove_prefix(input_string, prefix):\n \n \n if True:\n-    with open('run_ops.txt', 'r') as f:\n+    with open('run_ops.txt') as f:\n         opinfo_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n-    with open('count_ops.txt', 'r') as f:\n+    with open('count_ops.txt') as f:\n         opinfo_counts = [i.strip() for i in f.readlines()]\n         opinfo_counts = defaultdict(int, dict(zip(opinfo_ops, opinfo_counts)))\n \n     def count_fn(x):\n         return opinfo_counts[x['full_name']]\n \n-    with open('run_decompositions.txt', 'r') as f:\n+    with open('run_decompositions.txt') as f:\n         decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n \n-    with open('public_api', 'r') as f:\n+    with open('public_api') as f:\n         ref_api = [i.strip() for i in f.readlines()]\n \n     def has_ref_impl(x):\ndiff --git a/test/dynamo/test_autograd_function.py b/test/dynamo/test_autograd_function.py\nindex 55165edd61a41d..7de264e5051743 100644\n--- a/test/dynamo/test_autograd_function.py\n+++ b/test/dynamo/test_autograd_function.py\n@@ -207,7 +207,7 @@ def backward(ctx, grad_output):\n \n class ModuleWithGradFunc(torch.nn.Module):\n     def __init__(self, func):\n-        super(ModuleWithGradFunc, self).__init__()\n+        super().__init__()\n         self.f = func.apply\n \n     def forward(self, x):\n@@ -336,7 +336,7 @@ def backward(ctx, grad_output):\n \n         class MyMod(torch.nn.Module):\n             def __init__(self):\n-                super(MyMod, self).__init__()\n+                super().__init__()\n                 self.gamma = torch.nn.Parameter(torch.rand([4, 128, 32, 32]))\n \n             def forward(self, x):\ndiff --git a/test/dynamo/test_compile.py b/test/dynamo/test_compile.py\nindex e3847cbb2ae121..5b2de2b7b3867f 100644\n--- a/test/dynamo/test_compile.py\n+++ b/test/dynamo/test_compile.py\n@@ -11,7 +11,7 @@\n \n class ToyModel(torch.nn.Module):\n     def __init__(self):\n-        super(ToyModel, self).__init__()\n+        super().__init__()\n         self.linear = torch.nn.Linear(10, 10)\n         self.relu = torch.nn.ReLU()\n \ndiff --git a/test/dynamo/test_logging.py b/test/dynamo/test_logging.py\nindex 183910f6db3510..eed99681e2c04e 100644\n--- a/test/dynamo/test_logging.py\n+++ b/test/dynamo/test_logging.py\n@@ -157,7 +157,7 @@ def throw(x):\n     def test_ddp_graphs(self, records):\n         class ToyModel(torch.nn.Module):\n             def __init__(self):\n-                super(ToyModel, self).__init__()\n+                super().__init__()\n                 self.layers = torch.nn.Sequential(\n                     torch.nn.Linear(1024, 1024),\n                     torch.nn.Linear(1024, 1024),\ndiff --git a/test/dynamo/test_misc.py b/test/dynamo/test_misc.py\nindex 6b020adc0d8007..6ec0acf8054fd3 100644\n--- a/test/dynamo/test_misc.py\n+++ b/test/dynamo/test_misc.py\n@@ -823,7 +823,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 3)\n@@ -837,7 +837,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 2)\n@@ -2202,7 +2202,7 @@ def fn():\n def fn():\n     foo.bar(1, 2, 3)\n {str(chr(10)).join(' ' * 4 + 'x' + str(i) + ' = 1' for i in range(1 << 9))}\n-    l = [{str(' ').join('x' + str(i) + ',' for i in range(1 << 9))}]\n+    l = [{' '.join('x' + str(i) + ',' for i in range(1 << 9))}]\n         \"\"\"\n         locals = {}\n         exec(fn_str, {}, locals)\n@@ -3087,7 +3087,7 @@ def foo(self, memo=None, prefix=\"\", remove_duplicate=False):\n                     memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n                 ):\n                     for pn, p in self.named_parameters():\n-                        fpn = \"%s.%s\" % (mn, pn) if mn else pn\n+                        fpn = f\"{mn}.{pn}\" if mn else pn\n                         self.names.append(fpn)\n \n         # Test plain recurse\n@@ -5032,11 +5032,11 @@ def test_compute_exception_table_nested(self):\n             (15, 16, 7),\n             (17, 17, 6),\n         ]\n-        self.assertEquals(len(tab), len(expected))\n+        self.assertEqual(len(tab), len(expected))\n         for entry, exp in zip(tab, expected):\n-            self.assertEquals(entry.start, exp[0] * 2)\n-            self.assertEquals(entry.end, exp[1] * 2)\n-            self.assertEquals(entry.target, exp[2] * 2)\n+            self.assertEqual(entry.start, exp[0] * 2)\n+            self.assertEqual(entry.end, exp[1] * 2)\n+            self.assertEqual(entry.target, exp[2] * 2)\n \n     @skipIfNotPy311\n     def test_remove_dead_code_with_exn_table_entries(self):\n@@ -5060,17 +5060,17 @@ def test_remove_dead_code_with_exn_table_entries(self):\n         )\n         bytecode_transformation.propagate_inst_exn_table_entries(insts)\n         insts = bytecode_analysis.remove_dead_code(insts)\n-        self.assertEquals(len(insts), 5)\n+        self.assertEqual(len(insts), 5)\n         self.assertNotIn(exn_start, insts)\n         self.assertNotIn(exn_end, insts)\n         self.assertIn(target2, insts)\n         self.assertIn(target3, insts)\n         bytecode_transformation.update_offsets(insts)\n         tab = bytecode_transformation.compute_exception_table(insts)\n-        self.assertEquals(len(tab), 1)\n-        self.assertEquals(tab[0].start, 2)\n-        self.assertEquals(tab[0].end, 4)\n-        self.assertEquals(tab[0].target, 6)\n+        self.assertEqual(len(tab), 1)\n+        self.assertEqual(tab[0].start, 2)\n+        self.assertEqual(tab[0].end, 4)\n+        self.assertEqual(tab[0].target, 6)\n \n     def test_unhandled_exception_in_dynamo(self):\n         # traceback.format_exc() approximates an unhandled exception\n@@ -5757,7 +5757,7 @@ def guard(L):\n     def test_dynamo_compiling_fake_tensor_to_vararg_int(self):\n         class MyModule(torch.nn.Module):\n             def __init__(self):\n-                super(MyModule, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 # use numpy int so it's wrapped as fake tensor in dynamo\n@@ -5776,7 +5776,7 @@ def forward(self, x):\n     def test_scalar_tensor_is_equivalent_to_symint_argument(self):\n         class GumbelTopKSampler(torch.nn.Module):\n             def __init__(self, T, k):\n-                super(GumbelTopKSampler, self).__init__()\n+                super().__init__()\n                 self.T = torch.nn.Parameter(\n                     torch.tensor(T, dtype=torch.float32), requires_grad=False\n                 )\n@@ -5803,7 +5803,7 @@ def forward(self, logits):\n     def test_scalar_tensor_is_equivalent_to_symint_list_argument(self):\n         class Jitter(torch.nn.Module):\n             def __init__(self, jitter_val):\n-                super(Jitter, self).__init__()\n+                super().__init__()\n                 self.jitter_val = jitter_val\n \n             def roll_tensor(self, input):\n@@ -5986,7 +5986,7 @@ def _prepare_for_translation_validator(self):\n \n         # Z3 symbols.\n         [validator.add_var(s, int) for s in (s0, s1, s2)]\n-        z0, z1, z2 = [validator.z3var(s) for s in (s0, s1, s2)]\n+        z0, z1, z2 = (validator.z3var(s) for s in (s0, s1, s2))\n \n         return (s0, s1, s2), (z0, z1, z2), validator\n \ndiff --git a/test/dynamo/test_modules.py b/test/dynamo/test_modules.py\nindex 03ef4f07305454..5a881d0053ec1d 100644\n--- a/test/dynamo/test_modules.py\n+++ b/test/dynamo/test_modules.py\n@@ -762,7 +762,7 @@ def forward(self, x):\n \n class ConvCallSuperForwardDirectly(torch.nn.Conv1d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -770,13 +770,13 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n         )\n \n     def forward(self, inputs, mask=None):\n-        outputs = super(ConvCallSuperForwardDirectly, self).forward(inputs)\n+        outputs = super().forward(inputs)\n         return outputs\n \n \n class ConvTransposeCallSuperForwardDirectly(torch.nn.ConvTranspose2d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvTransposeCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -785,7 +785,7 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n \n     def forward(self, x):\n         if x.numel() > 0:\n-            return super(ConvTransposeCallSuperForwardDirectly, self).forward(x)\n+            return super().forward(x)\n         output_shape = [\n             ((i - 1) * d - 2 * p + (di * (k - 1) + 1) + op)\n             for i, p, di, k, d, op in zip(\n@@ -923,7 +923,7 @@ def forward(self, x):\n class SequentialWithDuplicatedModule(torch.nn.Module):\n     # Sequential module(self.layer) contains three duplicated ReLU module.\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             torch.nn.Linear(10, 20),\n@@ -940,7 +940,7 @@ def forward(self, x):\n \n class SequentialWithDuplicatedModule2(torch.nn.Module):\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule2, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             collections.OrderedDict(\ndiff --git a/test/dynamo/test_profiler.py b/test/dynamo/test_profiler.py\nindex 7f58d99863d093..bec7adb33eda98 100644\n--- a/test/dynamo/test_profiler.py\n+++ b/test/dynamo/test_profiler.py\n@@ -20,7 +20,7 @@ def inner_fn(x):\n         def outer_fn(x, y):\n             return inner_fn(x) * y\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             outer_fn(x, y)\n@@ -40,7 +40,7 @@ def test_dynamo_timed_profiling_backend_compile(self):\n         def fn(x, y):\n             return x.sin() * y.cos()\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             torch._dynamo.optimize(\"aot_eager\")(fn)(x, y)\ndiff --git a/test/dynamo/test_repros.py b/test/dynamo/test_repros.py\nindex 2e84776ea76580..77d8859541472c 100644\n--- a/test/dynamo/test_repros.py\n+++ b/test/dynamo/test_repros.py\n@@ -2632,7 +2632,7 @@ def test_error_return_without_exception_set(self):\n         # https://github.com/pytorch/pytorch/issues/93781\n         @torch.compile\n         def f():\n-            _generator_type = type((_ for _ in ()))\n+            _generator_type = type(_ for _ in ())\n \n         self.assertNoUnraisable(f)\n \ndiff --git a/test/error_messages/storage.py b/test/error_messages/storage.py\nindex f3053d862a220c..b33b86e0908a95 100644\n--- a/test/error_messages/storage.py\n+++ b/test/error_messages/storage.py\n@@ -14,7 +14,7 @@ def check_error(desc, fn, *required_substrings):\n         for sub in required_substrings:\n             assert sub in error_message\n         return\n-    raise AssertionError(\"given function ({}) didn't raise an error\".format(desc))\n+    raise AssertionError(f\"given function ({desc}) didn't raise an error\")\n \n check_error(\n     'Wrong argument types',\ndiff --git a/test/export/test_db.py b/test/export/test_db.py\nindex 10d149e096be9d..bfa57baf214c8f 100644\n--- a/test/export/test_db.py\n+++ b/test/export/test_db.py\n@@ -23,7 +23,7 @@ class ExampleTests(TestCase):\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.SUPPORTED).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -51,7 +51,7 @@ def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.NOT_SUPPORTED_YET).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -73,7 +73,7 @@ def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n             ).items()\n             for rewrite_case in get_rewrite_cases(case)\n         ],\n-        name_fn=lambda name, case: \"case_{}_{}\".format(name, case.name),\n+        name_fn=lambda name, case: f\"case_{name}_{case.name}\",\n     )\n     def test_exportdb_not_supported_rewrite(\n         self, name: str, rewrite_case: ExportCase\ndiff --git a/test/export/test_serialize.py b/test/export/test_serialize.py\nindex 01bb32ad2c791b..c3942936e2bc55 100644\n--- a/test/export/test_serialize.py\n+++ b/test/export/test_serialize.py\n@@ -361,7 +361,7 @@ def f(x, y):\n     @parametrize(\n         \"name,case\",\n         get_filtered_export_db_tests(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\ndiff --git a/test/forward_backward_compatibility/check_forward_backward_compatibility.py b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\nindex 886ad32a24bafb..cf4ce8def1adb7 100644\n--- a/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n+++ b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n@@ -501,7 +501,7 @@ def check_fc(existing_schemas):\n     args = parser.parse_args()\n     existing_schema_dict = {}\n     slist = []\n-    with open(args.existing_schemas, \"r\") as f:\n+    with open(args.existing_schemas) as f:\n         while True:\n             line = f.readline()\n             if not line:\ndiff --git a/test/functorch/test_aotdispatch.py b/test/functorch/test_aotdispatch.py\nindex e4357ce0bcafd8..ca92201fedc5ea 100644\n--- a/test/functorch/test_aotdispatch.py\n+++ b/test/functorch/test_aotdispatch.py\n@@ -1805,8 +1805,8 @@ def test_batch_norm_amp(self):\n         device = \"cuda\"\n         input_dtype = torch.float16\n         param_dtype = torch.float32\n-        weight, bias = [torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2)]\n-        running_mean, running_var = [torch.ones(64, device=device, dtype=param_dtype) for _ in range(2)]\n+        weight, bias = (torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2))\n+        running_mean, running_var = (torch.ones(64, device=device, dtype=param_dtype) for _ in range(2))\n \n         def bn(x):\n             return torch.ops.aten.cudnn_batch_norm(\ndiff --git a/test/functorch/test_vmap.py b/test/functorch/test_vmap.py\nindex a0f6e077004344..81b980edd6d006 100644\n--- a/test/functorch/test_vmap.py\n+++ b/test/functorch/test_vmap.py\n@@ -3438,7 +3438,7 @@ def test():\n             check_shape_only = op.name in ('empty_like', 'new_empty')\n             for sample_input in sample_inputs_itr:\n                 args = (sample_input.input,) + sample_input.args\n-                if not any((isinstance(arg, torch.Tensor) for arg in args)):\n+                if not any(isinstance(arg, torch.Tensor) for arg in args):\n                     # Atleast one tensor required for vmap.\n                     continue\n                 kwargs = sample_input.kwargs\ndiff --git a/test/fx/test_future.py b/test/fx/test_future.py\nindex 4f093de54b4f84..4525f678eaeb6c 100644\n--- a/test/fx/test_future.py\n+++ b/test/fx/test_future.py\n@@ -16,7 +16,7 @@ def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n \n # Forward references\n class M2(torch.nn.Module):\n-    def forward(self, x: 'torch.Tensor', a: 'A') -> 'torch.Tensor':\n+    def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n         return a(x)\n \n # Non-torch annotation with no internal forward references\n@@ -26,7 +26,7 @@ def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n \n # Non-torch annotation with internal forward references\n class M4(torch.nn.Module):\n-    def forward(self, x: typing.List['torch.Tensor'], a: A) -> 'torch.Tensor':\n+    def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n         return a(x[0])\n \n x = torch.rand(2, 3)\ndiff --git a/test/fx/test_gradual_type.py b/test/fx/test_gradual_type.py\nindex 23c6496b3a294f..e3f83756eb2668 100644\n--- a/test/fx/test_gradual_type.py\n+++ b/test/fx/test_gradual_type.py\n@@ -990,12 +990,12 @@ def forward(self, x : TensorType((4, 3, Dyn, Dyn))):\n \n         for n in traced.graph.nodes:\n             if n.target == 'conv1':\n-                assert n.type == TensorType((4, 6, sympy.floor((sympy.symbols('~0') - 4)),\n-                                             sympy.floor((sympy.symbols('~1') - 4))))\n+                assert n.type == TensorType((4, 6, sympy.floor(sympy.symbols('~0') - 4),\n+                                             sympy.floor(sympy.symbols('~1') - 4)))\n \n             elif n.target == 'conv2':\n-                assert n.type == TensorType((4, 16, sympy.floor((sympy.symbols('~4') - 4)),\n-                                             sympy.floor((sympy.symbols('~5') - 4))))\n+                assert n.type == TensorType((4, 16, sympy.floor(sympy.symbols('~4') - 4),\n+                                             sympy.floor(sympy.symbols('~5') - 4)))\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py\nindex 70c69ff5cef47a..f6b268abc72a3f 100644\n--- a/torch/_decomp/decompositions.py\n+++ b/torch/_decomp/decompositions.py\n@@ -1331,10 +1331,10 @@ def native_layer_norm_backward(\n     input_shape = input.shape\n     input_ndim = input.dim()\n     computation_dtype = utils.get_computation_dtype(input.dtype)\n-    grad_out_cast, input_cast, weight_cast, bias_cast = [\n+    grad_out_cast, input_cast, weight_cast, bias_cast = (\n         x.to(computation_dtype).contiguous() if x is not None else x\n         for x in (grad_out, input, weight, bias)\n-    ]\n+    )\n     assert grad_out_cast is not None\n \n     axis = input_ndim - len(normalized_shape)\n@@ -1745,7 +1745,7 @@ def native_batch_norm_backward(\n         running_var_cast,\n         save_mean_cast,\n         save_invstd_cast,\n-    ) = [\n+    ) = (\n         x.to(computation_dtype) if x is not None else x\n         for x in (\n             grad_out,\n@@ -1756,7 +1756,7 @@ def native_batch_norm_backward(\n             save_mean,\n             save_invstd,\n         )\n-    ]\n+    )\n     input_shape = input.shape\n     input_rank = input.dim()\n     assert input_rank >= 2, \"rank of the input must be at least 2\"\n@@ -3123,7 +3123,7 @@ def get_coeff(ofs: int) -> Tensor:\n             )\n             return _upsample_cubic_interp1d(cs, tx.unsqueeze(1))\n \n-        coeffs = tuple((get_coeff(ofs) for ofs in range(4)))\n+        coeffs = tuple(get_coeff(ofs) for ofs in range(4))\n         return _upsample_cubic_interp1d(coeffs, ty.unsqueeze(1))\n \n \n@@ -3371,10 +3371,10 @@ def load_bounded(ys, xs):\n         return aten._unsafe_index(a, [N_idx, C_idx, y_idx, x_idx])\n \n     def get_x_interp(y):\n-        coeffs_x = tuple((load_bounded(y, x_ofs) for x_ofs in ixs_ofs))\n+        coeffs_x = tuple(load_bounded(y, x_ofs) for x_ofs in ixs_ofs)\n         return _upsample_cubic_interp1d(coeffs_x, t_x)\n \n-    coeffs_y = tuple((get_x_interp(y_ofs) for y_ofs in iys_ofs))\n+    coeffs_y = tuple(get_x_interp(y_ofs) for y_ofs in iys_ofs)\n     result = _upsample_cubic_interp1d(coeffs_y, t_y)\n \n     # convert output to correct memory format, if necessary\ndiff --git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py\nindex 4d7c98aa222536..bf7a61b0793654 100644\n--- a/torch/_dynamo/debug_utils.py\n+++ b/torch/_dynamo/debug_utils.py\n@@ -385,11 +385,9 @@ def same_two_models(\n         # This means that the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return True.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph.\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph.\"\n         )\n         return True\n \n@@ -465,11 +463,9 @@ def backend_accuracy_fails(\n         # This means that the the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return False.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph\"\n         )\n         return False\n \ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..0bb1dbf7342a44 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -750,9 +750,7 @@ def __init__(\n \n         self.new_args = []\n         for i in range(0, len(flat_args)):\n-            arg = super(FlattenInputOutputSignature, self).placeholder(\n-                f\"arg{i}\", (), {}\n-            )\n+            arg = super().placeholder(f\"arg{i}\", (), {})\n             if i in matched_input_elements_to_fake:\n                 arg.node.meta[\"val\"] = matched_input_elements_to_fake[i]\n             else:\ndiff --git a/torch/_dynamo/output_graph.py b/torch/_dynamo/output_graph.py\nindex f6527f9b2de356..667b2363cff694 100644\n--- a/torch/_dynamo/output_graph.py\n+++ b/torch/_dynamo/output_graph.py\n@@ -1073,7 +1073,7 @@ class SubgraphTracer(fx.Tracer):\n     \"\"\"\n \n     def __init__(self, output_graph, parent=None):\n-        super(SubgraphTracer, self).__init__()\n+        super().__init__()\n         self.output_graph = weakref.proxy(output_graph)\n         self.graph = torch.fx.Graph()\n         # Map from graph input name to its placeholder proxy object, where the\ndiff --git a/torch/_dynamo/resume_execution.py b/torch/_dynamo/resume_execution.py\nindex a3344f3d69bca8..f3eafba1979470 100644\n--- a/torch/_dynamo/resume_execution.py\n+++ b/torch/_dynamo/resume_execution.py\n@@ -490,13 +490,13 @@ def find_new_offset(\n             instructions: List[Instruction], code_options: Dict[str, Any]\n         ):\n             nonlocal new_offset\n-            (target,) = [i for i in instructions if i.offset == offset]\n+            (target,) = (i for i in instructions if i.offset == offset)\n             # match the functions starting at the last instruction as we have added a prefix\n-            (new_target,) = [\n+            (new_target,) = (\n                 i2\n                 for i1, i2 in zip(reversed(instructions), reversed(meta.instructions))\n                 if i1 is target\n-            ]\n+            )\n             assert target.opcode == new_target.opcode\n             new_offset = new_target.offset\n \ndiff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py\nindex 75a44965b63a3e..46d7526ebbc57d 100644\n--- a/torch/_dynamo/symbolic_convert.py\n+++ b/torch/_dynamo/symbolic_convert.py\n@@ -888,7 +888,7 @@ def resolve_name(self, name, package, level):\n         if len(bits) < level:\n             raise ImportError(\"attempted relative import beyond top-level package\")\n         base = bits[0]\n-        return \"{}.{}\".format(base, name) if name else base\n+        return f\"{base}.{name}\" if name else base\n \n     def calc_package(self):\n         \"\"\"\n@@ -1840,7 +1840,7 @@ def format_frame_summary(self, additional_stack_frames=None):\n             additional_stack_frames = []\n         return \"\".join(\n             traceback.format_list(\n-                ([self.frame_summary()] + list(reversed(additional_stack_frames)))\n+                [self.frame_summary()] + list(reversed(additional_stack_frames))\n             )\n         )\n \ndiff --git a/torch/_dynamo/test_minifier_common.py b/torch/_dynamo/test_minifier_common.py\nindex 757e92d2f23b51..e1eadd6da8a595 100644\n--- a/torch/_dynamo/test_minifier_common.py\n+++ b/torch/_dynamo/test_minifier_common.py\n@@ -86,7 +86,7 @@ def _maybe_subprocess_run(self, args, *, isolate, cwd=None):\n                 args = [\"-c\"]\n             else:\n                 assert len(args) >= 2, args\n-                with open(args[1], \"r\") as f:\n+                with open(args[1]) as f:\n                     code = f.read()\n                 args = args[1:]\n \n@@ -156,7 +156,7 @@ def _run_test_code(self, code, *, isolate):\n     def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n         self.assertIsNotNone(repro_dir)\n         launch_file = os.path.join(repro_dir, \"minifier_launcher.py\")\n-        with open(launch_file, \"r\") as f:\n+        with open(launch_file) as f:\n             launch_code = f.read()\n         self.assertTrue(os.path.exists(launch_file))\n \n@@ -175,7 +175,7 @@ def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n     def _run_repro(self, repro_dir, *, isolate=True):\n         self.assertIsNotNone(repro_dir)\n         repro_file = os.path.join(repro_dir, \"repro.py\")\n-        with open(repro_file, \"r\") as f:\n+        with open(repro_file) as f:\n             repro_code = f.read()\n         self.assertTrue(os.path.exists(repro_file))\n \ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex d2aab5a65bd4d4..c720a0e637c478 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -368,12 +368,10 @@ def _wrap(self, value):\n         elif istype(\n             value, (dict, collections.defaultdict, collections.OrderedDict)\n         ) and all(\n-            (\n-                ConstantVariable.is_literal(k)\n-                or self.tensor_can_be_dict_key(k)\n-                or isinstance(k, enum.Enum)\n-                for k in value.keys()\n-            )\n+            ConstantVariable.is_literal(k)\n+            or self.tensor_can_be_dict_key(k)\n+            or isinstance(k, enum.Enum)\n+            for k in value.keys()\n         ):\n             if not value and self.get_source().is_nn_module():\n                 # It is faster to guard on 'false' property than to guard\ndiff --git a/torch/_dynamo/variables/misc.py b/torch/_dynamo/variables/misc.py\nindex b260eab0af0fdb..e049ccfb25269d 100644\n--- a/torch/_dynamo/variables/misc.py\n+++ b/torch/_dynamo/variables/misc.py\n@@ -880,7 +880,7 @@ def as_proxy(self):\n # Used to keep track of NULLs pushed on the stack for Python 3.11 function calls\n class NullVariable(VariableTracker):\n     def __init__(self, **kwargs):\n-        super(NullVariable, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n     def __str__(self):\n         return \"NullVariable\"\ndiff --git a/torch/_export/verifier.py b/torch/_export/verifier.py\nindex 41888230e02242..73906bb6a0d7a8 100644\n--- a/torch/_export/verifier.py\n+++ b/torch/_export/verifier.py\n@@ -38,7 +38,7 @@ def _check_is_fake_tensor(val):\n \n     val = node.meta.get(\"val\", None)\n     if val is None or not _check_is_fake_tensor(val):\n-        raise SpecViolationError(\"Node.meta {} is missing val field.\".format(node.name))\n+        raise SpecViolationError(f\"Node.meta {node.name} is missing val field.\")\n \n \n @compatibility(is_backward_compatible=False)\n@@ -71,7 +71,7 @@ def check_valid_op(self, op):\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         # All ops functional\n@@ -87,7 +87,7 @@ def check_valid(self, gm: GraphModule) -> None:  # noqa: C901\n             # TODO(T140410192): should have fake tensor for all dialects\n             if node.op in {\"call_module\", \"call_method\"}:\n                 raise SpecViolationError(\n-                    \"call_module is not valid: got a class '{}' \".format(node.target),\n+                    f\"call_module is not valid: got a class '{node.target}' \",\n                 )\n \n             if node.op == \"call_function\":\n@@ -122,7 +122,7 @@ def check_valid_op(self, op) -> None:\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         if (\ndiff --git a/torch/_functorch/eager_transforms.py b/torch/_functorch/eager_transforms.py\nindex a25a7bc456bec1..4ba72eb2152c6b 100644\n--- a/torch/_functorch/eager_transforms.py\n+++ b/torch/_functorch/eager_transforms.py\n@@ -548,7 +548,7 @@ def compute_jacobian_stacked():\n             # Iterate and concat the jacobians of different\n             # inputs.\n             for idx in range(len(flat_primals)):\n-                r = tuple((r_[idx] for r_ in chunked_results))\n+                r = tuple(r_[idx] for r_ in chunked_results)\n                 flat_results.append(torch.cat(r, 0))\n \n             return flat_results\ndiff --git a/torch/_functorch/pytree_hacks.py b/torch/_functorch/pytree_hacks.py\nindex 3694a53d7debb0..61bcdfbbf38b16 100644\n--- a/torch/_functorch/pytree_hacks.py\n+++ b/torch/_functorch/pytree_hacks.py\n@@ -13,7 +13,7 @@ def tree_map_(fn_, pytree):\n     return pytree\n \n \n-class PlaceHolder():\n+class PlaceHolder:\n     def __repr__(self):\n         return '*'\n \ndiff --git a/torch/_prims/__init__.py b/torch/_prims/__init__.py\nindex 7e8a37da76b06d..ac447dab410a07 100644\n--- a/torch/_prims/__init__.py\n+++ b/torch/_prims/__init__.py\n@@ -1437,7 +1437,7 @@ def expand_dims(\n     else:\n         dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]\n     if len(set(dims)) != len(dims):\n-        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))\n+        msg = f\"Received duplicate dimensions to expand in {str(dimensions)}\"\n         raise ValueError(msg)\n \n     new_shape = list(a.shape)\n@@ -1463,35 +1463,33 @@ def _slice_meta(\n     _strides = strides if strides is not None else [1] * len(start_indices)\n \n     if a.ndim != len(start_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with start_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with start_indices of length {}!\".format(\n             a.ndim, len(start_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(limit_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with limit_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with limit_indices of length {}!\".format(\n             a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(_strides):\n-        msg = (\n-            \"Attempting to slice tensor of rank {0} with strides of length {1}!\".format(\n-                a.ndim, len(limit_indices)\n-            )\n+        msg = \"Attempting to slice tensor of rank {} with strides of length {}!\".format(\n+            a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     for x, y in zip(start_indices, a.shape):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative start index of {0}!\".format(\n+            msg = \"Attempting to slice a tensor with a negative start index of {}!\".format(\n                 x\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than\"\n-                \" the length of its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a start index in {} is greater than\"\n+                \" the length of its corresponding dimension in shape {}\".format(\n                     start_indices, a.shape\n                 )\n             )\n@@ -1499,30 +1497,30 @@ def _slice_meta(\n \n     for x, y, z in zip(limit_indices, a.shape, start_indices):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative stop index of {0}!\".format(\n-                x\n+            msg = (\n+                \"Attempting to slice a tensor with a negative stop index of {}!\".format(\n+                    x\n+                )\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a stop index in {0} is greater than the length of \"\n-                \" its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a stop index in {} is greater than the length of \"\n+                \" its corresponding dimension in shape {}\".format(\n                     limit_indices, a.shape\n                 )\n             )\n             raise ValueError(msg)\n         if x < z:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than \"\n-                \" its corresponding stop index {1}\".format(x, z)\n+                \"Attempting to slice a tensor but a start index in {} is greater than \"\n+                \" its corresponding stop index {}\".format(x, z)\n             )\n \n     for x in _strides:\n         if x <= 0:\n-            msg = (\n-                \"Attempting to slice a tensor with a non-positive step of {0}!\".format(\n-                    x\n-                )\n+            msg = \"Attempting to slice a tensor with a non-positive step of {}!\".format(\n+                x\n             )\n             raise ValueError(msg)\n \n@@ -1581,38 +1579,38 @@ def _slice_in_dim_meta(\n     axis: int = 0,\n ) -> TensorLikeType:\n     if axis < 0:\n-        msg = \"slice_in_dim: received a negative axis {0}\".format(axis)\n+        msg = f\"slice_in_dim: received a negative axis {axis}\"\n         raise ValueError(msg)\n     if axis >= a.ndim:\n-        msg = \"slice_in_dim: axis {0} is greater or equal to the rank {1} of the tensor\".format(\n+        msg = \"slice_in_dim: axis {} is greater or equal to the rank {} of the tensor\".format(\n             axis, a.ndim\n         )\n         raise ValueError(msg)\n \n     if start_index < 0:\n-        msg = \"slice_in_dim: received a negative start_index {0}\".format(start_index)\n+        msg = f\"slice_in_dim: received a negative start_index {start_index}\"\n         raise ValueError(msg)\n \n     if start_index > a.shape[axis]:\n-        msg = \"slice_in_dim: start_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: start_index is greater than the length {} of dimension {}\".format(\n             start_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index > a.shape[axis]:\n-        msg = \"slice_in_dim: limit_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: limit_index is greater than the length {} of dimension {}\".format(\n             limit_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index < start_index:\n-        msg = \"slice_in_dim: received a limit_index {0} less than the start_index {1}\".format(\n+        msg = \"slice_in_dim: received a limit_index {} less than the start_index {}\".format(\n             limit_index, start_index\n         )\n         raise ValueError(msg)\n \n     if stride < 0:\n-        msg = \"slice_in_dim: received a non-positive stride of {0}!\".format(stride)\n+        msg = f\"slice_in_dim: received a non-positive stride of {stride}!\"\n         raise ValueError(msg)\n \n     start_indices = [0] * a.ndim\n@@ -1667,7 +1665,7 @@ def _split_dim_meta(a: TensorLikeType, dim: int, outer_length: int) -> TensorLik\n     inner_length = a.shape[dim] // outer_length\n \n     if (a.shape[dim] % outer_length) != 0:\n-        msg = \"Attempting to split dimension of length {0}, but outer length of {1} divides it with a remainder!\".format(\n+        msg = \"Attempting to split dimension of length {}, but outer length of {} divides it with a remainder!\".format(\n             a.shape[dim], outer_length\n         )\n         raise ValueError(msg)\n@@ -1746,13 +1744,13 @@ def _squeeze_meta(a: TensorLikeType, dimensions: Sequence) -> TensorLikeType:\n \n def _transpose_meta(a: TensorLikeType, permutation: DimsSequenceType) -> TensorLikeType:\n     if a.ndim != len(permutation):\n-        msg = \"Attempting to permute a tensor of rank {0}, but received a permutation of length {1}!\".format(\n+        msg = \"Attempting to permute a tensor of rank {}, but received a permutation of length {}!\".format(\n             a.ndim, len(permutation)\n         )\n         raise ValueError(msg)\n \n     if not utils.is_valid_permutation(a.ndim, permutation):\n-        msg = \"Received an invalid permutation, {0}!\".format(permutation)\n+        msg = f\"Received an invalid permutation, {permutation}!\"\n         raise ValueError(msg)\n \n     new_shape = [0] * a.ndim\n@@ -1938,7 +1936,7 @@ def _reshape_meta(a: TensorLikeType, shape: ShapeType):\n     # same number of elements\n     numel = reduce(operator.mul, shape)\n     if numel != a.numel():\n-        msg = \"Attempting to reshape a tensor with {0} elements to a shape with {1} elements!\".format(\n+        msg = \"Attempting to reshape a tensor with {} elements to a shape with {} elements!\".format(\n             a.numel(), numel\n         )\n         raise ValueError(msg)\n@@ -2190,7 +2188,7 @@ def _copy_to_meta(a: TensorLikeType, b: TensorLikeType):\n \n     # Validates the tensors have the same number of elements\n     if a.numel() != b.numel():\n-        msg = \"Attempting to copy {0} elements to a tensor with {1} elements!\".format(\n+        msg = \"Attempting to copy {} elements to a tensor with {} elements!\".format(\n             b.numel(), a.numel()\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_prims/executor.py b/torch/_prims/executor.py\nindex 2d8d815f063809..325ac67a665cc3 100644\n--- a/torch/_prims/executor.py\n+++ b/torch/_prims/executor.py\n@@ -28,7 +28,7 @@ def execute(\n     elif executor == \"strictly_nvfuser\":\n         return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)\n \n-    msg = \"Received unexpected value for 'executor': {0}. Allowed values are: aten, nvfuser.\".format(\n+    msg = \"Received unexpected value for 'executor': {}. Allowed values are: aten, nvfuser.\".format(\n         executor\n     )\n     raise ValueError(msg)\ndiff --git a/torch/_prims/nvfuser_executor.py b/torch/_prims/nvfuser_executor.py\nindex d0f51e928650c4..c1e61c1bb72f1f 100644\n--- a/torch/_prims/nvfuser_executor.py\n+++ b/torch/_prims/nvfuser_executor.py\n@@ -282,7 +282,7 @@ def nvfuser_execute(gm: GraphModule, *args, executor_parameters=None):\n \n         if get_nvprim_dump_nvtx():\n             torch.cuda.nvtx.range_push(\n-                \"fusion: {0}, graph: {1}\".format(\n+                \"fusion: {}, graph: {}\".format(\n                     fusion.id(),\n                     str(\n                         [\n@@ -475,7 +475,7 @@ def maybe_partition_graph(\n class NVTXInterpreter(torch.fx.Interpreter):\n     def run_node(self, n):\n         torch.cuda.nvtx.range_push(\n-            \"name: {0}, args: {1}, op: {2}, kwargs: {3}\".format(\n+            \"name: {}, args: {}, op: {}, kwargs: {}\".format(\n                 n.name, n.args, n.op, n.kwargs\n             )\n         )\ndiff --git a/torch/_prims_common/__init__.py b/torch/_prims_common/__init__.py\nindex f8033bb5780f74..4800966f3e2ff5 100644\n--- a/torch/_prims_common/__init__.py\n+++ b/torch/_prims_common/__init__.py\n@@ -120,11 +120,11 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n     assert isinstance(b, TensorLike)\n \n     if not same_shape(a.shape, b.shape):\n-        msg = \"Shapes {0} and {1} are not equal!\".format(a.shape, b.shape)\n+        msg = f\"Shapes {a.shape} and {b.shape} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.dtype != b.dtype:\n-        msg = \"Dtypes {0} and {1} are not equal!\".format(a.dtype, b.dtype)\n+        msg = f\"Dtypes {a.dtype} and {b.dtype} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.device != b.device:\n@@ -135,7 +135,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         ):\n             pass\n         else:\n-            msg = \"Devices {0} and {1} are not equal!\".format(a.device, b.device)\n+            msg = f\"Devices {a.device} and {b.device} are not equal!\"\n             raise AssertionError(msg)\n \n     # Stride checking is currently disabled, see https://github.com/pytorch/pytorch/issues/78050\n@@ -143,7 +143,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         same_strides, idx = check_significant_strides(a, b)\n         if not same_strides:\n             msg = (\n-                \"Stride mismatch! Strides are {0} and {1} (mismatched at {2})!\".format(\n+                \"Stride mismatch! Strides are {} and {} (mismatched at {})!\".format(\n                     a.stride(), b.stride(), idx\n                 )\n             )\n@@ -151,7 +151,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n \n         if a.storage_offset() != b.storage_offset():\n             msg = (\n-                \"Storage offset mismatch! Storage offsets are {0} and {1}!\".format(\n+                \"Storage offset mismatch! Storage offsets are {} and {}!\".format(\n                     a.storage_offset(), b.storage_offset()\n                 )\n             )\n@@ -584,7 +584,7 @@ def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int:\n \n     if _idx < 0 or _idx >= rank:\n         # Same error message as in aten/src/ATen/WrapDimUtils.h:49\n-        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(\n+        msg = \"Dimension out of range (expected to be in range of [{}, {}], but got {})\".format(\n             -rank, rank - 1, idx\n         )\n         raise IndexError(msg)\n@@ -710,7 +710,7 @@ def check_same_shape(*args, allow_cpu_scalar_tensors: bool):\n                 shape = arg.shape\n \n             if not is_same_shape(shape, arg.shape):\n-                msg = \"Shape {0} is not the expected shape {1}!\".format(\n+                msg = \"Shape {} is not the expected shape {}!\".format(\n                     arg.shape, shape\n                 )\n                 raise RuntimeError(msg)\n@@ -1102,7 +1102,7 @@ def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool:\n         if fn(cast_from):\n             return False\n \n-    raise ValueError(\"Received unknown dtypes {0}, {1}!\".format(cast_to, cast_from))\n+    raise ValueError(f\"Received unknown dtypes {cast_to}, {cast_from}!\")\n \n \n def check_same_dtype(*args):\n@@ -1340,7 +1340,7 @@ def elementwise_dtypes(\n     for x in args:\n         if not isinstance(x, (Number, TensorLike, sympy.Symbol)):\n             msg = (\n-                \"Unexpected type {0} when computing elementwise type promotion!\".format(\n+                \"Unexpected type {} when computing elementwise type promotion!\".format(\n                     str(type(x))\n                 )\n             )\n@@ -1424,7 +1424,7 @@ def _find_highest_dtype_filtered(\n         return get_computation_dtype(result_dtype), torch.bool\n     else:\n         raise ValueError(\n-            \"Unknown type promotion kind {0}\".format(str(type_promotion_kind))\n+            f\"Unknown type promotion kind {str(type_promotion_kind)}\"\n         )\n \n \n@@ -1648,8 +1648,8 @@ def check_in_bounds_for_storage(\n     required_length = compute_required_storage_length(shape, strides, storage_offset)\n     if a.size() < required_length:\n         msg = (\n-            \"Can't view a storage of size {0} with an offset of {1}, shape of {2}, and strides of {3}, \"\n-            \"which requires a storage of size {4}\".format(\n+            \"Can't view a storage of size {} with an offset of {}, shape of {}, and strides of {}, \"\n+            \"which requires a storage of size {}\".format(\n                 a.size(), storage_offset, str(shape), str(strides), required_length\n             )\n         )\n@@ -1671,9 +1671,9 @@ def check(\n     .. note:: This function is planned for removal in the future. Please use\n         `torch._check*` functions instead.\n     \"\"\"\n-    warnings.warn(DeprecationWarning((\n+    warnings.warn(DeprecationWarning(\n         \"'torch._prims_common.check' will be removed in the future. Please use \"\n-        \"'torch._check*' functions instead\")))\n+        \"'torch._check*' functions instead\"))\n     torch._check_with(exc_type, b, s)\n \n \ndiff --git a/torch/_prims_common/wrappers.py b/torch/_prims_common/wrappers.py\nindex 938465cac36318..c9755de3e0da63 100644\n--- a/torch/_prims_common/wrappers.py\n+++ b/torch/_prims_common/wrappers.py\n@@ -48,16 +48,16 @@ def _maybe_convert_to_dtype(a, dtype):\n         return None\n \n     raise ValueError(\n-        \"Received type {0} that is neither a tensor or a number!\".format(type(a))\n+        f\"Received type {type(a)} that is neither a tensor or a number!\"\n     )\n \n \n def _maybe_convert_to_type(a: NumberType, typ: type) -> NumberType:\n     if not isinstance(a, Number):\n-        msg = \"Found unknown type {0} when trying to convert scalars!\".format(type(a))\n+        msg = f\"Found unknown type {type(a)} when trying to convert scalars!\"\n         raise ValueError(msg)\n     if not utils.is_weakly_lesser_type(type(a), typ):\n-        msg = \"Scalar {0} of type {1} cannot be safely cast to type {2}!\".format(\n+        msg = \"Scalar {} of type {} cannot be safely cast to type {}!\".format(\n             a, type(a), typ\n         )\n         raise ValueError(msg)\n@@ -169,7 +169,7 @@ def _safe_copy_out(\n ):\n     # Checks same device\n     if copy_from.device != copy_to.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             copy_from.device, copy_to.device\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_refs/__init__.py b/torch/_refs/__init__.py\nindex 611c6e3b9a625a..53851d62a5eb44 100644\n--- a/torch/_refs/__init__.py\n+++ b/torch/_refs/__init__.py\n@@ -597,7 +597,7 @@ def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType:\n \n     python_type = utils.dtype_to_type(a.dtype)\n     if not utils.is_weakly_lesser_type(type(value), python_type):\n-        msg = \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+        msg = \"value argument of type {} cannot be safely cast to type {}!\".format(\n             type(value), python_type\n         )\n         raise ValueError(msg)\n@@ -997,10 +997,8 @@ def add(\n         if python_type != bool and not utils.is_weakly_lesser_type(\n             type(alpha), python_type\n         ):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         b = prims.mul(b, alpha)\n@@ -1069,7 +1067,7 @@ def copysign(\n     if isinstance(b, Number) and isinstance(a, Tensor):\n         b = scalar_tensor(b, dtype=a.dtype, device=a.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n-        msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+        msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n             a.device, b.device\n         )\n         raise RuntimeError(msg)\n@@ -1100,7 +1098,7 @@ def div(\n     else:\n         msg = (\n             \"div expected rounding_mode to be one of None, 'trunc', or 'floor' \"\n-            \"but found {0}.\".format(rounding_mode)\n+            \"but found {}.\".format(rounding_mode)\n         )\n         raise ValueError(msg)\n \n@@ -1218,7 +1216,7 @@ def floor_divide(\n         a = scalar_tensor(a, dtype=b.dtype, device=b.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n         if a.device == torch.device(\"cpu\"):\n-            msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+            msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n                 a.device, b.device\n             )\n             raise RuntimeError(msg)\n@@ -1378,19 +1376,19 @@ def _check_close_args(\n ) -> None:\n     torch._check_value(\n         a.dtype == b.dtype,\n-        lambda: \"{0}: Attempting to compare tensors of different dtypes {1} and {2}!\".format(\n+        lambda: \"{}: Attempting to compare tensors of different dtypes {} and {}!\".format(\n             name, a.dtype, b.dtype\n         ),\n     )\n     torch._check(\n         rtol >= 0,\n-        lambda: \"{0}: rtol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: rtol must be greater than or equal to zero, but got {}!\".format(\n             name, rtol\n         ),\n     )\n     torch._check(\n         atol >= 0,\n-        lambda: \"{0}: atol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: atol must be greater than or equal to zero, but got {}!\".format(\n             name, atol\n         ),\n     )\n@@ -1664,10 +1662,8 @@ def sub(\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         if isinstance(b, torch.Tensor):\n@@ -1759,7 +1755,7 @@ def addcdiv(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1788,7 +1784,7 @@ def addcmul(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1892,7 +1888,7 @@ def clone(\n \n def copy_to(a: Tensor, b: Tensor, *, allow_cross_device=True):\n     if not allow_cross_device and a.device != b.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             b.device, a.device\n         )\n         raise RuntimeError(msg)\n@@ -2098,7 +2094,7 @@ def _reduction(\n     assert isinstance(a, TensorLike)\n     if a.ndim > 64:\n         raise RuntimeError(\n-            \"Received a tensor with {0} dimensions, but only tensors with up to 64 dims are supported!\".format(\n+            \"Received a tensor with {} dimensions, but only tensors with up to 64 dims are supported!\".format(\n                 a.ndim\n             )\n         )\n@@ -2864,7 +2860,7 @@ def expand_as(a: Tensor, b: Tensor) -> Tensor:\n \n def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> Tuple[TensorLikeType, ...]:\n     if chunks <= 0:\n-        msg = \"Expected at least one chunk, but got {0}!\".format(chunks)\n+        msg = f\"Expected at least one chunk, but got {chunks}!\"\n         raise ValueError(msg)\n \n     dim = utils.canonicalize_dim(a.ndim, dim)\n@@ -3346,7 +3342,7 @@ def _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorL\n                 if allow_copy:\n                     return prims.reshape(a, shape)\n \n-                msg = \"Cannot view a tensor with shape {0} and strides {1} as a tensor with shape {2}!\".format(\n+                msg = \"Cannot view a tensor with shape {} and strides {} as a tensor with shape {}!\".format(\n                     a.shape, a.stride(), shape\n                 )\n                 raise ValueError(msg)\n@@ -3704,13 +3700,13 @@ def tensor_split(\n     # If indices_or_sections is a tensor, it must be a CPU Long tensor\n     if isinstance(indices_or_sections, TensorLike):\n         if not indices_or_sections.device.type == \"cpu\":\n-            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {0}\".format(\n+            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {}\".format(\n                 indices_or_sections.device\n             )\n             raise ValueError(msg)\n         if indices_or_sections.dtype != torch.long:\n             msg = \"tensor_split: if indices_or_sections is a tensor it must have long dtype, \"\n-            \" but received one with dtype {0}\".format(indices_or_sections.dtype)\n+            f\" but received one with dtype {indices_or_sections.dtype}\"\n             raise ValueError(msg)\n \n     # Case 0 -- indices_or_sections is an integer or a scalar tensor n and a is split along dim into n parts of equal-ish length\n@@ -3724,7 +3720,7 @@ def tensor_split(\n         )\n \n         if sections <= 0:\n-            msg = \"tensor_split: number of sections must be greater than 0, but was {0}\".format(\n+            msg = \"tensor_split: number of sections must be greater than 0, but was {}\".format(\n                 sections\n             )\n             raise ValueError(msg)\n@@ -3751,7 +3747,7 @@ def tensor_split(\n         if isinstance(indices_or_sections, TensorLike):\n             if indices_or_sections.ndim != 1:\n                 msg = \"tensor_split: non-scalar indices_or_sections tensors must have only one dimension, \"\n-                \"but received a tensor with {0} dimensions\".format(\n+                \"but received a tensor with {} dimensions\".format(\n                     indices_or_sections.ndim\n                 )\n                 raise ValueError(msg)\ndiff --git a/torch/_refs/nn/functional/__init__.py b/torch/_refs/nn/functional/__init__.py\nindex eaa6618379f356..ba00179c4b2d6f 100644\n--- a/torch/_refs/nn/functional/__init__.py\n+++ b/torch/_refs/nn/functional/__init__.py\n@@ -167,10 +167,8 @@ def celu(\n     if alpha is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         rhs = alpha * torch.expm1(torch.true_divide(a, alpha))  # type: ignore[arg-type]\n@@ -437,7 +435,7 @@ def softplus(\n     if beta is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(beta), python_type):\n-            msg = \"beta argument of type {0} cannot be safely cast to type {1}!\".format(\n+            msg = \"beta argument of type {} cannot be safely cast to type {}!\".format(\n                 type(beta), python_type\n             )\n             raise ValueError(msg)\n@@ -610,11 +608,9 @@ def margin_ranking_loss(\n     # loss_without_reduction = max(0, \u2212target * (input1 \u2212 input2) + margin)\n     if input1.ndim != input2.ndim or input1.ndim != target.ndim:\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(\n-                    input1.shape, input2.shape, target.shape\n-                )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(\n+                input1.shape, input2.shape, target.shape\n             )\n         )\n     _check_reduction_value(reduction)\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint.py b/torch/fx/experimental/migrate_gradual_types/constraint.py\nindex bab7c62347bbd9..0f0d23d0187490 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from torch.fx.experimental.migrate_gradual_types.operation import op_add, op_sub, op_mul, op_div, \\\n     op_mod, op_gt, op_lt, op_neq, op_eq\n from torch.fx.tensor_type import TensorType, Dyn\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\nindex fc1fae790d8300..153a8407fc4113 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n@@ -75,7 +75,7 @@ def transform_index_select(constraint, counter):\n     # if the index is valid then replace the input dimension with the new dimension\n     # otherwise the dimension will not be replaced and the clause will contain False\n     if is_valid_index == T():\n-        new_dims = copy.deepcopy((dims))\n+        new_dims = copy.deepcopy(dims)\n         new_dims[constraint.index] = constraint.dim_replace\n \n     transformed_constraint = Conj([BinConstraintT(constraint.input_var, TensorType(dims), op_eq),\n@@ -803,7 +803,7 @@ def apply_padding(e1_var: TVar,\n         broadcast_padding = []\n \n         # for every padding size, we also consider broadcasting\n-        for j in range((len(d2) - i)):\n+        for j in range(len(d2) - i):\n             broadcast_padding.append(broadcast_dim(simulate_padding, d2, d11, d12, j, True))\n \n         # we consider the possibilities for broadcasting for every dimension. Since we already\ndiff --git a/torch/fx/experimental/migrate_gradual_types/operation.py b/torch/fx/experimental/migrate_gradual_types/operation.py\nindex 68bba2d59a7608..ec2cb91bbcc179 100644\n--- a/torch/fx/experimental/migrate_gradual_types/operation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/operation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n op_add = '+'\n op_sub = '-'\n op_mul = '*'\ndiff --git a/torch/fx/experimental/symbolic_shapes.py b/torch/fx/experimental/symbolic_shapes.py\nindex 90a3c519b3c44b..70762cf1f81b8c 100644\n--- a/torch/fx/experimental/symbolic_shapes.py\n+++ b/torch/fx/experimental/symbolic_shapes.py\n@@ -1488,7 +1488,7 @@ def _print_Symbol(self, expr) -> str:\n         return self.print_source(self.symbol_to_source[expr][0])\n \n     def _print_Relational(self, expr):\n-        return '%s %s %s' % (\n+        return '{} {} {}'.format(\n             self.parenthesize(expr.lhs, precedence(expr)),\n             expr.rel_op,\n             self.parenthesize(expr.rhs, precedence(expr))\n@@ -1887,7 +1887,7 @@ def print_results(grouped, indent, result_fn):\n class ShapeEnvLoggerAdapter(logging.LoggerAdapter):\n     def process(self, msg, kwargs):\n         # TODO: Maybe suppress the envid if not DEBUG?\n-        return '%s: %s' % (self.extra['envid'], msg), kwargs\n+        return '{}: {}'.format(self.extra['envid'], msg), kwargs\n \n \n ENV_COUNTER = collections.Counter()\ndiff --git a/torch/fx/experimental/unification/multipledispatch/dispatcher.py b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\nindex c76d8c60b097c7..ac8bc7d8dd159c 100644\n--- a/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n+++ b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n@@ -205,10 +205,9 @@ def add(self, signature, func):\n             if not isinstance(typ, (type, list)):\n                 str_sig = ', '.join(c.__name__ if isinstance(c, type)\n                                     else str(c) for c in signature)\n-                raise TypeError(\"Tried to dispatch on non-type: %s\\n\"\n-                                \"In signature: <%s>\\n\"\n-                                \"In function: %s\" %\n-                                (typ, str_sig, self.name))\n+                raise TypeError(\"Tried to dispatch on non-type: {}\\n\"\n+                                \"In signature: <{}>\\n\"\n+                                \"In function: {}\".format(typ, str_sig, self.name))\n \n             # handle variadic signatures\n             if isinstance(typ, list):\n@@ -257,8 +256,7 @@ def __call__(self, *args, **kwargs):\n             func = self.dispatch(*types)\n             if not func:\n                 raise NotImplementedError(\n-                    'Could not find signature for %s: <%s>' %\n-                    (self.name, str_signature(types))) from e\n+                    f'Could not find signature for {self.name}: <{str_signature(types)}>') from e\n             self._cache[types] = func\n         try:\n             return func(*args, **kwargs)\n@@ -274,7 +272,7 @@ def __call__(self, *args, **kwargs):\n \n             raise NotImplementedError(\n                 \"Matching functions for \"\n-                \"%s: <%s> found, but none completed successfully\" % (\n+                \"{}: <{}> found, but none completed successfully\".format(\n                     self.name, str_signature(types),),) from e\n \n     def __str__(self):\n@@ -408,8 +406,7 @@ def __call__(self, *args, **kwargs):\n         types = tuple([type(arg) for arg in args])\n         func = self.dispatch(*types)\n         if not func:\n-            raise NotImplementedError('Could not find signature for %s: <%s>' %\n-                                      (self.name, str_signature(types)))\n+            raise NotImplementedError(f'Could not find signature for {self.name}: <{str_signature(types)}>')\n         return func(self.obj, *args, **kwargs)\n \n \ndiff --git a/torch/fx/interpreter.py b/torch/fx/interpreter.py\nindex 7bc5e55288b03c..6ee5706f92ee34 100644\n--- a/torch/fx/interpreter.py\n+++ b/torch/fx/interpreter.py\n@@ -139,7 +139,7 @@ def run(self, *args, initial_env : Optional[Dict[Node, Any]] = None, enable_io_p\n             except Exception as e:\n                 if self.extra_traceback:\n                     msg = f\"While executing {node.format_node()}\"\n-                    msg = '{}\\n\\n{}'.format(e.args[0], msg) if e.args else str(msg)\n+                    msg = f'{e.args[0]}\\n\\n{msg}' if e.args else str(msg)\n                     msg += f\"\\nOriginal traceback:\\n{node.stack_trace}\"\n                     e.args = (msg,) + e.args[1:]\n                     if isinstance(e, KeyError):\ndiff --git a/torch/fx/passes/utils/matcher_utils.py b/torch/fx/passes/utils/matcher_utils.py\nindex 1037b48d8eb61d..8b66561a6e280d 100644\n--- a/torch/fx/passes/utils/matcher_utils.py\n+++ b/torch/fx/passes/utils/matcher_utils.py\n@@ -30,7 +30,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class InternalMatch():\n+class InternalMatch:\n     # Nodes from which the match was found\n     anchors: List[Node]\n     # Maps nodes in the pattern subgraph to nodes in the larger graph\ndiff --git a/torch/fx/passes/utils/source_matcher_utils.py b/torch/fx/passes/utils/source_matcher_utils.py\nindex e00b9695742e36..da8cf9f0f168c4 100644\n--- a/torch/fx/passes/utils/source_matcher_utils.py\n+++ b/torch/fx/passes/utils/source_matcher_utils.py\n@@ -29,7 +29,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class SourcePartition():\n+class SourcePartition:\n     # Nodes in a particular partition\n     nodes: List[Node]\n \n"
  },
  {
    "number": 105431,
    "title": "[BE] Enable ruff's UP rules and autoformat inductor/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* #105433\n* #105432\n* __->__ #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "10d6d187c3a75962d5d8a3b7f62bb0bdb99488bb",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105431",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105431/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105431.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105431.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105431/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105431/comments",
    "labels": [
      "open source",
      "module: inductor",
      "topic: not user facing",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T01:22:26.269608Z",
    "state": "open",
    "patch": "From e25ff7a33f3108282c335b7946cddc450f7d2e86 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:22:20 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat inductor/\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py                    |  2 +-\n test/inductor/test_cuda_repro.py                   |  2 +-\n test/inductor/test_cudagraph_trees.py              |  2 +-\n test/inductor/test_fused_attention.py              |  4 ++--\n test/inductor/test_mkldnn_pattern_matcher.py       |  8 ++++----\n test/inductor/test_profiler.py                     |  4 ++--\n test/inductor/test_standalone_compile.py           |  2 +-\n test/inductor/test_torchinductor.py                |  6 +++---\n .../test_torchinductor_codegen_dynamic_shapes.py   | 14 +++++++-------\n torch/_inductor/codecache.py                       |  6 +++---\n torch/_inductor/codegen/cpp.py                     |  2 +-\n torch/_inductor/ir.py                              | 10 +++++-----\n torch/_inductor/lowering.py                        |  6 +++---\n torch/_inductor/triton_heuristics.py               |  6 ++----\n torch/_inductor/utils.py                           |  2 +-\n 15 files changed, 37 insertions(+), 39 deletions(-)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 89ea4315b1de6c..2074d8451be9cc 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -93,7 +93,7 @@ def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n     def test_conv2d_bn_mixed_dtype(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     3,\n                     16,\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..7df54f0d35a146 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -783,7 +783,7 @@ def forward(inductor_seeds, mul_4, view_15):\n     def test_issue100806(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.linear1 = torch.nn.Linear(10, 20)\n                 self.linear2 = torch.nn.Linear(20, 30)\n                 self.relu = torch.nn.ReLU()\ndiff --git a/test/inductor/test_cudagraph_trees.py b/test/inductor/test_cudagraph_trees.py\nindex 85e44c1e272f9d..8181484c419b58 100644\n--- a/test/inductor/test_cudagraph_trees.py\n+++ b/test/inductor/test_cudagraph_trees.py\n@@ -137,7 +137,7 @@ def tearDown(self):\n \n         def get_manager(self, device_index=None):\n             return torch._inductor.cudagraph_trees.get_container(\n-                (self.device_idx if not device_index else device_index)\n+                self.device_idx if not device_index else device_index\n             ).tree_manager\n \n         def get_roots(self):\ndiff --git a/test/inductor/test_fused_attention.py b/test/inductor/test_fused_attention.py\nindex e509c97b0cb04a..7163ac24de5cf3 100644\n--- a/test/inductor/test_fused_attention.py\n+++ b/test/inductor/test_fused_attention.py\n@@ -297,7 +297,7 @@ def test_pattern_fails_with_tensor_factor(self):\n         # https://github.com/pytorch/pytorch/issues/99124\n         class Model(torch.nn.Module):\n             def __init__(self, is_inv_factor):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.is_inv_factor = is_inv_factor\n \n             def forward(self, query, key, value, scale_factor) -> torch.Tensor:\n@@ -328,7 +328,7 @@ class Model(torch.nn.Module):\n             def __init__(\n                 self,\n             ):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, query, key, value, attn_mask) -> torch.Tensor:\n                 attn_weight = torch.softmax(\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex 3e07c0181994cf..9337a70b2f857e 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -374,7 +374,7 @@ def forward(self, x, negative_slope):\n     def test_conv2d_add_scalar(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n                 )\n@@ -476,7 +476,7 @@ def forward(self, x, other, alpha):\n         # we can't do the fusion when add's inputs are same tensor.\n         class Model2(torch.nn.Module):\n             def __init__(self):\n-                super(Model2, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -490,7 +490,7 @@ def forward(self, x):\n         # we can't do the fusion when add's inputs are mixed dtype.\n         class Model3(torch.nn.Module):\n             def __init__(self):\n-                super(Model3, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -526,7 +526,7 @@ def forward(self, x):\n     def test_reproduce_99842_issue(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n \n             def forward(self, input_tensor):\ndiff --git a/test/inductor/test_profiler.py b/test/inductor/test_profiler.py\nindex eebe884da46b33..68f44df8b9f052 100644\n--- a/test/inductor/test_profiler.py\n+++ b/test/inductor/test_profiler.py\n@@ -21,14 +21,14 @@ def test_inductor_profiling_triton_launch(self):\n         def fn(x, y):\n             return (x + y).sin().cos()\n \n-        x, y = [torch.rand((4, 4), device=\"cuda\") for _ in range(2)]\n+        x, y = (torch.rand((4, 4), device=\"cuda\") for _ in range(2))\n \n         with torch.profiler.profile() as prof:\n             fn(x, y)\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with open(fname, \"r\") as f:\n+            with open(fname) as f:\n                 trace_json = json.load(f)\n \n         self.assertTrue(\"traceEvents\" in trace_json)\ndiff --git a/test/inductor/test_standalone_compile.py b/test/inductor/test_standalone_compile.py\nindex c424c76244cc0e..88c528c891a4fc 100644\n--- a/test/inductor/test_standalone_compile.py\n+++ b/test/inductor/test_standalone_compile.py\n@@ -100,7 +100,7 @@ def test_inductor_via_export2(self):\n     def test_inductor_via_op_with_multiple_outputs(self):\n         x1 = torch.randn((2, 512, 128))\n         x2 = [128]\n-        x3 = torch.randn((128))\n+        x3 = torch.randn(128)\n         x4 = torch.randn((128,))\n         x5 = 1e-6\n         mod, inp = gen_gm_and_inputs(\ndiff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py\nindex 520837d3865a8d..45ee70cd8067ee 100644\n--- a/test/inductor/test_torchinductor.py\n+++ b/test/inductor/test_torchinductor.py\n@@ -2354,7 +2354,7 @@ def fn(x):\n     def test_adaptive_avg_pool2d_low_prec(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n \n             def forward(self, x):\n@@ -5995,7 +5995,7 @@ def fn(x, y):\n \n         self.common(\n             fn,\n-            [torch.randn((4, 2)), torch.randn((4))],\n+            [torch.randn((4, 2)), torch.randn(4)],\n         )\n \n     # Shape padding causes the inputs to all get specialized, so the codegen\n@@ -6047,7 +6047,7 @@ def test_sqrt_dynamic_shapes(self):\n \n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 B, N, C = x.shape\ndiff --git a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\nindex 8f4086af099e64..0cdfbd54ffbe3b 100644\n--- a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n+++ b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n@@ -124,14 +124,14 @@ def run(*ex, **kwargs):\n     \"test_expand_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_glu_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_isinf2_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_layer_norm_dynamic_shapes\": TestFailure((\"cuda\")),\n+    \"test_layer_norm_dynamic_shapes\": TestFailure(\"cuda\"),\n     \"test_linspace1_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_stack_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor2_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor3_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_to_device_constant_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_to_device_constant_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_upsample_nearest2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views3_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views4_dynamic_shapes\": TestFailure((\"cpu\",)),\n@@ -161,9 +161,9 @@ def run(*ex, **kwargs):\n     \"test_empty2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_empty_strided_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_index3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_inductor_bucketize_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_inductor_bucketize_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_like_rands_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n@@ -194,7 +194,7 @@ def run(*ex, **kwargs):\n     \"test_views6_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_view_detach_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_view_on_aliased_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_linear_float64_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_linear_float64_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_adaptive_avg_pool_with_output_size_0_dynamic_shapes\": TestFailure(\n         (\"cpu\", \"cuda\")\n     ),\n@@ -288,7 +288,7 @@ def run(*ex, **kwargs):\n \n if TEST_WITH_ROCM:\n     # aten.miopen_batch_norm is not registered for lowering\n-    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure((\"cuda\"))\n+    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure(\"cuda\")\n \n DynamicShapesCodegenCommonTemplate = make_dynamic_cls(\n     CommonTemplate, xfail_prop=\"_expected_failure_codegen_dynamic\"\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex fd1f4228c9d29a..8ac73569fe1c2a 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -158,7 +158,7 @@ def __init__(self):\n     def get_local_cache(self):\n         if not self.local_cache_path.is_file():\n             return {}\n-        with open(self.local_cache_path, \"r\") as local_cache_fp:\n+        with open(self.local_cache_path) as local_cache_fp:\n             local_cache = json.load(local_cache_fp)\n         return local_cache[\"cache\"]\n \n@@ -201,7 +201,7 @@ class PersistentCache(CacheBase):\n     def get_global_cache(self):\n         if self.global_cache_path is None or not self.global_cache_path.is_file():\n             return {}\n-        with open(self.global_cache_path, \"r\") as global_cache_fp:\n+        with open(self.global_cache_path) as global_cache_fp:\n             global_cache = json.load(global_cache_fp)\n         return global_cache[\"cache\"]\n \n@@ -844,7 +844,7 @@ def wrapper_call(*args):\n # - valid_vec_isa_list()\n # - VecISA.__bool__() <-- takes out a lock\n # - compile_file() <-- imports cpp_prefix_path from cpp, which causes us to try to take out the same lock.\n-@functools.lru_cache()\n+@functools.lru_cache\n def cpp_prefix_path():\n     path = Path(__file__).parent / \"codegen/cpp_prefix.h\"\n     with path.open() as f:\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex bb2469ebffd33d..f844160b7255c0 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -278,7 +278,7 @@ def parallel_num_threads():\n     return threads\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def stride_at(var: sympy.Symbol, index: sympy.Expr):\n     replacement = {var: var + 1}\n     new_index = sympy_subs(index, replacement)\ndiff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py\nindex 869a0fb60861f6..c8172ff36b3dcf 100644\n--- a/torch/_inductor/ir.py\n+++ b/torch/_inductor/ir.py\n@@ -3042,7 +3042,7 @@ class InplaceBernoulliFallback(ExternKernel):\n     kernel = \"aten.bernoulli_\"\n \n     def codegen(self, wrapper):\n-        (x,) = [t.codegen_reference() for t in self.inputs]\n+        (x,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(\n             f\"{self.kernel}({x}, {', '.join(map(repr, self.constant_args))})\"\n         )\n@@ -3073,9 +3073,9 @@ class ScatterFallback(ExternKernel):\n \n     def codegen(self, wrapper):\n         if self.src_is_tensor:\n-            (x, index, src) = [t.codegen_reference() for t in self.inputs]\n+            (x, index, src) = (t.codegen_reference() for t in self.inputs)\n         else:\n-            (x, index) = [t.codegen_reference() for t in self.inputs]\n+            (x, index) = (t.codegen_reference() for t in self.inputs)\n             src = self.constant_args[1]\n         wrapper.generate_scatter_fallback(\n             x,\n@@ -3156,7 +3156,7 @@ class IndexPutFallback(ExternKernel):\n     \"\"\"\n \n     def codegen(self, wrapper):\n-        (x, values, *valid_indices) = [t.codegen_reference() for t in self.inputs]\n+        (x, values, *valid_indices) = (t.codegen_reference() for t in self.inputs)\n         indices = []\n         iter_valid_indices = iter(valid_indices)\n         for i, _ in enumerate(self.indices):\n@@ -4694,7 +4694,7 @@ def codegen(self, wrapper):\n         wrapper.add_import_once(\n             \"from torch.distributed._functional_collectives_impl import _wait_tensor\"\n         )\n-        (input_collective,) = [t.codegen_reference() for t in self.inputs]\n+        (input_collective,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(f\"{input_collective} = _wait_tensor({input_collective})\")\n \n         # wait op still needs to produce a 'buffer' that represents the tensor output.\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 3f17db1e5fed3a..14430ace100534 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -2871,11 +2871,11 @@ def load_bounded(fy, fx):\n \n         iy = ops.to_dtype(in_y, get_int_dtype(iH + 1))\n         ix = ops.to_dtype(in_x, get_int_dtype(iW + 1))\n-        iys_ofs = tuple((ops.add(iy, ofs) for ofs in (-1, 0, 1, 2)))\n-        ixs_ofs = tuple((ops.add(ix, ofs) for ofs in (-1, 0, 1, 2)))\n+        iys_ofs = tuple(ops.add(iy, ofs) for ofs in (-1, 0, 1, 2))\n+        ixs_ofs = tuple(ops.add(ix, ofs) for ofs in (-1, 0, 1, 2))\n \n         def get_x_interp(y):\n-            coeffs_x = tuple((load_bounded(y, x) for x in ixs_ofs))\n+            coeffs_x = tuple(load_bounded(y, x) for x in ixs_ofs)\n             return cubic_interp1d(coeffs_x, t_x)\n \n         coeffs_y = tuple(get_x_interp(y) for y in iys_ofs)\ndiff --git a/torch/_inductor/triton_heuristics.py b/torch/_inductor/triton_heuristics.py\nindex 61027661111e5a..88fa275f8b0102 100644\n--- a/torch/_inductor/triton_heuristics.py\n+++ b/torch/_inductor/triton_heuristics.py\n@@ -482,9 +482,7 @@ def hash_configs(configs: List[Config]):\n     hasher = hashlib.sha256()\n     for cfg in configs:\n         hasher.update(\n-            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode(\n-                \"utf-8\"\n-            )\n+            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode()\n         )\n     return hasher.hexdigest()\n \n@@ -498,7 +496,7 @@ def load_cached_autotuning(\n     if not os.path.exists(cache_filename):\n         return None\n \n-    with open(cache_filename, \"r\") as fd:\n+    with open(cache_filename) as fd:\n         best_config = json.loads(fd.read())\n     if best_config.pop(\"configs_hash\", None) != configs_hash:\n         return None\ndiff --git a/torch/_inductor/utils.py b/torch/_inductor/utils.py\nindex 538d0a2040fb93..c604c45d53d32a 100644\n--- a/torch/_inductor/utils.py\n+++ b/torch/_inductor/utils.py\n@@ -688,7 +688,7 @@ def run_and_get_code(fn, *args, **kwargs):\n \n     def patched_compile_to_module(self):\n         mod = compile_to_module(self)\n-        with open(mod.__file__, \"r\") as f:\n+        with open(mod.__file__) as f:\n             source_codes.append(f.read())\n         return mod\n \n"
  },
  {
    "number": 105430,
    "title": "[BE] Enable ruff's UP rules and autoformat ao/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* #105433\n* #105432\n* #105431\n* __->__ #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "08e4135215f7419d22b6f42b00670ba35cad584f",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105430",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105430/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105430.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105430.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105430/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105430/comments",
    "labels": [
      "release notes: AO frontend",
      "open source",
      "release notes: quantization",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:22:22.373970Z",
    "state": "open",
    "patch": "From 3435d6a173402dba19f7edaa5d69f3f208625d50 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:22:15 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat ao/\n\n[ghstack-poisoned]\n---\n .../ao/sparsity/test_activation_sparsifier.py |  1 -\n test/ao/sparsity/test_composability.py        |  1 -\n test/ao/sparsity/test_data_scheduler.py       |  1 -\n test/ao/sparsity/test_data_sparsifier.py      |  1 -\n test/ao/sparsity/test_kernels.py              |  1 -\n test/ao/sparsity/test_parametrization.py      |  1 -\n test/ao/sparsity/test_scheduler.py            |  1 -\n test/ao/sparsity/test_sparsifier.py           |  1 -\n test/ao/sparsity/test_sparsity_utils.py       |  1 -\n .../ao/sparsity/test_structured_sparsifier.py |  1 -\n torch/ao/nn/intrinsic/modules/fused.py        |  2 +-\n .../ao/nn/intrinsic/qat/modules/conv_fused.py | 12 +++----\n .../nn/intrinsic/qat/modules/linear_relu.py   |  2 +-\n .../quantized/dynamic/modules/linear_relu.py  |  2 +-\n .../nn/intrinsic/quantized/modules/bn_relu.py |  4 +--\n .../intrinsic/quantized/modules/conv_relu.py  |  6 ++--\n .../quantized/modules/linear_relu.py          |  2 +-\n torch/ao/nn/quantizable/modules/activation.py |  6 ++--\n torch/ao/nn/quantized/dynamic/modules/conv.py |  1 -\n .../ao/nn/quantized/dynamic/modules/linear.py |  2 +-\n torch/ao/nn/quantized/dynamic/modules/rnn.py  | 32 +++++++++----------\n torch/ao/nn/quantized/modules/__init__.py     |  2 +-\n torch/ao/nn/quantized/modules/conv.py         |  5 ++-\n torch/ao/nn/quantized/modules/linear.py       |  2 +-\n .../ao/nn/quantized/reference/modules/rnn.py  |  2 +-\n .../ao/nn/sparse/quantized/dynamic/linear.py  |  2 +-\n torch/ao/ns/fx/ns_types.py                    |  8 ++---\n .../data_scheduler/base_data_scheduler.py     |  4 +--\n torch/ao/pruning/scheduler/base_scheduler.py  |  4 +--\n torch/ao/pruning/scheduler/cubic_scheduler.py |  1 -\n .../quantization/_learnable_fake_quantize.py  |  4 +--\n .../backend_config/backend_config.py          |  3 +-\n .../ao/quantization/backend_config/onednn.py  | 12 +++----\n .../quantization/experimental/APoT_tensor.py  |  2 +-\n .../ao/quantization/experimental/quantizer.py |  2 +-\n torch/ao/quantization/fake_quantize.py        |  2 +-\n torch/ao/quantization/fuse_modules.py         |  2 +-\n .../ao/quantization/fuser_method_mappings.py  | 10 +++---\n torch/ao/quantization/fx/_equalize.py         |  2 +-\n .../fx/_lower_to_native_backend.py            |  2 +-\n .../quantization/fx/_model_report/detector.py |  8 ++---\n .../_model_report/model_report_visualizer.py  |  2 +-\n torch/ao/quantization/fx/convert.py           |  2 +-\n torch/ao/quantization/fx/custom_config.py     |  9 ++----\n torch/ao/quantization/fx/prepare.py           |  2 +-\n torch/ao/quantization/fx/utils.py             | 24 ++++++--------\n torch/ao/quantization/observer.py             | 18 +++++------\n torch/ao/quantization/pt2e/prepare.py         |  4 +--\n .../quantization/pt2e/quantizer/quantizer.py  | 21 ++----------\n torch/ao/quantization/qconfig.py              |  8 ++---\n .../ao/quantization/quantization_mappings.py  | 10 +++---\n torch/ao/quantization/quantize.py             |  2 +-\n torch/ao/quantization/utils.py                |  2 +-\n 53 files changed, 114 insertions(+), 150 deletions(-)\n\ndiff --git a/test/ao/sparsity/test_activation_sparsifier.py b/test/ao/sparsity/test_activation_sparsifier.py\nindex 573a40762c31cc..01bdfa045da9d1 100644\n--- a/test/ao/sparsity/test_activation_sparsifier.py\n+++ b/test/ao/sparsity/test_activation_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import copy\ndiff --git a/test/ao/sparsity/test_composability.py b/test/ao/sparsity/test_composability.py\nindex 85d78c49ea54ae..cb799f714ca17b 100644\n--- a/test/ao/sparsity/test_composability.py\n+++ b/test/ao/sparsity/test_composability.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_data_scheduler.py b/test/ao/sparsity/test_data_scheduler.py\nindex 9c33a160e76836..ab7c051c21077a 100644\n--- a/test/ao/sparsity/test_data_scheduler.py\n+++ b/test/ao/sparsity/test_data_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_data_sparsifier.py b/test/ao/sparsity/test_data_sparsifier.py\nindex 81a899f6932a0d..9248a371826ebd 100644\n--- a/test/ao/sparsity/test_data_sparsifier.py\n+++ b/test/ao/sparsity/test_data_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_kernels.py b/test/ao/sparsity/test_kernels.py\nindex 4786557ceb3be7..111d51465be109 100644\n--- a/test/ao/sparsity/test_kernels.py\n+++ b/test/ao/sparsity/test_kernels.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.testing._internal.common_utils import run_tests\ndiff --git a/test/ao/sparsity/test_parametrization.py b/test/ao/sparsity/test_parametrization.py\nindex 54b6f778d9fa8f..02f7cc6db7fddf 100644\n--- a/test/ao/sparsity/test_parametrization.py\n+++ b/test/ao/sparsity/test_parametrization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_scheduler.py b/test/ao/sparsity/test_scheduler.py\nindex 52eb54cb9ecb96..835c5143f18bc2 100644\n--- a/test/ao/sparsity/test_scheduler.py\n+++ b/test/ao/sparsity/test_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch import nn\ndiff --git a/test/ao/sparsity/test_sparsifier.py b/test/ao/sparsity/test_sparsifier.py\nindex 4c79416a78dd69..c9309d4b81fe5b 100644\n--- a/test/ao/sparsity/test_sparsifier.py\n+++ b/test/ao/sparsity/test_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import itertools\ndiff --git a/test/ao/sparsity/test_sparsity_utils.py b/test/ao/sparsity/test_sparsity_utils.py\nindex 90aad10ab18db6..9a4fc79e6c454e 100644\n--- a/test/ao/sparsity/test_sparsity_utils.py\n+++ b/test/ao/sparsity/test_sparsity_utils.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_structured_sparsifier.py b/test/ao/sparsity/test_structured_sparsifier.py\nindex f50420c89a199d..13ab245a2efc27 100644\n--- a/test/ao/sparsity/test_structured_sparsifier.py\n+++ b/test/ao/sparsity/test_structured_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n import copy\n import logging\ndiff --git a/torch/ao/nn/intrinsic/modules/fused.py b/torch/ao/nn/intrinsic/modules/fused.py\nindex f70a5430e65c21..7c87154b0e6225 100644\n--- a/torch/ao/nn/intrinsic/modules/fused.py\n+++ b/torch/ao/nn/intrinsic/modules/fused.py\n@@ -125,7 +125,7 @@ class LinearBn1d(_FusedModule):\n     During quantization this will be replaced with the corresponding fused module.\"\"\"\n     def __init__(self, linear, bn):\n         assert type_before_parametrizations(linear) == Linear and type_before_parametrizations(bn) == BatchNorm1d, \\\n-            'Incorrect types for input modules{}{}'.format(type_before_parametrizations(linear), type_before_parametrizations(bn))\n+            f'Incorrect types for input modules{type_before_parametrizations(linear)}{type_before_parametrizations(bn)}'\n         super().__init__(linear, bn)\n \n class LinearLeakyReLU(_FusedModule):\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\nindex 3f457ad5917eba..161280ca079d53 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n@@ -453,7 +453,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU1d(nnqat.Conv1d, nni._FusedModule):\n     r\"\"\"A ConvReLU1d module is a fused module of Conv1d and ReLU, attached with\n@@ -490,7 +490,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn2d(_ConvBnNd, nn.Conv2d):\n     r\"\"\"\n@@ -585,7 +585,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU2d(nnqat.Conv2d, nni._FusedModule):\n     r\"\"\"A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\n@@ -622,7 +622,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn3d(_ConvBnNd, nn.Conv3d):\n     r\"\"\"\n@@ -758,7 +758,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU3d(nnqat.Conv3d, nni._FusedModule):\n     r\"\"\"A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with\n@@ -813,7 +813,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n def update_bn_stats(mod):\n     if type(mod) in {ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvBn1d, ConvBn2d, ConvBn3d}:\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\nindex 93b19537083427..11d11047c2c723 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n@@ -37,7 +37,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     def to_float(self):\n         linear = torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)\ndiff --git a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nindex 9a6502d546641b..a0bccdc0e3d3d4 100644\n--- a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n@@ -48,7 +48,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qlinear_relu):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\nindex 5cd2ed8a757cee..856fa43aac9941 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n@@ -39,7 +39,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU2d\n-        return super(BNReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\n@@ -75,7 +75,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU3d\n-        return super(BNReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\nindex 7a88a7b8f92d3b..30d00474e4a5ad 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n@@ -58,7 +58,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -107,7 +107,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -163,7 +163,7 @@ def from_float(cls, mod):\n                 mod.bn.weight,\n                 mod.bn.bias,\n             )\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\nindex 9c3a7bcd3b4a0c..17cb48f80fda91 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n@@ -41,7 +41,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_linear_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/quantizable/modules/activation.py b/torch/ao/nn/quantizable/modules/activation.py\nindex b7ba9dd8dc72c2..6a25d0f591021d 100644\n--- a/torch/ao/nn/quantizable/modules/activation.py\n+++ b/torch/ao/nn/quantizable/modules/activation.py\n@@ -317,7 +317,7 @@ def _forward_impl(self,\n             raise AssertionError(\"causal mask not supported by AO MHA module\")\n \n         if self.batch_first:\n-            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n+            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n \n         tgt_len, bsz, embed_dim_to_check = query.size()\n         assert self.embed_dim == embed_dim_to_check\n@@ -339,7 +339,7 @@ def _forward_impl(self,\n                 warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n                 attn_mask = attn_mask.to(torch.bool)\n             assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n-                'Only float and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n+                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n \n             if attn_mask.dim() == 2:\n                 attn_mask = attn_mask.unsqueeze(0)\n@@ -349,7 +349,7 @@ def _forward_impl(self,\n                 if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n                     raise RuntimeError('The size of the 3D attn_mask is not correct.')\n             else:\n-                raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n+                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n             # attn_mask's dim is 3 now.\n \n         # convert ByteTensor key_padding_mask to bool\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/conv.py b/torch/ao/nn/quantized/dynamic/modules/conv.py\nindex 125b48edaacde5..f1af7796413655 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/conv.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Dynamically quantized convolution modules.\"\"\"\n \n import torch\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/linear.py b/torch/ao/nn/quantized/dynamic/modules/linear.py\nindex 78e459f9bc63c5..22f483f32fd7a8 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/linear.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/linear.py\n@@ -68,7 +68,7 @@ def extra_repr(self):\n             self.in_features, self.out_features, self._packed_params.dtype\n         )\n         if self._packed_params.dtype == torch.qint8:\n-            extra_repr_str += ', qscheme={}'.format(self.weight().qscheme())\n+            extra_repr_str += f', qscheme={self.weight().qscheme()}'\n         return extra_repr_str\n \n     def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/rnn.py b/torch/ao/nn/quantized/dynamic/modules/rnn.py\nindex 3e78948b5447b2..47c8a9ac2fb43c 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/rnn.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/rnn.py\n@@ -231,8 +231,8 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n     def set_weight_bias(self, weight_bias_dict):\n \n         def weight_bias_name(ihhh, layer, suffix):\n-            weight_name = \"weight_{}_l{}{}\".format(ihhh, layer, suffix)\n-            bias_name = \"bias_{}_l{}{}\".format(ihhh, layer, suffix)\n+            weight_name = f\"weight_{ihhh}_l{layer}{suffix}\"\n+            bias_name = f\"bias_{ihhh}_l{layer}{suffix}\"\n             return weight_name, bias_name\n \n         num_directions = 2 if self.bidirectional else 1\n@@ -286,7 +286,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n         # RNNBase can be either LSTM or GRU\n         qRNNBase: Union[LSTM, GRU]\n         if mod.mode == 'LSTM':\n@@ -308,8 +308,8 @@ def from_float(cls, mod):\n                 suffix = '_reverse' if direction == 1 else ''\n \n                 def retrieve_weight_bias(ihhh):\n-                    weight_name = 'weight_{}_l{}{}'.format(ihhh, layer, suffix)\n-                    bias_name = 'bias_{}_l{}{}'.format(ihhh, layer, suffix)\n+                    weight_name = f'weight_{ihhh}_l{layer}{suffix}'\n+                    bias_name = f'bias_{ihhh}_l{layer}{suffix}'\n                     weight = getattr(mod, weight_name)\n                     bias = getattr(mod, bias_name)\n                     return weight, bias\n@@ -358,15 +358,15 @@ def _weight_bias(self):\n         for layer in range(self.num_layers):\n             for direction in range(num_directions):\n                 suffix = '_reverse' if direction == 1 else ''\n-                key_name1 = 'weight_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'weight_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'weight_ih_l{layer}{suffix}'\n+                key_name2 = f'weight_hh_l{layer}{suffix}'\n                 # packed weights are part of torchbind class, CellParamsSerializationType\n                 # Within the packed weight class, the weight and bias are accessible as Tensors\n                 packed_weight_bias = self._all_weight_values[count].param.__getstate__()[0][4]\n                 weight_bias_dict['weight'][key_name1] = packed_weight_bias[0].__getstate__()[0][0]\n                 weight_bias_dict['weight'][key_name2] = packed_weight_bias[1].__getstate__()[0][0]\n-                key_name1 = 'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'bias_ih_l{layer}{suffix}'\n+                key_name2 = f'bias_hh_l{layer}{suffix}'\n                 weight_bias_dict['bias'][key_name1] = packed_weight_bias[0].__getstate__()[0][1]\n                 weight_bias_dict['bias'][key_name2] = packed_weight_bias[1].__getstate__()[0][1]\n                 count = count + 1\n@@ -494,7 +494,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTM, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -746,7 +746,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -860,7 +860,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n \n         qRNNCellBase: Union[LSTMCell, GRUCell, RNNCell]\n \n@@ -1009,12 +1009,12 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n         return ret\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(RNNCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class LSTMCell(RNNCellBase):\n@@ -1057,7 +1057,7 @@ def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) ->\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTMCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class GRUCell(RNNCellBase):\n@@ -1098,4 +1098,4 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRUCell, cls).from_float(mod)\n+        return super().from_float(mod)\ndiff --git a/torch/ao/nn/quantized/modules/__init__.py b/torch/ao/nn/quantized/modules/__init__.py\nindex 05866f6da4066a..668f765fe3ef0a 100644\n--- a/torch/ao/nn/quantized/modules/__init__.py\n+++ b/torch/ao/nn/quantized/modules/__init__.py\n@@ -104,7 +104,7 @@ def from_float(mod):\n         return Quantize(scale.float().item(), zero_point.long().item(), mod.activation_post_process.dtype)\n \n     def extra_repr(self):\n-        return 'scale={}, zero_point={}, dtype={}'.format(self.scale, self.zero_point, self.dtype)\n+        return f'scale={self.scale}, zero_point={self.zero_point}, dtype={self.dtype}'\n \n \n class DeQuantize(torch.nn.Module):\ndiff --git a/torch/ao/nn/quantized/modules/conv.py b/torch/ao/nn/quantized/modules/conv.py\nindex 727447841ca43c..22a11014375948 100644\n--- a/torch/ao/nn/quantized/modules/conv.py\n+++ b/torch/ao/nn/quantized/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Quantized convolution modules.\"\"\"\n \n from typing import Optional, List, TypeVar\n@@ -64,7 +63,7 @@ def _init(self, in_channels, out_channels, kernel_size, stride,\n         self.output_padding = output_padding\n         self.groups = groups\n         if padding_mode not in _SUPPORTED_PADDING:\n-            raise ValueError(\"'padding_mode' {} is not supported by quantized convolution\".format(padding_mode))\n+            raise ValueError(f\"'padding_mode' {padding_mode} is not supported by quantized convolution\")\n         self.padding_mode = padding_mode\n         # Initialize as NCHW. set_weight will internally transpose to NHWC.\n         if self.transposed:\n@@ -593,7 +592,7 @@ def __init__(self, in_channels, out_channels, kernel_size, stride,\n                  padding, dilation, transposed, output_padding,\n                  groups, bias, padding_mode, device=None, dtype=None):\n         if padding_mode != 'zeros':\n-            raise ValueError('Only \"zeros\" padding mode is supported for {}'.format(self.__class__.__name__))\n+            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n         factory_kwargs = {'device': device, 'dtype': dtype}\n         # Subclasses of _ConvNd need to call _init rather than __init__. See\n         # discussion on PR #49702\ndiff --git a/torch/ao/nn/quantized/modules/linear.py b/torch/ao/nn/quantized/modules/linear.py\nindex e592c5f9b4d015..213934e62962a0 100644\n--- a/torch/ao/nn/quantized/modules/linear.py\n+++ b/torch/ao/nn/quantized/modules/linear.py\n@@ -262,7 +262,7 @@ def from_float(cls, mod):\n             if not isinstance(cls._FLOAT_MODULE, Iterable):\n                 cls._FLOAT_MODULE = [cls._FLOAT_MODULE]  # type: ignore[assignment]\n             supported_modules = ', '.join([float_mod.__name__ for float_mod in cls._FLOAT_MODULE])  # type: ignore[attr-defined]\n-            error_msg = 'nnq.{}.from_float only works for {}, but got: {}'.format(cls.__name__, supported_modules, type(mod))\n+            error_msg = f'nnq.{cls.__name__}.from_float only works for {supported_modules}, but got: {type(mod)}'\n             assert type_before_parametrizations(mod) in cls._FLOAT_MODULE, error_msg.format()  # type: ignore[attr-defined]\n             assert hasattr(mod, 'qconfig'), 'Input float module must have qconfig defined'\n             activation_post_process = mod.activation_post_process\ndiff --git a/torch/ao/nn/quantized/reference/modules/rnn.py b/torch/ao/nn/quantized/reference/modules/rnn.py\nindex 566642832a544d..9f44667c270b56 100644\n--- a/torch/ao/nn/quantized/reference/modules/rnn.py\n+++ b/torch/ao/nn/quantized/reference/modules/rnn.py\n@@ -152,7 +152,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n \n         if not is_batched:\n             ret = ret.squeeze(0)\ndiff --git a/torch/ao/nn/sparse/quantized/dynamic/linear.py b/torch/ao/nn/sparse/quantized/dynamic/linear.py\nindex 87d174db8098ac..4190ebe38c2f93 100644\n--- a/torch/ao/nn/sparse/quantized/dynamic/linear.py\n+++ b/torch/ao/nn/sparse/quantized/dynamic/linear.py\n@@ -60,7 +60,7 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                               missing_keys, unexpected_keys, error_msgs):\n         op_type = int(state_dict[prefix + 'op_type'])\n         assert op_type == 'sparse', \\\n-            \"Cannot load from op_type [{}], expecting [{}]\".format(op_type, self._op_type)\n+            f\"Cannot load from op_type [{op_type}], expecting [{self._op_type}]\"\n         state_dict.pop(prefix + 'op_type')\n \n         version = local_metadata.get('version', None)\ndiff --git a/torch/ao/ns/fx/ns_types.py b/torch/ao/ns/fx/ns_types.py\nindex cf0451a155dd44..5c3c422dd4ae9d 100644\n--- a/torch/ao/ns/fx/ns_types.py\n+++ b/torch/ao/ns/fx/ns_types.py\n@@ -10,10 +10,10 @@ class NSSingleResultValuesType(str, enum.Enum):\n     NODE_OUTPUT = 'node_output'\n     NODE_INPUT = 'node_input'\n \n-NSSubgraph = NamedTuple(\n-    'NSSubgraph',\n-    [('start_node', Node), ('end_node', Node), ('base_op_node', Node)]\n-)\n+class NSSubgraph(NamedTuple):\n+    start_node: Node\n+    end_node: Node\n+    base_op_node: Node\n \n # TODO(future PR): see if we can use typing_extensions's TypedDict instead\n # to properly type the various keys\ndiff --git a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\nindex 26da2146952ffd..0e4060f95435b6 100644\n--- a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n+++ b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n@@ -103,8 +103,8 @@ def get_schedule_param(self):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Data Sparsifier {0}\\n'.format(self.data_sparsifier)\n-        format_string += '    {0}: {1}\\n'.format(self.schedule_param, self.base_param)\n+        format_string += f'Data Sparsifier {self.data_sparsifier}\\n'\n+        format_string += f'    {self.schedule_param}: {self.base_param}\\n'\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/base_scheduler.py b/torch/ao/pruning/scheduler/base_scheduler.py\nindex 8986d5bbdf630f..66863b31c5f8d8 100644\n--- a/torch/ao/pruning/scheduler/base_scheduler.py\n+++ b/torch/ao/pruning/scheduler/base_scheduler.py\n@@ -106,8 +106,8 @@ def print_sl(self, is_verbose, group, sl, epoch=None):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Sparsifier {0}\\n'.format(self.sparsifier)\n-        format_string += '    {0}: {1}\\n'.format('base_sl', self.base_sl)\n+        format_string += f'Sparsifier {self.sparsifier}\\n'\n+        format_string += '    {}: {}\\n'.format('base_sl', self.base_sl)\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/cubic_scheduler.py b/torch/ao/pruning/scheduler/cubic_scheduler.py\nindex 49ee9f51b42ae6..76fc61daa288a6 100644\n--- a/torch/ao/pruning/scheduler/cubic_scheduler.py\n+++ b/torch/ao/pruning/scheduler/cubic_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import warnings\n \n from .base_scheduler import BaseScheduler\ndiff --git a/torch/ao/quantization/_learnable_fake_quantize.py b/torch/ao/quantization/_learnable_fake_quantize.py\nindex df86cd50a2a775..f21fedeb3bc28f 100644\n--- a/torch/ao/quantization/_learnable_fake_quantize.py\n+++ b/torch/ao/quantization/_learnable_fake_quantize.py\n@@ -114,8 +114,8 @@ def toggle_fake_quant(self, enabled=True):\n \n     @torch.jit.export\n     def observe_quant_params(self):\n-        print('_LearnableFakeQuantize Scale: {}'.format(self.scale.detach()))\n-        print('_LearnableFakeQuantize Zero Point: {}'.format(self.zero_point.detach()))\n+        print(f'_LearnableFakeQuantize Scale: {self.scale.detach()}')\n+        print(f'_LearnableFakeQuantize Zero Point: {self.zero_point.detach()}')\n \n     @torch.jit.export\n     def calculate_qparams(self):\ndiff --git a/torch/ao/quantization/backend_config/backend_config.py b/torch/ao/quantization/backend_config/backend_config.py\nindex ef31166b5cdab1..32abcc42e402fb 100644\n--- a/torch/ao/quantization/backend_config/backend_config.py\n+++ b/torch/ao/quantization/backend_config/backend_config.py\n@@ -599,8 +599,7 @@ def _get_dtype_config(obj: Any) -> DTypeConfig:\n                 return obj\n             if isinstance(obj, Dict):\n                 return DTypeConfig.from_dict(obj)\n-            raise ValueError(\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (DTYPE_CONFIGS_DICT_KEY, type(obj)))\n+            raise ValueError(f\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"{DTYPE_CONFIGS_DICT_KEY}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         if PATTERN_DICT_KEY in backend_pattern_config_dict:\ndiff --git a/torch/ao/quantization/backend_config/onednn.py b/torch/ao/quantization/backend_config/onednn.py\nindex 6a896608c9b5a8..8c14637ae3d3f7 100644\n--- a/torch/ao/quantization/backend_config/onednn.py\n+++ b/torch/ao/quantization/backend_config/onednn.py\n@@ -89,7 +89,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n         \"Linear, BN and LeakyReLU all must be in the same mode (train or eval).\"\n \n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((linear, bn, leaky_relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(linear, bn, leaky_relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Linear: nni.LinearLeakyReLU,\n@@ -100,7 +100,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n             fm = fused_module(fused_linear, leaky_relu)\n             return fm\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((linear, bn, leaky_relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(linear, bn, leaky_relu)}\")\n \n # ======================\n # |  CONFIGS FOR CONV  |\n@@ -144,7 +144,7 @@ def _conv_add_extra_inputs_getter_left(pattern):\n def _fuse_conv_bn_add_left(is_qat, add, bn_conv, _):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -216,7 +216,7 @@ def _conv_add_extra_inputs_getter_right(pattern):\n def _fuse_conv_bn_add_right(is_qat, add, _, bn_conv):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -305,7 +305,7 @@ def _fuse_conv_bn_add_relu_left(is_qat, relu, add_pattern):\n     add, bn_conv, _ = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\n@@ -387,7 +387,7 @@ def _fuse_conv_bn_add_relu_right(is_qat, relu, add_pattern):\n     add, _, bn_conv = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\ndiff --git a/torch/ao/quantization/experimental/APoT_tensor.py b/torch/ao/quantization/experimental/APoT_tensor.py\nindex f780e204154147..debda7aea8c0d1 100644\n--- a/torch/ao/quantization/experimental/APoT_tensor.py\n+++ b/torch/ao/quantization/experimental/APoT_tensor.py\n@@ -2,7 +2,7 @@\n from torch.ao.quantization.experimental.quantizer import APoTQuantizer\n \n # class to store APoT quantized tensor\n-class TensorAPoT():\n+class TensorAPoT:\n     quantizer: APoTQuantizer\n     data: torch.Tensor\n \ndiff --git a/torch/ao/quantization/experimental/quantizer.py b/torch/ao/quantization/experimental/quantizer.py\nindex e7e6048fb00e08..df9c0f27847e13 100644\n--- a/torch/ao/quantization/experimental/quantizer.py\n+++ b/torch/ao/quantization/experimental/quantizer.py\n@@ -5,7 +5,7 @@\n \n # class to store APoT quantizer and\n # implement quantize and dequantize\n-class APoTQuantizer():\n+class APoTQuantizer:\n     alpha: torch.Tensor\n     gamma: torch.Tensor\n     quantization_levels: torch.Tensor\ndiff --git a/torch/ao/quantization/fake_quantize.py b/torch/ao/quantization/fake_quantize.py\nindex 881d431dcccb18..0da19e9f09b5a8 100644\n--- a/torch/ao/quantization/fake_quantize.py\n+++ b/torch/ao/quantization/fake_quantize.py\n@@ -268,7 +268,7 @@ class FixedQParamsFakeQuantize(FakeQuantize):\n     def __init__(self, observer):\n         super().__init__(observer=observer)\n         assert type(self.activation_post_process) == FixedQParamsObserver,\\\n-            \"%s's observer must be a %s\" % (self.__class__.__name__, FixedQParamsObserver.__name__)\n+            f\"{self.__class__.__name__}'s observer must be a {FixedQParamsObserver.__name__}\"\n         self._observer_ctr = observer\n         self.scale = self.activation_post_process.scale\n         self.zero_point = self.activation_post_process.zero_point\ndiff --git a/torch/ao/quantization/fuse_modules.py b/torch/ao/quantization/fuse_modules.py\nindex 80c3933ddc06a1..7c7ef1a88e83a7 100644\n--- a/torch/ao/quantization/fuse_modules.py\n+++ b/torch/ao/quantization/fuse_modules.py\n@@ -51,7 +51,7 @@ def fuse_known_modules(mod_list, is_qat, additional_fuser_method_mapping=None):\n     types = tuple(type_before_parametrizations(m) for m in mod_list)\n     fuser_method = get_fuser_method(types, additional_fuser_method_mapping)\n     if fuser_method is None:\n-        raise NotImplementedError(\"Cannot fuse modules: {}\".format(types))\n+        raise NotImplementedError(f\"Cannot fuse modules: {types}\")\n     new_mod : List[Optional[nn.Module]] = [None] * len(mod_list)\n     fused = fuser_method(is_qat, *mod_list)\n     # NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\ndiff --git a/torch/ao/quantization/fuser_method_mappings.py b/torch/ao/quantization/fuser_method_mappings.py\nindex 9971326de1d102..3140f13008ac3d 100644\n--- a/torch/ao/quantization/fuser_method_mappings.py\n+++ b/torch/ao/quantization/fuser_method_mappings.py\n@@ -47,7 +47,7 @@ def fuse_conv_bn(is_qat, conv, bn):\n         if fused_module_class is not None:\n             return fused_module_class(conv, bn)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn)}\")\n     else:\n         return nn.utils.fuse_conv_bn_eval(conv, bn)\n \n@@ -84,7 +84,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n         if fused_module is not None:\n             return fused_module(conv, bn, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Conv1d: nni.ConvReLU1d,\n@@ -96,7 +96,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n             fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n             return fused_module(fused_conv, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(conv, bn, relu)}\")\n \n def fuse_linear_bn(is_qat, linear, bn):\n     r\"\"\"Given the linear and bn modules, fuses them and returns the fused module\n@@ -187,7 +187,7 @@ def get_fuser_method(op_list, additional_fuser_method_mapping=None):\n     all_mappings = get_combined_dict(_DEFAULT_OP_LIST_TO_FUSER_METHOD,\n                                      additional_fuser_method_mapping)\n     fuser_method = all_mappings.get(op_list, None)\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_list)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_list} \"\n     return fuser_method\n \n def _reverse2(f):\n@@ -244,5 +244,5 @@ def get_fuser_method_new(\n         fuser_method = fuser_method_mapping.get(op_pattern, None)\n         if fuser_method is not None:\n             break\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_pattern)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_pattern} \"\n     return fuser_method\ndiff --git a/torch/ao/quantization/fx/_equalize.py b/torch/ao/quantization/fx/_equalize.py\nindex 357db6454032e7..883ddf19682f08 100644\n--- a/torch/ao/quantization/fx/_equalize.py\n+++ b/torch/ao/quantization/fx/_equalize.py\n@@ -227,7 +227,7 @@ def __new__(cls, input_activation=torch.nn.Identity, weight=torch.nn.Identity):\n         if isinstance(input_activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"EqualizationQConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        self = super(EqualizationQConfig, cls).__new__(cls, input_activation, weight)\n+        self = super().__new__(cls, input_activation, weight)\n         return self\n \n \ndiff --git a/torch/ao/quantization/fx/_lower_to_native_backend.py b/torch/ao/quantization/fx/_lower_to_native_backend.py\nindex b897a7e80ab7cf..f1e9c81896f6c5 100644\n--- a/torch/ao/quantization/fx/_lower_to_native_backend.py\n+++ b/torch/ao/quantization/fx/_lower_to_native_backend.py\n@@ -514,7 +514,7 @@ def _match_static_pattern(\n     matched_dequantize = False\n     for i in dequantize_node_arg_indices:\n         assert i < len(ref_node.args),\\\n-            \"Dequantize index %s exceeded reference node's arg length %s\" % (i, len(ref_node.args))\n+            f\"Dequantize index {i} exceeded reference node's arg length {len(ref_node.args)}\"\n         arg = ref_node.args[i]\n         if is_dequantize_node(arg):\n             matched_dequantize = True\ndiff --git a/torch/ao/quantization/fx/_model_report/detector.py b/torch/ao/quantization/fx/_model_report/detector.py\nindex 60c9b26ffacecd..2e3cdc0a316611 100644\n--- a/torch/ao/quantization/fx/_model_report/detector.py\n+++ b/torch/ao/quantization/fx/_model_report/detector.py\n@@ -32,7 +32,7 @@\n DETECTOR_OBS_ARGS_KEY = \"observer_args\"\n \n # Mapping related code\n-class DetectorQConfigInfo():\n+class DetectorQConfigInfo:\n     r\"\"\"\n     This class contains the QConfig information for a single module.\n     The list of variables / values this contains can grow depending on the\n@@ -234,7 +234,7 @@ def __init__(self, backend: str = torch.backends.quantized.engine):\n         if self.backend_chosen in self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES:\n             self.supported_modules = self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES[self.backend_chosen]\n         else:\n-            raise ValueError(\"Not configured to work with {}. Try a different default backend\".format(self.backend_chosen))\n+            raise ValueError(f\"Not configured to work with {self.backend_chosen}. Try a different default backend\")\n \n     def get_detector_name(self) -> str:\n         r\"\"\" returns the string name of this detector\"\"\"\n@@ -352,7 +352,7 @@ def generate_detector_report(self, model: nn.Module) -> Tuple[str, Dict[str, Any\n         per_channel_info = self._detect_per_channel_helper(model)\n \n         # String to let the user know of further optimizations\n-        further_optims_str = \"Further Optimizations for backend {}: \\n\".format(self.backend_chosen)\n+        further_optims_str = f\"Further Optimizations for backend {self.backend_chosen}: \\n\"\n \n         optimizations_possible = False\n         for fqn in per_channel_info:\n@@ -1019,7 +1019,7 @@ def _generate_comparison_values(self, input_info: Dict, weight_info: Dict) -> Di\n \n             # raise error if not in weight info\n             if module_fqn not in weight_info:\n-                raise KeyError(\"Unable to find weight range stats for module {}\".format(module_fqn))\n+                raise KeyError(f\"Unable to find weight range stats for module {module_fqn}\")\n \n             # calculate the ratios of the weight info and input info\n             weight_ratio = self._calculate_range_ratio(weight_info[module_fqn], self.WEIGHT_STR, module_fqn)\ndiff --git a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\nindex f1f17e80982e54..8e04338446dab1 100644\n--- a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n+++ b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n@@ -587,7 +587,7 @@ def generate_plot_visualization(self, feature_filter: str, module_fqn_filter: st\n             avg_vals = [sum(y_data[:][index]) / num_channels for index in range(num_modules)]\n \n             # plot the three things we measured\n-            ax.plot(x_data, avg_vals, label=\"Average Value Across {} Channels\".format(num_channels))\n+            ax.plot(x_data, avg_vals, label=f\"Average Value Across {num_channels} Channels\")\n             ax.legend(loc='upper right')\n         else:\n             ax.set_xlabel(\"idx\")\ndiff --git a/torch/ao/quantization/fx/convert.py b/torch/ao/quantization/fx/convert.py\nindex 14e6ec094ede48..687342cc0ed321 100644\n--- a/torch/ao/quantization/fx/convert.py\n+++ b/torch/ao/quantization/fx/convert.py\n@@ -970,7 +970,7 @@ def convert(\n         # all the values either match what was set in prepare node_name_to_qconfig\n         # or are set to None in the convert_node_name_to_qconfig.\n         for k, v in node_name_to_qconfig.items():\n-            assert k in convert_node_name_to_qconfig, 'Expected key {} in convert node_name_to_qconfig'.format(k)\n+            assert k in convert_node_name_to_qconfig, f'Expected key {k} in convert node_name_to_qconfig'\n             if convert_node_name_to_qconfig[k] is not None:\n                 assert qconfig_equals(v, convert_node_name_to_qconfig[k]), \\\n                     \"Expected k {} to have the same value in prepare and convert QConfigMappings, \" \\\ndiff --git a/torch/ao/quantization/fx/custom_config.py b/torch/ao/quantization/fx/custom_config.py\nindex ef29061796d3a3..4fb2c3a28cb0a5 100644\n--- a/torch/ao/quantization/fx/custom_config.py\n+++ b/torch/ao/quantization/fx/custom_config.py\n@@ -197,8 +197,7 @@ def _get_qconfig_mapping(obj: Any, dict_key: str) -> Optional[QConfigMapping]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return QConfigMapping.from_dict(obj)\n-            raise ValueError(\"Expected QConfigMapping in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected QConfigMapping in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCustomConfig]:\n             \"\"\"\n@@ -208,8 +207,7 @@ def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCusto\n                 return obj\n             if isinstance(obj, Dict):\n                 return PrepareCustomConfig.from_dict(obj)\n-            raise ValueError(\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n             \"\"\"\n@@ -219,8 +217,7 @@ def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return BackendConfig.from_dict(obj)\n-            raise ValueError(\"Expected BackendConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected BackendConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         for (module_name, qconfig_dict, example_inputs, _prepare_custom_config_dict, backend_config_dict) in\\\ndiff --git a/torch/ao/quantization/fx/prepare.py b/torch/ao/quantization/fx/prepare.py\nindex d14bf444ccd8c6..aa9f1f7467f932 100644\n--- a/torch/ao/quantization/fx/prepare.py\n+++ b/torch/ao/quantization/fx/prepare.py\n@@ -238,7 +238,7 @@ def _needs_obs_or_fq(\n     # be converted to choose_qparams -> q -> dq in convert step\n     if cur_target_is_dynamic:\n         assert cur_target_dtype in _OBS_DTYPE_LIST, \\\n-            \"Expected cur_target_dtype to be torch.float, but got: {}\".format(cur_target_dtype)\n+            f\"Expected cur_target_dtype to be torch.float, but got: {cur_target_dtype}\"\n         assert prev_output_dtype not in _DO_NOT_OBS_DTYPE_LIST\n         return is_zeroth_arg\n     if reuse_input_obs_or_fq:\ndiff --git a/torch/ao/quantization/fx/utils.py b/torch/ao/quantization/fx/utils.py\nindex 2e0b6bbb130530..0942fd9462b0a1 100644\n--- a/torch/ao/quantization/fx/utils.py\n+++ b/torch/ao/quantization/fx/utils.py\n@@ -149,7 +149,7 @@ def get_qconv_prepack_op(conv_op: Callable) -> Callable:\n         torch.nn.functional.conv_transpose3d: torch.ops.quantized.conv_transpose3d_prepack,\n     }\n     prepack_op = prepack_ops.get(conv_op, None)\n-    assert prepack_op, \"Didn't find prepack op for {}\".format(conv_op)\n+    assert prepack_op, f\"Didn't find prepack op for {conv_op}\"\n     return prepack_op\n \n # Returns a function that can get a new attribute name for module with given\n@@ -811,24 +811,21 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n         # check quantization ranges\n         if backend_quant_min is not None and backend_quant_max is not None:\n             if app_quant_min is None or app_quant_max is None:\n-                warnings.warn(\"QConfig %s must specify 'quant_min' and 'quant_max', ignoring %s\" %\n-                              (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'quant_min' and 'quant_max', ignoring {qconfig}\")\n                 return False\n             elif app_quant_min < backend_quant_min or app_quant_max > backend_quant_max:\n-                warnings.warn((\"QConfig %s quantization range must fall within the backend's:\\n\"\n-                              \"QConfig range = (%s, %s), BackendConfig range = (%s, %s), ignoring %s\") %\n-                              (debug_string, app_quant_min, app_quant_max,\n+                warnings.warn((\"QConfig {} quantization range must fall within the backend's:\\n\"\n+                              \"QConfig range = ({}, {}), BackendConfig range = ({}, {}), ignoring {}\").format(debug_string, app_quant_min, app_quant_max,\n                               backend_quant_min, backend_quant_max, qconfig))\n                 return False\n         # check scale min\n         if backend_scale_min is not None:\n             if app_scale_min is None:\n-                warnings.warn(\"QConfig %s must specify 'eps', ignoring %s\" % (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'eps', ignoring {qconfig}\")\n                 return False\n             elif app_scale_min < backend_scale_min:\n-                warnings.warn((\"QConfig %s eps (%s) must be greater than or equal to \"\n-                              \"the backend's min scale value (%s), ignoring %s\") %\n-                              (debug_string, app_scale_min, backend_scale_min, qconfig))\n+                warnings.warn((\"QConfig {} eps ({}) must be greater than or equal to \"\n+                              \"the backend's min scale value ({}), ignoring {}\").format(debug_string, app_scale_min, backend_scale_min, qconfig))\n                 return False\n         # check fixed scale and zero point\n         if backend_scale_exact_match is not None and backend_zero_point_exact_match is not None:\n@@ -846,12 +843,11 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n             if not isinstance(activation_post_process, FixedQParamsObserver) and \\\n                     not isinstance(activation_post_process, FixedQParamsFakeQuantize):\n                 warnings.warn((\"QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize \"\n-                              \"for fixed qparams ops, ignoring %s.\\n%s\") % (qconfig, suggestion_str))\n+                              \"for fixed qparams ops, ignoring {}.\\n{}\").format(qconfig, suggestion_str))\n                 return False\n             if observer.scale != backend_scale_exact_match or observer.zero_point != backend_zero_point_exact_match:\n-                warnings.warn((\"QConfig fixed scale (%s) and zero point (%s) do not match the backend's \"\n-                              \"(%s and %s), ignoring %s.\\n%s\") %\n-                              (observer.scale, observer.zero_point, backend_scale_exact_match,\n+                warnings.warn((\"QConfig fixed scale ({}) and zero point ({}) do not match the backend's \"\n+                              \"({} and {}), ignoring {}.\\n{}\").format(observer.scale, observer.zero_point, backend_scale_exact_match,\n                               backend_zero_point_exact_match, qconfig, suggestion_str))\n                 return False\n         return True\ndiff --git a/torch/ao/quantization/observer.py b/torch/ao/quantization/observer.py\nindex 3263ae11564129..f5d24c45787265 100644\n--- a/torch/ao/quantization/observer.py\n+++ b/torch/ao/quantization/observer.py\n@@ -118,7 +118,7 @@ def _with_callable_args(cls_or_self, **kwargs):\n     return r.with_callable_args(**kwargs)\n \n \n-ABC: Any = ABCMeta(str(\"ABC\"), (object,), {})  # compatible with Python 2 *and* 3:\n+ABC: Any = ABCMeta(\"ABC\", (object,), {})  # compatible with Python 2 *and* 3:\n \n \n class ObserverBase(ABC, nn.Module):\n@@ -509,7 +509,7 @@ def calculate_qparams(self):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     @torch.jit.export\n     def reset_min_max_vals(self):\n@@ -712,7 +712,7 @@ def calculate_qparams(self):\n         return self._calculate_qparams(self.min_val, self.max_val)\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     def _load_from_state_dict(\n         self,\n@@ -746,7 +746,7 @@ def _load_from_state_dict(\n                 elif name == expected_max_name:\n                     self.max_val.resize_(val.shape)\n                 else:\n-                    warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                    warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n                 # For torchscript module we need to update the attributes here since we do not\n                 # call the `_load_from_state_dict` function defined module.py\n                 if torch.jit.is_scripting():\n@@ -755,7 +755,7 @@ def _load_from_state_dict(\n                     elif name == expected_max_name:\n                         self.max_val.copy_(val)\n                     else:\n-                        warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                        warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n             elif strict:\n                 missing_keys.append(key)\n \n@@ -1265,7 +1265,7 @@ def _load_from_state_dict(\n         )\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n \n class FixedQParamsObserver(ObserverBase):\n@@ -1363,7 +1363,7 @@ def forward(self, x):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"dtype={}, is_dynamic={}\".format(self.dtype, self.is_dynamic)\n+        return f\"dtype={self.dtype}, is_dynamic={self.is_dynamic}\"\n \n     @torch.jit.export\n     def calculate_qparams(self):\n@@ -1518,10 +1518,10 @@ def load_observer_state_dict(mod, obs_dict):\n                 )\n     for k in missing_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Missing keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Missing keys for observer {k} in state_dict\")\n     for k in unexpected_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Unexpected keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Unexpected keys for observer {k} in state_dict\")\n \n \n # Restrict activations to be in the range (0,127)\ndiff --git a/torch/ao/quantization/pt2e/prepare.py b/torch/ao/quantization/pt2e/prepare.py\nindex 2a29a92a986ced..13f73acca354b4 100644\n--- a/torch/ao/quantization/pt2e/prepare.py\n+++ b/torch/ao/quantization/pt2e/prepare.py\n@@ -68,9 +68,9 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n             assert _is_activation_post_process_node(arg, named_modules)\n             assert arg_as_input_act_obs_or_fq is not None\n             observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), \"expect observed argument to be a Node, but got: {}\".format(type(observed_arg))\n+            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n             assert observed_arg in obs_or_fq_map, \\\n-                \"can't refer to a node that does not have observer/fake_quant inserted yet: {}\".format(observed_arg)\n+                f\"can't refer to a node that does not have observer/fake_quant inserted yet: {observed_arg}\"\n             arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n             new_arg = arg\n             obs_or_fq_map[(observed_arg, node)] = arg_as_input_act_obs_or_fq\ndiff --git a/torch/ao/quantization/pt2e/quantizer/quantizer.py b/torch/ao/quantization/pt2e/quantizer/quantizer.py\nindex 3f7531c33a4e8b..46d7286756d17c 100644\n--- a/torch/ao/quantization/pt2e/quantizer/quantizer.py\n+++ b/torch/ao/quantization/pt2e/quantizer/quantizer.py\n@@ -128,24 +128,9 @@ class QuantizationConfig:\n OperatorPatternType = List[Callable]\n OperatorPatternType.__module__ = \"torch.ao.quantization.pt2e.quantizer.quantizer\"\n \n-OperatorConfig = NamedTuple(\n-    \"OperatorConfig\",\n-    # fix List[str] with List[List[Union[nn.Module, FunctionType, BuiltinFunctionType]]]\n-    # Basically we are mapping a quantization config to some list of patterns.\n-    # a pattern is defined as a list of nn module, function or builtin function names\n-    # e.g. [nn.Conv2d, torch.relu, torch.add]\n-    # We have not resolved whether fusion can be considered internal details of the\n-    # quantizer hence it does not need communication to user.\n-    # Note this pattern is not really informative since it does not really\n-    # tell us the graph structure resulting from the list of ops.\n-    [\n-        (\"config\", QuantizationConfig),\n-        (\n-            \"operators\",\n-            List[OperatorPatternType],\n-        ),\n-    ],\n-)\n+class OperatorConfig(NamedTuple):\n+    config: QuantizationConfig\n+    operators: List[OperatorPatternType]\n \n @dataclass\n class QuantizationAnnotation:\ndiff --git a/torch/ao/quantization/qconfig.py b/torch/ao/quantization/qconfig.py\nindex f7489c1ed4d05a..dc8353d6172990 100644\n--- a/torch/ao/quantization/qconfig.py\n+++ b/torch/ao/quantization/qconfig.py\n@@ -103,7 +103,7 @@ def __new__(cls, activation, weight):\n         if isinstance(activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"QConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        return super(QConfig, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n class QConfigDynamic(namedtuple('QConfigDynamic', ['activation', 'weight'])):\n@@ -128,7 +128,7 @@ def __new__(cls, activation=torch.nn.Identity, weight=torch.nn.Identity):\n             raise ValueError(\"QConfigDynamic received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n         warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n-        return super(QConfigDynamic, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n default_qconfig = QConfig(activation=default_observer,\n@@ -236,7 +236,7 @@ def get_default_qconfig(backend='x86', version=0):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     if version == 0:\n@@ -326,7 +326,7 @@ def get_default_qat_qconfig(backend='x86', version=1):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     # Histogram observer is too slow for quantization aware training\ndiff --git a/torch/ao/quantization/quantization_mappings.py b/torch/ao/quantization/quantization_mappings.py\nindex 96db52624acd34..4b3f4d26b2ac09 100644\n--- a/torch/ao/quantization/quantization_mappings.py\n+++ b/torch/ao/quantization/quantization_mappings.py\n@@ -251,7 +251,7 @@ def get_static_quant_module_class(\n         else DEFAULT_STATIC_QUANT_MODULE_MAPPINGS, additional_static_quant_mapping)\n     static_quant_module_class = all_mappings.get(float_module_class, None)\n     assert static_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(static_quant_module_class)\n \n@@ -266,7 +266,7 @@ def get_dynamic_quant_module_class(\n     all_mappings = get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping)\n     dynamic_quant_module_class = all_mappings.get(float_module_class, None)\n     assert dynamic_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(dynamic_quant_module_class)\n \n@@ -300,10 +300,10 @@ def get_default_qconfig_propagation_list() -> Set[Callable]:\n     attribute to in prepare\n     '''\n     QCONFIG_PROPAGATE_MODULE_CLASS_LIST = (\n-        (set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         _INCLUDE_QCONFIG_PROPAGATE_LIST)\n+         _INCLUDE_QCONFIG_PROPAGATE_LIST\n     )\n     return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n \n@@ -332,7 +332,7 @@ def get_quantized_operator(float_op: Union[Callable, str]) -> Callable:\n     '''\n     quantized_op = DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS.get(float_op, None)\n     assert quantized_op is not None, \\\n-        'Operator {} does not have corresponding quantized op'.format(str(float_op))\n+        f'Operator {str(float_op)} does not have corresponding quantized op'\n     return quantized_op\n \n def _get_special_act_post_process(module: torch.nn.Module) -> Optional[Callable]:\ndiff --git a/torch/ao/quantization/quantize.py b/torch/ao/quantization/quantize.py\nindex ca60475fc95a79..23c234c3f35103 100644\n--- a/torch/ao/quantization/quantize.py\n+++ b/torch/ao/quantization/quantize.py\n@@ -442,7 +442,7 @@ def quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8,\n             }\n         else:\n             raise ValueError(\n-                \"Don't know how to quantize with default settings for {}. Provide full qconfig please\".format(dtype))\n+                f\"Don't know how to quantize with default settings for {dtype}. Provide full qconfig please\")\n     elif isinstance(qconfig_spec, set):\n         if dtype is torch.qint8:\n             default_qconfig = default_dynamic_qconfig\ndiff --git a/torch/ao/quantization/utils.py b/torch/ao/quantization/utils.py\nindex 1a82c0fcf432ae..d97b76fb253389 100644\n--- a/torch/ao/quantization/utils.py\n+++ b/torch/ao/quantization/utils.py\n@@ -327,7 +327,7 @@ def check_min_max_valid(min_val: torch.Tensor, max_val: torch.Tensor) -> bool:\n     else:\n         assert torch.all(\n             min_val <= max_val\n-        ), \"min {} should be less than max {}\".format(min_val, max_val)\n+        ), f\"min {min_val} should be less than max {max_val}\"\n \n     return True\n \n"
  },
  {
    "number": 105429,
    "title": "[BE] Enable ruff's UP rules and autoformat benchmarks/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* __->__ #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "fa8ab975fe84d01c544b3392cd7f81aa6d3ca25e",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105429",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105429/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105429.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105429.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105429/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105429/comments",
    "labels": [
      "release notes: distributed (ddp)",
      "open source",
      "module: dynamo",
      "ciflow/inductor",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:22:17.870261Z",
    "state": "open",
    "patch": "From 23d7c3ce7149bd69f018c0cdcfea839e002f2a74 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:22:10 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat benchmarks/\n\n[ghstack-poisoned]\n---\n benchmarks/compare-fastrnn-results.py              |  4 ++--\n benchmarks/cpp/tensorexpr/bench_ops.py             |  4 ++--\n benchmarks/distributed/ddp/benchmark.py            | 14 +++++++-------\n benchmarks/distributed/ddp/diff.py                 | 10 +++++-----\n benchmarks/distributed/pipeline/pipe.py            |  4 ++--\n .../distributed/rpc/parameter_server/launcher.py   |  2 +-\n benchmarks/distributed/rpc/rl/coordinator.py       |  2 +-\n benchmarks/dynamo/_onnx/reporter.py                | 12 ++++++------\n benchmarks/dynamo/benchmarks.py                    |  2 +-\n benchmarks/dynamo/combine_csv.py                   |  2 +-\n benchmarks/dynamo/common.py                        |  4 ++--\n .../dynamo/microbenchmarks/operator_inp_utils.py   |  2 +-\n benchmarks/dynamo/parse_logs.py                    |  2 +-\n benchmarks/dynamo/runner.py                        | 10 +++++-----\n benchmarks/dynamo/timm_models.py                   |  4 ++--\n benchmarks/fastrnns/bench.py                       |  2 +-\n benchmarks/fastrnns/profile.py                     |  8 ++++----\n benchmarks/fastrnns/runner.py                      |  6 +++---\n benchmarks/fastrnns/test.py                        |  4 ++--\n .../framework_overhead_benchmark/C2Module.py       |  4 ++--\n .../framework_overhead_benchmark.py                |  6 +++---\n .../pt_wrapper_module.py                           |  4 ++--\n benchmarks/framework_overhead_benchmark/utils.py   |  2 +-\n .../functional_autograd_benchmark/compare.py       |  4 ++--\n .../functional_autograd_benchmark.py               |  6 +++---\n benchmarks/functional_autograd_benchmark/utils.py  |  2 +-\n benchmarks/instruction_counts/applications/ci.py   |  2 +-\n benchmarks/instruction_counts/core/expand.py       |  2 +-\n benchmarks/operator_benchmark/benchmark_caffe2.py  |  6 +++---\n benchmarks/operator_benchmark/benchmark_core.py    |  8 ++++----\n benchmarks/operator_benchmark/benchmark_pytorch.py |  2 +-\n .../operator_benchmark/common/repeat_benchmark.py  |  2 +-\n benchmarks/overrides_benchmark/bench.py            |  4 ++--\n benchmarks/sparse/dlmc/matmul_bench.py             |  4 ++--\n benchmarks/sparse/dlmc/utils.py                    |  8 ++++----\n benchmarks/tensorexpr/__main__.py                  |  9 ++++-----\n benchmarks/tensorexpr/benchmark.py                 |  4 ++--\n benchmarks/tensorexpr/reduction.py                 |  2 +-\n benchmarks/upload_scribe.py                        |  2 +-\n 39 files changed, 90 insertions(+), 91 deletions(-)\n\ndiff --git a/benchmarks/compare-fastrnn-results.py b/benchmarks/compare-fastrnn-results.py\nindex bc4a55688b17ed..45961ca78de53b 100644\n--- a/benchmarks/compare-fastrnn-results.py\n+++ b/benchmarks/compare-fastrnn-results.py\n@@ -23,9 +23,9 @@ def get_times(json_data):\n parser.add_argument('--format', default='md', type=str, help='output format (csv, md, json, table)')\n args = parser.parse_args()\n \n-with open(args.base, \"r\") as base:\n+with open(args.base) as base:\n     base_times = get_times(json.load(base))\n-with open(args.diff, \"r\") as diff:\n+with open(args.diff) as diff:\n     diff_times = get_times(json.load(diff))\n \n all_keys = set(base_times.keys()).union(diff_times.keys())\ndiff --git a/benchmarks/cpp/tensorexpr/bench_ops.py b/benchmarks/cpp/tensorexpr/bench_ops.py\nindex 12d766ae74862c..fef18f912e75c0 100644\n--- a/benchmarks/cpp/tensorexpr/bench_ops.py\n+++ b/benchmarks/cpp/tensorexpr/bench_ops.py\n@@ -83,8 +83,8 @@ def test_batch_norm():\n         [5, 512, 7, 7]]\n     for n, c, h, w in batch_norm_shapes:\n         x = torch.rand((n, c, h, w))\n-        y = torch.rand((c))\n-        z = torch.rand((c))\n+        y = torch.rand(c)\n+        z = torch.rand(c)\n         traced = torch.jit.trace(lambda x, y, z: op(x, y, z), (x, y, z))\n \n         # Warmup.\ndiff --git a/benchmarks/distributed/ddp/benchmark.py b/benchmarks/distributed/ddp/benchmark.py\nindex c72e3e6a27d961..db592cf6bd32d6 100644\n--- a/benchmarks/distributed/ddp/benchmark.py\n+++ b/benchmarks/distributed/ddp/benchmark.py\n@@ -181,7 +181,7 @@ def __init__(self, device, distributed_backend, bucket_size, model):\n         self.model = model\n \n     def __str__(self):\n-        return \"{} with batch size {}\".format(self.model, self.batch_size)\n+        return f\"{self.model} with batch size {self.batch_size}\"\n \n     def create_model(self):\n         return torchvision.models.__dict__[self.model]().to(self.device)\n@@ -212,7 +212,7 @@ def main():\n     # metadata, like measurements. Not for benchmarking itself.\n     dist.init_process_group(\n         backend=\"gloo\",\n-        init_method=\"tcp://{}:{}\".format(args.master_addr, args.master_port),\n+        init_method=f\"tcp://{args.master_addr}:{args.master_port}\",\n         rank=args.rank,\n         world_size=args.world_size,\n     )\n@@ -227,10 +227,10 @@ def main():\n         print(\"PyTorch distributed benchmark suite\")\n         print(\"-----------------------------------\")\n         print(\"\")\n-        print(\"* PyTorch version: {}\".format(torch.__version__))\n-        print(\"* CUDA version: {}\".format(torch.version.cuda))\n-        print(\"* Distributed backend: {}\".format(args.distributed_backend))\n-        print(\"* Maximum bucket size: {}MB\".format(args.bucket_size))\n+        print(f\"* PyTorch version: {torch.__version__}\")\n+        print(f\"* CUDA version: {torch.version.cuda}\")\n+        print(f\"* Distributed backend: {args.distributed_backend}\")\n+        print(f\"* Maximum bucket size: {args.bucket_size}MB\")\n         print(\"\")\n         print(\"--- nvidia-smi topo -m ---\")\n         print(\"\")\n@@ -261,7 +261,7 @@ def main():\n     benchmark_results = []\n     for benchmark in benchmarks:\n         if args.rank == 0:\n-            print(\"\\nBenchmark: {}\".format(str(benchmark)))\n+            print(f\"\\nBenchmark: {str(benchmark)}\")\n         result = sweep(benchmark)\n         benchmark_results.append({\n             \"model\": benchmark.model,\ndiff --git a/benchmarks/distributed/ddp/diff.py b/benchmarks/distributed/ddp/diff.py\nindex d427a5b29d9199..bce7a8db56c13e 100644\n--- a/benchmarks/distributed/ddp/diff.py\n+++ b/benchmarks/distributed/ddp/diff.py\n@@ -10,7 +10,7 @@\n \n \n def load(path):\n-    with open(path, 'r') as f:\n+    with open(path) as f:\n         return json.load(f)\n \n \n@@ -44,8 +44,8 @@ def main():\n \n         model = ra[\"model\"]\n         batch_size = int(ra[\"batch_size\"])\n-        name = \"{} with batch size {}\".format(model, batch_size)\n-        print(\"Benchmark: {}\".format(name))\n+        name = f\"{model} with batch size {batch_size}\"\n+        print(f\"Benchmark: {name}\")\n \n         # Print header\n         print(\"\")\n@@ -66,13 +66,13 @@ def main():\n             ngpus = len(xa[\"ranks\"])\n             ma = sorted(xa[\"measurements\"])\n             mb = sorted(xb[\"measurements\"])\n-            print(\"{:>4d} GPUs:\".format(ngpus), end='')  # noqa: E999\n+            print(f\"{ngpus:>4d} GPUs:\", end='')  # noqa: E999\n             for p in [75, 95]:\n                 va = np.percentile(ma, p)\n                 vb = np.percentile(mb, p)\n                 # We're measuring time, so lower is better (hence the negation)\n                 delta = -100 * ((vb - va) / va)\n-                print(\"  p{:02d}: {:8.3f}s {:7d}/s {:+8.1f}%\".format(p, vb, int(batch_size / vb), delta), end='')  # noqa: E999\n+                print(f\"  p{p:02d}: {vb:8.3f}s {int(batch_size / vb):7d}/s {delta:+8.1f}%\", end='')  # noqa: E999\n             print(\"\")\n         print(\"\")\n \ndiff --git a/benchmarks/distributed/pipeline/pipe.py b/benchmarks/distributed/pipeline/pipe.py\nindex 8a08d25ca4c940..58f2850e34868e 100644\n--- a/benchmarks/distributed/pipeline/pipe.py\n+++ b/benchmarks/distributed/pipeline/pipe.py\n@@ -16,7 +16,7 @@\n def sizeof_fmt(num, suffix='B'):\n     for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti']:\n         if abs(num) < 1024.0:\n-            return \"%3.2f%sB\" % (num, unit)\n+            return f\"{num:3.2f}{unit}B\"\n         num /= 1024.0\n \n \n@@ -146,7 +146,7 @@ def get_last_device(model):\n             return torch.cuda.current_device()\n \n \n-    print('Number of parameters for model: {}'.format(sum(p.numel() for p in model.parameters())))\n+    print(f'Number of parameters for model: {sum(p.numel() for p in model.parameters())}')\n     for i, batch in enumerate(lm_dataloader):\n         bi = batch[\"input\"]\n         if args.max_batch and i > args.max_batch:\ndiff --git a/benchmarks/distributed/rpc/parameter_server/launcher.py b/benchmarks/distributed/rpc/parameter_server/launcher.py\nindex a4c13cdb29b696..ec6559c8508f9c 100644\n--- a/benchmarks/distributed/rpc/parameter_server/launcher.py\n+++ b/benchmarks/distributed/rpc/parameter_server/launcher.py\n@@ -362,7 +362,7 @@ def get_json_config(file_name, id):\n         file_name (str): name of configuration file to load\n         id (str): configuration that will be loaded\n     \"\"\"\n-    with open(os.path.join(Path(__file__).parent, file_name), \"r\") as f:\n+    with open(os.path.join(Path(__file__).parent, file_name)) as f:\n         json_config = json.load(f)[id]\n     return json_config\n \ndiff --git a/benchmarks/distributed/rpc/rl/coordinator.py b/benchmarks/distributed/rpc/rl/coordinator.py\nindex b488378d5aee58..8b6d246f0861f6 100644\n--- a/benchmarks/distributed/rpc/rl/coordinator.py\n+++ b/benchmarks/distributed/rpc/rl/coordinator.py\n@@ -102,7 +102,7 @@ def run_coordinator(self, episodes, episode_steps, queue):\n                              'observer throughput': {}}\n \n \n-        print(\"For batch size {0}\".format(self.batch_size))\n+        print(f\"For batch size {self.batch_size}\")\n         print(\"\\nAgent Latency - \", len(agent_latency_final))\n         agent_latency_final = sorted(agent_latency_final)\n         for p in [50, 75, 90, 95]:\ndiff --git a/benchmarks/dynamo/_onnx/reporter.py b/benchmarks/dynamo/_onnx/reporter.py\nindex f10b50c4e323df..9318b7a6dc2e47 100644\n--- a/benchmarks/dynamo/_onnx/reporter.py\n+++ b/benchmarks/dynamo/_onnx/reporter.py\n@@ -22,7 +22,7 @@\n _COMPACT_ERROR_GROUP = False\n \n \n-class ErrorAggregator(object):\n+class ErrorAggregator:\n     \"\"\"\n     Collect and group error messages for report at the end.\n \n@@ -47,7 +47,7 @@ class ErrorAggregator(object):\n     ]\n \n     def __init__(self, log: Optional[logging.Logger] = None):\n-        super(ErrorAggregator, self).__init__()\n+        super().__init__()\n         self.error_groups = []\n         self.bigram_to_group_ids = collections.defaultdict(list)\n         self.log = log or logging.getLogger(__name__)\n@@ -141,7 +141,7 @@ def __len__(self):\n         return sum(map(len, self.error_groups))\n \n \n-class ErrorAggregatorDict(object):\n+class ErrorAggregatorDict:\n     \"\"\"\n     Collect error types and individually group their error messages for a debug report at the end.\n \n@@ -152,7 +152,7 @@ class ErrorAggregatorDict(object):\n     \"\"\"\n \n     def __init__(self):\n-        super(ErrorAggregatorDict, self).__init__()\n+        super().__init__()\n         self.aggregator: Dict[str, ErrorAggregator] = dict()\n \n     def __getitem__(self, item: str):\n@@ -179,7 +179,7 @@ def record(self, error_type: str, error: str, module: str):\n             log.exception(\"%s error from %s\", error_type, module)\n \n \n-class ExportErrorCsvParser(object):\n+class ExportErrorCsvParser:\n     \"\"\"Parses `*_export_error.csv` produced by onnxbench, aggregates errors and produces report.\n \n     Two types of aggregations are performed.\n@@ -310,7 +310,7 @@ def row(self) -> List[str]:\n         return [getattr(self, field.name) for field in dataclasses.fields(self)]\n \n \n-class ExportErrorParser(object):\n+class ExportErrorParser:\n     def __init__(self, device: str, model_name: str, batch_size: int):\n         self.device = device\n         self.model_name = model_name\ndiff --git a/benchmarks/dynamo/benchmarks.py b/benchmarks/dynamo/benchmarks.py\nindex 36aaf33df96b1f..cb4cc84867ca9b 100755\n--- a/benchmarks/dynamo/benchmarks.py\n+++ b/benchmarks/dynamo/benchmarks.py\n@@ -9,7 +9,7 @@\n # TOOD(voz): Someday, consolidate all the files into one runner instead of a shim like this...\n def model_names(filename: str) -> Set[str]:\n     names = set()\n-    with open(filename, \"r\") as fh:\n+    with open(filename) as fh:\n         lines = fh.readlines()\n         lines = [line.rstrip() for line in lines]\n         for line in lines:\ndiff --git a/benchmarks/dynamo/combine_csv.py b/benchmarks/dynamo/combine_csv.py\nindex b579e0a1bbbd5c..560b8a3cf2405a 100644\n--- a/benchmarks/dynamo/combine_csv.py\n+++ b/benchmarks/dynamo/combine_csv.py\n@@ -11,7 +11,7 @@\n RESULTS = defaultdict(dict)\n \n for side, f in zip([\"static\", \"dynamic\"], sys.argv[1:]):\n-    with open(f, \"r\") as f:\n+    with open(f) as f:\n         reader = csv.DictReader(f)\n         for row in reader:\n             RESULTS[(row[\"bench\"], row[\"name\"])][side] = row\ndiff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py\nindex cabc18f35697a7..0e1ce36461a122 100644\n--- a/benchmarks/dynamo/common.py\n+++ b/benchmarks/dynamo/common.py\n@@ -341,7 +341,7 @@ def load_model_from_path(path_and_class_str):\n \n def output_csv(filename, headers, row):\n     if os.path.exists(filename):\n-        with open(filename, \"r\") as fd:\n+        with open(filename) as fd:\n             lines = list(csv.reader(fd)) or [[]]\n             if headers and len(headers) > len(lines[0]):\n                 # if prior results failed the header might not be filled in yet\n@@ -1417,7 +1417,7 @@ def read_batch_size_from_file(args, filename, model_name):\n     if os.path.exists(\"benchmarks\"):\n         filename = os.path.join(\"benchmarks\", filename)\n     assert os.path.exists(filename), filename\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n         lines = [i.split(\",\") for i in lines if len(i.strip()) > 0]\n         for val in lines:\ndiff --git a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\nindex 997624f583b4c8..f83568c4db6cc6 100644\n--- a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n+++ b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n@@ -240,7 +240,7 @@ class OperatorInputsLoader:\n     def __init__(self, json_file_path):\n         self.operator_db = defaultdict(Counter)\n \n-        with open(json_file_path, \"r\") as f:\n+        with open(json_file_path) as f:\n             lines = f.readlines()\n \n         i = 0\ndiff --git a/benchmarks/dynamo/parse_logs.py b/benchmarks/dynamo/parse_logs.py\nindex aeb72c231c5462..8ae272897903f9 100644\n--- a/benchmarks/dynamo/parse_logs.py\n+++ b/benchmarks/dynamo/parse_logs.py\n@@ -15,7 +15,7 @@\n \n assert len(sys.argv) == 2\n \n-full_log = open(sys.argv[1], \"r\").read()\n+full_log = open(sys.argv[1]).read()\n \n # If the log contains a gist URL, extract it so we can include it in the CSV\n gist_url = \"\"\ndiff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py\nindex c3c3e8bc046c8e..5a544914ca74bd 100755\n--- a/benchmarks/dynamo/runner.py\n+++ b/benchmarks/dynamo/runner.py\n@@ -608,7 +608,7 @@ def __init__(\n \n     def has_header(self, output_filename):\n         header_present = False\n-        with open(output_filename, \"r\") as f:\n+        with open(output_filename) as f:\n             line = f.readline()\n             if \"dev\" in line:\n                 header_present = True\n@@ -1026,7 +1026,7 @@ def __init__(self, args):\n         assert os.path.exists(self.lookup_file)\n \n     def generate_diff(self, last2, filename, caption):\n-        df_cur, df_prev = [pd.read_csv(os.path.join(path, filename)) for path in last2]\n+        df_cur, df_prev = (pd.read_csv(os.path.join(path, filename)) for path in last2)\n         df_merge = df_cur.merge(df_prev, on=\"Compiler\", suffixes=(\"_cur\", \"_prev\"))\n         data = {col: [] for col in (\"compiler\", \"suite\", \"prev_value\", \"cur_value\")}\n         for _, row in df_merge.iterrows():\n@@ -1145,10 +1145,10 @@ def generate_comment(self):\n                     if last2[compiler] is None:\n                         continue\n \n-                    df_cur, df_prev = [\n+                    df_cur, df_prev = (\n                         last2[compiler][i].untouched_parsed_frames[suite][metric]\n                         for i in (0, 1)\n-                    ]\n+                    )\n                     df_merge = df_cur.merge(\n                         df_prev, on=\"name\", suffixes=(\"_cur\", \"_prev\")\n                     )\n@@ -1367,7 +1367,7 @@ def gen_comment(self):\n         all_lines = []\n         for f in files:\n             try:\n-                with open(os.path.join(self.output_dir, f), \"r\") as fh:\n+                with open(os.path.join(self.output_dir, f)) as fh:\n                     all_lines.extend(fh.readlines())\n             except FileNotFoundError:\n                 pass\ndiff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py\nindex 75769f7cb6c50a..587dbb93683f3f 100755\n--- a/benchmarks/dynamo/timm_models.py\n+++ b/benchmarks/dynamo/timm_models.py\n@@ -31,7 +31,7 @@ def pip_install(package):\n TIMM_MODELS = dict()\n filename = os.path.join(os.path.dirname(__file__), \"timm_models_list.txt\")\n \n-with open(filename, \"r\") as fh:\n+with open(filename) as fh:\n     lines = fh.readlines()\n     lines = [line.rstrip() for line in lines]\n     for line in lines:\n@@ -92,7 +92,7 @@ def read_models_from_docs():\n         models = set()\n         # TODO - set the path to pytorch-image-models repo\n         for fn in glob.glob(\"../pytorch-image-models/docs/models/*.md\"):\n-            with open(fn, \"r\") as f:\n+            with open(fn) as f:\n                 while True:\n                     line = f.readline()\n                     if not line:\ndiff --git a/benchmarks/fastrnns/bench.py b/benchmarks/fastrnns/bench.py\nindex d4b70ff78b7a72..f0e9679b80f275 100644\n--- a/benchmarks/fastrnns/bench.py\n+++ b/benchmarks/fastrnns/bench.py\n@@ -187,7 +187,7 @@ def bench(rnn_runners, group_name, print_json=False, sep=' ', **params):\n \n \n def bench_group(model_list, bench_name, bench_group, bench_args):\n-    print_stderr('Benchmarking {}s...'.format(bench_name))\n+    print_stderr(f'Benchmarking {bench_name}s...')\n     nn_results = bench(get_nn_runners(*model_list), bench_group, **bench_args)\n     print_stderr('')\n     return nn_results\ndiff --git a/benchmarks/fastrnns/profile.py b/benchmarks/fastrnns/profile.py\nindex 7f3de61ef9c39f..10707fab986bc7 100644\n--- a/benchmarks/fastrnns/profile.py\n+++ b/benchmarks/fastrnns/profile.py\n@@ -54,7 +54,7 @@ def profile(rnns, sleep_between_seconds=1, nloops=5,\n \n def system(command):\n     \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n-    print('[system] {}'.format(command))\n+    print(f'[system] {command}')\n     p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, shell=True)\n     output, err = p.communicate()\n@@ -87,13 +87,13 @@ def nvprof_output_filename(rnns, **params):\n \n \n def nvprof(cmd, outpath):\n-    return system('nvprof -o {} {}'.format(outpath, cmd))\n+    return system(f'nvprof -o {outpath} {cmd}')\n \n \n def full_profile(rnns, **args):\n     profile_args = []\n     for k, v in args.items():\n-        profile_args.append('--{}={}'.format(k, v))\n+        profile_args.append(f'--{k}={v}')\n     profile_args.append('--rnns {}'.format(' '.join(rnns)))\n     profile_args.append('--internal-run')\n \n@@ -103,7 +103,7 @@ def full_profile(rnns, **args):\n         sys.executable, ' '.join(profile_args))\n     rc, stdout, stderr = nvprof(cmd, outpath)\n     if rc != 0:\n-        raise RuntimeError('stderr: {}\\nstdout: {}'.format(stderr, stdout))\n+        raise RuntimeError(f'stderr: {stderr}\\nstdout: {stdout}')\n \n \n if __name__ == '__main__':\ndiff --git a/benchmarks/fastrnns/runner.py b/benchmarks/fastrnns/runner.py\nindex c6bf3727f38071..c33f3c92ad0f04 100644\n--- a/benchmarks/fastrnns/runner.py\n+++ b/benchmarks/fastrnns/runner.py\n@@ -11,7 +11,7 @@\n                       varlen_lstm_creator, varlen_pytorch_lstm_creator)\n \n \n-class DisableCuDNN():\n+class DisableCuDNN:\n     def __enter__(self):\n         self.saved = torch.backends.cudnn.enabled\n         torch.backends.cudnn.enabled = False\n@@ -20,7 +20,7 @@ def __exit__(self, *args, **kwargs):\n         torch.backends.cudnn.enabled = self.saved\n \n \n-class DummyContext():\n+class DummyContext:\n     def __enter__(self):\n         pass\n \n@@ -28,7 +28,7 @@ def __exit__(self, *args, **kwargs):\n         pass\n \n \n-class AssertNoJIT():\n+class AssertNoJIT:\n     def __enter__(self):\n         import os\n         enabled = os.environ.get('PYTORCH_JIT', 1)\ndiff --git a/benchmarks/fastrnns/test.py b/benchmarks/fastrnns/test.py\nindex a56cf928fd7add..640af10b95c042 100644\n--- a/benchmarks/fastrnns/test.py\n+++ b/benchmarks/fastrnns/test.py\n@@ -71,7 +71,7 @@ def test_vl_py(**test_args):\n     control_creator = varlen_pytorch_lstm_creator\n     name, experim_creator, context = get_nn_runners('vl_py')[0]\n     with context():\n-        print('testing {}...'.format(name))\n+        print(f'testing {name}...')\n         creator_keys = [\n             'seqLength', 'numLayers', 'inputSize',\n             'hiddenSize', 'miniBatch', 'device', 'seed'\n@@ -154,5 +154,5 @@ def test_vl_py(**test_args):\n \n     for name, creator, context in rnn_runners:\n         with context():\n-            print('testing {}...'.format(name))\n+            print(f'testing {name}...')\n             test_rnns(creator, pytorch_lstm_creator, **test_args)\ndiff --git a/benchmarks/framework_overhead_benchmark/C2Module.py b/benchmarks/framework_overhead_benchmark/C2Module.py\nindex dfc5e6e79098a6..b6b80e83db07d0 100644\n--- a/benchmarks/framework_overhead_benchmark/C2Module.py\n+++ b/benchmarks/framework_overhead_benchmark/C2Module.py\n@@ -20,14 +20,14 @@ class C2SimpleNet:\n     def __init__(self, op_name, num_inputs=1, debug=False):\n         self.input_names = []\n         self.net = core.Net(\"framework_benchmark_net\")\n-        self.input_names = [\"in_{}\".format(i) for i in range(num_inputs)]\n+        self.input_names = [f\"in_{i}\" for i in range(num_inputs)]\n         for i in range(num_inputs):\n             add_blob(workspace, self.input_names[i], [1])\n         self.net.AddExternalInputs(self.input_names)\n         op_constructor = getattr(self.net, op_name)\n         op_constructor(self.input_names)\n         self.output_name = self.net._net.op[-1].output\n-        print(\"Benchmarking op {}:\".format(op_name))\n+        print(f\"Benchmarking op {op_name}:\")\n         for _ in range(NUM_LOOP_ITERS):\n             output_name = self.net._net.op[-1].output\n             self.input_names[-1] = output_name[0]\ndiff --git a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\nindex 727b78197b39bc..fd02a00c43655d 100644\n--- a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n+++ b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n@@ -31,7 +31,7 @@ def parse_op_args(op):\n def print_results(result):\n     print(\"===================================\")\n     for key, value in result.items():\n-        print(\"{}, latency per iter (us):{}\".format(key, ms_to_us(value)))\n+        print(f\"{key}, latency per iter (us):{ms_to_us(value)}\")\n     print(\"===================================\")\n \n def benchmark_simple_fn(args, config, module_config, module_type, result):\n@@ -46,7 +46,7 @@ def benchmark_simple_fn(args, config, module_config, module_type, result):\n         result:         dictionary instance to be populated with the benchmark result (latency per iter).\n     \"\"\"\n     benchmark_c2_net = args.benchmark_c2_net\n-    print(\"Benchmarking {}\".format(module_type.__name__))\n+    print(f\"Benchmarking {module_type.__name__}\")\n     if benchmark_c2_net:\n         op_name = module_config.c2_op\n         num_inputs = module_config.num_params\n@@ -86,7 +86,7 @@ def main():\n     args = parser.parse_args()\n \n     if args.op not in SUPPORTED_OPS:\n-        print(\"Op {} is not supported: Supported ops are:{}\".format(args.op, SUPPORTED_OPS))\n+        print(f\"Op {args.op} is not supported: Supported ops are:{SUPPORTED_OPS}\")\n         return\n     assert not (args.benchmark_c2_net and args.use_throughput_benchmark), \\\n         \"Benchmarking of C2 net via throughput benchmarking is not yet supported\"\ndiff --git a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\nindex 154564f1c6d799..19f2471cbbfaaa 100644\n--- a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n+++ b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n@@ -31,8 +31,8 @@ def __init__(self, wrapped_type, module_config, debug, save=False):\n             if save:\n                 file_name = self.module_name + \"_\" + pt_fn.__name__ + \".pt\"\n                 torch.jit.save(self.module, file_name)\n-                print(\"Generated graph is saved in {}\".format(file_name))\n-        print(\"Benchmarking module {} with fn {}: Graph mode:{}\".format(self.module_name, pt_fn.__name__, module_config.graph_mode))\n+                print(f\"Generated graph is saved in {file_name}\")\n+        print(f\"Benchmarking module {self.module_name} with fn {pt_fn.__name__}: Graph mode:{module_config.graph_mode}\")\n         if (debug and isinstance(self.module, torch.jit.ScriptModule)):\n             print(self.module.graph)\n             print(self.module.code)\ndiff --git a/benchmarks/framework_overhead_benchmark/utils.py b/benchmarks/framework_overhead_benchmark/utils.py\nindex 9e760d404339ed..2efb67a51f7887 100644\n--- a/benchmarks/framework_overhead_benchmark/utils.py\n+++ b/benchmarks/framework_overhead_benchmark/utils.py\n@@ -26,7 +26,7 @@ def benchmark_module(config, module, use_throughput_benchmark=False):\n     if use_throughput_benchmark:\n         return benchmark_using_throughput_benchmark(config, module)\n     module.forward(config.num_warmup_iters)\n-    print(\"Running module for {} iterations\".format(config.num_iters))\n+    print(f\"Running module for {config.num_iters} iterations\")\n     start = time.time()\n     module.forward(config.num_iters)\n     end = time.time()\ndiff --git a/benchmarks/functional_autograd_benchmark/compare.py b/benchmarks/functional_autograd_benchmark/compare.py\nindex c2c4ef6c95d5be..65a4a3afcea881 100644\n--- a/benchmarks/functional_autograd_benchmark/compare.py\n+++ b/benchmarks/functional_autograd_benchmark/compare.py\n@@ -10,11 +10,11 @@ def main():\n     parser.add_argument(\"--output\", type=str, default=\"\", help=\"Text file where to write the output\")\n     args = parser.parse_args()\n \n-    with open(args.before, \"r\") as f:\n+    with open(args.before) as f:\n         content = f.read()\n     res_before = from_markdown_table(content)\n \n-    with open(args.after, \"r\") as f:\n+    with open(args.after) as f:\n         content = f.read()\n     res_after = from_markdown_table(content)\n \ndiff --git a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\nindex 1b0ef20902da9b..76c447f04a496f 100644\n--- a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n+++ b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n@@ -198,7 +198,7 @@ def noop():\n             pass\n         do_sync = noop\n     else:\n-        device = torch.device(\"cuda:{}\".format(args.gpu))\n+        device = torch.device(f\"cuda:{args.gpu}\")\n         do_sync = torch.cuda.synchronize\n \n     model, inp = model_getter(device)\n@@ -257,7 +257,7 @@ def main():\n             runtimes = torch.tensor(runtimes)\n             mean, var = runtimes.mean(), runtimes.var()\n             results[name][task] = (mean.item(), var.item())\n-            print(\"Results for model {} on task {}: {}s (var: {})\".format(name, task, mean, var))\n+            print(f\"Results for model {name} on task {task}: {mean}s (var: {var})\")\n \n             if has_functorch:\n                 try:\n@@ -269,7 +269,7 @@ def main():\n                 runtimes = torch.tensor(runtimes)\n                 mean, var = runtimes.mean(), runtimes.var()\n                 results[name][f\"functorch {task}\"] = (mean.item(), var.item())\n-                print(\"Results for model {} on task {} using Functorch: {}s (var: {})\".format(name, task, mean, var))\n+                print(f\"Results for model {name} on task {task} using Functorch: {mean}s (var: {var})\")\n \n     if args.output:\n         with open(args.output, \"w\") as f:\ndiff --git a/benchmarks/functional_autograd_benchmark/utils.py b/benchmarks/functional_autograd_benchmark/utils.py\nindex dcf03e7a28d085..23f3481cbde117 100644\n--- a/benchmarks/functional_autograd_benchmark/utils.py\n+++ b/benchmarks/functional_autograd_benchmark/utils.py\n@@ -97,7 +97,7 @@ def from_markdown_table(data: str) -> TimingResultType:\n     res = defaultdict(defaultdict)\n \n     for line in out:\n-        model, task, mean, var = [f.strip() for f in line.strip().split(\"|\") if f]\n+        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n         res[model][task] = (float(mean), float(var))\n \n     return res\ndiff --git a/benchmarks/instruction_counts/applications/ci.py b/benchmarks/instruction_counts/applications/ci.py\nindex 85ee9881d83b55..c34c60b094a857 100644\n--- a/benchmarks/instruction_counts/applications/ci.py\n+++ b/benchmarks/instruction_counts/applications/ci.py\n@@ -70,7 +70,7 @@ def main(argv: List[str]) -> None:\n     }\n \n     if args.destination:\n-        with open(args.destination, \"wt\") as f:\n+        with open(args.destination, \"w\") as f:\n             json.dump(final_results, f)\n \n     if in_debug_mode:\ndiff --git a/benchmarks/instruction_counts/core/expand.py b/benchmarks/instruction_counts/core/expand.py\nindex f6713ee65cb93c..c60925d9e14e91 100644\n--- a/benchmarks/instruction_counts/core/expand.py\n+++ b/benchmarks/instruction_counts/core/expand.py\n@@ -58,7 +58,7 @@ def _generate_torchscript_file(model_src: str, name: str) -> Optional[str]:\n         # to confirm.\n         raise ValueError(f\"File {module_path} already exists.\")\n \n-    with open(module_path, \"wt\") as f:\n+    with open(module_path, \"w\") as f:\n         f.write(model_src)\n \n     # Import magic to actually load our function.\ndiff --git a/benchmarks/operator_benchmark/benchmark_caffe2.py b/benchmarks/operator_benchmark/benchmark_caffe2.py\nindex d5939030d03c1a..df27a172739bf6 100644\n--- a/benchmarks/operator_benchmark/benchmark_caffe2.py\n+++ b/benchmarks/operator_benchmark/benchmark_caffe2.py\n@@ -122,7 +122,7 @@ def run_forward(self, num_runs, print_per_iter=False, cuda_sync=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.forward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator test case: {self.test_name}\")\n \n     def run_backward(self, num_runs, print_per_iter=False):\n         \"\"\" Run the backward path of an operator in a loop\n@@ -130,7 +130,7 @@ def run_backward(self, num_runs, print_per_iter=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.backward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator gradient test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator gradient test case: {self.test_name}\")\n \n     def _print_per_iter(self):\n         pass\n@@ -140,7 +140,7 @@ def create_caffe2_op_test_case(op_bench, test_config):\n     test_case = Caffe2OperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\n \n \ndiff --git a/benchmarks/operator_benchmark/benchmark_core.py b/benchmarks/operator_benchmark/benchmark_core.py\nindex d6fd00f0522b38..3dfb6e3b00d42c 100644\n--- a/benchmarks/operator_benchmark/benchmark_core.py\n+++ b/benchmarks/operator_benchmark/benchmark_core.py\n@@ -197,7 +197,7 @@ def _print_header(self):\n             print(\"# List of Operators to run:\")\n             self.printed_ops_list = set()\n             if self.args.operators:\n-                print(\"# {}\".format(self.args.operators))\n+                print(f\"# {self.args.operators}\")\n \n     def _print_perf_result(self, reported_run_time_us, test_case):\n         if self.args.report_aibench:\n@@ -206,7 +206,7 @@ def _print_perf_result(self, reported_run_time_us, test_case):\n             return\n             test_name = '_'.join([test_case.framework, test_case.test_config.test_name])\n             for run in range(self.num_runs):\n-                print(\"{}Observer \".format(test_case.framework) + json.dumps(\n+                print(f\"{test_case.framework}Observer \" + json.dumps(\n                     {\n                         \"type\": test_name,\n                         \"metric\": \"latency\",\n@@ -349,14 +349,14 @@ def _keep_test(self, test_case):\n     def _print_test_case_info(self, test_case):\n         # Print out the test name and skip the real execution\n         if self.args.list_tests:\n-            print(\"# {}\".format(test_case.test_config.test_name))\n+            print(f\"# {test_case.test_config.test_name}\")\n             return True\n         elif self.args.list_ops:\n             if self.args.operators is None:\n                 op_name = test_case.op_bench.module_name()\n \n                 if op_name not in self.printed_ops_list:\n-                    print(\"# {}\".format(op_name))\n+                    print(f\"# {op_name}\")\n                     self.printed_ops_list.add(op_name)\n             return True\n \ndiff --git a/benchmarks/operator_benchmark/benchmark_pytorch.py b/benchmarks/operator_benchmark/benchmark_pytorch.py\nindex e9a9b3c5de42ad..c4a82dff2ba5c1 100644\n--- a/benchmarks/operator_benchmark/benchmark_pytorch.py\n+++ b/benchmarks/operator_benchmark/benchmark_pytorch.py\n@@ -192,5 +192,5 @@ def create_pytorch_op_test_case(op_bench, test_config):\n     test_case = PyTorchOperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\ndiff --git a/benchmarks/operator_benchmark/common/repeat_benchmark.py b/benchmarks/operator_benchmark/common/repeat_benchmark.py\nindex b744a95d217363..337bf525b29caf 100644\n--- a/benchmarks/operator_benchmark/common/repeat_benchmark.py\n+++ b/benchmarks/operator_benchmark/common/repeat_benchmark.py\n@@ -54,4 +54,4 @@ def pt_repeat_n_times(niters):\n     total_time_s = (time.time() - s)\n     total_time_per_iter_s = total_time_s / NUM_BENCHMARK_ITERS\n     achieved_bandwidth = (total_bytes * BYTES_TO_MB) / total_time_per_iter_s\n-    print(\"Time:{} Achieved Bandwidth:{} MB/s\".format(total_time_per_iter_s, achieved_bandwidth))\n+    print(f\"Time:{total_time_per_iter_s} Achieved Bandwidth:{achieved_bandwidth} MB/s\")\ndiff --git a/benchmarks/overrides_benchmark/bench.py b/benchmarks/overrides_benchmark/bench.py\nindex b6dbd0c2f8d64c..2c591a6e569793 100644\n--- a/benchmarks/overrides_benchmark/bench.py\n+++ b/benchmarks/overrides_benchmark/bench.py\n@@ -56,8 +56,8 @@ def main():\n \n         bench_min, bench_std = bench(tensor_1, tensor_2)\n         print(\n-            \"Type {0} had a minimum time of {1} us\"\n-            \" and a standard deviation of {2} us.\".format(\n+            \"Type {} had a minimum time of {} us\"\n+            \" and a standard deviation of {} us.\".format(\n                 t.__name__, (10 ** 6 * bench_min), (10 ** 6) * bench_std\n             )\n         )\ndiff --git a/benchmarks/sparse/dlmc/matmul_bench.py b/benchmarks/sparse/dlmc/matmul_bench.py\nindex 6b896ddf34a635..8d37d3242dd1bf 100644\n--- a/benchmarks/sparse/dlmc/matmul_bench.py\n+++ b/benchmarks/sparse/dlmc/matmul_bench.py\n@@ -62,8 +62,8 @@ def filter_ops(operation):\n             test_name = device + \":matmul-forward\"\n             return list(filter(None, [\n                 (test_name, device, \"torch:\" + operation.replace(\"sparse\", \"dense\"),\n-                 \"{}(dx, dy)\".format(OPS_MAP[operation])),\n-                (test_name, device, \"torch:\" + operation, \"{}(x, y)\".format(OPS_MAP[operation])),\n+                 f\"{OPS_MAP[operation]}(dx, dy)\"),\n+                (test_name, device, \"torch:\" + operation, f\"{OPS_MAP[operation]}(x, y)\"),\n                 (test_name, device, \"scipy:\" + operation, \"scipy_matmul(sx, sy)\") if device == \"cpu\" else None\n             ]))\n \ndiff --git a/benchmarks/sparse/dlmc/utils.py b/benchmarks/sparse/dlmc/utils.py\nindex 3079abf6e1dff2..8fad391f63f83d 100644\n--- a/benchmarks/sparse/dlmc/utils.py\n+++ b/benchmarks/sparse/dlmc/utils.py\n@@ -21,7 +21,7 @@ def sparse_grad_output(a, b):\n \n \n def read_matrix_params(path):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         line = file.readline()\n         nrows, ncols, nnz = (int(el) for el in line.split(', '))\n         return (nrows, ncols), nnz\n@@ -38,7 +38,7 @@ def csr_to_coo(indices, indptr, shape):\n \n \n def load_sparse_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -51,7 +51,7 @@ def load_sparse_matrix(path, device):\n \n \n def gen_vector(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -59,7 +59,7 @@ def gen_vector(path, device):\n \n \n def gen_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\ndiff --git a/benchmarks/tensorexpr/__main__.py b/benchmarks/tensorexpr/__main__.py\nindex ed632e966b2cb4..fa81ea6bb1c611 100644\n--- a/benchmarks/tensorexpr/__main__.py\n+++ b/benchmarks/tensorexpr/__main__.py\n@@ -157,7 +157,7 @@ def main():\n         torch._C._jit_set_nvfuser_enabled(True)\n         torch._C._get_graph_executor_optimize(True)\n     else :\n-        raise ValueError(\"Undefined fuser: {}\".format(args.cuda_fuser))\n+        raise ValueError(f\"Undefined fuser: {args.cuda_fuser}\")\n \n     if args.cpu_fusion:\n         import torch\n@@ -207,7 +207,7 @@ def set_global_threads(num_threads):\n     for index, dtype in enumerate(datatypes):\n         datatypes[index] = getattr(torch, dtype)\n         if not datatypes[index] :\n-            raise AttributeError(\"DataType: {} is not valid!\".format(dtype))\n+            raise AttributeError(f\"DataType: {dtype} is not valid!\")\n \n     tensor_engine.set_engine_mode(args.engine)\n \n@@ -282,7 +282,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                         run_with_input_iter(bench_cls, args.input_iter, allow_skip=True)\n                     else :\n                         if args.input_iter is not None :\n-                            print(\"WARNING: Incompatible benchmark class called with input_iter arg: {}\".format(name))\n+                            print(f\"WARNING: Incompatible benchmark class called with input_iter arg: {name}\")\n                         run_default_configs(bench_cls, allow_skip=True)\n \n             if match_class_name:\n@@ -321,8 +321,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                     [bench_cls.module() for bench_cls in benchmark_classes]\n                 )\n                 raise ValueError(\n-                    \"invalid name: %s\\nAvailable benchmark classes:\\n%s\"\n-                    % (name, available_classes)\n+                    f\"invalid name: {name}\\nAvailable benchmark classes:\\n{available_classes}\"\n                 )\n \n \ndiff --git a/benchmarks/tensorexpr/benchmark.py b/benchmarks/tensorexpr/benchmark.py\nindex 7a5b255da904aa..2730a1be24784b 100644\n--- a/benchmarks/tensorexpr/benchmark.py\n+++ b/benchmarks/tensorexpr/benchmark.py\n@@ -66,7 +66,7 @@ def desc(self):\n         if \"NNC_NUM_THREADS\" in os.environ:\n             num_threads_str = os.environ[\"NNC_NUM_THREADS\"]\n             device += num_threads_str\n-        return \"%s: %s_%s_%s_%s\" % (\n+        return \"{}: {}_{}_{}_{}\".format(\n             self.engine.mode,\n             self.module(),\n             self.mode,\n@@ -203,7 +203,7 @@ def dump_result(self, result_dict):\n         if self.output_type == \"json\":\n             print(json.dumps(result_dict))\n         elif self.output_type == \"stdout\":\n-            msg = \"%s: %.2f us, SOL %.2f GB/s, algorithmic %.2f GB/s\" % (\n+            msg = \"{}: {:.2f} us, SOL {:.2f} GB/s, algorithmic {:.2f} GB/s\".format(\n                 result_dict[\"desc\"],\n                 result_dict[\"us\"],\n                 result_dict[\"sol\"],\ndiff --git a/benchmarks/tensorexpr/reduction.py b/benchmarks/tensorexpr/reduction.py\nindex 77d64074eb81d1..3613001667746d 100644\n--- a/benchmarks/tensorexpr/reduction.py\n+++ b/benchmarks/tensorexpr/reduction.py\n@@ -139,7 +139,7 @@ def __init__(self, mode, device, dtype, red_dim, dim0, dim1):\n         )]\n \n         if red_dim != 0 and red_dim != 1 :\n-            raise ValueError(\"invalid reduction dimension: {}\".format(red_dim))\n+            raise ValueError(f\"invalid reduction dimension: {red_dim}\")\n \n     def forward(self, inputs):\n         x = self.add(inputs, 0.001)\ndiff --git a/benchmarks/upload_scribe.py b/benchmarks/upload_scribe.py\nindex d476ade1b8df4d..551544b2d288ae 100644\n--- a/benchmarks/upload_scribe.py\n+++ b/benchmarks/upload_scribe.py\n@@ -95,7 +95,7 @@ def post_pytest_benchmarks(self, pytest_json):\n         for b in pytest_json['benchmarks']:\n             test = b['name'].split('[')[0]\n             net_name = b['params']['net_name']\n-            benchmark_name = '{}[{}]'.format(test, net_name)\n+            benchmark_name = f'{test}[{net_name}]'\n             executor = b['params']['executor']\n             fuser = b['params']['fuser']\n             m = self.format_message({\n"
  },
  {
    "number": 105428,
    "title": "[BE] Enable ruff's UP rules and autoformat tools and scripts",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* __->__ #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "b82fcad264ff04059b1098f7b6a676969f1b53f1",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105428",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105428/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105428.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105428.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105428/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105428/comments",
    "labels": [
      "open source",
      "ciflow/trunk",
      "merging",
      "release notes: releng",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:20:58.373591Z",
    "state": "open",
    "patch": "From 3de2e9db6089b29b70b6d15597de0c64f1452c60 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:20:51 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat tools and scripts\n\n[ghstack-poisoned]\n---\n .ci/pytorch/create_test_cert.py               |  4 +-\n .../perf_test/compare_with_baseline.py        |  4 +-\n .../cimodel/data/binary_build_definitions.py  |  2 +-\n .../cimodel/data/pytorch_build_definitions.py |  4 +-\n .circleci/generate_config_yml.py              |  4 +-\n .github/scripts/ensure_actions_will_cancel.py |  4 +-\n .github/scripts/file_io_utils.py              |  2 +-\n .github/scripts/filter_test_configs.py        |  2 +-\n .github/scripts/generate_pytorch_version.py   |  2 +-\n .github/scripts/label_utils.py                |  2 +-\n .github/scripts/lint_native_functions.py      |  2 +-\n .github/scripts/run_torchbench.py             |  4 +-\n .github/scripts/test_check_labels.py          |  2 +-\n .github/scripts/test_trymerge.py              |  2 +-\n .github/scripts/trymerge.py                   |  6 +--\n .github/scripts/trymerge_explainer.py         |  2 +-\n docs/caffe2/process.py                        |  4 +-\n docs/cpp/source/conf.py                       |  3 +-\n docs/source/conf.py                           |  3 +-\n .../scripts/exportdb/generate_example_rst.py  |  4 +-\n setup.py                                      | 38 +++++++++----------\n tools/amd_build/build_amd.py                  |  8 ++--\n tools/autograd/gen_python_functions.py        |  8 ++--\n tools/autograd/load_derivatives.py            |  2 +-\n .../gen_op_registration_allowlist.py          |  4 +-\n tools/code_analyzer/gen_oplist.py             |  2 +-\n tools/coverage_plugins_package/setup.py       |  2 +-\n tools/download_mnist.py                       | 10 ++---\n tools/gen_vulkan_spv.py                       | 20 +++++-----\n tools/generate_torch_version.py               | 12 +++---\n tools/jit/gen_unboxing.py                     |  2 +-\n tools/linter/adapters/constexpr_linter.py     |  2 +-\n tools/linter/adapters/grep_linter.py          |  2 +-\n tools/nightly.py                              |  8 ++--\n tools/onnx/gen_diagnostics.py                 |  2 +-\n tools/onnx/update_default_opset_version.py    |  2 +-\n tools/pyi/gen_pyi.py                          | 20 +++++-----\n tools/setup_helpers/cmake.py                  | 18 ++++-----\n tools/setup_helpers/cmake_utils.py            |  2 +-\n tools/setup_helpers/generate_code.py          |  2 +-\n tools/stats/import_test_stats.py              |  2 +-\n tools/stats/upload_stats_lib.py               |  4 +-\n tools/substitute.py                           |  2 +-\n tools/test/test_executorch_gen.py             |  4 +-\n tools/test/test_executorch_signatures.py      |  8 ++--\n tools/test/test_vulkan_codegen.py             |  4 +-\n tools/testing/explicit_ci_jobs.py             |  2 +-\n tools/testing/test_selections.py              |  2 +-\n torch/package/package_exporter.py             | 20 ++++------\n 49 files changed, 134 insertions(+), 142 deletions(-)\n\ndiff --git a/.ci/pytorch/create_test_cert.py b/.ci/pytorch/create_test_cert.py\nindex d3ead7ae259434..4e31f97878f41b 100644\n--- a/.ci/pytorch/create_test_cert.py\n+++ b/.ci/pytorch/create_test_cert.py\n@@ -88,9 +88,9 @@ def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n \n \n ca_key = genrsa(temp_dir + \"/ca.key\")\n-ca_cert = create_cert(temp_dir + \"/ca.pem\", u\"US\", u\"New York\", u\"New York\", u\"Gloo Certificate Authority\", ca_key)\n+ca_cert = create_cert(temp_dir + \"/ca.pem\", \"US\", \"New York\", \"New York\", \"Gloo Certificate Authority\", ca_key)\n \n pkey = genrsa(temp_dir + \"/pkey.key\")\n-csr = create_req(temp_dir + \"/csr.csr\", u\"US\", u\"California\", u\"San Francisco\", u\"Gloo Testing Company\", pkey)\n+csr = create_req(temp_dir + \"/csr.csr\", \"US\", \"California\", \"San Francisco\", \"Gloo Testing Company\", pkey)\n \n cert = sign_certificate_request(temp_dir + \"/cert.pem\", csr, ca_cert, ca_key)\ndiff --git a/.ci/pytorch/perf_test/compare_with_baseline.py b/.ci/pytorch/perf_test/compare_with_baseline.py\nindex 6d2839ac1db412..f7b962632cd79f 100644\n--- a/.ci/pytorch/perf_test/compare_with_baseline.py\n+++ b/.ci/pytorch/perf_test/compare_with_baseline.py\n@@ -19,7 +19,7 @@\n elif 'gpu' in test_name:\n     backend = 'gpu'\n \n-data_file_path = '../{}_runtime.json'.format(backend)\n+data_file_path = f'../{backend}_runtime.json'\n \n with open(data_file_path) as data_file:\n     data = json.load(data_file)\n@@ -69,7 +69,7 @@\n     print(\"z-value < 3, no perf regression detected.\")\n     if args.update:\n         print(\"We will use these numbers as new baseline.\")\n-        new_data_file_path = '../new_{}_runtime.json'.format(backend)\n+        new_data_file_path = f'../new_{backend}_runtime.json'\n         with open(new_data_file_path) as new_data_file:\n             new_data = json.load(new_data_file)\n         new_data[test_name] = {}\ndiff --git a/.circleci/cimodel/data/binary_build_definitions.py b/.circleci/cimodel/data/binary_build_definitions.py\nindex 45981e8e9ea77b..7dccbdc7cbf689 100644\n--- a/.circleci/cimodel/data/binary_build_definitions.py\n+++ b/.circleci/cimodel/data/binary_build_definitions.py\n@@ -5,7 +5,7 @@\n import cimodel.lib.conf_tree as conf_tree\n import cimodel.lib.miniutils as miniutils\n \n-class Conf(object):\n+class Conf:\n     def __init__(self, os, gpu_version, pydistro, parms, smoke, libtorch_variant, gcc_config_variant, libtorch_config_variant):\n \n         self.os = os\ndiff --git a/.circleci/cimodel/data/pytorch_build_definitions.py b/.circleci/cimodel/data/pytorch_build_definitions.py\nindex 76e87b07c1889f..e6e44bd2b5aeb0 100644\n--- a/.circleci/cimodel/data/pytorch_build_definitions.py\n+++ b/.circleci/cimodel/data/pytorch_build_definitions.py\n@@ -143,7 +143,7 @@ def gen_workflow_job(self, phase):\n \n \n # TODO This is a hack to special case some configs just for the workflow list\n-class HiddenConf(object):\n+class HiddenConf:\n     def __init__(self, name, parent_build=None, filters=None):\n         self.name = name\n         self.parent_build = parent_build\n@@ -160,7 +160,7 @@ def gen_workflow_job(self, phase):\n     def gen_build_name(self, _):\n         return self.name\n \n-class DocPushConf(object):\n+class DocPushConf:\n     def __init__(self, name, parent_build=None, branch=\"master\"):\n         self.name = name\n         self.parent_build = parent_build\ndiff --git a/.circleci/generate_config_yml.py b/.circleci/generate_config_yml.py\nindex b3e47eed8b4317..d1ef439941d4b2 100755\n--- a/.circleci/generate_config_yml.py\n+++ b/.circleci/generate_config_yml.py\n@@ -18,7 +18,7 @@\n import cimodel.lib.miniyaml as miniyaml\n \n \n-class File(object):\n+class File:\n     \"\"\"\n     Verbatim copy the contents of a file into config.yml\n     \"\"\"\n@@ -57,7 +57,7 @@ def horizontal_rule():\n     return \"\".join(\"#\" * 78)\n \n \n-class Header(object):\n+class Header:\n     def __init__(self, title, summary=None):\n         self.title = title\n         self.summary_lines = summary or []\ndiff --git a/.github/scripts/ensure_actions_will_cancel.py b/.github/scripts/ensure_actions_will_cancel.py\nindex 92eb3441acd3cf..8d53f2bed5e18b 100755\n--- a/.github/scripts/ensure_actions_will_cancel.py\n+++ b/.github/scripts/ensure_actions_will_cancel.py\n@@ -17,7 +17,7 @@\n \n \n def should_check(filename: Path) -> bool:\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         content = f.read()\n \n     data = yaml.safe_load(content)\n@@ -37,7 +37,7 @@ def should_check(filename: Path) -> bool:\n     files = [f for f in files if should_check(f)]\n     names = set()\n     for filename in files:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             data = yaml.safe_load(f)\n \n         name = data.get(\"name\")\ndiff --git a/.github/scripts/file_io_utils.py b/.github/scripts/file_io_utils.py\nindex 097b092bc904af..faba9f06d2ac65 100644\n--- a/.github/scripts/file_io_utils.py\n+++ b/.github/scripts/file_io_utils.py\n@@ -44,7 +44,7 @@ def load_json_file(file_path: Path) -> Any:\n     \"\"\"\n     Returns the deserialized json object\n     \"\"\"\n-    with open(file_path, \"r\") as f:\n+    with open(file_path) as f:\n         return json.load(f)\n \n \ndiff --git a/.github/scripts/filter_test_configs.py b/.github/scripts/filter_test_configs.py\nindex 9d1f39a833c571..92968179702273 100755\n--- a/.github/scripts/filter_test_configs.py\n+++ b/.github/scripts/filter_test_configs.py\n@@ -319,7 +319,7 @@ def process_jobs(\n     try:\n         # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking\n         # it into its two components first\n-        current_platform, _ = [n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n]\n+        current_platform, _ = (n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n)\n     except ValueError as error:\n         warnings.warn(f\"Invalid job name {job_name}, returning\")\n         return test_matrix\ndiff --git a/.github/scripts/generate_pytorch_version.py b/.github/scripts/generate_pytorch_version.py\nindex f9a6d49505b308..e70b97165e61d7 100755\n--- a/.github/scripts/generate_pytorch_version.py\n+++ b/.github/scripts/generate_pytorch_version.py\n@@ -50,7 +50,7 @@ def get_tag() -> str:\n \n def get_base_version() -> str:\n     root = get_pytorch_root()\n-    dirty_version = open(root / \"version.txt\", \"r\").read().strip()\n+    dirty_version = open(root / \"version.txt\").read().strip()\n     # Strips trailing a0 from version.txt, not too sure why it's there in the\n     # first place\n     return re.sub(LEGACY_BASE_VERSION_SUFFIX_PATTERN, \"\", dirty_version)\ndiff --git a/.github/scripts/label_utils.py b/.github/scripts/label_utils.py\nindex 812c33b426f441..e3ce0f52fe853f 100644\n--- a/.github/scripts/label_utils.py\n+++ b/.github/scripts/label_utils.py\n@@ -51,7 +51,7 @@ def get_last_page_num_from_header(header: Any) -> int:\n     )\n \n \n-@lru_cache()\n+@lru_cache\n def gh_get_labels(org: str, repo: str) -> List[str]:\n     prefix = f\"https://api.github.com/repos/{org}/{repo}/labels?per_page=100\"\n     header, info = request_for_labels(prefix + \"&page=1\")\ndiff --git a/.github/scripts/lint_native_functions.py b/.github/scripts/lint_native_functions.py\nindex 9bde9e8d84e5f9..4dfe9fd63e2e4e 100755\n--- a/.github/scripts/lint_native_functions.py\n+++ b/.github/scripts/lint_native_functions.py\n@@ -26,7 +26,7 @@ def fn(base: str) -> str:\n     return str(base / Path(\"aten/src/ATen/native/native_functions.yaml\"))\n \n \n-with open(Path(__file__).parent.parent.parent / fn(\".\"), \"r\") as f:\n+with open(Path(__file__).parent.parent.parent / fn(\".\")) as f:\n     contents = f.read()\n \n yaml = ruamel.yaml.YAML()  # type: ignore[attr-defined]\ndiff --git a/.github/scripts/run_torchbench.py b/.github/scripts/run_torchbench.py\nindex 3a80ebbeb9970a..e5e3c7a03dea07 100644\n--- a/.github/scripts/run_torchbench.py\n+++ b/.github/scripts/run_torchbench.py\n@@ -129,7 +129,7 @@ def extract_models_from_pr(\n     model_list = []\n     userbenchmark_list = []\n     pr_list = []\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(filter(lambda x: x.startswith(MAGIC_PREFIX), lines))\n         if magic_lines:\n@@ -157,7 +157,7 @@ def extract_models_from_pr(\n \n def find_torchbench_branch(prbody_file: str) -> str:\n     branch_name: str = \"\"\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(\n             filter(lambda x: x.startswith(MAGIC_TORCHBENCH_PREFIX), lines)\ndiff --git a/.github/scripts/test_check_labels.py b/.github/scripts/test_check_labels.py\nindex 17d33158f2efb4..2b2cd7b6c5204b 100644\n--- a/.github/scripts/test_check_labels.py\n+++ b/.github/scripts/test_check_labels.py\n@@ -15,7 +15,7 @@\n \n \n def mock_parse_args() -> object:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.pr_num = 76123\n \ndiff --git a/.github/scripts/test_trymerge.py b/.github/scripts/test_trymerge.py\nindex 7cf70882ed3d40..a46ef9032a6459 100755\n--- a/.github/scripts/test_trymerge.py\n+++ b/.github/scripts/test_trymerge.py\n@@ -114,7 +114,7 @@ def mocked_rockset_results(head_sha: str, merge_base: str, num_retries: int = 3)\n \n \n def mock_parse_args(revert: bool = False, force: bool = False) -> Any:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.revert = revert\n             self.force = force\ndiff --git a/.github/scripts/trymerge.py b/.github/scripts/trymerge.py\nindex 3cffcaa14efaf8..cc253f36cbd1b7 100755\n--- a/.github/scripts/trymerge.py\n+++ b/.github/scripts/trymerge.py\n@@ -1628,10 +1628,8 @@ def validate_revert(\n         allowed_reverters.append(\"CONTRIBUTOR\")\n     if author_association not in allowed_reverters:\n         raise PostCommentError(\n-            (\n-                f\"Will not revert as @{author_login} is not one of \"\n-                f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n-            )\n+            f\"Will not revert as @{author_login} is not one of \"\n+            f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n         )\n     skip_internal_checks = can_skip_internal_checks(pr, comment_id)\n \ndiff --git a/.github/scripts/trymerge_explainer.py b/.github/scripts/trymerge_explainer.py\nindex ebc74cf63eb833..8aa6ab59b94cbc 100644\n--- a/.github/scripts/trymerge_explainer.py\n+++ b/.github/scripts/trymerge_explainer.py\n@@ -17,7 +17,7 @@ def has_label(labels: List[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:\n     return len(list(filter(pattern.match, labels))) > 0\n \n \n-class TryMergeExplainer(object):\n+class TryMergeExplainer:\n     force: bool\n     labels: List[str]\n     pr_num: int\ndiff --git a/docs/caffe2/process.py b/docs/caffe2/process.py\nindex 3b94b9d38502a2..4a59eec388d90b 100644\n--- a/docs/caffe2/process.py\n+++ b/docs/caffe2/process.py\n@@ -8,7 +8,7 @@\n \n # Module caffe2...caffe2.python.control_test\n def insert(originalfile, first_line, description):\n-    with open(originalfile, 'r') as f:\n+    with open(originalfile) as f:\n         f1 = f.readline()\n         if(f1.find(first_line) < 0):\n             docs = first_line + description + f1\n@@ -30,7 +30,7 @@ def insert(originalfile, first_line, description):\n     for file in files:\n         if (file.endswith(\".py\") and not file.endswith(\"_test.py\") and not file.endswith(\"__.py\")):\n             filepath = os.path.join(root, file)\n-            print((\"filepath: \" + filepath))\n+            print(\"filepath: \" + filepath)\n             directory = os.path.dirname(filepath)[2:]\n             directory = directory.replace(\"/\", \".\")\n             print(\"directory: \" + directory)\ndiff --git a/docs/cpp/source/conf.py b/docs/cpp/source/conf.py\nindex 88648787fa8c8e..2b94cfdb5058fb 100644\n--- a/docs/cpp/source/conf.py\n+++ b/docs/cpp/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -99,7 +98,7 @@\n     ############################################################################\n     # Main library page layout example configuration.                          #\n     ############################################################################\n-    \"afterTitleDescription\": textwrap.dedent(u'''\n+    \"afterTitleDescription\": textwrap.dedent('''\n         Welcome to the developer reference for the PyTorch C++ API.\n     '''),\n }\ndiff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 8bbb189853eec7..8fec5f16f9852c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -624,7 +623,7 @@ def visit_reference(self, node):\n                 anchor = ref_anchor[1]\n                 txt = node.parent.astext()\n                 if txt == anchor or txt == anchor.split('.')[-1]:\n-                    self.body.append('<p id=\"{}\"/>'.format(ref_anchor[1]))\n+                    self.body.append(f'<p id=\"{ref_anchor[1]}\"/>')\n         return old_call(self, node)\n     Klass.visit_reference = visit_reference\n \ndiff --git a/docs/source/scripts/exportdb/generate_example_rst.py b/docs/source/scripts/exportdb/generate_example_rst.py\nindex 58dca5f31ea73d..38f71b905245c3 100644\n--- a/docs/source/scripts/exportdb/generate_example_rst.py\n+++ b/docs/source/scripts/exportdb/generate_example_rst.py\n@@ -31,7 +31,7 @@ def generate_example_rst(example_case: ExportCase):\n         if isinstance(model, torch.nn.Module)\n         else inspect.getfile(model)\n     )\n-    with open(source_file, \"r\") as file:\n+    with open(source_file) as file:\n         source_code = file.read()\n     source_code = re.sub(r\"from torch\\._export\\.db\\.case import .*\\n\", \"\", source_code)\n     source_code = re.sub(r\"@export_case\\((.|\\n)*?\\)\\n\", \"\", source_code)\n@@ -114,7 +114,7 @@ def generate_index_rst(example_cases, tag_to_modules, support_level_to_modules):\n \n     tag_names = \"\\n    \".join(t for t in tag_to_modules.keys())\n \n-    with open(os.path.join(PWD, \"blurb.txt\"), \"r\") as file:\n+    with open(os.path.join(PWD, \"blurb.txt\")) as file:\n         blurb = file.read()\n \n     # Generate contents of the .rst file\ndiff --git a/setup.py b/setup.py\nindex 5f0180cb0c8ac6..d454b2e62f33b4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -323,7 +323,7 @@ def report(*args):\n package_name = os.getenv('TORCH_PACKAGE_NAME', 'torch')\n package_type = os.getenv('PACKAGE_TYPE', 'wheel')\n version = get_torch_version()\n-report(\"Building wheel {}-{}\".format(package_name, version))\n+report(f\"Building wheel {package_name}-{version}\")\n \n cmake = CMake()\n \n@@ -361,7 +361,7 @@ def not_exists_or_empty(folder):\n             start = time.time()\n             subprocess.check_call([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd)\n             end = time.time()\n-            print(' --- Submodule initialization took {:.2f} sec'.format(end - start))\n+            print(f' --- Submodule initialization took {end - start:.2f} sec')\n         except Exception:\n             print(' --- Submodule initalization failed')\n             print('Please run:\\n\\tgit submodule update --init --recursive')\n@@ -616,16 +616,16 @@ def build_extensions(self):\n                 continue\n             fullname = self.get_ext_fullname(ext.name)\n             filename = self.get_ext_filename(fullname)\n-            report(\"\\nCopying extension {}\".format(ext.name))\n+            report(f\"\\nCopying extension {ext.name}\")\n \n             relative_site_packages = sysconfig.get_path('purelib').replace(sysconfig.get_path('data'), '').lstrip(os.path.sep)\n             src = os.path.join(\"torch\", relative_site_packages, filename)\n             if not os.path.exists(src):\n-                report(\"{} does not exist\".format(src))\n+                report(f\"{src} does not exist\")\n                 del self.extensions[i]\n             else:\n                 dst = os.path.join(os.path.realpath(self.build_lib), filename)\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -642,7 +642,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -658,7 +658,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"nvfuser\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -670,7 +670,7 @@ def build_extensions(self):\n     def get_outputs(self):\n         outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n         outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n-        report(\"setup.py::get_outputs returning {}\".format(outputs))\n+        report(f\"setup.py::get_outputs returning {outputs}\")\n         return outputs\n \n     def create_compile_commands(self):\n@@ -694,13 +694,13 @@ def load(filename):\n         new_contents = json.dumps(all_commands, indent=2)\n         contents = ''\n         if os.path.exists('compile_commands.json'):\n-            with open('compile_commands.json', 'r') as f:\n+            with open('compile_commands.json') as f:\n                 contents = f.read()\n         if contents != new_contents:\n             with open('compile_commands.json', 'w') as f:\n                 f.write(new_contents)\n \n-class concat_license_files():\n+class concat_license_files:\n     \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n \n     LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n@@ -723,7 +723,7 @@ def __enter__(self):\n         finally:\n             sys.path = old_path\n \n-        with open(self.f1, 'r') as f1:\n+        with open(self.f1) as f1:\n             self.bsd_text = f1.read()\n \n         with open(self.f1, 'a') as f1:\n@@ -771,7 +771,7 @@ def finalize_options(self):\n     def run(self):\n         import glob\n         import re\n-        with open('.gitignore', 'r') as f:\n+        with open('.gitignore') as f:\n             ignores = f.read()\n             pat = re.compile(r'^#( BEGIN NOT-CLEAN-FILES )?')\n             for wildcard in filter(None, ignores.split('\\n')):\n@@ -934,31 +934,31 @@ def make_relative_rpath_args(path):\n     if cmake_cache_vars['BUILD_CAFFE2']:\n         extensions.append(\n             Extension(\n-                name=str('caffe2.python.caffe2_pybind11_state'),\n+                name='caffe2.python.caffe2_pybind11_state',\n                 sources=[]),\n         )\n         if cmake_cache_vars['USE_CUDA']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_gpu'),\n+                    name='caffe2.python.caffe2_pybind11_state_gpu',\n                     sources=[]),\n             )\n         if cmake_cache_vars['USE_ROCM']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_hip'),\n+                    name='caffe2.python.caffe2_pybind11_state_hip',\n                     sources=[]),\n             )\n     if cmake_cache_vars['BUILD_FUNCTORCH']:\n         extensions.append(\n             Extension(\n-                name=str('functorch._C'),\n+                name='functorch._C',\n                 sources=[]),\n         )\n     if cmake_cache_vars['BUILD_NVFUSER']:\n         extensions.append(\n             Extension(\n-                name=str('nvfuser._C'),\n+                name='nvfuser._C',\n                 sources=[]),\n         )\n \n@@ -1272,7 +1272,7 @@ def main():\n         download_url='https://github.com/pytorch/pytorch/tags',\n         author='PyTorch Team',\n         author_email='packages@pytorch.org',\n-        python_requires='>={}'.format(python_min_version_str),\n+        python_requires=f'>={python_min_version_str}',\n         # PyPI package information.\n         classifiers=[\n             'Development Status :: 5 - Production/Stable',\n@@ -1288,7 +1288,7 @@ def main():\n             'Topic :: Software Development :: Libraries :: Python Modules',\n             'Programming Language :: C++',\n             'Programming Language :: Python :: 3',\n-        ] + ['Programming Language :: Python :: 3.{}'.format(i) for i in range(python_min_version[1], version_range_max)],\n+        ] + [f'Programming Language :: Python :: 3.{i}' for i in range(python_min_version[1], version_range_max)],\n         license='BSD-3',\n         keywords='pytorch, machine learning',\n     )\ndiff --git a/tools/amd_build/build_amd.py b/tools/amd_build/build_amd.py\nindex 59f806b361102e..5d14e9266f3b4a 100755\n--- a/tools/amd_build/build_amd.py\n+++ b/tools/amd_build/build_amd.py\n@@ -140,7 +140,7 @@ def is_hip_clang() -> bool:\n         hip_path = os.getenv(\"HIP_PATH\", \"/opt/rocm/hip\")\n         with open(hip_path + \"/lib/.hipInfo\") as f:\n             return \"HIP_COMPILER=clang\" in f.read()\n-    except IOError:\n+    except OSError:\n         return False\n \n \n@@ -149,7 +149,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Hip.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\" hip_hcc \", \" amdhip64 \") for line in lines]\n         if lines == newlines:\n@@ -163,7 +163,7 @@ def is_hip_clang() -> bool:\n gloo_cmake_file = \"third_party/gloo/cmake/Modules/Findrccl.cmake\"\n if os.path.exists(gloo_cmake_file):\n     do_write = False\n-    with open(gloo_cmake_file, \"r\") as sources:\n+    with open(gloo_cmake_file) as sources:\n         lines = sources.readlines()\n     newlines = [line.replace(\"RCCL_LIBRARY\", \"RCCL_LIB_PATH\") for line in lines]\n     if lines == newlines:\n@@ -179,7 +179,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Dependencies.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\"HIP_HCC_FLAGS\", \"HIP_CLANG_FLAGS\") for line in lines]\n         if lines == newlines:\ndiff --git a/tools/autograd/gen_python_functions.py b/tools/autograd/gen_python_functions.py\nindex 211aacafaa8728..d1e9d60737defc 100644\n--- a/tools/autograd/gen_python_functions.py\n+++ b/tools/autograd/gen_python_functions.py\n@@ -553,7 +553,7 @@ def load_deprecated_signatures(\n     # find matching original signatures for each deprecated signature\n     results: List[PythonSignatureNativeFunctionPair] = []\n \n-    with open(deprecated_yaml_path, \"r\") as f:\n+    with open(deprecated_yaml_path) as f:\n         deprecated_defs = yaml.load(f, Loader=YamlLoader)\n \n     for deprecated in deprecated_defs:\n@@ -873,7 +873,7 @@ def method_impl(\n         name=name,\n         pycname=pycname,\n         method_header=method_header,\n-        max_args=max((o.signature.arguments_count() for o in overloads)),\n+        max_args=max(o.signature.arguments_count() for o in overloads),\n         signatures=signatures,\n         traceable=traceable,\n         check_has_torch_function=gen_has_torch_function_check(\n@@ -1255,10 +1255,10 @@ def go(f: NativeFunction) -> str:\n         # dispatch lambda signature\n         name = cpp.name(f.func)\n         lambda_formals = \", \".join(\n-            (\n+\n                 f\"{a.type_str} {a.name}\"\n                 for a in dispatch_lambda_args(ps, f, symint=symint)\n-            )\n+\n         )\n         lambda_return = dispatch_lambda_return_str(f)\n \ndiff --git a/tools/autograd/load_derivatives.py b/tools/autograd/load_derivatives.py\nindex b51b625b2ea28f..b846892b0e3ed4 100644\n--- a/tools/autograd/load_derivatives.py\n+++ b/tools/autograd/load_derivatives.py\n@@ -98,7 +98,7 @@ def load_derivatives(\n     global _GLOBAL_LOAD_DERIVATIVE_CACHE\n     key = (derivatives_yaml_path, native_yaml_path)\n     if key not in _GLOBAL_LOAD_DERIVATIVE_CACHE:\n-        with open(derivatives_yaml_path, \"r\") as f:\n+        with open(derivatives_yaml_path) as f:\n             definitions = yaml.load(f, Loader=YamlLoader)\n \n         funcs = parse_native_yaml(native_yaml_path, tags_yaml_path).native_functions\ndiff --git a/tools/code_analyzer/gen_op_registration_allowlist.py b/tools/code_analyzer/gen_op_registration_allowlist.py\nindex b01142c872f1c2..b5d15ca1ae8417 100644\n--- a/tools/code_analyzer/gen_op_registration_allowlist.py\n+++ b/tools/code_analyzer/gen_op_registration_allowlist.py\n@@ -24,7 +24,7 @@ def canonical_name(opname: str) -> str:\n \n \n def load_op_dep_graph(fname: str) -> DepGraph:\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         result = defaultdict(set)\n         for op in yaml.safe_load(stream):\n             op_name = canonical_name(op[\"name\"])\n@@ -36,7 +36,7 @@ def load_op_dep_graph(fname: str) -> DepGraph:\n \n def load_root_ops(fname: str) -> List[str]:\n     result = []\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         for op in yaml.safe_load(stream):\n             result.append(canonical_name(op))\n     return result\ndiff --git a/tools/code_analyzer/gen_oplist.py b/tools/code_analyzer/gen_oplist.py\nindex 7c1764deda5b2f..0a9e2a1539b6a7 100644\n--- a/tools/code_analyzer/gen_oplist.py\n+++ b/tools/code_analyzer/gen_oplist.py\n@@ -79,7 +79,7 @@ def gen_supported_mobile_models(model_dicts: List[Any], output_dir: str) -> None\n \n     supported_hashes = \"\"\n     for md5 in md5_hashes:\n-        supported_hashes += '\"{}\",\\n'.format(md5)\n+        supported_hashes += f'\"{md5}\",\\n'\n     with open(\n         os.path.join(output_dir, \"SupportedMobileModelsRegistration.cpp\"), \"wb\"\n     ) as out_file:\ndiff --git a/tools/coverage_plugins_package/setup.py b/tools/coverage_plugins_package/setup.py\nindex 01250694550423..e3e88067cb08db 100644\n--- a/tools/coverage_plugins_package/setup.py\n+++ b/tools/coverage_plugins_package/setup.py\n@@ -1,6 +1,6 @@\n import setuptools  # type: ignore[import]\n \n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n+with open(\"README.md\", encoding=\"utf-8\") as fh:\n     long_description = fh.read()\n \n setuptools.setup(\ndiff --git a/tools/download_mnist.py b/tools/download_mnist.py\nindex 52fa411eda9f88..ac9c049bdeedb6 100644\n--- a/tools/download_mnist.py\n+++ b/tools/download_mnist.py\n@@ -32,16 +32,16 @@ def report_download_progress(\n def download(destination_path: str, resource: str, quiet: bool) -> None:\n     if os.path.exists(destination_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ...\".format(destination_path))\n+            print(f\"{destination_path} already exists, skipping ...\")\n     else:\n         for mirror in MIRRORS:\n             url = mirror + resource\n-            print(\"Downloading {} ...\".format(url))\n+            print(f\"Downloading {url} ...\")\n             try:\n                 hook = None if quiet else report_download_progress\n                 urlretrieve(url, destination_path, reporthook=hook)\n             except (URLError, ConnectionError) as e:\n-                print(\"Failed to download (trying next):\\n{}\".format(e))\n+                print(f\"Failed to download (trying next):\\n{e}\")\n                 continue\n             finally:\n                 if not quiet:\n@@ -56,13 +56,13 @@ def unzip(zipped_path: str, quiet: bool) -> None:\n     unzipped_path = os.path.splitext(zipped_path)[0]\n     if os.path.exists(unzipped_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ... \".format(unzipped_path))\n+            print(f\"{unzipped_path} already exists, skipping ... \")\n         return\n     with gzip.open(zipped_path, \"rb\") as zipped_file:\n         with open(unzipped_path, \"wb\") as unzipped_file:\n             unzipped_file.write(zipped_file.read())\n             if not quiet:\n-                print(\"Unzipped {} ...\".format(zipped_path))\n+                print(f\"Unzipped {zipped_path} ...\")\n \n \n def main() -> None:\ndiff --git a/tools/gen_vulkan_spv.py b/tools/gen_vulkan_spv.py\nindex 02c9c39270b107..8d38dfa0e82bcb 100644\n--- a/tools/gen_vulkan_spv.py\n+++ b/tools/gen_vulkan_spv.py\n@@ -74,7 +74,7 @@ def __init__(self: \"VulkanShaderGenerator\") -> None:\n \n     def add_params_yaml(self, parameters_yaml_file):  # type: ignore[no-untyped-def]\n         all_template_params = OrderedDict()\n-        with open(parameters_yaml_file, \"r\") as f:\n+        with open(parameters_yaml_file) as f:\n             contents = yaml.load(f, Loader=UniqueKeyLoader)\n             for key in contents:\n                 all_template_params[key] = contents[key]\n@@ -205,7 +205,7 @@ def determineDescriptorType(lineStr: str) -> str:\n \n def getShaderInfo(srcFilePath: str) -> ShaderInfo:\n     shader_info = ShaderInfo([], [], \"\")\n-    with open(srcFilePath, 'r') as srcFile:\n+    with open(srcFilePath) as srcFile:\n         for line in srcFile:\n             if isDescriptorLine(line):\n                 shader_info.layouts.append(determineDescriptorType(line))\n@@ -272,13 +272,13 @@ def genCppH(\n         if len(f) > 1:\n             templateSrcPaths.append(f)\n             templateSrcPaths.sort()\n-    print(\"templateSrcPaths:{}\".format(templateSrcPaths))\n+    print(f\"templateSrcPaths:{templateSrcPaths}\")\n \n     spvPaths = {}\n     for templateSrcPath in templateSrcPaths:\n-        print(\"templateSrcPath {}\".format(templateSrcPath))\n+        print(f\"templateSrcPath {templateSrcPath}\")\n         name = getName(templateSrcPath).replace(\"_glsl\", \"\")\n-        print(\"name {}\".format(name))\n+        print(f\"name {name}\")\n \n         codeTemplate = CodeTemplate.from_file(templateSrcPath)\n         srcPath = tmpDirPath + \"/\" + name + \".glsl\"\n@@ -287,7 +287,7 @@ def genCppH(\n             fw.write(content)\n \n         spvPath = tmpDirPath + \"/\" + name + \".spv\"\n-        print(\"spvPath {}\".format(spvPath))\n+        print(f\"spvPath {spvPath}\")\n \n         cmd = [\n             glslcPath, \"-fshader-stage=compute\",\n@@ -328,7 +328,7 @@ def genCppH(\n     h += nsend\n \n     cpp = \"#include <ATen/native/vulkan/api/Shader.h>\\n\"\n-    cpp += \"#include <ATen/native/vulkan/{}>\\n\".format(H_NAME)\n+    cpp += f\"#include <ATen/native/vulkan/{H_NAME}>\\n\"\n     cpp += \"#include <stdint.h>\\n\"\n     cpp += \"#include <vector>\\n\"\n     cpp += nsbegin\n@@ -340,7 +340,7 @@ def genCppH(\n     for spvPath, srcPath in spvPaths.items():\n         name = getName(spvPath).replace(\"_spv\", \"\")\n \n-        print(\"spvPath:{}\".format(spvPath))\n+        print(f\"spvPath:{spvPath}\")\n         with open(spvPath, 'rb') as fr:\n             next_bin = array.array('I', fr.read())\n             sizeBytes = 4 * len(next_bin)\n@@ -362,8 +362,8 @@ def genCppH(\n         shader_info_layouts = \"{{{}}}\".format(\",\\n \".join(shader_info.layouts))\n \n         shader_info_args = [\n-            \"\\\"vulkan.{}\\\"\".format(name),\n-            \"{}_bin\".format(name),\n+            f\"\\\"vulkan.{name}\\\"\",\n+            f\"{name}_bin\",\n             str(sizeBytes),\n             shader_info_layouts,\n             tile_size,\ndiff --git a/tools/generate_torch_version.py b/tools/generate_torch_version.py\nindex 9e9f73b031f810..d90d3646ab1910 100644\n--- a/tools/generate_torch_version.py\n+++ b/tools/generate_torch_version.py\n@@ -41,7 +41,7 @@ def get_tag(pytorch_root: Union[str, Path]) -> str:\n \n def get_torch_version(sha: Optional[str] = None) -> str:\n     pytorch_root = Path(__file__).parent.parent\n-    version = open(pytorch_root / \"version.txt\", \"r\").read().strip()\n+    version = open(pytorch_root / \"version.txt\").read().strip()\n \n     if os.getenv(\"PYTORCH_BUILD_VERSION\"):\n         assert os.getenv(\"PYTORCH_BUILD_NUMBER\") is not None\n@@ -86,11 +86,11 @@ def get_torch_version(sha: Optional[str] = None) -> str:\n         version = tagged_version\n \n     with open(version_path, \"w\") as f:\n-        f.write(\"__version__ = '{}'\\n\".format(version))\n+        f.write(f\"__version__ = '{version}'\\n\")\n         # NB: This is not 100% accurate, because you could have built the\n         # library code with DEBUG, but csrc without DEBUG (in which case\n         # this would claim to be a release build when it's not.)\n-        f.write(\"debug = {}\\n\".format(repr(bool(args.is_debug))))\n-        f.write(\"cuda = {}\\n\".format(repr(args.cuda_version)))\n-        f.write(\"git_version = {}\\n\".format(repr(sha)))\n-        f.write(\"hip = {}\\n\".format(repr(args.hip_version)))\n+        f.write(f\"debug = {repr(bool(args.is_debug))}\\n\")\n+        f.write(f\"cuda = {repr(args.cuda_version)}\\n\")\n+        f.write(f\"git_version = {repr(sha)}\\n\")\n+        f.write(f\"hip = {repr(args.hip_version)}\\n\")\ndiff --git a/tools/jit/gen_unboxing.py b/tools/jit/gen_unboxing.py\nindex 6179d6afe482ff..ee4e2fc2ddb188 100644\n--- a/tools/jit/gen_unboxing.py\n+++ b/tools/jit/gen_unboxing.py\n@@ -250,7 +250,7 @@ def main(args: List[str]) -> None:\n     if options.op_registration_allowlist:\n         op_registration_allowlist = options.op_registration_allowlist\n     elif options.TEST_ONLY_op_registration_allowlist_yaml_path:\n-        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path, \"r\") as f:\n+        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path) as f:\n             op_registration_allowlist = yaml.safe_load(f)\n     else:\n         op_registration_allowlist = None\ndiff --git a/tools/linter/adapters/constexpr_linter.py b/tools/linter/adapters/constexpr_linter.py\nindex 8992f30ac46b29..24ecc83b238e30 100644\n--- a/tools/linter/adapters/constexpr_linter.py\n+++ b/tools/linter/adapters/constexpr_linter.py\n@@ -35,7 +35,7 @@ class LintMessage(NamedTuple):\n def check_file(filename: str) -> Optional[LintMessage]:\n     logging.debug(\"Checking file %s\", filename)\n \n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n \n     for idx, line in enumerate(lines):\ndiff --git a/tools/linter/adapters/grep_linter.py b/tools/linter/adapters/grep_linter.py\nindex 21c8a210b2b697..64dac4cdc079cd 100644\n--- a/tools/linter/adapters/grep_linter.py\n+++ b/tools/linter/adapters/grep_linter.py\n@@ -108,7 +108,7 @@ def lint_file(\n     original = None\n     replacement = None\n     if replace_pattern:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             original = f.read()\n \n         try:\ndiff --git a/tools/nightly.py b/tools/nightly.py\nindex 1544eb0692b661..28a8c6eb2331e7 100755\n--- a/tools/nightly.py\n+++ b/tools/nightly.py\n@@ -105,7 +105,7 @@ def redact(self, needle: str, replace: str = \"<REDACTED>\") -> None:\n         self.redactions[needle] = replace\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_base_dir() -> str:\n     meta_dir = os.getcwd()\n     base_dir = os.path.join(meta_dir, \"nightly\", \"log\")\n@@ -113,17 +113,17 @@ def logging_base_dir() -> str:\n     return base_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_run_dir() -> str:\n     cur_dir = os.path.join(\n         logging_base_dir(),\n-        \"{}_{}\".format(datetime.datetime.now().strftime(DATETIME_FORMAT), uuid.uuid1()),\n+        f\"{datetime.datetime.now().strftime(DATETIME_FORMAT)}_{uuid.uuid1()}\",\n     )\n     os.makedirs(cur_dir, exist_ok=True)\n     return cur_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_record_argv() -> None:\n     s = subprocess.list2cmdline(sys.argv)\n     with open(os.path.join(logging_run_dir(), \"argv\"), \"w\") as f:\ndiff --git a/tools/onnx/gen_diagnostics.py b/tools/onnx/gen_diagnostics.py\nindex 2aeb61a06318d7..4cf70289296050 100644\n--- a/tools/onnx/gen_diagnostics.py\n+++ b/tools/onnx/gen_diagnostics.py\n@@ -205,7 +205,7 @@ def gen_diagnostics(\n     out_cpp_dir: str,\n     out_docs_dir: str,\n ) -> None:\n-    with open(rules_path, \"r\") as f:\n+    with open(rules_path) as f:\n         rules = yaml.load(f, Loader=YamlLoader)\n \n     template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"templates\")\ndiff --git a/tools/onnx/update_default_opset_version.py b/tools/onnx/update_default_opset_version.py\nindex 6dc6ffbd2890f4..6463c6271b6ee6 100755\n--- a/tools/onnx/update_default_opset_version.py\n+++ b/tools/onnx/update_default_opset_version.py\n@@ -23,7 +23,7 @@\n def read_sub_write(path: str, prefix_pat: str, new_default: int) -> None:\n     with open(path, encoding=\"utf-8\") as f:\n         content_str = f.read()\n-    content_str = re.sub(prefix_pat, r\"\\g<1>{}\".format(new_default), content_str)\n+    content_str = re.sub(prefix_pat, fr\"\\g<1>{new_default}\", content_str)\n     with open(path, \"w\", encoding=\"utf-8\") as f:\n         f.write(content_str)\n     print(\"modified\", path)\ndiff --git a/tools/pyi/gen_pyi.py b/tools/pyi/gen_pyi.py\nindex 24ee8ad1bfe580..c74d737416870a 100644\n--- a/tools/pyi/gen_pyi.py\n+++ b/tools/pyi/gen_pyi.py\n@@ -191,15 +191,15 @@ def sig_for_ops(opname: str) -> List[str]:\n \n     name = opname[2:-2]\n     if name in binary_ops:\n-        return [\"def {}(self, other: Any) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self, other: Any) -> Tensor: ...\"]\n     elif name in comparison_ops:\n-        sig = \"def {}(self, other: Any) -> Tensor: ...\".format(opname)\n+        sig = f\"def {opname}(self, other: Any) -> Tensor: ...\"\n         if name in symmetric_comparison_ops:\n             # unsafe override https://github.com/python/mypy/issues/5704\n             sig += \"  # type: ignore[override]\"\n         return [sig]\n     elif name in unary_ops:\n-        return [\"def {}(self) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self) -> Tensor: ...\"]\n     elif name in to_py_type_ops:\n         if name in {\"bool\", \"float\", \"complex\"}:\n             tname = name\n@@ -209,7 +209,7 @@ def sig_for_ops(opname: str) -> List[str]:\n             tname = \"int\"\n         if tname in {\"float\", \"int\", \"bool\", \"complex\"}:\n             tname = \"builtins.\" + tname\n-        return [\"def {}(self) -> {}: ...\".format(opname, tname)]\n+        return [f\"def {opname}(self) -> {tname}: ...\"]\n     else:\n         raise Exception(\"unknown op\", opname)\n \n@@ -1120,7 +1120,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     for name in simple_conversions:\n         unsorted_tensor_method_hints[name].append(\n-            \"def {}(self) -> Tensor: ...\".format(name)\n+            f\"def {name}(self) -> Tensor: ...\"\n         )\n \n     # pyi tensor methods don't currently include deprecated signatures for some reason\n@@ -1150,7 +1150,7 @@ def replace_special_case(hint: str) -> str:\n                 namedtuples[tuple_name] = tuple_def\n \n     for op in all_ops:\n-        name = \"__{}__\".format(op)\n+        name = f\"__{op}__\"\n         unsorted_tensor_method_hints[name] += sig_for_ops(name)\n \n     tensor_method_hints = []\n@@ -1164,7 +1164,7 @@ def replace_special_case(hint: str) -> str:\n     # Generate namedtuple definitions\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-    namedtuple_defs = [\"{}\\n\".format(defn) for defn in namedtuples.values()]\n+    namedtuple_defs = [f\"{defn}\\n\" for defn in namedtuples.values()]\n \n     # Generate type signatures for legacy classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1183,7 +1183,7 @@ def replace_special_case(hint: str) -> str:\n         \"ByteTensor\",\n         \"BoolTensor\",\n     ):\n-        legacy_class_hints.append(\"class {}(Tensor): ...\".format(c))\n+        legacy_class_hints.append(f\"class {c}(Tensor): ...\")\n \n     # Generate type signatures for dtype classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1191,7 +1191,7 @@ def replace_special_case(hint: str) -> str:\n     # TODO: don't explicitly list dtypes here; get it from canonical\n     # source\n     dtype_class_hints = [\n-        \"{}: dtype = ...\".format(n)\n+        f\"{n}: dtype = ...\"\n         for n in [\n             \"float32\",\n             \"float\",\n@@ -1232,7 +1232,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     all_symbols = sorted(list(namedtuples.keys()) + hinted_function_names)\n     all_directive = pformat(all_symbols, width=100, compact=True).split(\"\\n\")\n-    all_directive[0] = \"__all__ = {}\".format(all_directive[0])\n+    all_directive[0] = f\"__all__ = {all_directive[0]}\"\n \n     # Dispatch key hints\n     # ~~~~~~~~~~~~~~~~~~\ndiff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py\nindex 0bd6e5d4c2adc9..cf80bd3eb1e62c 100644\n--- a/tools/setup_helpers/cmake.py\n+++ b/tools/setup_helpers/cmake.py\n@@ -61,10 +61,10 @@ def _get_cmake_command() -> str:\n \n         _cmake_min_version = LooseVersion(\"3.13.0\")\n         if all(\n-            (\n+\n                 ver is None or ver < _cmake_min_version\n                 for ver in [cmake_version, cmake3_version]\n-            )\n+\n         ):\n             raise RuntimeError(\"no cmake or cmake3 with version >= 3.13.0 found\")\n \n@@ -108,7 +108,7 @@ def defines(args: List[str], **kwargs: CMakeValue) -> None:\n         \"Adds definitions to a cmake argument list.\"\n         for key, value in sorted(kwargs.items()):\n             if value is not None:\n-                args.append(\"-D{}={}\".format(key, value))\n+                args.append(f\"-D{key}={value}\")\n \n     def get_cmake_cache_variables(self) -> Dict[str, CMakeValue]:\n         r\"\"\"Gets values in CMakeCache.txt into a dictionary.\n@@ -173,7 +173,7 @@ def generate(\n                     toolset_dict[\"host\"] = \"x64\"\n             if toolset_dict:\n                 toolset_expr = \",\".join(\n-                    [\"{}={}\".format(k, v) for k, v in toolset_dict.items()]\n+                    [f\"{k}={v}\" for k, v in toolset_dict.items()]\n                 )\n                 args.append(\"-T\" + toolset_expr)\n \n@@ -322,10 +322,10 @@ def generate(\n         expected_wrapper = \"/usr/local/opt/ccache/libexec\"\n         if IS_DARWIN and os.path.exists(expected_wrapper):\n             if \"CMAKE_C_COMPILER\" not in build_options and \"CC\" not in os.environ:\n-                CMake.defines(args, CMAKE_C_COMPILER=\"{}/gcc\".format(expected_wrapper))\n+                CMake.defines(args, CMAKE_C_COMPILER=f\"{expected_wrapper}/gcc\")\n             if \"CMAKE_CXX_COMPILER\" not in build_options and \"CXX\" not in os.environ:\n                 CMake.defines(\n-                    args, CMAKE_CXX_COMPILER=\"{}/g++\".format(expected_wrapper)\n+                    args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\"\n                 )\n \n         for env_var_name in my_env:\n@@ -336,10 +336,10 @@ def generate(\n                     my_env[env_var_name] = str(my_env[env_var_name].encode(\"utf-8\"))\n                 except UnicodeDecodeError as e:\n                     shex = \":\".join(\n-                        \"{:02x}\".format(ord(c)) for c in my_env[env_var_name]\n+                        f\"{ord(c):02x}\" for c in my_env[env_var_name]\n                     )\n                     print(\n-                        \"Invalid ENV[{}] = {}\".format(env_var_name, shex),\n+                        f\"Invalid ENV[{env_var_name}] = {shex}\",\n                         file=sys.stderr,\n                     )\n                     print(e, file=sys.stderr)\n@@ -396,7 +396,7 @@ def build(self, my_env: Dict[str, str]) -> None:\n             build_args += [\"--\"]\n             if IS_WINDOWS and not USE_NINJA:\n                 # We are likely using msbuild here\n-                build_args += [\"/p:CL_MPCount={}\".format(max_jobs)]\n+                build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n             else:\n                 build_args += [\"-j\", max_jobs]\n         self.run(build_args, my_env)\ndiff --git a/tools/setup_helpers/cmake_utils.py b/tools/setup_helpers/cmake_utils.py\nindex dabd66a4e838bb..c15b6f7592c015 100644\n--- a/tools/setup_helpers/cmake_utils.py\n+++ b/tools/setup_helpers/cmake_utils.py\n@@ -72,7 +72,7 @@ def get_cmake_cache_variables_from_file(\n         )\n         if matched is None:  # Illegal line\n             raise ValueError(\n-                \"Unexpected line {} in {}: {}\".format(i, repr(cmake_cache_file), line)\n+                f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\"\n             )\n         _, variable, type_, value = matched.groups()\n         if type_ is None:\ndiff --git a/tools/setup_helpers/generate_code.py b/tools/setup_helpers/generate_code.py\nindex c03fd87f25b6aa..afdd168d179fd6 100644\n--- a/tools/setup_helpers/generate_code.py\n+++ b/tools/setup_helpers/generate_code.py\n@@ -75,7 +75,7 @@ def generate_code(\n def get_selector_from_legacy_operator_selection_list(\n     selected_op_list_path: str,\n ) -> Any:\n-    with open(selected_op_list_path, \"r\") as f:\n+    with open(selected_op_list_path) as f:\n         # strip out the overload part\n         # It's only for legacy config - do NOT copy this code!\n         selected_op_list = {\ndiff --git a/tools/stats/import_test_stats.py b/tools/stats/import_test_stats.py\nindex b0719fc56d97b6..28d8ee0961bd9e 100644\n--- a/tools/stats/import_test_stats.py\n+++ b/tools/stats/import_test_stats.py\n@@ -46,7 +46,7 @@ def is_cached_file_valid() -> bool:\n \n     if os.path.exists(path) and is_cached_file_valid():\n         # Another test process already download the file, so don't re-do it\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             return cast(Dict[str, Any], json.load(f))\n \n     for _ in range(3):\ndiff --git a/tools/stats/upload_stats_lib.py b/tools/stats/upload_stats_lib.py\nindex c9223c528fe163..c7e90286d00a2b 100644\n--- a/tools/stats/upload_stats_lib.py\n+++ b/tools/stats/upload_stats_lib.py\n@@ -249,10 +249,10 @@ def value(self) -> Any:\n         value = os.environ.get(self.env_var)\n         if value is None and self.required:\n             raise ValueError(\n-                (\n+\n                     f\"Missing {self.name}. Please set the {self.env_var}\"\n                     \"environment variable to pass in this value.\"\n-                )\n+\n             )\n         if self.type_conversion_fn:\n             return self.type_conversion_fn(value)\ndiff --git a/tools/substitute.py b/tools/substitute.py\nindex c3b353bf740115..e9c05990c75f9a 100644\n--- a/tools/substitute.py\n+++ b/tools/substitute.py\n@@ -11,7 +11,7 @@\n     parser.add_argument(\"--replace\", action=\"append\", nargs=2)\n     options = parser.parse_args()\n \n-    with open(options.input_file, \"r\") as f:\n+    with open(options.input_file) as f:\n         contents = f.read()\n \n     output_file = os.path.join(options.install_dir, options.output_file)\ndiff --git a/tools/test/test_executorch_gen.py b/tools/test/test_executorch_gen.py\nindex b2a0f6768271bf..c9d6c79b85c1ea 100644\n--- a/tools/test/test_executorch_gen.py\n+++ b/tools/test/test_executorch_gen.py\n@@ -181,7 +181,7 @@ def test_translate_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\n@@ -268,7 +268,7 @@ def test_translate_kernel_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\ndiff --git a/tools/test/test_executorch_signatures.py b/tools/test/test_executorch_signatures.py\nindex c137f6982ec2b7..543926d4c31ef0 100644\n--- a/tools/test/test_executorch_signatures.py\n+++ b/tools/test/test_executorch_signatures.py\n@@ -21,7 +21,7 @@ def test_runtime_signature_contains_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=True)\n-            self.assertEquals(len(args), 3)\n+            self.assertEqual(len(args), 3)\n             self.assertTrue(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n@@ -30,7 +30,7 @@ def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=False)\n-            self.assertEquals(len(args), 2)\n+            self.assertEqual(len(args), 2)\n             self.assertFalse(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_declaration_correct(self) -> None:\n@@ -38,7 +38,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             decl = self.sig.decl(include_context=True)\n-            self.assertEquals(\n+            self.assertEqual(\n                 decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\n@@ -48,7 +48,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n                 ),\n             )\n             no_context_decl = self.sig.decl(include_context=False)\n-            self.assertEquals(\n+            self.assertEqual(\n                 no_context_decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\ndiff --git a/tools/test/test_vulkan_codegen.py b/tools/test/test_vulkan_codegen.py\nindex ae87c27e7aeb8e..196be229b348d2 100644\n--- a/tools/test/test_vulkan_codegen.py\n+++ b/tools/test/test_vulkan_codegen.py\n@@ -92,9 +92,9 @@ def test_missing_key_default_val(self) -> None:\n                     file_name_2 = os.path.join(tmp_dir, \"conv2d_pw_1x2.glsl\")\n                     self.assertTrue(os.path.exists(file_name_1))\n                     self.assertTrue(os.path.exists(file_name_2))\n-                    with open(file_name_1, \"r\") as f:\n+                    with open(file_name_1) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 1\" in contents)\n-                    with open(file_name_2, \"r\") as f:\n+                    with open(file_name_2) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 2\" in contents)\ndiff --git a/tools/testing/explicit_ci_jobs.py b/tools/testing/explicit_ci_jobs.py\nindex daff3cce8956ff..594e00d437f9f7 100755\n--- a/tools/testing/explicit_ci_jobs.py\n+++ b/tools/testing/explicit_ci_jobs.py\n@@ -127,7 +127,7 @@ def commit_ci(files: List[str], message: str) -> None:\n     args = parser.parse_args()\n \n     touched_files = [CONFIG_YML]\n-    with open(CONFIG_YML, \"r\") as f:\n+    with open(CONFIG_YML) as f:\n         config_yml = yaml.safe_load(f.read())\n \n     config_yml[\"workflows\"] = get_filtered_circleci_config(\ndiff --git a/tools/testing/test_selections.py b/tools/testing/test_selections.py\nindex 24fb7278d206f9..76f841b8902bdd 100644\n--- a/tools/testing/test_selections.py\n+++ b/tools/testing/test_selections.py\n@@ -163,7 +163,7 @@ def _get_previously_failing_tests() -> Set[str]:\n         )\n         return set()\n \n-    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH, \"r\") as f:\n+    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH) as f:\n         last_failed_tests = json.load(f)\n \n     prioritized_tests = _parse_prev_failing_test_files(last_failed_tests)\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex 053ce0c0a89552..ebd24383e0b53f 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -156,14 +156,12 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-\n-                            \"      Note: While we usually use modules in the python standard library \"\n-                            f\"from the local environment, `{module_name}` has a lot of system \"\n-                            \"level access and therefore can pose a security risk. We heavily \"\n-                            f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n-                            \"is not possible, add it to the extern list by calling \"\n-                            f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-\n+                        \"      Note: While we usually use modules in the python standard library \"\n+                        f\"from the local environment, `{module_name}` has a lot of system \"\n+                        \"level access and therefore can pose a security risk. We heavily \"\n+                        f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n+                        \"is not possible, add it to the extern list by calling \"\n+                        f'PackageExporter.extern(\"`{module_name}`\")\\n'\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +171,8 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-\n-                    \"Set debug=True when invoking PackageExporter for a visualization of where \"\n-                    \"broken modules are coming from!\\n\"\n-\n+                \"Set debug=True when invoking PackageExporter for a visualization of where \"\n+                \"broken modules are coming from!\\n\"\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\n"
  },
  {
    "number": 105427,
    "title": "[BE] Enable ruff's UP rules and autoformat onnx/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* __->__ #105427\n* #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "a305635bd8a06c2d41c5e71d5e9c3d042cc4441c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105427",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105427/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105427.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105427.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105427/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105427/comments",
    "labels": [
      "Merged",
      "open source",
      "ciflow/trunk",
      "release notes: onnx",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:20:53.592201Z",
    "state": "closed",
    "patch": "From 01c22a61cea2d179f56906cdf7215c084f0c74eb Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:20:47 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat onnx/\n\n[ghstack-poisoned]\n---\n torch/onnx/_internal/diagnostics/infra/utils.py    | 2 +-\n torch/onnx/_internal/exporter.py                   | 4 +---\n torch/onnx/_internal/fx/onnxfunction_dispatcher.py | 6 +++---\n torch/onnx/_internal/fx/passes/type_promotion.py   | 2 +-\n torch/onnx/_internal/fx/registration.py            | 4 ++--\n torch/onnx/_internal/io_adapter.py                 | 6 +++---\n torch/onnx/_internal/jit_utils.py                  | 2 +-\n torch/onnx/symbolic_opset11.py                     | 2 +-\n torch/onnx/symbolic_opset17.py                     | 8 ++++----\n torch/onnx/verification.py                         | 4 ++--\n 10 files changed, 19 insertions(+), 21 deletions(-)\n\ndiff --git a/torch/onnx/_internal/diagnostics/infra/utils.py b/torch/onnx/_internal/diagnostics/infra/utils.py\nindex f287268df5727b..4648b477515025 100644\n--- a/torch/onnx/_internal/diagnostics/infra/utils.py\n+++ b/torch/onnx/_internal/diagnostics/infra/utils.py\n@@ -43,7 +43,7 @@ def python_call_stack(frames_to_skip: int = 0, frames_to_log: int = 16) -> _infr\n     return stack\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def _function_source_info(fn: Callable) -> Tuple[Sequence[str], int, Optional[str]]:\n     \"\"\"Returns the source lines, line number, and source file path for the given function.\n \ndiff --git a/torch/onnx/_internal/exporter.py b/torch/onnx/_internal/exporter.py\nindex aae7eb6f8e3804..33bf8c7afe44f5 100644\n--- a/torch/onnx/_internal/exporter.py\n+++ b/torch/onnx/_internal/exporter.py\n@@ -145,9 +145,7 @@ class ResolvedExportOptions(ExportOptions):\n     logging diagnostics, and generating the SARIF log.\"\"\"\n \n     @_beartype.beartype\n-    def __init__(\n-        self, options: Optional[Union[ExportOptions, \"ResolvedExportOptions\"]]\n-    ):\n+    def __init__(self, options: Optional[Union[ExportOptions, ResolvedExportOptions]]):\n         if options is None:\n             options = ExportOptions()\n         if isinstance(options, ResolvedExportOptions):\ndiff --git a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\nindex a312cf1aed3d80..2b489ca076b430 100644\n--- a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n+++ b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n@@ -105,7 +105,7 @@ def dispatch(\n         ],\n         onnx_kwargs: Dict[str, fx_type_utils.Argument],\n         diagnostic_context: diagnostics.DiagnosticContext,\n-    ) -> Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]:\n+    ) -> Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]:\n         \"\"\"Dispatches an ONNX function based on the given FX node, arguments, and keyword arguments.\n         Args:\n             node: The TorchFX node to dispatch the function for.\n@@ -405,7 +405,7 @@ def aten_new_full_dtype(self: TTensor, size: INT64, fill_value: TTensor, dtype:\n \n     \"\"\"\n \n-    def __init__(self, onnxfunction: \"onnxscript.OnnxFunction\"):\n+    def __init__(self, onnxfunction: onnxscript.OnnxFunction):\n         \"\"\"Initialize the OnnxSchemaChecker .\n \n         Args:\n@@ -579,7 +579,7 @@ def _record_matching_score(\n     @_beartype.beartype\n     def _separate_input_attributes_from_arguments(\n         self,\n-        param_schemas: Sequence[\"onnxscript.values.ParamSchema\"],\n+        param_schemas: Sequence[onnxscript.values.ParamSchema],\n         args: Sequence[\n             Optional[Union[fx_type_utils.TensorLike, str, int, float, bool, list]]\n         ],\ndiff --git a/torch/onnx/_internal/fx/passes/type_promotion.py b/torch/onnx/_internal/fx/passes/type_promotion.py\nindex e100afefe7814a..c8a10cc322fe90 100644\n--- a/torch/onnx/_internal/fx/passes/type_promotion.py\n+++ b/torch/onnx/_internal/fx/passes/type_promotion.py\n@@ -1217,7 +1217,7 @@ def add_rule(self, rule: TypePromotionRule) -> None:\n             ValueError: If the rule is invalid.\n         \"\"\"\n         if not rule.is_valid():\n-            raise ValueError(\"Invalid type promotion rule: {}\".format(rule))\n+            raise ValueError(f\"Invalid type promotion rule: {rule}\")\n         self._rule_table[f\"{rule.namespace}.{rule.op_name}\"] = rule\n \n     @_beartype.beartype\ndiff --git a/torch/onnx/_internal/fx/registration.py b/torch/onnx/_internal/fx/registration.py\nindex 135c9afe9cdd57..b7c8c3521e55f5 100644\n--- a/torch/onnx/_internal/fx/registration.py\n+++ b/torch/onnx/_internal/fx/registration.py\n@@ -29,7 +29,7 @@ class SymbolicFunction:\n \n     \"\"\"\n \n-    onnx_function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]\n+    onnx_function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]\n     op_full_name: str\n     is_custom: bool = False\n \n@@ -99,7 +99,7 @@ def _register(\n     @_beartype.beartype\n     def register_custom_op(\n         self,\n-        function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"],\n+        function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction],\n         namespace: str,\n         op_name: str,\n         overload: Optional[str] = None,\ndiff --git a/torch/onnx/_internal/io_adapter.py b/torch/onnx/_internal/io_adapter.py\nindex 2654a1ade32ac4..1a80e179ac0b67 100644\n--- a/torch/onnx/_internal/io_adapter.py\n+++ b/torch/onnx/_internal/io_adapter.py\n@@ -60,7 +60,7 @@ def append_step(self, step: InputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, *model_args, **model_kwargs\n-    ) -> Sequence[Union[int, float, bool, str, \"torch.Tensor\", None]]:\n+    ) -> Sequence[Union[int, float, bool, str, torch.Tensor, None]]:\n         \"\"\"Converts the PyTorch model inputs to exported ONNX model inputs format.\n \n         Args:\n@@ -113,7 +113,7 @@ def append_step(self, step: OutputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, model_outputs: Any\n-    ) -> Sequence[Union[\"torch.Tensor\", int, float, bool, str]]:\n+    ) -> Sequence[Union[torch.Tensor, int, float, bool, str]]:\n         \"\"\"Converts the PyTorch model outputs to exported ONNX model outputs format.\n \n         Args:\n@@ -228,7 +228,7 @@ def apply(\n class LiftParametersAndBuffersIntoArgsStep:\n     \"\"\"Append parameters and buffers to model's positional argument list.\"\"\"\n \n-    def __init__(self, inputs: Tuple[\"torch.Tensor\", ...]) -> None:\n+    def __init__(self, inputs: Tuple[torch.Tensor, ...]) -> None:\n         self.inputs = inputs\n \n     def apply(\ndiff --git a/torch/onnx/_internal/jit_utils.py b/torch/onnx/_internal/jit_utils.py\nindex 9052961fc7a646..c46a82c40dfec8 100644\n--- a/torch/onnx/_internal/jit_utils.py\n+++ b/torch/onnx/_internal/jit_utils.py\n@@ -40,7 +40,7 @@ class GraphContext:\n     block: _C.Block\n     opset: int\n     original_node: _C.Node\n-    params_dict: Dict[str, \"_C.IValue\"]\n+    params_dict: Dict[str, _C.IValue]\n     env: Dict[_C.Value, _C.Value]\n \n     # Relay methods from _C.Graph for compatibility with symbolic functions that expect\ndiff --git a/torch/onnx/symbolic_opset11.py b/torch/onnx/symbolic_opset11.py\nindex b432244c42aaa7..3bb63e0e8fa36b 100644\n--- a/torch/onnx/symbolic_opset11.py\n+++ b/torch/onnx/symbolic_opset11.py\n@@ -888,7 +888,7 @@ def _get_arange_dtype(dtype):\n         dtype = symbolic_helper._maybe_get_const(dtype, \"i\")\n         return dtype\n \n-    if len(args) == 2 and all((isinstance(val, int) for val in args)):\n+    if len(args) == 2 and all(isinstance(val, int) for val in args):\n         # aten::arange(Scalar start, Scalar end)\n         dtype = torch.int64\n         # Start index.\ndiff --git a/torch/onnx/symbolic_opset17.py b/torch/onnx/symbolic_opset17.py\nindex 92151a5e58d977..3fa03ab8e11915 100644\n--- a/torch/onnx/symbolic_opset17.py\n+++ b/torch/onnx/symbolic_opset17.py\n@@ -144,8 +144,8 @@ def stft(\n         # Center window around zeros if needed (required by ONNX's STFT)\n         if n_win < n_fft:\n             left, right = _compute_edge_sizes(n_fft, n_win)\n-            left_win = g.op(\"Constant\", value_t=torch.zeros((left)))\n-            right_win = g.op(\"Constant\", value_t=torch.zeros((right)))\n+            left_win = g.op(\"Constant\", value_t=torch.zeros(left))\n+            right_win = g.op(\"Constant\", value_t=torch.zeros(right))\n             window = g.op(\"Concat\", left_win, window, right_win, axis_i=0)\n \n     # Create window, if needed\n@@ -161,11 +161,11 @@ def stft(\n             # Center window, if needed\n             left, right = _compute_edge_sizes(n_fft, win_length)\n             torch_window = torch.hstack(\n-                (torch.zeros((left)), torch.ones((win_length)), torch.zeros((right)))\n+                (torch.zeros(left), torch.ones(win_length), torch.zeros(right))\n             )\n         else:\n             # Rectangle window\n-            torch_window = torch.ones((n_fft))\n+            torch_window = torch.ones(n_fft)\n         assert torch_window.shape[0] == n_fft\n         window = g.op(\"Constant\", value_t=torch_window)\n     window = g.op(\ndiff --git a/torch/onnx/verification.py b/torch/onnx/verification.py\nindex abfa4677eb21cb..27fe4e28e32cf6 100644\n--- a/torch/onnx/verification.py\n+++ b/torch/onnx/verification.py\n@@ -1310,7 +1310,7 @@ def essential_node_kinds(self) -> Set[str]:\n         }\n \n     @_beartype.beartype\n-    def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n+    def all_mismatch_leaf_graph_info(self) -> List[GraphInfo]:\n         \"\"\"Return a list of all leaf `GraphInfo` objects that have mismatch.\"\"\"\n         if not self.has_mismatch():\n             return []\n@@ -1333,7 +1333,7 @@ def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n         return results\n \n     @_beartype.beartype\n-    def find_partition(self, id: str) -> Optional[\"GraphInfo\"]:\n+    def find_partition(self, id: str) -> Optional[GraphInfo]:\n         \"\"\"Find the `GraphInfo` object with the given id.\"\"\"\n         if id == self.id:\n             return self\n"
  },
  {
    "number": 105426,
    "title": "[BE] Enable ruff's UP rules and autoformat optim/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105435\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* __->__ #105426\n* #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "eb5261d73ff68b38a1ff57ef212a0b3addbcdcd3",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105426",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105426/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105426.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105426.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105426/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105426/comments",
    "labels": [
      "Merged",
      "open source",
      "release notes: optim",
      "ciflow/trunk",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:20:49.373709Z",
    "state": "closed",
    "patch": "From 94e7270603df5dff64eb024eaa7a5c7faa894ba0 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:20:42 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat optim/\n\n[ghstack-poisoned]\n---\n test/distributions/test_constraints.py        |  14 +-\n test/distributions/test_distributions.py      | 238 +++++++++---------\n test/distributions/test_transforms.py         |  20 +-\n test/optim/test_optim.py                      |   4 +-\n torch/distributions/constraints.py            |  16 +-\n torch/distributions/independent.py            |   2 +-\n torch/distributions/kl.py                     |   6 +-\n .../lowrank_multivariate_normal.py            |   2 +-\n torch/distributions/mixture_same_family.py    |   8 +-\n .../distributions/transformed_distribution.py |   2 +-\n torch/distributions/transforms.py             |   6 +-\n torch/optim/adadelta.py                       |   8 +-\n torch/optim/adagrad.py                        |   8 +-\n torch/optim/adam.py                           |  10 +-\n torch/optim/adamax.py                         |  10 +-\n torch/optim/adamw.py                          |  10 +-\n torch/optim/asgd.py                           |   4 +-\n torch/optim/lr_scheduler.py                   |  18 +-\n torch/optim/nadam.py                          |  12 +-\n torch/optim/optimizer.py                      |  12 +-\n torch/optim/radam.py                          |  10 +-\n torch/optim/rmsprop.py                        |  10 +-\n torch/optim/rprop.py                          |   4 +-\n torch/optim/sgd.py                            |   6 +-\n torch/optim/sparse_adam.py                    |   8 +-\n torch/package/_importlib.py                   |   6 +-\n .../package/file_structure_representation.py  |   1 -\n torch/package/package_exporter.py             |  10 +-\n torch/package/package_importer.py             |   6 +-\n torch/profiler/_memory_profiler.py            |   4 +-\n torch/profiler/_pattern_matcher.py            |   2 +-\n torch/signal/windows/windows.py               |   1 -\n torch/sparse/semi_structured.py               |  16 +-\n 33 files changed, 246 insertions(+), 248 deletions(-)\n\ndiff --git a/test/distributions/test_constraints.py b/test/distributions/test_constraints.py\nindex b733cbc021e153..0753b246e37948 100644\n--- a/test/distributions/test_constraints.py\n+++ b/test/distributions/test_constraints.py\n@@ -83,7 +83,7 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         t = biject_to(constraint)\n     except NotImplementedError:\n         pytest.skip('`biject_to` not implemented.')\n-    assert t.bijective, \"biject_to({}) is not bijective\".format(constraint)\n+    assert t.bijective, f\"biject_to({constraint}) is not bijective\"\n     if constraint_fn is constraints.corr_cholesky:\n         # (D * (D-1)) / 2 (where D = 4) = 6 (size of last dim)\n         x = torch.randn(6, 6, dtype=torch.double)\n@@ -93,12 +93,12 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         x = x.cuda()\n     y = t(x)\n     assert constraint.check(y).all(), '\\n'.join([\n-        \"Failed to biject_to({})\".format(constraint),\n-        \"x = {}\".format(x),\n-        \"biject_to(...)(x) = {}\".format(y),\n+        f\"Failed to biject_to({constraint})\",\n+        f\"x = {x}\",\n+        f\"biject_to(...)(x) = {y}\",\n     ])\n     x2 = t.inv(y)\n-    assert torch.allclose(x, x2), \"Error in biject_to({}) inverse\".format(constraint)\n+    assert torch.allclose(x, x2), f\"Error in biject_to({constraint}) inverse\"\n \n     j = t.log_abs_det_jacobian(x, y)\n     assert j.shape == x.shape[:x.dim() - t.domain.event_dim]\n@@ -119,10 +119,10 @@ def test_transform_to(constraint_fn, args, is_cuda):\n     if is_cuda:\n         x = x.cuda()\n     y = t(x)\n-    assert constraint.check(y).all(), \"Failed to transform_to({})\".format(constraint)\n+    assert constraint.check(y).all(), f\"Failed to transform_to({constraint})\"\n     x2 = t.inv(y)\n     y2 = t(x2)\n-    assert torch.allclose(y, y2), \"Error in transform_to({}) pseudoinverse\".format(constraint)\n+    assert torch.allclose(y, y2), f\"Error in transform_to({constraint}) pseudoinverse\"\n \n \n if __name__ == \"__main__\":\ndiff --git a/test/distributions/test_distributions.py b/test/distributions/test_distributions.py\nindex 69591d31c5ed20..2f4d256516c849 100644\n--- a/test/distributions/test_distributions.py\n+++ b/test/distributions/test_distributions.py\n@@ -862,7 +862,7 @@ def _check_sampler_sampler(self, torch_dist, ref_dist, message, multivariate=Fal\n         bins = samples.reshape((num_bins, samples_per_bin)).mean(axis=1)\n         stddev = samples_per_bin ** -0.5\n         threshold = stddev * scipy.special.erfinv(1 - 2 * failure_rate / num_bins)\n-        message = '{}.sample() is biased:\\n{}'.format(message, bins)\n+        message = f'{message}.sample() is biased:\\n{bins}'\n         for bias in bins:\n             self.assertLess(-threshold, bias, message)\n             self.assertLess(bias, threshold, message)\n@@ -971,7 +971,7 @@ def test_has_examples(self):\n             if isinstance(Dist, type) and issubclass(Dist, Distribution) \\\n                     and Dist is not Distribution and Dist is not ExponentialFamily:\n                 self.assertIn(Dist, distributions_with_examples,\n-                              \"Please add {} to the EXAMPLES list in test_distributions.py\".format(Dist.__name__))\n+                              f\"Please add {Dist.__name__} to the EXAMPLES list in test_distributions.py\")\n \n     def test_support_attributes(self):\n         for Dist, params in EXAMPLES:\n@@ -1120,7 +1120,7 @@ def test_geometric_sample(self):\n         for prob in [0.01, 0.18, 0.8]:\n             self._check_sampler_discrete(Geometric(prob),\n                                          scipy.stats.geom(p=prob, loc=-1),\n-                                         'Geometric(prob={})'.format(prob))\n+                                         f'Geometric(prob={prob})')\n \n     def test_binomial(self):\n         p = torch.arange(0.05, 1, 0.1).requires_grad_()\n@@ -1136,7 +1136,7 @@ def test_binomial_sample(self):\n             for count in [2, 10, 100, 500]:\n                 self._check_sampler_discrete(Binomial(total_count=count, probs=prob),\n                                              scipy.stats.binom(count, prob),\n-                                             'Binomial(total_count={}, probs={})'.format(count, prob))\n+                                             f'Binomial(total_count={count}, probs={prob})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_binomial_log_prob_and_entropy(self):\n@@ -1431,7 +1431,7 @@ def test_poisson_sample(self):\n         for rate in [0.1, 1.0, 5.0]:\n             self._check_sampler_discrete(Poisson(rate),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={})'.format(rate),\n+                                         f'Poisson(lambda={rate})',\n                                          failure_rate=1e-3)\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n@@ -1441,7 +1441,7 @@ def test_poisson_gpu_sample(self):\n         for rate in [0.12, 0.9, 4.0]:\n             self._check_sampler_discrete(Poisson(torch.tensor([rate]).cuda()),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={}, cuda)'.format(rate),\n+                                         f'Poisson(lambda={rate}, cuda)',\n                                          failure_rate=1e-3)\n \n     def test_relaxed_bernoulli(self):\n@@ -1476,7 +1476,7 @@ def sample(self, *args, **kwargs):\n         for probs, temp in product([0.1, 0.2, 0.8], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(Rounded(RelaxedBernoulli(temp, probs)),\n                                          scipy.stats.bernoulli(probs),\n-                                         'Rounded(RelaxedBernoulli(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedBernoulli(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [0.001, 0.2, 0.999]:\n@@ -1534,7 +1534,7 @@ def pmf(self, samples):\n         for probs, temp in product([torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(ArgMax(RelaxedOneHotCategorical(temp, probs)),\n                                          ScipyCategorical(scipy.stats.multinomial(1, probs)),\n-                                         'Rounded(RelaxedOneHotCategorical(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedOneHotCategorical(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])]:\n@@ -1588,7 +1588,7 @@ def test_vonmises_sample(self):\n             for concentration in [0.03, 0.3, 1.0, 10.0, 100.0]:\n                 self._check_sampler_sampler(VonMises(loc, concentration),\n                                             scipy.stats.vonmises(loc=loc, kappa=concentration),\n-                                            \"VonMises(loc={}, concentration={})\".format(loc, concentration),\n+                                            f\"VonMises(loc={loc}, concentration={concentration})\",\n                                             num_samples=int(1e5), circular=True)\n \n     def test_vonmises_logprob(self):\n@@ -1694,7 +1694,7 @@ def test_halfnormal_sample(self):\n         for std in [0.1, 1.0, 10.0]:\n             self._check_sampler_sampler(HalfNormal(std),\n                                         scipy.stats.halfnorm(scale=std),\n-                                        'HalfNormal(scale={})'.format(std))\n+                                        f'HalfNormal(scale={std})')\n \n     def test_lognormal(self):\n         mean = torch.randn(5, 5, requires_grad=True)\n@@ -1746,7 +1746,7 @@ def test_lognormal_sample(self):\n         for mean, std in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(LogNormal(mean, std),\n                                         scipy.stats.lognorm(scale=math.exp(mean), s=std),\n-                                        'LogNormal(loc={}, scale={})'.format(mean, std))\n+                                        f'LogNormal(loc={mean}, scale={std})')\n \n     def test_logisticnormal(self):\n         set_rng_seed(1)  # see Note [Randomized statistical tests]\n@@ -1814,7 +1814,7 @@ def test_logisticnormal_sample(self):\n             std_th = torch.tensor(np.sqrt(np.diag(cov)))\n             self._check_sampler_sampler(\n                 LogisticNormal(mean_th, std_th), ref_dist,\n-                'LogisticNormal(loc={}, scale={})'.format(mean_th, std_th),\n+                f'LogisticNormal(loc={mean_th}, scale={std_th})',\n                 multivariate=True)\n \n     def test_mixture_same_family_shape(self):\n@@ -1958,7 +1958,7 @@ def test_normal_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Normal(loc, scale),\n                                         scipy.stats.norm(loc=loc, scale=scale),\n-                                        'Normal(mean={}, std={})'.format(loc, scale))\n+                                        f'Normal(mean={loc}, std={scale})')\n \n     def test_lowrank_multivariate_normal_shape(self):\n         mean = torch.randn(5, 3, requires_grad=True)\n@@ -2191,15 +2191,15 @@ def test_multivariate_normal_sample(self):\n \n         self._check_sampler_sampler(MultivariateNormal(mean, cov),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, cov={})'.format(mean, cov),\n+                                    f'MultivariateNormal(loc={mean}, cov={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, precision_matrix=prec),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, atol={})'.format(mean, prec),\n+                                    f'MultivariateNormal(loc={mean}, atol={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, scale_tril=scale_tril),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, scale_tril={})'.format(mean, scale_tril),\n+                                    f'MultivariateNormal(loc={mean}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_multivariate_normal_properties(self):\n@@ -2352,15 +2352,15 @@ def test_wishart_sample(self):\n \n         self._check_sampler_sampler(Wishart(df, cov),\n                                     ref_dist,\n-                                    'Wishart(df={}, covariance_matrix={})'.format(df, cov),\n+                                    f'Wishart(df={df}, covariance_matrix={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, precision_matrix=prec),\n                                     ref_dist,\n-                                    'Wishart(df={}, precision_matrix={})'.format(df, prec),\n+                                    f'Wishart(df={df}, precision_matrix={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, scale_tril=scale_tril),\n                                     ref_dist,\n-                                    'Wishart(df={}, scale_tril={})'.format(df, scale_tril),\n+                                    f'Wishart(df={df}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_wishart_properties(self):\n@@ -2431,7 +2431,7 @@ def test_exponential_sample(self):\n         for rate in [1e-5, 1.0, 10.]:\n             self._check_sampler_sampler(Exponential(rate),\n                                         scipy.stats.expon(scale=1. / rate),\n-                                        'Exponential(rate={})'.format(rate))\n+                                        f'Exponential(rate={rate})')\n \n     def test_laplace(self):\n         loc = torch.randn(5, 5, requires_grad=True)\n@@ -2482,7 +2482,7 @@ def test_laplace_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Laplace(loc, scale),\n                                         scipy.stats.laplace(loc=loc, scale=scale),\n-                                        'Laplace(loc={}, scale={})'.format(loc, scale))\n+                                        f'Laplace(loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gamma_shape(self):\n@@ -2533,7 +2533,7 @@ def test_gamma_sample(self):\n         for alpha, beta in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gamma(alpha, beta),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(concentration={}, rate={})'.format(alpha, beta))\n+                                        f'Gamma(concentration={alpha}, rate={beta})')\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n@@ -2543,7 +2543,7 @@ def test_gamma_gpu_sample(self):\n             a, b = torch.tensor([alpha]).cuda(), torch.tensor([beta]).cuda()\n             self._check_sampler_sampler(Gamma(a, b),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(alpha={}, beta={})'.format(alpha, beta),\n+                                        f'Gamma(alpha={alpha}, beta={beta})',\n                                         failure_rate=1e-4)\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -2575,7 +2575,7 @@ def test_pareto_sample(self):\n         for scale, alpha in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Pareto(scale, alpha),\n                                         scipy.stats.pareto(alpha, scale=scale),\n-                                        'Pareto(scale={}, alpha={})'.format(scale, alpha))\n+                                        f'Pareto(scale={scale}, alpha={alpha})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gumbel(self):\n@@ -2616,7 +2616,7 @@ def test_gumbel_sample(self):\n         for loc, scale in product([-5.0, -1.0, -0.1, 0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gumbel(loc, scale),\n                                         scipy.stats.gumbel_r(loc=loc, scale=scale),\n-                                        'Gumbel(loc={}, scale={})'.format(loc, scale))\n+                                        f'Gumbel(loc={loc}, scale={scale})')\n \n     def test_kumaraswamy_shape(self):\n         concentration1 = torch.randn(2, 3).abs().requires_grad_()\n@@ -2646,13 +2646,13 @@ def test_kumaraswamy_mean_variance(self):\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .mean\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .mean\")\n             expected = samples.var(0)\n             actual = m.variance\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .variance\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .variance\")\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_fishersnedecor(self):\n@@ -2683,7 +2683,7 @@ def test_fishersnedecor_sample(self):\n         for df1, df2 in product([0.1, 0.5, 1.0, 5.0, 10.0], [0.1, 0.5, 1.0, 5.0, 10.0]):\n             self._check_sampler_sampler(FisherSnedecor(df1, df2),\n                                         scipy.stats.f(df1, df2),\n-                                        'FisherSnedecor(loc={}, scale={})'.format(df1, df2))\n+                                        f'FisherSnedecor(loc={df1}, scale={df2})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_chi2_shape(self):\n@@ -2710,7 +2710,7 @@ def test_chi2_sample(self):\n         for df in [0.1, 1.0, 5.0]:\n             self._check_sampler_sampler(Chi2(df),\n                                         scipy.stats.chi2(df),\n-                                        'Chi2(df={})'.format(df))\n+                                        f'Chi2(df={df})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT(self):\n@@ -2740,7 +2740,7 @@ def test_studentT_sample(self):\n         for df, loc, scale in product([0.1, 1.0, 5.0, 10.0], [-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(StudentT(df=df, loc=loc, scale=scale),\n                                         scipy.stats.t(df=df, loc=loc, scale=scale),\n-                                        'StudentT(df={}, loc={}, scale={})'.format(df, loc, scale))\n+                                        f'StudentT(df={df}, loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT_log_prob(self):\n@@ -2793,7 +2793,7 @@ def test_dirichlet_sample(self):\n         alpha = torch.exp(torch.randn(3))\n         self._check_sampler_sampler(Dirichlet(alpha),\n                                     scipy.stats.dirichlet(alpha.numpy()),\n-                                    'Dirichlet(alpha={})'.format(list(alpha)),\n+                                    f'Dirichlet(alpha={list(alpha)})',\n                                     multivariate=True)\n \n     def test_dirichlet_mode(self):\n@@ -2837,11 +2837,11 @@ def test_beta_sample(self):\n         for con1, con0 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Beta(con1, con0),\n                                         scipy.stats.beta(con1, con0),\n-                                        'Beta(alpha={}, beta={})'.format(con1, con0))\n+                                        f'Beta(alpha={con1}, beta={con0})')\n         # Check that small alphas do not cause NANs.\n         for Tensor in [torch.FloatTensor, torch.DoubleTensor]:\n             x = Beta(Tensor([1e-6]), Tensor([1e-6])).sample()[0]\n-            self.assertTrue(np.isfinite(x) and x > 0, 'Invalid Beta.sample(): {}'.format(x))\n+            self.assertTrue(np.isfinite(x) and x > 0, f'Invalid Beta.sample(): {x}')\n \n     def test_beta_underflow(self):\n         # For low values of (alpha, beta), the gamma samples can underflow\n@@ -2997,10 +2997,10 @@ def test_cdf_icdf_inverse(self):\n                     continue\n                 rel_error = torch.abs(actual - samples) / (1e-10 + torch.abs(samples))\n                 self.assertLess(rel_error.max(), 1e-4, msg='\\n'.join([\n-                    '{} example {}/{}, icdf(cdf(x)) != x'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf(x) = {}'.format(cdf),\n-                    'icdf(cdf(x)) = {}'.format(actual),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, icdf(cdf(x)) != x',\n+                    f'x = {samples}',\n+                    f'cdf(x) = {cdf}',\n+                    f'icdf(cdf(x)) = {actual}',\n                 ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3029,11 +3029,11 @@ def test_cdf_log_prob(self):\n                     continue\n                 cdfs_derivative = grad(cdfs.sum(), [samples])[0]  # this should not be wrapped in torch.abs()\n                 self.assertEqual(cdfs_derivative, pdfs, msg='\\n'.join([\n-                    '{} example {}/{}, d(cdf)/dx != pdf(x)'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf = {}'.format(cdfs),\n-                    'pdf = {}'.format(pdfs),\n-                    'grad(cdf) = {}'.format(cdfs_derivative),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, d(cdf)/dx != pdf(x)',\n+                    f'x = {samples}',\n+                    f'cdf = {cdfs}',\n+                    f'pdf = {pdfs}',\n+                    f'grad(cdf) = {cdfs_derivative}',\n                 ]))\n \n     def test_valid_parameter_broadcasting(self):\n@@ -3144,13 +3144,13 @@ def test_valid_parameter_broadcasting(self):\n         for dist, expected_size in valid_examples:\n             actual_size = dist.sample().size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n             sample_shape = torch.Size((2,))\n             expected_size = sample_shape + expected_size\n             actual_size = dist.sample(sample_shape).size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n     def test_invalid_parameter_broadcasting(self):\n         # invalid broadcasting cases; should throw error\n@@ -3303,13 +3303,13 @@ def test_gamma(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.0005, '\\n'.join([\n-                'Bad gradient dx/alpha for x ~ Gamma({}, 1)'.format(alpha),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at alpha={}, x={}'.format(alpha, x[rel_error.argmax()]),\n+                f'Bad gradient dx/alpha for x ~ Gamma({alpha}, 1)',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at alpha={alpha}, x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3331,12 +3331,12 @@ def test_chi2(self):\n             expected_grad = -cdf_df / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx/ddf for x ~ Chi2({})'.format(df),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n+                f'Bad gradient dx/ddf for x ~ Chi2({df})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3361,13 +3361,13 @@ def test_dirichlet_on_diagonal(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx[0]/dalpha[0] for Dirichlet([{}, {}, {}])'.format(a0, a1, a2),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x={}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx[0]/dalpha[0] for Dirichlet([{a0}, {a1}, {a2}])',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3391,13 +3391,13 @@ def test_beta_wrt_alpha(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon1 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon1 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3421,13 +3421,13 @@ def test_beta_wrt_beta(self):\n             expected_grad = -cdf_beta / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon0 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {!r}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon0 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]!r}',\n             ]))\n \n     def test_dirichlet_multivariate(self):\n@@ -3485,8 +3485,8 @@ def compute_v(x, alpha):\n             # expression in terms of log_prob rather than the less numerically stable log_prob.exp().\n             error = dlogp_da + (dlogp_dx * v).sum(-1) + div_v\n             self.assertLess(torch.abs(error).max(), 0.005, '\\n'.join([\n-                'Dirichlet([{}, {}, {}]) gradient violates continuity equation:'.format(a1, a2, a3),\n-                'error = {}'.format(error),\n+                f'Dirichlet([{a1}, {a2}, {a3}]) gradient violates continuity equation:',\n+                f'error = {error}',\n             ]))\n \n \n@@ -4147,9 +4147,9 @@ def test_kl_monte_carlo(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                'Expected ({} Monte Carlo samples): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                f'Expected ({denominator} Monte Carlo samples): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     # Multivariate normal has a separate Monte Carlo based test due to the requirement of random generation of\n@@ -4174,9 +4174,9 @@ def test_kl_multivariate_normal(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected ({} Monte Carlo sample): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected ({denominator} Monte Carlo sample): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     def test_kl_multivariate_normal_batched(self):\n@@ -4223,23 +4223,23 @@ def test_kl_lowrank_multivariate_normal(self):\n \n             error_lowrank_lowrank = torch.abs(actual_lowrank_lowrank - expected).max()\n             self.assertLess(error_lowrank_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_lowrank),\n+                f'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_lowrank}',\n             ]))\n \n             error_lowrank_full = torch.abs(actual_lowrank_full - expected).max()\n             self.assertLess(error_lowrank_full, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_full),\n+                f'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_full}',\n             ]))\n \n             error_full_lowrank = torch.abs(actual_full_lowrank - expected).max()\n             self.assertLess(error_full_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_full_lowrank),\n+                f'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_full_lowrank}',\n             ]))\n \n     def test_kl_lowrank_multivariate_normal_batched(self):\n@@ -4261,16 +4261,16 @@ def test_kl_exponential_family(self):\n                 actual = kl_divergence(p, q)\n                 expected = _kl_expfamily_expfamily(p, q)\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                    'Expected (using Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                    f'Expected (using Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n     def test_kl_infinite(self):\n         for p, q in self.infinite_examples:\n             self.assertTrue((kl_divergence(p, q) == inf).all(),\n-                            'Incorrect KL({}, {})'.format(type(p).__name__, type(q).__name__))\n+                            f'Incorrect KL({type(p).__name__}, {type(q).__name__})')\n \n     def test_kl_edgecases(self):\n         self.assertEqual(kl_divergence(Bernoulli(0), Bernoulli(0)), 0)\n@@ -4287,9 +4287,9 @@ def test_kl_shape(self):\n                     continue\n                 expected_shape = dist.batch_shape if dist.batch_shape else torch.Size()\n                 self.assertEqual(kl.shape, expected_shape, msg='\\n'.join([\n-                    '{} example {}/{}'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected {}'.format(expected_shape),\n-                    'Actual {}'.format(kl.shape),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}',\n+                    f'Expected {expected_shape}',\n+                    f'Actual {kl.shape}',\n                 ]))\n \n     def test_kl_transformed(self):\n@@ -4316,10 +4316,10 @@ def test_entropy_monte_carlo(self):\n                 ignore = (expected == inf) | (expected == -inf)\n                 expected[ignore] = actual[ignore]\n                 self.assertEqual(actual, expected, atol=0.2, rtol=0, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (monte carlo) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max()),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (monte carlo) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}',\n                 ]))\n \n     def test_entropy_exponential_family(self):\n@@ -4337,10 +4337,10 @@ def test_entropy_exponential_family(self):\n                 except NotImplementedError:\n                     continue\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n \n@@ -4632,7 +4632,7 @@ def test_lazy_logits_initialization(self):\n             dist = Dist(**param)\n             # Create new instance to generate a valid sample\n             dist.log_prob(Dist(**param).sample())\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('probs', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -4649,7 +4649,7 @@ def test_lazy_probs_initialization(self):\n                 continue\n             dist = Dist(**param)\n             dist.sample()\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('logits', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -5161,7 +5161,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_enumerate_support(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5185,7 +5185,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_mean(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5207,7 +5207,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_variance(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5231,7 +5231,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_entropy(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5255,7 +5255,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_cdf(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5276,7 +5276,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n \n if __name__ == '__main__' and torch._C.has_lapack:\ndiff --git a/test/distributions/test_transforms.py b/test/distributions/test_transforms.py\nindex a4a025b83fd36e..6fd4cf818d6b83 100644\n--- a/test/distributions/test_transforms.py\n+++ b/test/distributions/test_transforms.py\n@@ -156,7 +156,7 @@ def generate_data(transform):\n         x /= x.norm(dim=-1, keepdim=True)\n         x.diagonal(dim1=-1).copy_(x.diagonal(dim1=-1).abs())\n         return x\n-    raise ValueError('Unsupported domain: {}'.format(domain))\n+    raise ValueError(f'Unsupported domain: {domain}')\n \n \n TRANSFORMS_CACHE_ACTIVE = get_transforms(cache_size=1)\n@@ -215,19 +215,19 @@ def test_forward_inverse(transform, test_cached):\n     if transform.bijective:\n         # verify function inverse\n         assert torch.allclose(x2, x, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t.inv(t(-)) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n+            f'{transform} t.inv(t(-)) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n         ])\n     else:\n         # verify weaker function pseudo-inverse\n         assert torch.allclose(y2, y, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t(t.inv(t(-))) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n-            'y2 = t(x2) = {}'.format(y2),\n+            f'{transform} t(t.inv(t(-))) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n+            f'y2 = t(x2) = {y2}',\n         ])\n \n \ndiff --git a/test/optim/test_optim.py b/test/optim/test_optim.py\nindex 54307b2417eaf6..2f1f5536fc2fe1 100644\n--- a/test/optim/test_optim.py\n+++ b/test/optim/test_optim.py\n@@ -1701,8 +1701,8 @@ def test_fused_optimizer_does_not_step_if_foundinf(self):\n \n         num_tensors = 5\n         for functional_optim, amsgrad, no_grad_scale in itertools.product((adam.adam, adamw.adamw), (False, True), (False, True)):\n-            params, grads, exp_avgs, exp_avg_sqs = [\n-                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4)]\n+            params, grads, exp_avgs, exp_avg_sqs = (\n+                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4))\n             prev_params = [t.clone().detach() for t in params]\n             max_exp_avg_sqs = [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] if amsgrad else []\n             state_steps = [torch.ones((), dtype=torch.float32, device=\"cuda\") for _ in range(num_tensors)]\ndiff --git a/torch/distributions/constraints.py b/torch/distributions/constraints.py\nindex a4e3c08461cde7..5f284959beb372 100644\n--- a/torch/distributions/constraints.py\n+++ b/torch/distributions/constraints.py\n@@ -258,7 +258,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -277,7 +277,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -296,7 +296,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -321,7 +321,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -338,7 +338,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -355,7 +355,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -373,7 +373,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -391,7 +391,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \ndiff --git a/torch/distributions/independent.py b/torch/distributions/independent.py\nindex 48442650ddcb77..44a01fd62f9130 100644\n--- a/torch/distributions/independent.py\n+++ b/torch/distributions/independent.py\n@@ -109,4 +109,4 @@ def enumerate_support(self, expand=True):\n         return self.base_dist.enumerate_support(expand=expand)\n \n     def __repr__(self):\n-        return self.__class__.__name__ + '({}, {})'.format(self.base_dist, self.reinterpreted_batch_ndims)\n+        return self.__class__.__name__ + f'({self.base_dist}, {self.reinterpreted_batch_ndims})'\ndiff --git a/torch/distributions/kl.py b/torch/distributions/kl.py\nindex 26d7b47d2f51a8..4eda85ef75b68a 100644\n--- a/torch/distributions/kl.py\n+++ b/torch/distributions/kl.py\n@@ -65,9 +65,9 @@ def kl_version2(p, q): ...\n         type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n     \"\"\"\n     if not isinstance(type_p, type) and issubclass(type_p, Distribution):\n-        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n+        raise TypeError(f'Expected type_p to be a Distribution subclass but got {type_p}')\n     if not isinstance(type_q, type) and issubclass(type_q, Distribution):\n-        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n+        raise TypeError(f'Expected type_q to be a Distribution subclass but got {type_q}')\n \n     def decorator(fun):\n         _KL_REGISTRY[type_p, type_q] = fun\n@@ -735,7 +735,7 @@ def _kl_uniform_beta(p, q):\n     common_term = p.high - p.low\n     t1 = torch.log(common_term)\n     t2 = (q.concentration1 - 1) * (_x_log_x(p.high) - _x_log_x(p.low) - common_term) / common_term\n-    t3 = (q.concentration0 - 1) * (_x_log_x((1 - p.high)) - _x_log_x((1 - p.low)) + common_term) / common_term\n+    t3 = (q.concentration0 - 1) * (_x_log_x(1 - p.high) - _x_log_x(1 - p.low) + common_term) / common_term\n     t4 = q.concentration1.lgamma() + q.concentration0.lgamma() - (q.concentration1 + q.concentration0).lgamma()\n     result = t3 + t4 - t1 - t2\n     result[(p.high > q.support.upper_bound) | (p.low < q.support.lower_bound)] = inf\ndiff --git a/torch/distributions/lowrank_multivariate_normal.py b/torch/distributions/lowrank_multivariate_normal.py\nindex f74ea47a7e53a4..5ca125a92dd006 100644\n--- a/torch/distributions/lowrank_multivariate_normal.py\n+++ b/torch/distributions/lowrank_multivariate_normal.py\n@@ -93,7 +93,7 @@ def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n             raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\"\n                              .format(event_shape[0]))\n         if cov_diag.shape[-1:] != event_shape:\n-            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n+            raise ValueError(f\"cov_diag must be a batch of vectors with shape {event_shape}\")\n \n         loc_ = loc.unsqueeze(-1)\n         cov_diag_ = cov_diag.unsqueeze(-1)\ndiff --git a/torch/distributions/mixture_same_family.py b/torch/distributions/mixture_same_family.py\nindex f12bef1da2c54d..a4d7bd6ff4610b 100644\n--- a/torch/distributions/mixture_same_family.py\n+++ b/torch/distributions/mixture_same_family.py\n@@ -71,17 +71,17 @@ def __init__(self,\n         cdbs = self._component_distribution.batch_shape[:-1]\n         for size1, size2 in zip(reversed(mdbs), reversed(cdbs)):\n             if size1 != 1 and size2 != 1 and size1 != size2:\n-                raise ValueError(\"`mixture_distribution.batch_shape` ({0}) is not \"\n+                raise ValueError(\"`mixture_distribution.batch_shape` ({}) is not \"\n                                  \"compatible with `component_distribution.\"\n-                                 \"batch_shape`({1})\".format(mdbs, cdbs))\n+                                 \"batch_shape`({})\".format(mdbs, cdbs))\n \n         # Check that the number of mixture component matches\n         km = self._mixture_distribution.logits.shape[-1]\n         kc = self._component_distribution.batch_shape[-1]\n         if km is not None and kc is not None and km != kc:\n-            raise ValueError(\"`mixture_distribution component` ({0}) does not\"\n+            raise ValueError(\"`mixture_distribution component` ({}) does not\"\n                              \" equal `component_distribution.batch_shape[-1]`\"\n-                             \" ({1})\".format(km, kc))\n+                             \" ({})\".format(km, kc))\n         self._num_component = km\n \n         event_shape = self._component_distribution.event_shape\ndiff --git a/torch/distributions/transformed_distribution.py b/torch/distributions/transformed_distribution.py\nindex d31064210d4ba7..cd7b5f088a99fe 100644\n--- a/torch/distributions/transformed_distribution.py\n+++ b/torch/distributions/transformed_distribution.py\n@@ -51,7 +51,7 @@ def __init__(self, base_distribution, transforms, validate_args=None):\n                 raise ValueError(\"transforms must be a Transform or a list of Transforms\")\n             self.transforms = transforms\n         else:\n-            raise ValueError(\"transforms must be a Transform or list, but was {}\".format(transforms))\n+            raise ValueError(f\"transforms must be a Transform or list, but was {transforms}\")\n \n         # Reshape base_distribution according to transforms.\n         base_shape = base_distribution.batch_shape + base_distribution.event_shape\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 06d21548384e3c..6745d1f6fbd51e 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -135,7 +135,7 @@ def with_cache(self, cache_size=1):\n             return self\n         if type(self).__init__ is Transform.__init__:\n             return type(self)(cache_size=cache_size)\n-        raise NotImplementedError(\"{}.with_cache is not implemented\".format(type(self)))\n+        raise NotImplementedError(f\"{type(self)}.with_cache is not implemented\")\n \n     def __eq__(self, other):\n         return self is other\n@@ -506,7 +506,7 @@ def forward_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.in_shape)\n         if shape[cut:] != self.in_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.in_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.in_shape}\")\n         return shape[:cut] + self.out_shape\n \n     def inverse_shape(self, shape):\n@@ -514,7 +514,7 @@ def inverse_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.out_shape)\n         if shape[cut:] != self.out_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.out_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.out_shape}\")\n         return shape[:cut] + self.in_shape\n \n \ndiff --git a/torch/optim/adadelta.py b/torch/optim/adadelta.py\nindex d4cbd41883af65..a38337426313db 100644\n--- a/torch/optim/adadelta.py\n+++ b/torch/optim/adadelta.py\n@@ -22,13 +22,13 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= rho <= 1.0:\n-            raise ValueError(\"Invalid rho value: {}\".format(rho))\n+            raise ValueError(f\"Invalid rho value: {rho}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adagrad.py b/torch/optim/adagrad.py\nindex 5909818e5bfb6e..1a3e5120004f98 100644\n--- a/torch/optim/adagrad.py\n+++ b/torch/optim/adagrad.py\n@@ -23,11 +23,11 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= lr_decay:\n-            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n+            raise ValueError(f\"Invalid lr_decay value: {lr_decay}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= initial_accumulator_value:\n             raise ValueError(\n                 \"Invalid initial_accumulator_value value: {}\".format(\n@@ -35,7 +35,7 @@ def __init__(\n                 )\n             )\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adam.py b/torch/optim/adam.py\nindex 3c0d550f414b45..c7e4ed45a92156 100644\n--- a/torch/optim/adam.py\n+++ b/torch/optim/adam.py\n@@ -16,15 +16,15 @@ def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                  maximize: bool = False, capturable: bool = False,\n                  differentiable: bool = False, fused: Optional[bool] = None):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, amsgrad=amsgrad,\ndiff --git a/torch/optim/adamax.py b/torch/optim/adamax.py\nindex 9a5bf9131993ad..1ee927274558f1 100644\n--- a/torch/optim/adamax.py\n+++ b/torch/optim/adamax.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adamw.py b/torch/optim/adamw.py\nindex da202d95c2a032..ff8dbef1d46e8a 100644\n--- a/torch/optim/adamw.py\n+++ b/torch/optim/adamw.py\n@@ -26,15 +26,15 @@ def __init__(\n         fused: Optional[bool] = None,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/asgd.py b/torch/optim/asgd.py\nindex 5e5bd759c1d540..e483e1c31fbc7c 100644\n--- a/torch/optim/asgd.py\n+++ b/torch/optim/asgd.py\n@@ -28,9 +28,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/lr_scheduler.py b/torch/optim/lr_scheduler.py\nindex b531f5149d1aff..d0f85a5daea0c8 100644\n--- a/torch/optim/lr_scheduler.py\n+++ b/torch/optim/lr_scheduler.py\n@@ -1366,11 +1366,11 @@ class CosineAnnealingWarmRestarts(LRScheduler):\n \n     def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n         if T_0 <= 0 or not isinstance(T_0, int):\n-            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n+            raise ValueError(f\"Expected positive integer T_0, but got {T_0}\")\n         if T_mult < 1 or not isinstance(T_mult, int):\n-            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n+            raise ValueError(f\"Expected integer T_mult >= 1, but got {T_mult}\")\n         if not isinstance(eta_min, (float, int)):\n-            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))\n+            raise ValueError(f\"Expected float or int eta_min, but got {eta_min} of type {type(eta_min)}\")\n         self.T_0 = T_0\n         self.T_i = T_0\n         self.T_mult = T_mult\n@@ -1425,7 +1425,7 @@ def step(self, epoch=None):\n                 self.T_i = self.T_i * self.T_mult\n         else:\n             if epoch < 0:\n-                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n+                raise ValueError(f\"Expected non-negative epoch, but got {epoch}\")\n             if epoch >= self.T_0:\n                 if self.T_mult == 1:\n                     self.T_cur = epoch % self.T_0\n@@ -1590,13 +1590,13 @@ def __init__(self,\n             raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n         elif total_steps is not None:\n             if total_steps <= 0 or not isinstance(total_steps, int):\n-                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n+                raise ValueError(f\"Expected positive integer total_steps, but got {total_steps}\")\n             self.total_steps = total_steps\n         else:\n             if epochs <= 0 or not isinstance(epochs, int):\n-                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n+                raise ValueError(f\"Expected positive integer epochs, but got {epochs}\")\n             if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n-                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n+                raise ValueError(f\"Expected positive integer steps_per_epoch, but got {steps_per_epoch}\")\n             self.total_steps = epochs * steps_per_epoch\n \n         if three_phase:\n@@ -1643,11 +1643,11 @@ def __init__(self,\n \n         # Validate pct_start\n         if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n-            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n+            raise ValueError(f\"Expected float between 0 and 1 pct_start, but got {pct_start}\")\n \n         # Validate anneal_strategy\n         if anneal_strategy not in ['cos', 'linear']:\n-            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n+            raise ValueError(f\"anneal_strategy must by one of 'cos' or 'linear', instead got {anneal_strategy}\")\n         elif anneal_strategy == 'cos':\n             self.anneal_func = self._annealing_cos\n         elif anneal_strategy == 'linear':\ndiff --git a/torch/optim/nadam.py b/torch/optim/nadam.py\nindex 23fa563f044d0d..aeb3fc8b77dd2c 100644\n--- a/torch/optim/nadam.py\n+++ b/torch/optim/nadam.py\n@@ -11,17 +11,17 @@ def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                  weight_decay=0, momentum_decay=4e-3, *, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= momentum_decay:\n-            raise ValueError(\"Invalid momentum_decay value: {}\".format(momentum_decay))\n+            raise ValueError(f\"Invalid momentum_decay value: {momentum_decay}\")\n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, momentum_decay=momentum_decay,\n                         foreach=foreach, differentiable=differentiable)\ndiff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py\nindex 34d27bdaca6058..2356a073f3719d 100644\n--- a/torch/optim/optimizer.py\n+++ b/torch/optim/optimizer.py\n@@ -246,10 +246,10 @@ def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         for i, group in enumerate(self.param_groups):\n             format_string += '\\n'\n-            format_string += 'Parameter Group {0}\\n'.format(i)\n+            format_string += f'Parameter Group {i}\\n'\n             for key in sorted(group.keys()):\n                 if key != 'params':\n-                    format_string += '    {0}: {1}\\n'.format(key, group[key])\n+                    format_string += f'    {key}: {group[key]}\\n'\n         format_string += ')'\n         return format_string\n \n@@ -304,7 +304,7 @@ def profile_hook_step(func):\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             self, *_ = args\n-            profile_name = \"Optimizer.step#{}.step\".format(self.__class__.__name__)\n+            profile_name = f\"Optimizer.step#{self.__class__.__name__}.step\"\n             with torch.autograd.profiler.record_function(profile_name):\n                 # call optimizer step pre hooks\n                 for pre_hook in chain(_global_optimizer_pre_hooks.values(), self._optimizer_step_pre_hooks.values()):\n@@ -337,7 +337,7 @@ def _group_tensors_by_device_and_dtype(tensorlistlist, with_indices=False):\n             return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)\n \n     def _patch_step_function(self):\n-        self._zero_grad_profile_name = \"Optimizer.zero_grad#{}.zero_grad\".format(self.__class__.__name__)\n+        self._zero_grad_profile_name = f\"Optimizer.zero_grad#{self.__class__.__name__}.zero_grad\"\n         hooked = getattr(self.__class__.step, \"hooked\", None)\n         if not hooked:\n             self.__class__.step = self.profile_hook_step(self.__class__.step)  # type: ignore[method-assign]\n@@ -468,8 +468,8 @@ def load_state_dict(self, state_dict):\n                              \"that doesn't match the size of optimizer's group\")\n \n         # Update the state\n-        id_map = dict(zip(chain.from_iterable((g['params'] for g in saved_groups)),\n-                      chain.from_iterable((g['params'] for g in groups))))\n+        id_map = dict(zip(chain.from_iterable(g['params'] for g in saved_groups),\n+                      chain.from_iterable(g['params'] for g in groups)))\n \n         def cast(param, value, param_id=None, param_groups=None, key=None):\n             r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\ndiff --git a/torch/optim/radam.py b/torch/optim/radam.py\nindex 3078db48cfd2fc..120620ab949cc1 100644\n--- a/torch/optim/radam.py\n+++ b/torch/optim/radam.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/rmsprop.py b/torch/optim/rmsprop.py\nindex 88acf98a1bcbda..cec27d95506840 100644\n--- a/torch/optim/rmsprop.py\n+++ b/torch/optim/rmsprop.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= momentum:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= alpha:\n-            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n+            raise ValueError(f\"Invalid alpha value: {alpha}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/rprop.py b/torch/optim/rprop.py\nindex a0812f5fbc903f..93e7241010500a 100644\n--- a/torch/optim/rprop.py\n+++ b/torch/optim/rprop.py\n@@ -20,9 +20,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < etas[0] < 1.0 < etas[1]:\n-            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n+            raise ValueError(f\"Invalid eta values: {etas[0]}, {etas[1]}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/sgd.py b/torch/optim/sgd.py\nindex c34761d5e48555..d22fb2a697fd41 100644\n--- a/torch/optim/sgd.py\n+++ b/torch/optim/sgd.py\n@@ -11,11 +11,11 @@ def __init__(self, params, lr=required, momentum=0, dampening=0,\n                  weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if lr is not required and lr < 0.0:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if momentum < 0.0:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if weight_decay < 0.0:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                         weight_decay=weight_decay, nesterov=nesterov,\ndiff --git a/torch/optim/sparse_adam.py b/torch/optim/sparse_adam.py\nindex 383b6866e822af..c68441cb389c04 100644\n--- a/torch/optim/sparse_adam.py\n+++ b/torch/optim/sparse_adam.py\n@@ -7,13 +7,13 @@\n class SparseAdam(Optimizer):\n     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, maximize: bool = False):\n         if not 0.0 < lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n \n         params = list(params)\n \ndiff --git a/torch/package/_importlib.py b/torch/package/_importlib.py\nindex 327c79c67ef9d7..011567b89f5d6c 100644\n--- a/torch/package/_importlib.py\n+++ b/torch/package/_importlib.py\n@@ -31,13 +31,13 @@ def _resolve_name(name, package, level):\n     if len(bits) < level:\n         raise ValueError(\"attempted relative import beyond top-level package\")\n     base = bits[0]\n-    return \"{}.{}\".format(base, name) if name else base\n+    return f\"{base}.{name}\" if name else base\n \n \n def _sanity_check(name, package, level):\n     \"\"\"Verify arguments are \"sane\".\"\"\"\n     if not isinstance(name, str):\n-        raise TypeError(\"module name must be str, not {}\".format(type(name)))\n+        raise TypeError(f\"module name must be str, not {type(name)}\")\n     if level < 0:\n         raise ValueError(\"level must be >= 0\")\n     if level > 0:\n@@ -90,6 +90,6 @@ def _normalize_path(path):\n     \"\"\"\n     parent, file_name = os.path.split(path)\n     if parent:\n-        raise ValueError(\"{!r} must be only a file name\".format(path))\n+        raise ValueError(f\"{path!r} must be only a file name\")\n     else:\n         return file_name\ndiff --git a/torch/package/file_structure_representation.py b/torch/package/file_structure_representation.py\nindex 6ea69173ed3f69..cc5f055c1a20ef 100644\n--- a/torch/package/file_structure_representation.py\n+++ b/torch/package/file_structure_representation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Dict, List\n \n from .glob_group import GlobGroup, GlobPattern\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex f9478b66327605..053ce0c0a89552 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -79,7 +79,7 @@ class PackagingErrorReason(Enum):\n     \"\"\"\n \n     def __repr__(self):\n-        return \"<%s.%s>\" % (self.__class__.__name__, self.name)\n+        return f\"<{self.__class__.__name__}.{self.name}>\"\n \n     IS_EXTENSION_MODULE = (\n         \"Module is a C extension module. torch.package supports Python modules only.\"\n@@ -156,14 +156,14 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-                        (\n+\n                             \"      Note: While we usually use modules in the python standard library \"\n                             f\"from the local environment, `{module_name}` has a lot of system \"\n                             \"level access and therefore can pose a security risk. We heavily \"\n                             f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n                             \"is not possible, add it to the extern list by calling \"\n                             f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-                        )\n+\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +173,10 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-                (\n+\n                     \"Set debug=True when invoking PackageExporter for a visualization of where \"\n                     \"broken modules are coming from!\\n\"\n-                )\n+\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\ndiff --git a/torch/package/package_importer.py b/torch/package/package_importer.py\nindex 8369e79e783ad7..2d313c8f14eb45 100644\n--- a/torch/package/package_importer.py\n+++ b/torch/package/package_importer.py\n@@ -539,7 +539,7 @@ def _handle_fromlist(self, module, fromlist, *, recursive=False):\n                     if not recursive and hasattr(module, \"__all__\"):\n                         self._handle_fromlist(module, module.__all__, recursive=True)\n                 elif not hasattr(module, x):\n-                    from_name = \"{}.{}\".format(module_name, x)\n+                    from_name = f\"{module_name}.{x}\"\n                     try:\n                         self._gcd_import(from_name)\n                     except ModuleNotFoundError as exc:\n@@ -587,13 +587,13 @@ def _get_package(self, package):\n         \"\"\"\n         if hasattr(package, \"__spec__\"):\n             if package.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package.__spec__.name))\n+                raise TypeError(f\"{package.__spec__.name!r} is not a package\")\n             else:\n                 return package\n         else:\n             module = self.import_module(package)\n             if module.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package))\n+                raise TypeError(f\"{package!r} is not a package\")\n             else:\n                 return module\n \ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex 7ade85a85caa11..fbbcd4d67b7889 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -738,11 +738,11 @@ def _category_snapshot(self) -> Dict[TensorAndID, Optional[Category]]:\n \n         for node in self._data_flow_graph.flow_nodes:\n             all_tensor_versions.update(((k, v) for k, (_, v) in node.inputs.items()))\n-            all_tensor_versions.update(((key, 0) for key in node.intermediates))\n+            all_tensor_versions.update((key, 0) for key in node.intermediates)\n             all_tensor_versions.update(node.outputs.items())\n \n         for i in self._categories._values.values():\n-            all_tensor_versions.update(((key, 0) for key in i._by_id_keyset))\n+            all_tensor_versions.update((key, 0) for key in i._by_id_keyset)\n \n         return {\n             (key, version): self._categories.get(key, version)\ndiff --git a/torch/profiler/_pattern_matcher.py b/torch/profiler/_pattern_matcher.py\nindex ae95faf0d2bae7..1d85d193ecf894 100644\n--- a/torch/profiler/_pattern_matcher.py\n+++ b/torch/profiler/_pattern_matcher.py\n@@ -642,7 +642,7 @@ def report_all_anti_patterns(prof,\n         json_report_path = os.path.join(json_report_dir,\n                                         \"torchtidy_report.json\")\n         if os.path.exists(json_report_path):\n-            with open(json_report_path, \"r\") as f:\n+            with open(json_report_path) as f:\n                 exisiting_report = json.load(f)\n                 exisiting_report.update(report_dict)\n                 report_dict = exisiting_report\ndiff --git a/torch/signal/windows/windows.py b/torch/signal/windows/windows.py\nindex 1ddfff96228927..d1b8e2529bb97e 100644\n--- a/torch/signal/windows/windows.py\n+++ b/torch/signal/windows/windows.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Optional, Iterable\n \n import torch\ndiff --git a/torch/sparse/semi_structured.py b/torch/sparse/semi_structured.py\nindex 0e4c217a50aafb..d1e4321d66f36b 100644\n--- a/torch/sparse/semi_structured.py\n+++ b/torch/sparse/semi_structured.py\n@@ -136,28 +136,28 @@ def __init__(\n             # check device\n             if not original_tensor.is_cuda:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.device= {original_tensor.device} is not supported! \"\n                         \"Only CUDA tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dim\n             if original_tensor.dim() != 2:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dim = {original_tensor.dim()} is not supported! \"\n                         \"Only 2d tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dtype\n             if original_tensor.dtype not in _DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dtype {original_tensor.dtype} is not a supported dtype! \"\n                         \"dtype must be one of: {_DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG}\"\n-                    )\n+\n                 )\n \n             # check shape\n@@ -167,10 +167,10 @@ def __init__(\n             if m < min_rows or m % min_rows or n < min_cols or n % min_cols:\n                 # TODO in the future we can add in padding to support dimensions that aren't perfect multiples\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.shape {original_tensor.shape} is not supported! \"\n                         \"Both dimensions must be larger or equal than and a multiple of ({min_rows}, {min_cols})\"\n-                    )\n+\n                 )\n \n             # This code calculates the size of the compressed tensor.\n"
  },
  {
    "number": 105425,
    "title": "[BE] Enable ruff's UP rules and autoformat testing/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105435\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* __->__ #105425\n* #105424\n* #105423\n\n",
    "merge_commit_sha": "35c156814dd66eeae63f8939bcc20a120bf85d5c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105425",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105425/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105425.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105425.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105425/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105425/comments",
    "labels": [
      "Merged",
      "open source",
      "release notes: distributed (rpc)",
      "ciflow/trunk",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:20:45.052111Z",
    "state": "closed",
    "patch": "From 1b298bffad8ca05d8840672cdbdc2efa386935c6 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:20:37 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat testing/\n\n[ghstack-poisoned]\n---\n torch/testing/_comparison.py                  | 10 +--\n .../_internal/check_kernel_launches.py        |  2 +-\n .../_internal/codegen/random_topo_test.py     | 16 ++---\n torch/testing/_internal/common_cuda.py        |  2 +-\n torch/testing/_internal/common_device_type.py | 50 +++++++--------\n torch/testing/_internal/common_distributed.py |  4 +-\n torch/testing/_internal/common_fsdp.py        |  2 +-\n .../_internal/common_methods_invocations.py   | 18 +++---\n torch/testing/_internal/common_modules.py     | 64 +++++++++----------\n torch/testing/_internal/common_nn.py          | 12 ++--\n torch/testing/_internal/common_pruning.py     |  1 -\n .../testing/_internal/common_quantization.py  |  5 +-\n torch/testing/_internal/common_utils.py       | 34 +++++-----\n torch/testing/_internal/dist_utils.py         |  4 +-\n .../_internal/distributed/distributed_test.py | 14 ++--\n .../distributed/nn/api/remote_module_test.py  | 20 +++---\n .../distributed/rpc/dist_autograd_test.py     | 22 +++----\n .../reinforcement_learning_rpc_test.py        |  2 +-\n .../distributed/rpc/faulty_agent_rpc_test.py  |  8 +--\n .../rpc/faulty_rpc_agent_test_fixture.py      |  2 +-\n .../_internal/distributed/rpc/jit/rpc_test.py |  2 +-\n .../distributed/rpc/jit/rpc_test_faulty.py    |  8 +--\n .../_internal/distributed/rpc/rpc_test.py     |  4 +-\n .../rpc/tensorpipe_rpc_agent_test_fixture.py  |  2 +-\n .../_internal/jit_metaprogramming_utils.py    | 18 +++---\n torch/testing/_internal/jit_utils.py          | 12 ++--\n torch/testing/_internal/opinfo/core.py        |  9 +--\n .../_internal/opinfo/definitions/linalg.py    |  2 +-\n .../_internal/opinfo/definitions/sparse.py    |  2 +-\n 29 files changed, 172 insertions(+), 179 deletions(-)\n\ndiff --git a/torch/testing/_comparison.py b/torch/testing/_comparison.py\nindex 1ccc7447ed7486..4204a6c0e69441 100644\n--- a/torch/testing/_comparison.py\n+++ b/torch/testing/_comparison.py\n@@ -465,9 +465,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[bool, bool]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_bool(bool_like, id=id) for bool_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_bool(self, bool_like: Any, *, id: Tuple[Any, ...]) -> bool:\n@@ -559,9 +559,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[Union[int, float, complex], Union[int, float, complex]]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_number(number_like, id=id) for number_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_number(\n@@ -675,7 +675,7 @@ def _process_inputs(\n         if not allow_subclasses and type(actual) is not type(expected):\n             self._inputs_not_supported()\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\ndiff --git a/torch/testing/_internal/check_kernel_launches.py b/torch/testing/_internal/check_kernel_launches.py\nindex 667e3412ceaf2c..131ea461ce544a 100644\n--- a/torch/testing/_internal/check_kernel_launches.py\n+++ b/torch/testing/_internal/check_kernel_launches.py\n@@ -111,7 +111,7 @@ def check_file(filename):\n         return 0\n     if should_exclude_file(filename):\n         return 0\n-    with open(filename, \"r\") as fo:\n+    with open(filename) as fo:\n         contents = fo.read()\n         unsafeCount = check_code_for_cuda_kernel_launches(contents, filename)\n     return unsafeCount\ndiff --git a/torch/testing/_internal/codegen/random_topo_test.py b/torch/testing/_internal/codegen/random_topo_test.py\nindex fdb13d4ef139c0..b94f8f60a301d3 100644\n--- a/torch/testing/_internal/codegen/random_topo_test.py\n+++ b/torch/testing/_internal/codegen/random_topo_test.py\n@@ -96,7 +96,7 @@ def get_root(x, dependency_map):\n         out_tensor = None\n \n         if DEBUG_PRINT:\n-            print(\"iteration {0}, num_sets{1}, candidates {2}, tensor_list {3}, lh_index {4}, op_index {5}\".format(\n+            print(\"iteration {}, num_sets{}, candidates {}, tensor_list {}, lh_index {}, op_index {}\".format(\n                 num_operations, num_sets, candidate, len(tensor_list), lh_index, op_index))\n         if num_operations >= 0:\n             num_operations -= 1\n@@ -125,7 +125,7 @@ def get_root(x, dependency_map):\n                     #  right = tensor_list[lh_index]\n                     out_tensor = binary_operations[op_index - u_op_size](left, right)\n                 if DEBUG_PRINT:\n-                    print(\"binary, op_2_index {0}, rh_index ?{1}\".format(op_2_index, rh_index))\n+                    print(f\"binary, op_2_index {op_2_index}, rh_index ?{rh_index}\")\n         else:\n             # binary operation, we just randomly pick two candidates.\n             # this is not the most efficient way to close dependency, as we could have\n@@ -136,7 +136,7 @@ def get_root(x, dependency_map):\n             # [if rh_index: create binary operator output tensor]\n             rh_index = candidate[cand_index]\n             if DEBUG_PRINT:\n-                print(\"binary rh_index ?{0}\".format(rh_index))\n+                print(f\"binary rh_index ?{rh_index}\")\n \n         # update candidate should happen before we remove rh_index\n         candidate[index] = len(tensor_list)\n@@ -185,7 +185,7 @@ def get_root(x, dependency_map):\n             ret_list.append(tensor_list[ind])\n \n     if DEBUG_PRINT:\n-        print(\"ended with tensor_list: {0}\".format(len(tensor_list)))\n+        print(f\"ended with tensor_list: {len(tensor_list)}\")\n \n     return tuple(ret_list)\n \n@@ -248,7 +248,7 @@ def prepareInputTensorsToRandomTopoTest(seed,\n \n \n def reproString(current_seed, args):\n-    repro_str = \"python {0}\".format(__file__)\n+    repro_str = f\"python {__file__}\"\n     if args.cuda_fuser:\n         repro_str += \" --cuda-fuser\"\n     if args.legacy_fuser:\n@@ -259,8 +259,8 @@ def reproString(current_seed, args):\n         repro_str += \" --fp16\"\n     if args.cpu:\n         repro_str += \" --cpu\"\n-    repro_str += \" --max-num-tensor {0} --max-tensor-dim {1} --max-tensor-size {2}\"\\\n-        \" --depth-factor {3} --seed {4} --repro-run\".format(\n+    repro_str += \" --max-num-tensor {} --max-tensor-dim {} --max-tensor-size {}\"\\\n+        \" --depth-factor {} --seed {} --repro-run\".format(\n             args.max_num_tensor, args.max_tensor_dim, args.max_tensor_size,\n             args.depth_factor, current_seed)\n     return repro_str\n@@ -390,7 +390,7 @@ def parse_args():\n         if len(failing_repros) == 0:\n             print(\"test passed\")\n         else:\n-            print(\"{0} out of {1} tests failed;\".format(\n+            print(\"{} out of {} tests failed;\".format(\n                   len(failing_repros), args.iterations))\n             print(\"To repro failing tests, run\\n\")\n             for repro in failing_repros:\ndiff --git a/torch/testing/_internal/common_cuda.py b/torch/testing/_internal/common_cuda.py\nindex c380dd5e6d7250..f427f7b62b9f77 100644\n--- a/torch/testing/_internal/common_cuda.py\n+++ b/torch/testing/_internal/common_cuda.py\n@@ -48,7 +48,7 @@ def initialize_cuda_context_rng():\n     if not __cuda_ctx_rng_initialized:\n         # initialize cuda context and rng for memory tests\n         for i in range(torch.cuda.device_count()):\n-            torch.randn(1, device=\"cuda:{}\".format(i))\n+            torch.randn(1, device=f\"cuda:{i}\")\n         __cuda_ctx_rng_initialized = True\n \n \ndiff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py\nindex d9c362c332479d..891c878cc5f059 100644\n--- a/torch/testing/_internal/common_device_type.py\n+++ b/torch/testing/_internal/common_device_type.py\n@@ -276,9 +276,9 @@ def _dtype_test_suffix(dtypes):\n     if isinstance(dtypes, (list, tuple)):\n         if len(dtypes) == 0:\n             return ''\n-        return '_' + '_'.join((dtype_name(d) for d in dtypes))\n+        return '_' + '_'.join(dtype_name(d) for d in dtypes)\n     elif dtypes:\n-        return '_{}'.format(dtype_name(dtypes))\n+        return f'_{dtype_name(dtypes)}'\n     else:\n         return ''\n \n@@ -286,7 +286,7 @@ def _dtype_test_suffix(dtypes):\n def _update_param_kwargs(param_kwargs, name, value):\n     \"\"\" Adds a kwarg with the specified name and value to the param_kwargs dict. \"\"\"\n     # Make name plural (e.g. devices / dtypes) if the value is composite.\n-    plural_name = '{}s'.format(name)\n+    plural_name = f'{name}s'\n \n     # Clear out old entries of the arg if any.\n     if name in param_kwargs:\n@@ -432,7 +432,7 @@ def instantiated_test(self, param_kwargs=param_kwargs):\n \n                 return result\n \n-            assert not hasattr(cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(cls, name), f\"Redefinition of test {name}\"\n             setattr(cls, name, instantiated_test)\n \n         def default_parametrize_fn(test, generic_cls, device_cls):\n@@ -467,7 +467,7 @@ def dtype_parametrize_fn(test, generic_cls, device_cls, dtypes=dtypes):\n             dtype_kwarg = None\n             if 'dtype' in param_kwargs or 'dtypes' in param_kwargs:\n                 dtype_kwarg = param_kwargs['dtypes'] if 'dtypes' in param_kwargs else param_kwargs['dtype']\n-            test_name = '{}{}{}{}'.format(name, test_suffix, device_suffix, _dtype_test_suffix(dtype_kwarg))\n+            test_name = f'{name}{test_suffix}{device_suffix}{_dtype_test_suffix(dtype_kwarg)}'\n \n             instantiate_test_helper(cls=cls, name=test_name, test=test, param_kwargs=param_kwargs,\n                                     decorator_fn=decorator_fn)\n@@ -523,7 +523,7 @@ def setUpClass(cls):\n         cls.cudnn_version = None if cls.no_cudnn else torch.backends.cudnn.version()\n \n         # Acquires the current device as the primary (test) device\n-        cls.primary_device = 'cuda:{0}'.format(torch.cuda.current_device())\n+        cls.primary_device = f'cuda:{torch.cuda.current_device()}'\n \n # See Note [Lazy Tensor tests in device agnostic testing]\n lazy_ts_backend_init = False\n@@ -589,7 +589,7 @@ def setUpClass(cls):\n         cls.device_mod = getattr(torch, cls.device_type, None)\n         assert cls.device_mod is not None, f'''torch has no module of `{cls.device_type}`, you should register\n                                             a module by `torch._register_device_module`.'''\n-        cls.primary_device = '{device_type}:{id}'.format(device_type=cls.device_type, id=cls.device_mod.current_device())\n+        cls.primary_device = f'{cls.device_type}:{cls.device_mod.current_device()}'\n \n # Adds available device-type-specific test base classes\n def get_device_type_test_bases():\n@@ -744,7 +744,7 @@ def split_if_not_empty(x: str):\n                 else:\n                     device_type_test_class.instantiate_test(name, copy.deepcopy(test))\n             else:  # Ports non-test member\n-                assert name not in device_type_test_class.__dict__, \"Redefinition of directly defined member {0}\".format(name)\n+                assert name not in device_type_test_class.__dict__, f\"Redefinition of directly defined member {name}\"\n                 nontest = getattr(generic_test_class, name)\n                 setattr(device_type_test_class, name, nontest)\n \n@@ -913,7 +913,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for op {1}!\".format(test_name, op.name))\n+                    print(f\"Failed to instantiate {test_name} for op {op.name}!\")\n                     raise ex\n         if op is check_exhausted_iterator:\n             raise ValueError('An empty op_list was passed to @ops. '\n@@ -1034,7 +1034,7 @@ def dep_fn(self, *args, **kwargs):\n             size_bytes = size(self, *args, **kwargs) if callable(size) else size\n             _device = device if device is not None else self.get_primary_device()\n             if not _has_sufficient_memory(_device, size_bytes):\n-                raise unittest.SkipTest('Insufficient {} memory'.format(_device))\n+                raise unittest.SkipTest(f'Insufficient {_device} memory')\n \n             return fn(self, *args, **kwargs)\n         return dep_fn\n@@ -1072,7 +1072,7 @@ def __call__(self, fn):\n         @wraps(fn)\n         def only_fn(slf, *args, **kwargs):\n             if self.device_type != slf.device_type:\n-                reason = \"Only runs on {0}\".format(self.device_type)\n+                reason = f\"Only runs on {self.device_type}\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, *args, **kwargs)\n@@ -1090,13 +1090,13 @@ def __init__(self, num_required_devices):\n         self.num_required_devices = num_required_devices\n \n     def __call__(self, fn):\n-        assert not hasattr(fn, 'num_required_devices'), \"deviceCountAtLeast redefinition for {0}\".format(fn.__name__)\n+        assert not hasattr(fn, 'num_required_devices'), f\"deviceCountAtLeast redefinition for {fn.__name__}\"\n         fn.num_required_devices = self.num_required_devices\n \n         @wraps(fn)\n         def multi_fn(slf, devices, *args, **kwargs):\n             if len(devices) < self.num_required_devices:\n-                reason = \"fewer than {0} devices detected\".format(self.num_required_devices)\n+                reason = f\"fewer than {self.num_required_devices} devices detected\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, devices, *args, **kwargs)\n@@ -1108,7 +1108,7 @@ def onlyNativeDeviceTypes(fn):\n     @wraps(fn)\n     def only_fn(self, *args, **kwargs):\n         if self.device_type not in NATIVE_DEVICES:\n-            reason = \"onlyNativeDeviceTypes: doesn't run on {0}\".format(self.device_type)\n+            reason = f\"onlyNativeDeviceTypes: doesn't run on {self.device_type}\"\n             raise unittest.SkipTest(reason)\n \n         return fn(self, *args, **kwargs)\n@@ -1137,7 +1137,7 @@ class precisionOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"precisionOverride not given a dtype : precision dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"precisionOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"precisionOverride given unknown dtype {dtype}\"\n \n         self.d = d\n \n@@ -1168,7 +1168,7 @@ class toleranceOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"toleranceOverride not given a dtype : tol dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"toleranceOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"toleranceOverride given unknown dtype {dtype}\"\n             assert isinstance(prec, tol), \"toleranceOverride not given a dtype : tol dict!\"\n \n         self.d = d\n@@ -1195,17 +1195,17 @@ def __init__(self, *args, device_type=\"all\"):\n                 assert isinstance(arg, (list, tuple)), \\\n                     \"When one dtype variant is a tuple or list, \" \\\n                     \"all dtype variants must be. \" \\\n-                    \"Received non-list non-tuple dtype {0}\".format(str(arg))\n-                assert all(isinstance(dtype, torch.dtype) for dtype in arg), \"Unknown dtype in {0}\".format(str(arg))\n+                    \"Received non-list non-tuple dtype {}\".format(str(arg))\n+                assert all(isinstance(dtype, torch.dtype) for dtype in arg), f\"Unknown dtype in {str(arg)}\"\n         else:\n-            assert all(isinstance(arg, torch.dtype) for arg in args), \"Unknown dtype in {0}\".format(str(args))\n+            assert all(isinstance(arg, torch.dtype) for arg in args), f\"Unknown dtype in {str(args)}\"\n \n         self.args = args\n         self.device_type = device_type\n \n     def __call__(self, fn):\n         d = getattr(fn, 'dtypes', {})\n-        assert self.device_type not in d, \"dtypes redefinition for {0}\".format(self.device_type)\n+        assert self.device_type not in d, f\"dtypes redefinition for {self.device_type}\"\n         d[self.device_type] = self.args\n         fn.dtypes = d\n         return fn\n@@ -1244,7 +1244,7 @@ def onlyPRIVATEUSE1(fn):\n     device_type = torch._C._get_privateuse1_backend_name()\n     device_mod = getattr(torch, device_type, None)\n     if device_mod is None:\n-        reason = \"Skip as torch has no module of {0}\".format(device_type)\n+        reason = f\"Skip as torch has no module of {device_type}\"\n         return unittest.skip(reason)(fn)\n     return onlyOn(device_type)(fn)\n \n@@ -1358,7 +1358,7 @@ def wrap_fn(self, *args, **kwargs):\n                     raise unittest.SkipTest(reason)\n                 rocm_version_tuple = _get_torch_rocm_version()\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\n@@ -1375,7 +1375,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version in (versions or []):\n-                reason = \"test skipped for CUDA version {0}\".format(version)\n+                reason = f\"test skipped for CUDA version {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1391,7 +1391,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version < versions:\n-                reason = \"test skipped for CUDA versions < {0}\".format(version)\n+                reason = f\"test skipped for CUDA versions < {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1409,7 +1409,7 @@ def wrap_fn(self, *args, **kwargs):\n                     reason = \"cuDNN not available\"\n                     raise unittest.SkipTest(reason)\n                 if self.cudnn_version is None or self.cudnn_version < version:\n-                    reason = \"cuDNN version {0} is available but {1} required\".format(self.cudnn_version, version)\n+                    reason = f\"cuDNN version {self.cudnn_version} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\ndiff --git a/torch/testing/_internal/common_distributed.py b/torch/testing/_internal/common_distributed.py\nindex 8981aa78d06a09..d1cf02749b79ce 100644\n--- a/torch/testing/_internal/common_distributed.py\n+++ b/torch/testing/_internal/common_distributed.py\n@@ -770,7 +770,7 @@ def _check_no_test_errors(self, elapsed_time) -> None:\n         for i, p in enumerate(self.processes):\n             if p.exitcode is None:\n                 raise RuntimeError(\n-                    \"Process {} timed out after {} seconds\".format(i, elapsed_time)\n+                    f\"Process {i} timed out after {elapsed_time} seconds\"\n                 )\n             self.assertNotEqual(self.TEST_ERROR_EXIT_CODE, p.exitcode)\n \n@@ -1102,7 +1102,7 @@ def _check_return_codes(cls, failed_ranks, timeout, fn):\n                     \"Caught exception: \\n%s exiting thread %s\", msg, rank\n                 )\n                 error_msg += (\n-                    \"Thread {} exited with exception:\\n{}\\n\".format(rank, msg)\n+                    f\"Thread {rank} exited with exception:\\n{msg}\\n\"\n                 )\n             elif isinstance(exc, SystemExit):\n                 if type(exc.code) == int and skip_code < 0:\ndiff --git a/torch/testing/_internal/common_fsdp.py b/torch/testing/_internal/common_fsdp.py\nindex 589372eaa8e216..20a35930a7f5f4 100644\n--- a/torch/testing/_internal/common_fsdp.py\n+++ b/torch/testing/_internal/common_fsdp.py\n@@ -881,7 +881,7 @@ def process_group(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     def _check_cpu_offload(self, fsdp_model, cpu_offload):\n         self.assertEqual(cpu_offload, fsdp_model.cpu_offload)\ndiff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py\nindex 3b9d3269853a05..a9f58c76d90f2d 100644\n--- a/torch/testing/_internal/common_methods_invocations.py\n+++ b/torch/testing/_internal/common_methods_invocations.py\n@@ -851,7 +851,7 @@ def error_inputs_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"normal expects std >= 0.0, but found std {}\".format(invalid_std),\n+        error_regex=fr\"normal expects std >= 0.0, but found std {invalid_std}\",\n     )\n \n def sample_inputs_cauchy(op, device, dtype, requires_grad, **kwargs):\n@@ -871,7 +871,7 @@ def error_inputs_cauchy(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_scale,)),\n         error_type=RuntimeError,\n-        error_regex=r\"cauchy_ expects sigma > 0.0, but found sigma={}\".format(invalid_scale),\n+        error_regex=fr\"cauchy_ expects sigma > 0.0, but found sigma={invalid_scale}\",\n     )\n \n \n@@ -893,7 +893,7 @@ def error_inputs_exponential(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(invalid_rate,)),\n         error_type=RuntimeError,\n-        error_regex=r\"exponential_ expects lambda > 0.0, but found lambda={}\".format(invalid_rate),\n+        error_regex=fr\"exponential_ expects lambda > 0.0, but found lambda={invalid_rate}\",\n     )\n \n \n@@ -915,7 +915,7 @@ def error_inputs_geometric(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(neg_prob,)),\n         error_type=RuntimeError,\n-        error_regex=r\"geometric_ expects p to be in \\(0, 1\\), but got p={}\".format(neg_prob),\n+        error_regex=fr\"geometric_ expects p to be in \\(0, 1\\), but got p={neg_prob}\",\n     )\n \n \n@@ -937,7 +937,7 @@ def error_inputs_log_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"log_normal_ expects std > 0.0, but found std={}\".format(invalid_std),\n+        error_regex=fr\"log_normal_ expects std > 0.0, but found std={invalid_std}\",\n     )\n \n \n@@ -1889,9 +1889,9 @@ def sample_inputs_logcumsumexp(self, device, dtype, requires_grad, **kwargs):\n             yield SampleInput(t, dim)\n \n def sample_inputs_trace(self, device, dtype, requires_grad, **kwargs):\n-    yield SampleInput((make_tensor((S, S), dtype=dtype, device=device,\n+    yield SampleInput(make_tensor((S, S), dtype=dtype, device=device,\n                                    low=None, high=None,\n-                                   requires_grad=requires_grad)))\n+                                   requires_grad=requires_grad))\n \n \n def error_inputs_trace(op, device):\n@@ -4020,7 +4020,7 @@ def error_inputs_group_norm(opinfo, device, **kwargs):\n \n     # check that input has minimum number of dimensions\n     err_msg1 = \"Expected at least 2 dimensions for input tensor but received\"\n-    s1 = SampleInput(make_arg((1)), args=(1,))\n+    s1 = SampleInput(make_arg(1), args=(1,))\n     yield ErrorInput(s1, error_regex=err_msg1)\n \n     # check that the channels dimension is compatible with number of groups\n@@ -6950,7 +6950,7 @@ def make_bool_mask(shape):\n \n         if mask_t.sum() == 0:\n             def random_index(shape):\n-                return tuple((random.randrange(0, max_idx) for max_idx in shape))\n+                return tuple(random.randrange(0, max_idx) for max_idx in shape)\n \n             mask_t[random_index(mask_t.shape)] = True\n             return mask_t\ndiff --git a/torch/testing/_internal/common_modules.py b/torch/testing/_internal/common_modules.py\nindex 2119678a33f5ef..c3ac11454ab410 100644\n--- a/torch/testing/_internal/common_modules.py\n+++ b/torch/testing/_internal/common_modules.py\n@@ -123,7 +123,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for module {1}!\".format(test_name, module_info.name))\n+                    print(f\"Failed to instantiate {test_name} for module {module_info.name}!\")\n                     raise ex\n \n \n@@ -252,7 +252,7 @@ def bilinear_reference_fn(m, p, x1, x2, bias=True):\n                     desc='no_bias',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n         ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n-                    forward_input=FunctionInput(make_input((2)), make_input((3))),\n+                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                     desc='no_batch_dim',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n     ]\n@@ -312,9 +312,9 @@ def module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_\n     for desc, constructor_kwargs in cases:\n         module_inputs.append(\n             ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n-                        forward_input=FunctionInput(make_input((3)),\n-                                                    make_target((3)),\n-                                                    make_input((1)).abs()),\n+                        forward_input=FunctionInput(make_input(3),\n+                                                    make_target(3),\n+                                                    make_input(1).abs()),\n                         desc=desc,\n                         reference_fn=no_batch_dim_reference_fn)\n         )\n@@ -454,7 +454,7 @@ def generate_regression_criterion_inputs(make_input):\n             constructor_input=FunctionInput(reduction=reduction),\n             forward_input=FunctionInput(make_input((4, )), make_input(4,)),\n             reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True),\n-            desc='no_batch_dim_{}'.format(reduction)\n+            desc=f'no_batch_dim_{reduction}'\n         ) for reduction in ['none', 'mean', 'sum']]\n \n \n@@ -752,7 +752,7 @@ def module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, tra\n     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n     conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n     kernel_size, C_in, C_out = 3, 4, 5\n-    input_no_batch_shape = (C_in,) + tuple((i + 3 for i in range(N)))\n+    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n     input_batch_shape = (2,) + input_no_batch_shape\n     return [\n         ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n@@ -878,7 +878,7 @@ def module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad,\n         ModuleInput(constructor_input=FunctionInput(),\n                     forward_input=FunctionInput(make_input((3, 2, 5)))),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(0.5),\n@@ -897,10 +897,10 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n \n     return [\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(()))),\n+                    forward_input=FunctionInput(make_input(())),\n                     desc='scalar'),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -908,7 +908,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -916,7 +916,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -924,7 +924,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5, 6)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d_multiparam')]\n \n@@ -1216,11 +1216,11 @@ def module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3, 6, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 6, 5)))),\n+            forward_input=FunctionInput(make_input((4, 6, 5))),\n             desc='1d_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput(3, 12, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 12)))),\n+            forward_input=FunctionInput(make_input((4, 12))),\n             desc='1d_affine_GN'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 6, 1e-3),\n@@ -1334,13 +1334,13 @@ def module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_g\n             constructor_input=(\n                 FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape)))),\n+            forward_input=FunctionInput(make_input(input_batch_shape))),\n         ModuleInput(\n             constructor_input=(\n                 FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                 FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape))),\n+            forward_input=FunctionInput(make_input(input_batch_shape)),\n             desc='tracking_stats'),\n         ModuleInput(\n             constructor_input=(\n@@ -1365,15 +1365,15 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((128, 5, 5)))),\n+            forward_input=FunctionInput(make_input((128, 5, 5))),\n             desc='1d_elementwise_affine_large_batch'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3, False),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_no_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([2, 2, 5], 1e-3),\n@@ -1396,11 +1396,11 @@ def module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, require\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7))),\n             desc='1d'),\n         ModuleInput(\n             constructor_input=FunctionInput(2,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n             desc='2d_uneven_pad'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 1., 0.5, 2.),\n@@ -1415,7 +1415,7 @@ def module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, t\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(1.5, 2),\n-            forward_input=FunctionInput(make_input(((1, 3, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 7))),\n             desc='norm'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, 2, 3),\n@@ -1449,7 +1449,7 @@ def module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(4),\n-            forward_input=FunctionInput(make_input(((2, 10, 4)))),\n+            forward_input=FunctionInput(make_input((2, 10, 4))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput(4, 4),\n@@ -1468,7 +1468,7 @@ def module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n-            forward_input=FunctionInput(make_input(((3, 7, 7)))),\n+            forward_input=FunctionInput(make_input((3, 7, 7))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n@@ -1486,7 +1486,7 @@ def module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2)),\n-            forward_input=FunctionInput(make_input(((2, 3, 5, 5, 5))))),\n+            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n         ModuleInput(\n             constructor_input=FunctionInput(2, (2, 2, 2)),\n             forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n@@ -1511,7 +1511,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n@@ -1521,11 +1521,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((3, 5, 7))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\n@@ -1545,7 +1545,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n@@ -1559,11 +1559,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\ndiff --git a/torch/testing/_internal/common_nn.py b/torch/testing/_internal/common_nn.py\nindex 6bc41ab20f3fcd..85f9e35ac0a6bb 100644\n--- a/torch/testing/_internal/common_nn.py\n+++ b/torch/testing/_internal/common_nn.py\n@@ -2540,7 +2540,7 @@ def unsqueeze_inp(inp):\n         output_size = (2, 3) + tuple(p + 1 for p in padding)  # simplified from `(4 + 2 * p - 3) // 2 + 1`\n         new_module_tests.append(\n             dict(\n-                module_name='Conv{}d'.format(d),\n+                module_name=f'Conv{d}d',\n                 constructor_args=(2, 3, 3, 2, padding, 1, 1, True, padding_mode),\n                 cpp_constructor_args='''torch::nn::Conv{}dOptions(2, 3, 3)\n                                         .stride(2)\n@@ -2552,7 +2552,7 @@ def unsqueeze_inp(inp):\n                 input_size=input_size,\n                 output_size=output_size,\n                 cudnn=True,\n-                desc='{}_stride2_pad2'.format(padding_mode),\n+                desc=f'{padding_mode}_stride2_pad2',\n                 with_tf32=True,\n                 tf32_precision=0.05\n             ),\n@@ -3906,7 +3906,7 @@ def flatten(xs):\n reductions = ['none', 'mean', 'sum']\n for name, reduction in product(regression_criterion_no_batch, reductions):\n     regression_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_size=(3, ),\n         target_size=(3, ),\n@@ -3959,7 +3959,7 @@ def flatten(xs):\n for (name, input_fn, target_fn), reduction in product(classification_criterion_no_batch,\n                                                       reductions):\n     classification_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_fn=lambda f=input_fn: f(),\n         target_fn=lambda f=target_fn: f(),\n@@ -4152,7 +4152,7 @@ def _get_arg(self, name, unpack):\n                 self._arg_cache[name] = self._extra_kwargs[fn_name]()\n             else:\n                 assert size_name in self._extra_kwargs, \\\n-                    \"Missing `{}`, `{}` or `{}` for {}\".format(name, size_name, fn_name, self.get_name())\n+                    f\"Missing `{name}`, `{size_name}` or `{fn_name}` for {self.get_name()}\"\n \n                 def map_tensor_sizes(sizes):\n                     if isinstance(sizes, list):\n@@ -4281,7 +4281,7 @@ def test_cuda(self, test_case):\n         type_map = {torch.double: torch.float}\n         cpu_input_tuple = cpu_input if isinstance(cpu_input, tuple) else (cpu_input,)\n \n-        is_any_input_complex = any((isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple))\n+        is_any_input_complex = any(isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple)\n \n         gpu_input_tuple = to_gpu(cpu_input_tuple, type_map=type_map)\n \ndiff --git a/torch/testing/_internal/common_pruning.py b/torch/testing/_internal/common_pruning.py\nindex 32732818a25b02..b6cbd92105f3f4 100644\n--- a/torch/testing/_internal/common_pruning.py\n+++ b/torch/testing/_internal/common_pruning.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.ao.pruning import BaseSparsifier\ndiff --git a/torch/testing/_internal/common_quantization.py b/torch/testing/_internal/common_quantization.py\nindex 95686be4511264..dcc575c942bc84 100644\n--- a/torch/testing/_internal/common_quantization.py\n+++ b/torch/testing/_internal/common_quantization.py\n@@ -791,8 +791,7 @@ def _get_underlying_op_type(\n                     (exp_type_end_b is act_type_end_b)\n                 self.assertTrue(\n                     types_match,\n-                    'Type mismatch at %s: expected %s, got %s' %\n-                    (k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n+                    'Type mismatch at {}: expected {}, got {}'.format(k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n                         (act_type_start_a, act_type_end_a, act_type_start_b, act_type_end_b))\n                 )\n \n@@ -1601,7 +1600,7 @@ def __init__(self):\n         super().__init__()\n         self.quant = torch.ao.quantization.QuantStub()\n         self.fc1 = torch.nn.Linear(5, 8).to(dtype=torch.float)\n-        self.layer_norm = torch.nn.LayerNorm((8))\n+        self.layer_norm = torch.nn.LayerNorm(8)\n         self.group_norm = torch.nn.GroupNorm(2, 8)\n         self.instance_norm1d = torch.nn.InstanceNorm1d(8)\n         self.instance_norm2d = torch.nn.InstanceNorm2d(8)\ndiff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py\nindex 3ba2331e7cd40e..c96d13bd46ac53 100644\n--- a/torch/testing/_internal/common_utils.py\n+++ b/torch/testing/_internal/common_utils.py\n@@ -203,7 +203,7 @@ def repro_env_var_prefix() -> str:\n \n def maybe_load_json(filename):\n     if os.path.isfile(filename):\n-        with open(filename, 'r') as fp:\n+        with open(filename) as fp:\n             return json.load(fp)\n     log.warning(\"Attempted to load json file '%s' but it does not exist.\", filename)\n     return {}\n@@ -355,12 +355,12 @@ def instantiate_test_helper(cls, name, test, param_kwargs):\n             def instantiated_test(self, param_kwargs=param_kwargs):\n                 test(self, **param_kwargs)\n \n-            assert not hasattr(generic_cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(generic_cls, name), f\"Redefinition of test {name}\"\n             setattr(generic_cls, name, instantiated_test)\n \n         for (test, test_suffix, param_kwargs, decorator_fn) in class_attr.parametrize_fn(\n                 class_attr, generic_cls=generic_cls, device_cls=None):\n-            full_name = '{}_{}'.format(test.__name__, test_suffix)\n+            full_name = f'{test.__name__}_{test_suffix}'\n \n             # Apply decorators based on full param kwargs.\n             for decorator in decorator_fn(param_kwargs):\n@@ -830,7 +830,7 @@ def run_tests(argv=UNITTEST_ARGS):\n     # import test files.\n     if SLOW_TESTS_FILE:\n         if os.path.exists(SLOW_TESTS_FILE):\n-            with open(SLOW_TESTS_FILE, 'r') as fp:\n+            with open(SLOW_TESTS_FILE) as fp:\n                 global slow_tests_dict\n                 slow_tests_dict = json.load(fp)\n                 # use env vars so pytest-xdist subprocesses can still access them\n@@ -839,7 +839,7 @@ def run_tests(argv=UNITTEST_ARGS):\n             warnings.warn(f'slow test file provided but not found: {SLOW_TESTS_FILE}')\n     if DISABLED_TESTS_FILE:\n         if os.path.exists(DISABLED_TESTS_FILE):\n-            with open(DISABLED_TESTS_FILE, 'r') as fp:\n+            with open(DISABLED_TESTS_FILE) as fp:\n                 global disabled_tests_dict\n                 disabled_tests_dict = json.load(fp)\n                 os.environ['DISABLED_TESTS_FILE'] = DISABLED_TESTS_FILE\n@@ -905,7 +905,7 @@ def run_tests(argv=UNITTEST_ARGS):\n         test_batches = chunk_list(get_test_names(test_cases), RUN_PARALLEL)\n         processes = []\n         for i in range(RUN_PARALLEL):\n-            command = [sys.executable] + argv + ['--log-suffix=-shard-{}'.format(i + 1)] + test_batches[i]\n+            command = [sys.executable] + argv + [f'--log-suffix=-shard-{i + 1}'] + test_batches[i]\n             processes.append(subprocess.Popen(command, universal_newlines=True))\n         failed = False\n         for p in processes:\n@@ -1294,7 +1294,7 @@ def wrap_fn(self, *args, **kwargs):\n                 rocm_version = rocm_version.split(\"-\")[0]    # ignore git sha\n                 rocm_version_tuple = tuple(int(x) for x in rocm_version.split(\".\"))\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n         return wrap_fn\n@@ -1672,7 +1672,7 @@ def is_iterable_of_tensors(iterable, include_empty=False):\n     return True\n \n \n-class CudaNonDefaultStream():\n+class CudaNonDefaultStream:\n     def __enter__(self):\n         # Before starting CUDA test save currently active streams on all\n         # CUDA devices and set new non default streams to all CUDA devices\n@@ -1698,7 +1698,7 @@ def __exit__(self, exec_type, exec_value, traceback):\n                                      device_type=self.beforeStreams[d].device_type)\n         torch._C._cuda_setDevice(beforeDevice)\n \n-class CudaMemoryLeakCheck():\n+class CudaMemoryLeakCheck:\n     def __init__(self, testcase, name=None):\n         self.name = testcase.id() if name is None else name\n         self.testcase = testcase\n@@ -2104,7 +2104,7 @@ def __init__(self, actual, expected, *, rtol_override=0.0, atol_override=0.0, **\n     def _process_inputs(self, actual, expected, *, id, allow_subclasses):\n         self._check_inputs_isinstance(actual, expected, cls=(torch.Tensor, np.ndarray))\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\n@@ -2201,7 +2201,7 @@ def set_warn_always_context(new_val: bool):\n         torch.set_warn_always(old_val)\n \n \n-class NoTest():\n+class NoTest:\n     # causes pytest to not recognize this class as a test\n     __test__ = False\n \n@@ -3408,12 +3408,12 @@ def remove_prefix(text, prefix):\n         subname_output = \"\"\n         if subname:\n             expected_file += \"-\" + subname\n-            subname_output = \" ({})\".format(subname)\n+            subname_output = f\" ({subname})\"\n         expected_file += \".expect\"\n         expected = None\n \n         def accept_output(update_type):\n-            print(\"Accepting {} for {}{}:\\n\\n{}\".format(update_type, munged_id, subname_output, s))\n+            print(f\"Accepting {update_type} for {munged_id}{subname_output}:\\n\\n{s}\")\n             with open(expected_file, 'w') as f:\n                 # Adjust for producer_version, leave s unmodified\n                 s_tag = re.sub(r'(producer_version): \"[0-9.]*\"',\n@@ -3423,7 +3423,7 @@ def accept_output(update_type):\n         try:\n             with open(expected_file) as f:\n                 expected = f.read()\n-        except IOError as e:\n+        except OSError as e:\n             if e.errno != errno.ENOENT:\n                 raise\n             elif expecttest.ACCEPT:\n@@ -3442,7 +3442,7 @@ def accept_output(update_type):\n         # Adjust for producer_version\n         expected = expected.replace(\n             'producer_version: \"CURRENT_VERSION\"',\n-            'producer_version: \"{}\"'.format(torch.onnx.producer_version)\n+            f'producer_version: \"{torch.onnx.producer_version}\"'\n         )\n         if expecttest.ACCEPT:\n             if expected != s:\n@@ -3600,7 +3600,7 @@ def download_file(url, binary=True):\n             f.write(data)\n         return path\n     except error.URLError as e:\n-        msg = \"could not download test file '{}'\".format(url)\n+        msg = f\"could not download test file '{url}'\"\n         warnings.warn(msg, RuntimeWarning)\n         raise unittest.SkipTest(msg) from e\n \n@@ -4359,7 +4359,7 @@ def wrap_fn(*args, **kwargs):\n     return wrap_fn\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def get_cycles_per_ms() -> float:\n     \"\"\"Measure and return approximate number of cycles per millisecond for torch.cuda._sleep\n     \"\"\"\ndiff --git a/torch/testing/_internal/dist_utils.py b/torch/testing/_internal/dist_utils.py\nindex 94aafe31773261..daee88e4580cd5 100644\n--- a/torch/testing/_internal/dist_utils.py\n+++ b/torch/testing/_internal/dist_utils.py\n@@ -101,7 +101,7 @@ def wait_until_node_failure(rank: int, expected_error_regex: str = \".*\") -> str:\n     \"\"\"\n     while True:\n         try:\n-            rpc.rpc_sync(\"worker{}\".format(rank), noop, args=())\n+            rpc.rpc_sync(f\"worker{rank}\", noop, args=())\n             time.sleep(0.1)\n         except Exception as e:\n             if re.search(pattern=expected_error_regex, string=str(e)):\n@@ -187,7 +187,7 @@ def initialize_pg(init_method, rank: int, world_size: int) -> None:\n \n \n def worker_name(rank: int) -> str:\n-    return \"worker{}\".format(rank)\n+    return f\"worker{rank}\"\n \n \n def get_function_event(function_events, partial_event_name):\ndiff --git a/torch/testing/_internal/distributed/distributed_test.py b/torch/testing/_internal/distributed/distributed_test.py\nindex 19a0c3f50d1ae4..5fdc796310d44d 100644\n--- a/torch/testing/_internal/distributed/distributed_test.py\n+++ b/torch/testing/_internal/distributed/distributed_test.py\n@@ -517,7 +517,7 @@ def sync(cls, wait_for=None, timeout=10):\n             arrived = 0\n             with _lock():\n                 for f_name in os.listdir(barrier_dir):\n-                    with open(os.path.join(barrier_dir, f_name), \"r\") as f:\n+                    with open(os.path.join(barrier_dir, f_name)) as f:\n                         data = f.read()\n                         if int(data) >= cls.barrier_id:\n                             arrived += 1\n@@ -552,7 +552,7 @@ def tearDown(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     @classmethod\n     def _run(cls, rank, test_name, file_name, pipe):\n@@ -654,7 +654,7 @@ def test_dump_DDP_relevant_env_vars(self):\n                 lines = out.getvalue().splitlines()\n \n             def format_line(var):\n-                return \"env:%s=%s\" % (\n+                return \"env:{}={}\".format(\n                     var,\n                     os.environ[var] if var in os.environ else \"N/A\",\n                 )\n@@ -692,7 +692,7 @@ def test_get_rank(self):\n \n             all_ranks = set()\n             for f_name in os.listdir(test_dir):\n-                with open(os.path.join(test_dir, f_name), \"r\") as f:\n+                with open(os.path.join(test_dir, f_name)) as f:\n                     all_ranks.add(int(f.read()))\n             self.assertEqual(len(all_ranks), num_processes)\n \n@@ -9640,7 +9640,7 @@ def backward(ctx, grad_output):\n \n             class MyModel(nn.Module):\n                 def __init__(self, device):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.error = True\n                     self.fc1 = nn.Linear(10, 10).cuda(device)\n \n@@ -9683,12 +9683,12 @@ def forward(self, inp):\n         def test_ddp_has_finalized(self):\n \n             @dataclass\n-            class MyClass():\n+            class MyClass:\n                 obj: torch.Tensor\n \n             class MyModel(nn.Module):\n                 def __init__(self, rank):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.rank = rank\n                     self.fc1 = nn.Linear(1024, 1024).cuda(rank)\n                     self.fc2 = nn.Linear(1024, 2 * 1024).cuda(rank)\ndiff --git a/torch/testing/_internal/distributed/nn/api/remote_module_test.py b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\nindex f955c0fc1ace1a..4d9f1d9b53ddc4 100644\n--- a/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n+++ b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n@@ -131,7 +131,7 @@ def test_bad_module(self):\n         if self.rank != 0:\n             return\n         dst_worker_name = dist_utils.worker_name((self.rank + 1) % self.world_size)\n-        remote_device = \"{}/cpu\".format(dst_worker_name)\n+        remote_device = f\"{dst_worker_name}/cpu\"\n         args = (1,)\n         kwargs = dict(first_kwarg=2)\n \n@@ -575,7 +575,7 @@ def test_valid_device(self):\n         dst_worker_name = dist_utils.worker_name(dst_rank)\n \n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -585,7 +585,7 @@ def test_valid_device(self):\n \n         # Test rank works as well.\n         for remote_module in self._create_remote_module_iter(\n-            \"rank:{}/cuda:0\".format(dst_rank), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"rank:{dst_rank}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -607,7 +607,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/foo\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/foo\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -618,7 +618,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:100\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:100\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -627,7 +627,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cpu2\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cpu2\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -636,7 +636,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -648,7 +648,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:0/cuda:1\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:0/cuda:1\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -692,7 +692,7 @@ def test_input_moved_to_cuda_device(self):\n \n         # Only test Python nn.Module, because script module methods don't support taking kwargs.\n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             ret_fut = remote_module.forward_async(*args, **kwargs)\n             ret = ret_fut.wait()\n@@ -716,7 +716,7 @@ def test_input_moved_to_cuda_device_script(self):\n \n         scripted_remote_module = next(\n             self._create_remote_module_iter(\n-                \"{}/cuda:0\".format(dst_worker_name),\n+                f\"{dst_worker_name}/cuda:0\",\n                 modes=[ModuleCreationMode.MODULE_CTOR_WITH_INTERFACE],\n             )\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\nindex 8e8c353460e6ff..b08b51c31d9f7d 100644\n--- a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n+++ b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n@@ -226,7 +226,7 @@ def _exec_func_with_dst(self, dst, exec_mode, method, *args):\n             fut = rpc.rpc_async(worker_name(dst), method, args=(args))\n             return fut.wait()\n         else:\n-            raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+            raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n     def _exec_func(self, exec_mode, method, *args):\n         return self._exec_func_with_dst(\n@@ -288,7 +288,7 @@ def _test_graph(self, fn, exec_mode, sparse):\n                     worker_name(dst_rank), fn, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -355,7 +355,7 @@ def _test_graph_for_py_nested_call(self, exec_mode, sparse):\n                     args=(t1, t2, dst_rank, self.world_size, 1),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             # Barrier to ensure all RPCs are done.\n             dist.barrier()\n@@ -449,7 +449,7 @@ def _test_graph_for_py_nested_call_itself(self, exec_mode, sparse):\n                     ),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name((self.rank + 1) % self.world_size),\n@@ -505,7 +505,7 @@ def _test_no_graph_with_tensors_not_require_grad(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.add, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -548,7 +548,7 @@ def _test_rpc_complex_args(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.stack, args=(tensors,)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             self.assertEqual(torch.stack(tensors), ret)\n \n@@ -1292,7 +1292,7 @@ def test_autograd_context(self):\n         for context_id in context_ids:\n             with self.assertRaisesRegex(\n                 RuntimeError,\n-                \"Could not find autograd context with id: {}\".format(context_id),\n+                f\"Could not find autograd context with id: {context_id}\",\n             ):\n                 dist_autograd._retrieve_context(context_id)\n \n@@ -1357,7 +1357,7 @@ def _test_grad_only_on_return_value(self, exec_mode):\n                     worker_name(dst_rank), ret_requires_grad\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             dist_autograd.backward(context_id, [ret.sum()])\n \n@@ -1748,7 +1748,7 @@ def test_backward_without_context(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             res = rpc.rpc_sync(\n                 worker_name(self._next_rank()), torch.add, args=(t1, t2)\n@@ -2031,7 +2031,7 @@ def test_clean_context_during_backward(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             dist_autograd.backward(context_id, [t1.sum()])\n \n@@ -2234,7 +2234,7 @@ def test_multiple_backward_with_errors(self):\n         t2 = torch.rand((3, 3), requires_grad=True)\n         with dist_autograd.context() as context_id:\n             loss = rpc.rpc_sync(\n-                'worker{}'.format(self._next_rank()),\n+                f'worker{self._next_rank()}',\n                 DistAutogradTest._python_udf_with_backward_error,\n                 args=(t1, t2)).sum()\n \ndiff --git a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\nindex 9f8b71911a07cc..98db73d7401845 100644\n--- a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n@@ -225,7 +225,7 @@ def run_agent(agent, n_steps):\n         last_reward = agent.finish_episode()\n \n         if agent.running_reward > agent.reward_threshold:\n-            print(\"Solved! Running reward is now {}!\".format(agent.running_reward))\n+            print(f\"Solved! Running reward is now {agent.running_reward}!\")\n             break\n \n \ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\nindex d050a2138b7922..b7683064dcfd13 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n@@ -70,7 +70,7 @@ def _test_remote_message_dropped_pickle(self, dst=None):\n         if self.rank != 0:\n             return\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -100,7 +100,7 @@ def _test_remote_message_dropped_timeout(self, func, args, dst=None):\n \n         # test the case where rpc.remote() message creation is completely dropped.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -143,7 +143,7 @@ def _test_remote_message_delay_timeout(self, func, args, dst=None):\n         # Test the case where remote message is eventually processed on the owner,\n         # but the future on the creator times out before the response comes back.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, func, args=args, timeout=0.001)\n         # Future corresponding to the remote creation should time out.\n@@ -233,7 +233,7 @@ def test_rref_to_here_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\nindex b08e897ec464af..af73fef4794b06 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n@@ -54,7 +54,7 @@ def get_shutdown_error_regex(self):\n             \"Connection reset by peer\",\n             \"Connection closed by peer\"\n         ]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\nindex fce0b5e8802567..0bb45ddeadb186 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n@@ -310,7 +310,7 @@ def future_return_to_python(\n             dst_rank: int, inputs: Tuple[Tensor, Tensor]\n         ) -> Future[Tensor]:\n             return rpc.rpc_async(\n-                \"worker{}\".format(dst_rank), two_args_two_kwargs, inputs\n+                f\"worker{dst_rank}\", two_args_two_kwargs, inputs\n             )\n \n         fut_res = future_return_to_python(dst_rank, inputs)\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\nindex 96ede7231a9722..2e4eea3a365176 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n@@ -157,7 +157,7 @@ def test_remote_timeout_to_here_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -173,7 +173,7 @@ def test_rref_to_here_timeout_in_jit(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -188,7 +188,7 @@ def test_rref_timeout_pickle_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -205,7 +205,7 @@ def test_rref_timeout_pickle_script_func(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/rpc_test.py b/torch/testing/_internal/distributed/rpc/rpc_test.py\nindex 2d350d06cc6794..47b13a837a0355 100644\n--- a/torch/testing/_internal/distributed/rpc/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/rpc_test.py\n@@ -3153,7 +3153,7 @@ def test_rref_str(self):\n         rref1 = RRef(self.rank)\n         id_class = \"GloballyUniqueId\"\n         self.assertEqual(\n-            \"OwnerRRef({}(created_on={}, local_id=0))\".format(id_class, self.rank), rref1.__str__()\n+            f\"OwnerRRef({id_class}(created_on={self.rank}, local_id=0))\", rref1.__str__()\n         )\n \n         dst_rank = (self.rank + 1) % self.world_size\n@@ -4296,7 +4296,7 @@ def test_rref_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, my_sleep_func, args=(2, ), timeout=0.01)\n         # Future corresponding to the remote creation should time out.\ndiff --git a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\nindex 0f5cb0a4987a8f..191017caad139e 100644\n--- a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n@@ -26,7 +26,7 @@ def get_shutdown_error_regex(self):\n         # FIXME Once we consolidate the error messages returned by the\n         # TensorPipe agent put some more specific regex here.\n         error_regexes = [\".*\"]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/jit_metaprogramming_utils.py b/torch/testing/_internal/jit_metaprogramming_utils.py\nindex 72b7e477c76a49..88137fd1029a18 100644\n--- a/torch/testing/_internal/jit_metaprogramming_utils.py\n+++ b/torch/testing/_internal/jit_metaprogramming_utils.py\n@@ -338,11 +338,11 @@ def get_call(method_name, func_type, args, kwargs):\n     argument_str += kwargs_str\n \n     if func_type == 'functional' or func_type == 'function':\n-        call = 'torch.{}({})'.format(method_name, argument_str)\n+        call = f'torch.{method_name}({argument_str})'\n     elif func_type == 'method':\n-        call = '{}.{}({})'.format(self_arg, method_name, argument_str)\n+        call = f'{self_arg}.{method_name}({argument_str})'\n     elif func_type == 'nn_functional':\n-        call = 'torch.nn.functional.{}({})'.format(method_name, argument_str)\n+        call = f'torch.nn.functional.{method_name}({argument_str})'\n     else:\n         raise TypeError('Unsupported function type')\n \n@@ -361,17 +361,17 @@ def get_script_args(args):\n     actuals: List[str] = []\n     for arg in args:\n         if isinstance(arg, torch.Tensor):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name)\n             actuals.append(name)\n             tensors.append(arg)\n         elif is_iterable_of_tensors(arg):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name + ': List[torch.Tensor]')\n             actuals.append(name)\n             tensors.append(list(arg))\n         elif isinstance(arg, str):\n-            actuals.append(\"'{}'\".format(arg))\n+            actuals.append(f\"'{arg}'\")\n         else:\n             actuals.append(str(get_constant(arg)))\n     return (formals, tensors, actuals)\n@@ -399,7 +399,7 @@ def script_fn(*args, **kwargs):\n         return output\n     return script_fn\n \n-class SplitInputs():\n+class SplitInputs:\n     all_tensors: List[Any]\n     tensor_args: List[Any]\n     nontensor_args: List[Any]\n@@ -584,7 +584,7 @@ def script_module(*args, **kwargs):\n \n         method_args = ', '.join(['self'] + actuals)\n         call_args_str = ', '.join(actuals)\n-        call = \"self.submodule({})\".format(call_args_str)\n+        call = f\"self.submodule({call_args_str})\"\n         script = script_method_template.format(method_args, call)\n \n         submodule_constants = []\n@@ -640,7 +640,7 @@ def get_nn_mod_test_name(**kwargs):\n         test_name = get_nn_module_name_from_kwargs(**kwargs)\n         if 'desc' in kwargs:\n             test_name = \"{}_{}\".format(test_name, kwargs['desc'])\n-    return 'test_nn_{}'.format(test_name)\n+    return f'test_nn_{test_name}'\n \n def get_nn_module_class_from_kwargs(**kwargs):\n     name = get_nn_module_name_from_kwargs(**kwargs)\ndiff --git a/torch/testing/_internal/jit_utils.py b/torch/testing/_internal/jit_utils.py\nindex b72dd5dc1285a6..2f6675234d3e7c 100644\n--- a/torch/testing/_internal/jit_utils.py\n+++ b/torch/testing/_internal/jit_utils.py\n@@ -176,12 +176,12 @@ def get_nodes_and_parents_recursively(block, kind, acc):\n \n         fusion_groups : Dict[torch._C.Block, List[torch._C.Node]] = defaultdict(list)\n         get_nodes_and_parents_recursively(graph, FUSION_GROUP, fusion_groups)\n-        self.assertTrue(len(fusion_groups) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_groups) == 1, f'got {graph}')\n         (graph, fusion_nodes) = list(fusion_groups.items())[0]\n         # the block contains one FUSION_GROUP and the rest of nodes are `allowed_nodes`\n-        self.assertTrue(len(fusion_nodes) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_nodes) == 1, f'got {graph}')\n         self.assertTrue(all(node.kind() in allowed_nodes for node in graph.nodes()),\n-                        'got {}'.format(graph))\n+                        f'got {graph}')\n \n     def _isHookExceptionOk(self, e):\n         se = str(e)\n@@ -294,7 +294,7 @@ def assertGraphContains(self, graph, kind, consider_subgraphs=False):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             self.assertTrue(count > 0)\n             return\n \n@@ -321,7 +321,7 @@ def perform_assert(graph, kind, actual, expected, consider_subgraphs):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             perform_assert(graph, kind, count, num_kind_nodes,\n                            consider_subgraphs)\n             return\n@@ -768,7 +768,7 @@ def _get_py3_code(code, fn_name):\n         fn = getattr(module, fn_name)\n         return fn\n \n-class TensorExprTestOptions():\n+class TensorExprTestOptions:\n     def __init__(self):\n         self.old_profiling_executor = torch._C._jit_set_profiling_executor(True)\n         self.old_profiling_mode = torch._C._get_graph_executor_optimize(True)\ndiff --git a/torch/testing/_internal/opinfo/core.py b/torch/testing/_internal/opinfo/core.py\nindex 8f86cbd06af61a..5e7e1dcd5a49ea 100644\n--- a/torch/testing/_internal/opinfo/core.py\n+++ b/torch/testing/_internal/opinfo/core.py\n@@ -859,9 +859,7 @@ class OpInfo:\n     def __post_init__(self):\n         self._original_opinfo_args = asdict(self).copy()\n \n-        assert self.dtypes is not None, \"OpInfo for {0} has no dtypes!\".format(\n-            self.name\n-        )\n+        assert self.dtypes is not None, \"OpInfo for {} has no dtypes!\".format(self.name)\n \n         dtypes_args = (self.dtypes, self.dtypesIfCUDA, self.dtypesIfROCM)\n \n@@ -874,10 +872,7 @@ def __post_init__(self):\n \n         # Attribute to verify dynamic_dtypes are used.\n         self.dynamic_dtypes = any(\n-            (\n-                isinstance(dtypes, utils._dynamic_dispatch_dtypes)\n-                for dtypes in dtypes_args\n-            )\n+            isinstance(dtypes, utils._dynamic_dispatch_dtypes) for dtypes in dtypes_args\n         )\n \n         if self.dynamic_dtypes:\ndiff --git a/torch/testing/_internal/opinfo/definitions/linalg.py b/torch/testing/_internal/opinfo/definitions/linalg.py\nindex ca84eca5d3d027..a8c29dbf09309d 100644\n--- a/torch/testing/_internal/opinfo/definitions/linalg.py\n+++ b/torch/testing/_internal/opinfo/definitions/linalg.py\n@@ -1007,7 +1007,7 @@ def sample_inputs_linalg_solve(\n         nrhs = [(1,), (3,)]\n \n     for n, batch, rhs in product(ns, batches, nrhs):\n-        yield SampleInput(make_a(*batch, n, n), args=(make_b((batch + (n,) + rhs)),))\n+        yield SampleInput(make_a(*batch, n, n), args=(make_b(batch + (n,) + rhs),))\n \n \n def sample_inputs_linalg_solve_triangular(\ndiff --git a/torch/testing/_internal/opinfo/definitions/sparse.py b/torch/testing/_internal/opinfo/definitions/sparse.py\nindex 6baff3b2f86fea..570b2c546f099a 100644\n--- a/torch/testing/_internal/opinfo/definitions/sparse.py\n+++ b/torch/testing/_internal/opinfo/definitions/sparse.py\n@@ -331,7 +331,7 @@ def _validate_sample_input_sparse_reduction_sum(sample, check_validate=False):\n     }:\n         if (isinstance(dim, int) and (t_inp.dim() != 2 or keepdim)) or (\n             isinstance(dim, (list, tuple))\n-            and (((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim))\n+            and ((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim)\n         ):\n             if layout in {torch.sparse_bsr, torch.sparse_bsc}:\n                 return ErrorInput(\n"
  },
  {
    "number": 105424,
    "title": "[BE] Enable ruff's UP rules and autoformat utils/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105435\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* __->__ #105424\n* #105423\n\n",
    "merge_commit_sha": "8cbc9dbbdd1104bd7dd03aa80e93af88fd4f4537",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105424",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105424/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105424.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105424.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105424/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105424/comments",
    "labels": [
      "Merged",
      "open source",
      "ciflow/trunk",
      "release notes: dataloader",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:20:40.033718Z",
    "state": "closed",
    "patch": "From 63ac12c0d479739fa3b1c09c257c5430b2f2e384 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:20:32 +0000\nSubject: [PATCH 1/3] [BE] Enable ruff's UP rules and autoformat utils/\n\n[ghstack-poisoned]\n---\n torch/utils/_freeze.py                        |  2 +-\n torch/utils/benchmark/examples/end_to_end.py  |  5 ++--\n .../utils/benchmark/examples/op_benchmark.py  |  4 +--\n .../benchmark/examples/sparse/op_benchmark.py |  4 +--\n .../examples/spectral_ops_fuzz_test.py        |  2 +-\n torch/utils/benchmark/utils/common.py         |  4 +--\n torch/utils/benchmark/utils/cpp_jit.py        |  6 ++--\n .../utils/valgrind_wrapper/timer_interface.py | 12 ++++----\n torch/utils/bottleneck/__main__.py            |  6 ++--\n torch/utils/bundled_inputs.py                 |  8 +++---\n torch/utils/checkpoint.py                     |  2 +-\n torch/utils/collect_env.py                    | 26 ++++++++---------\n torch/utils/cpp_extension.py                  |  8 +++---\n torch/utils/data/_utils/pin_memory.py         |  2 +-\n torch/utils/data/_utils/worker.py             |  8 +++---\n torch/utils/data/dataloader.py                |  6 ++--\n torch/utils/data/datapipes/_decorator.py      |  2 +-\n torch/utils/data/datapipes/_typing.py         |  8 +++---\n .../data/datapipes/dataframe/dataframes.py    | 18 ++++++------\n torch/utils/data/datapipes/datapipe.py        | 10 +++----\n torch/utils/data/datapipes/gen_pyi.py         |  2 +-\n torch/utils/data/datapipes/iter/callable.py   |  2 +-\n .../data/datapipes/iter/combinatorics.py      |  4 +--\n torch/utils/data/datapipes/iter/combining.py  |  6 ++--\n torch/utils/data/datapipes/iter/filelister.py |  2 +-\n torch/utils/data/datapipes/iter/fileopener.py |  4 +--\n torch/utils/data/datapipes/iter/grouping.py   |  2 +-\n .../data/datapipes/iter/routeddecoder.py      |  2 +-\n torch/utils/data/datapipes/iter/sharding.py   |  2 +-\n torch/utils/data/datapipes/map/combining.py   |  2 +-\n torch/utils/data/datapipes/map/grouping.py    |  2 +-\n torch/utils/data/datapipes/utils/common.py    |  2 +-\n torch/utils/data/datapipes/utils/decoder.py   |  6 ++--\n torch/utils/data/graph.py                     |  2 +-\n torch/utils/dlpack.py                         |  2 +-\n torch/utils/hipify/cuda_to_hip_mappings.py    |  2 +-\n torch/utils/hipify/hipify_python.py           | 28 +++++++++----------\n torch/utils/jit/log_extract.py                |  2 +-\n torch/utils/mobile_optimizer.py               |  4 +--\n torch/utils/tensorboard/_caffe2_graph.py      |  4 +--\n torch/utils/tensorboard/_embedding.py         |  2 +-\n torch/utils/tensorboard/_pytorch_graph.py     |  6 ++--\n torch/utils/tensorboard/writer.py             |  2 +-\n torch/utils/throughput_benchmark.py           | 10 +++----\n torch/utils/viz/_cycles.py                    | 12 ++++----\n torch/utils/weak.py                           |  5 ++--\n 46 files changed, 131 insertions(+), 131 deletions(-)\n\ndiff --git a/torch/utils/_freeze.py b/torch/utils/_freeze.py\nindex 5245ac011e19ac..6590ff4b769e42 100644\n--- a/torch/utils/_freeze.py\n+++ b/torch/utils/_freeze.py\n@@ -237,7 +237,7 @@ def compile_file(self, path: Path, top_package_path: Path):\n         module_mangled_name = \"__\".join(module_qualname)\n         c_name = \"M_\" + module_mangled_name\n \n-        with open(path, \"r\") as src_file:\n+        with open(path) as src_file:\n             co = self.compile_string(src_file.read())\n \n         bytecode = marshal.dumps(co)\ndiff --git a/torch/utils/benchmark/examples/end_to_end.py b/torch/utils/benchmark/examples/end_to_end.py\nindex 5e0f42712d7c7a..a6d05a91c94253 100644\n--- a/torch/utils/benchmark/examples/end_to_end.py\n+++ b/torch/utils/benchmark/examples/end_to_end.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"End-to-end example to test a PR for regressions:\n \n $ python -m examples.end_to_end --pr 39850\n@@ -111,7 +110,7 @@ def parse_args():\n \n def construct_stmt_and_label(pr, params):\n     if pr == \"39850\":\n-        k0, k1, k2, dim = [params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"]]\n+        k0, k1, k2, dim = (params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"])\n         state = np.random.RandomState(params[\"random_value\"])\n         topk_dim = state.randint(low=0, high=dim)\n         dim_size = [k0, k1, k2][topk_dim]\n@@ -291,7 +290,7 @@ def construct_table(results, device_str, test_variance):\n     )\n \n     _, result_log_file = tempfile.mkstemp(suffix=\".log\")\n-    with open(result_log_file, \"wt\") as f:\n+    with open(result_log_file, \"w\") as f:\n         f.write(f\"{device_str}\\n\\n{column_labels}\\n\")\n         print(f\"\\n{column_labels}\\n[First twenty omitted (these tend to be noisy) ]\")\n         for key, (r_ref, r_pr), rel_diff in results:\ndiff --git a/torch/utils/benchmark/examples/op_benchmark.py b/torch/utils/benchmark/examples/op_benchmark.py\nindex 65b69d84b41f44..b7536b9ec26bb8 100644\n--- a/torch/utils/benchmark/examples/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/op_benchmark.py\n@@ -37,13 +37,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/sparse/op_benchmark.py b/torch/utils/benchmark/examples/sparse/op_benchmark.py\nindex f9ee17d5617e08..d7e97d33cc1101 100644\n--- a/torch/utils/benchmark/examples/sparse/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/sparse/op_benchmark.py\n@@ -32,13 +32,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\nindex d8284ee4187c49..c70395573adb2c 100644\n--- a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n+++ b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n@@ -27,7 +27,7 @@ def run_benchmark(name: str, function: object, dtype: torch.dtype, seed: int, de\n     results = []\n     for tensors, tensor_params, params in spectral_fuzzer.take(samples):\n         shape = [params['k0'], params['k1'], params['k2']][:params['ndim']]\n-        str_shape = ' x '.join([\"{:<4}\".format(s) for s in shape])\n+        str_shape = ' x '.join([f\"{s:<4}\" for s in shape])\n         sub_label = f\"{str_shape} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n         for dim in _dim_options(params['ndim']):\n             for nthreads in (1, 4, 16) if not cuda else (1,):\ndiff --git a/torch/utils/benchmark/utils/common.py b/torch/utils/benchmark/utils/common.py\nindex a8bbef3bfbeb4f..c1636ddb78a2bf 100644\n--- a/torch/utils/benchmark/utils/common.py\n+++ b/torch/utils/benchmark/utils/common.py\n@@ -325,7 +325,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n                 if not os.path.exists(owner_file):\n                     continue\n \n-                with open(owner_file, \"rt\") as f:\n+                with open(owner_file) as f:\n                     owner_pid = int(f.read())\n \n                 if owner_pid == os.getpid():\n@@ -349,7 +349,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n     os.makedirs(path, exist_ok=False)\n \n     if use_dev_shm:\n-        with open(os.path.join(path, \"owner.pid\"), \"wt\") as f:\n+        with open(os.path.join(path, \"owner.pid\"), \"w\") as f:\n             f.write(str(os.getpid()))\n \n     return path\ndiff --git a/torch/utils/benchmark/utils/cpp_jit.py b/torch/utils/benchmark/utils/cpp_jit.py\nindex 65b8c70ee43e6c..a09f1a00aace6f 100644\n--- a/torch/utils/benchmark/utils/cpp_jit.py\n+++ b/torch/utils/benchmark/utils/cpp_jit.py\n@@ -137,7 +137,7 @@ def _compile_template(\n         os.makedirs(build_dir, exist_ok=True)\n \n         src_path = os.path.join(build_dir, \"timer_src.cpp\")\n-        with open(src_path, \"wt\") as f:\n+        with open(src_path, \"w\") as f:\n             f.write(src)\n \n     # `cpp_extension` has its own locking scheme, so we don't need our lock.\n@@ -154,7 +154,7 @@ def _compile_template(\n \n def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> TimeitModuleType:\n     template_path: str = os.path.join(SOURCE_ROOT, \"timeit_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     module = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)\n@@ -164,7 +164,7 @@ def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> Time\n \n def compile_callgrind_template(*, stmt: str, setup: str, global_setup: str) -> str:\n     template_path: str = os.path.join(SOURCE_ROOT, \"valgrind_wrapper\", \"timer_callgrind_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     target = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 71753bd59548ae..11ce6d90fc47f3 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -28,7 +28,9 @@\n     CompletedProcessType = subprocess.CompletedProcess\n \n \n-FunctionCount = NamedTuple(\"FunctionCount\", [(\"count\", int), (\"function\", str)])\n+class FunctionCount(NamedTuple):\n+    count: int\n+    function: str\n \n \n @dataclasses.dataclass(repr=False, eq=False, frozen=True)\n@@ -598,7 +600,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     stderr=subprocess.STDOUT,\n                     **kwargs,\n                 )\n-                with open(stdout_stderr_log, \"rt\") as f:\n+                with open(stdout_stderr_log) as f:\n                     return invocation, f.read()\n             finally:\n                 f_stdout_stderr.close()\n@@ -612,7 +614,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     )\n \n                 script_file = os.path.join(working_dir, \"timer_callgrind.py\")\n-                with open(script_file, \"wt\") as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(self._construct_script(\n                         task_spec,\n                         globals=GlobalsBridge(globals, data_dir),\n@@ -652,7 +654,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n             if valgrind_invocation.returncode:\n                 error_report = \"\"\n                 if os.path.exists(error_log):\n-                    with open(error_log, \"rt\") as f:\n+                    with open(error_log) as f:\n                         error_report = f.read()\n                 if not error_report:\n                     error_report = \"Unknown error.\\n\" + valgrind_invocation_output\n@@ -724,7 +726,7 @@ def read_results(i: int) -> Tuple[FunctionCounts, FunctionCounts, Optional[str]]\n                 fpath = f\"{callgrind_out}.{i + 1}\"  # Callgrind one-indexes files.\n                 callgrind_out_contents: Optional[str] = None\n                 if retain_out_file:\n-                    with open(fpath, \"rt\") as f:\n+                    with open(fpath) as f:\n                         callgrind_out_contents = f.read()\n \n                 return (\ndiff --git a/torch/utils/bottleneck/__main__.py b/torch/utils/bottleneck/__main__.py\nindex 86c1af04baa0e6..f7fd209e1438fa 100644\n--- a/torch/utils/bottleneck/__main__.py\n+++ b/torch/utils/bottleneck/__main__.py\n@@ -16,7 +16,7 @@ def redirect_argv(new_argv):\n \n def compiled_with_cuda(sysinfo):\n     if sysinfo.cuda_compiled_version:\n-        return 'compiled w/ CUDA {}'.format(sysinfo.cuda_compiled_version)\n+        return f'compiled w/ CUDA {sysinfo.cuda_compiled_version}'\n     return 'not compiled w/ CUDA'\n \n \n@@ -59,7 +59,7 @@ def run_env_analysis():\n         'debug_str': debug_str,\n         'pytorch_version': info.torch_version,\n         'cuda_compiled': compiled_with_cuda(info),\n-        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'py_version': f'{sys.version_info[0]}.{sys.version_info[1]}',\n         'cuda_runtime': cuda_avail,\n         'pip_version': pip_version,\n         'pip_list_output': pip_list_output,\n@@ -138,7 +138,7 @@ def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\n \n     result = {\n         'mode': mode,\n-        'description': 'top {} events sorted by {}'.format(topk, sortby),\n+        'description': f'top {topk} events sorted by {sortby}',\n         'output': torch.autograd.profiler_util._build_table(topk_events),\n         'cuda_warning': cuda_warning\n     }\ndiff --git a/torch/utils/bundled_inputs.py b/torch/utils/bundled_inputs.py\nindex 4ae39733ff2e4b..ad34e15e6bfa17 100644\n--- a/torch/utils/bundled_inputs.py\n+++ b/torch/utils/bundled_inputs.py\n@@ -261,11 +261,11 @@ def augment_many_model_functions_with_bundled_inputs(\n \n \n         if input_list is not None and not isinstance(input_list, Sequence):\n-            raise TypeError(\"Error inputs for function {0} is not a Sequence\".format(function_name))\n+            raise TypeError(f\"Error inputs for function {function_name} is not a Sequence\")\n \n         function_arg_types = [arg.type for arg in function.schema.arguments[1:]]  # type: ignore[attr-defined]\n         deflated_inputs_type: ListType = ListType(TupleType(function_arg_types))\n-        model._c._register_attribute(\"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs_type, [])\n+        model._c._register_attribute(f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs_type, [])\n \n         if hasattr(model, \"_generate_bundled_inputs_for_\" + function_name):\n             if input_list is not None:\n@@ -290,7 +290,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             for inp_idx, args in enumerate(input_list):\n                 if not isinstance(args, Tuple) and not isinstance(args, List):  # type: ignore[arg-type]\n                     raise TypeError(\n-                        \"Error bundled input for function {0} idx: {1} is not a Tuple or a List\".format(function_name, inp_idx)\n+                        f\"Error bundled input for function {function_name} idx: {inp_idx} is not a Tuple or a List\"\n                     )\n                 deflated_args = []\n                 parts.append(\"(\")\n@@ -314,7 +314,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             # Back-channel return this expr for debugging.\n             if _receive_inflate_expr is not None:\n                 _receive_inflate_expr.append(expr)\n-            setattr(model, \"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs)\n+            setattr(model, f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs)\n             definition = textwrap.dedent(\"\"\"\n                 def _generate_bundled_inputs_for_{name}(self):\n                     deflated = self._bundled_inputs_deflated_{name}\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..4da281d32ac3bd 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -66,7 +66,7 @@ def _get_device_module(device=\"cuda\"):\n     return device_module\n \n \n-class DefaultDeviceType(object):\n+class DefaultDeviceType:\n     r\"\"\"\n     A class that manages the default device type for checkpointing.\n     If no non-CPU tensors are present, the default device type will\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex de03564a2b75d1..2266d64c1944d5 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n+    out = run_and_read_all(run_lambda, f\"{conda} list\")\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n+        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return 'Probably one of the following:\\n{}'.format(result)\n+    return f'Probably one of the following:\\n{result}'\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = '\"{}\"'.format(candidate_smi)\n+                smi = f'\"{candidate_smi}\"'\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n+    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return 'macOS {} ({})'.format(version, machine())\n+        return f'macOS {version} ({machine()})'\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n-        return '{} ({})'.format(platform, machine())\n+        return f'{platform} ({machine()})'\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n+        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return '\\n{}\\n'.format(string)\n+            return f'\\n{string}\\n'\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               '[{}] '.format(envinfo.pip_version))\n+                                               f'[{envinfo.pip_version}] ')\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n+            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..e847f4e30915b9 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -150,10 +150,10 @@ def _join_rocm_home(*paths) -> str:\n     only once we need to get any ROCm-specific path.\n     '''\n     if ROCM_HOME is None:\n-        raise EnvironmentError('ROCM_HOME environment variable is not set. '\n+        raise OSError('ROCM_HOME environment variable is not set. '\n                                'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n-        raise EnvironmentError('Building PyTorch extensions using '\n+        raise OSError('Building PyTorch extensions using '\n                                'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n@@ -264,7 +264,7 @@ def _maybe_write(filename, new_content):\n     if it already had the right content (to avoid triggering recompile).\n     '''\n     if os.path.exists(filename):\n-        with open(filename, 'r') as f:\n+        with open(filename) as f:\n             content = f.read()\n \n         if content == new_content:\n@@ -2247,7 +2247,7 @@ def _join_cuda_home(*paths) -> str:\n     only once we need to get any CUDA-specific path.\n     '''\n     if CUDA_HOME is None:\n-        raise EnvironmentError('CUDA_HOME environment variable is not set. '\n+        raise OSError('CUDA_HOME environment variable is not set. '\n                                'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \ndiff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py\nindex 074b89b624b9d3..cdd53c2d9ea2b1 100644\n--- a/torch/utils/data/_utils/pin_memory.py\n+++ b/torch/utils/data/_utils/pin_memory.py\n@@ -37,7 +37,7 @@ def do_one_step():\n                 data = pin_memory(data, device)\n             except Exception:\n                 data = ExceptionWrapper(\n-                    where=\"in pin memory thread for device {}\".format(device_id))\n+                    where=f\"in pin memory thread for device {device_id}\")\n             r = (idx, data)\n         while not done_event.is_set():\n             try:\ndiff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py\nindex b4fc8e0748f0f1..0d43f63a6a2f20 100644\n--- a/torch/utils/data/_utils/worker.py\n+++ b/torch/utils/data/_utils/worker.py\n@@ -76,13 +76,13 @@ def __init__(self, **kwargs):\n \n     def __setattr__(self, key, val):\n         if self.__initialized:\n-            raise RuntimeError(\"Cannot assign attributes to {} objects\".format(self.__class__.__name__))\n+            raise RuntimeError(f\"Cannot assign attributes to {self.__class__.__name__} objects\")\n         return super().__setattr__(key, val)\n \n     def __repr__(self):\n         items = []\n         for k in self.__keys:\n-            items.append('{}={}'.format(k, getattr(self, k)))\n+            items.append(f'{k}={getattr(self, k)}')\n         return '{}({})'.format(self.__class__.__name__, ', '.join(items))\n \n \n@@ -252,7 +252,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n             fetcher = _DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)\n         except Exception:\n             init_exception = ExceptionWrapper(\n-                where=\"in DataLoader worker process {}\".format(worker_id))\n+                where=f\"in DataLoader worker process {worker_id}\")\n \n         # When using Iterable mode, some worker can exit earlier than others due\n         # to the IterableDataset behaving differently for different workers.\n@@ -318,7 +318,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n                         # `ExceptionWrapper` does the correct thing.\n                         # See NOTE [ Python Traceback Reference Cycle Problem ]\n                         data = ExceptionWrapper(\n-                            where=\"in DataLoader worker process {}\".format(worker_id))\n+                            where=f\"in DataLoader worker process {worker_id}\")\n             data_queue.put((idx, data))\n             del data, idx, index, r  # save memory\n     except KeyboardInterrupt:\ndiff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py\nindex ec86f778023ba6..1c33592f02f146 100644\n--- a/torch/utils/data/dataloader.py\n+++ b/torch/utils/data/dataloader.py\n@@ -604,7 +604,7 @@ def __init__(self, loader: DataLoader) -> None:\n         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n         self._persistent_workers = loader.persistent_workers\n         self._num_yielded = 0\n-        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n+        self._profile_name = f\"enumerate(DataLoader)#{self.__class__.__name__}.__next__\"\n \n     def __iter__(self) -> '_BaseDataLoaderIter':\n         return self\n@@ -1145,7 +1145,7 @@ def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n                     self._mark_worker_as_unavailable(worker_id)\n             if len(failed_workers) > 0:\n                 pids_str = ', '.join(str(w.pid) for w in failed_workers)\n-                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n+                raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n             if isinstance(e, queue.Empty):\n                 return (False, None)\n             import tempfile\n@@ -1281,7 +1281,7 @@ def _get_data(self):\n             if success:\n                 return data\n             else:\n-                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n+                raise RuntimeError(f'DataLoader timed out after {self._timeout} seconds')\n         elif self._pin_memory:\n             while self._pin_memory_thread.is_alive():\n                 success, data = self._try_get_data()\ndiff --git a/torch/utils/data/datapipes/_decorator.py b/torch/utils/data/datapipes/_decorator.py\nindex e4cc9e4e59365d..96b7e00e076f02 100644\n--- a/torch/utils/data/datapipes/_decorator.py\n+++ b/torch/utils/data/datapipes/_decorator.py\n@@ -80,7 +80,7 @@ def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:\n         elif isinstance(arg, Callable):  # type:ignore[arg-type]\n             self.deterministic_fn = arg  # type: ignore[assignment, misc]\n         else:\n-            raise TypeError(\"{} can not be decorated by non_deterministic\".format(arg))\n+            raise TypeError(f\"{arg} can not be decorated by non_deterministic\")\n \n     def __call__(self, *args, **kwargs):\n         global _determinism\ndiff --git a/torch/utils/data/datapipes/_typing.py b/torch/utils/data/datapipes/_typing.py\nindex 6377a2ec940860..68049ba30d9018 100644\n--- a/torch/utils/data/datapipes/_typing.py\n+++ b/torch/utils/data/datapipes/_typing.py\n@@ -234,7 +234,7 @@ def issubtype(self, other):\n             return issubtype(self.param, other.param)\n         if isinstance(other, type):\n             return issubtype(self.param, other)\n-        raise TypeError(\"Expected '_DataPipeType' or 'type', but found {}\".format(type(other)))\n+        raise TypeError(f\"Expected '_DataPipeType' or 'type', but found {type(other)}\")\n \n     def issubtype_of_instance(self, other):\n         return issubinstance(other, self.param)\n@@ -279,13 +279,13 @@ def __init__(self, name, bases, namespace, **kwargs):\n     @_tp_cache\n     def _getitem_(self, params):\n         if params is None:\n-            raise TypeError('{}[t]: t can not be None'.format(self.__name__))\n+            raise TypeError(f'{self.__name__}[t]: t can not be None')\n         if isinstance(params, str):\n             params = ForwardRef(params)\n         if not isinstance(params, tuple):\n             params = (params, )\n \n-        msg = \"{}[t]: t must be a type\".format(self.__name__)\n+        msg = f\"{self.__name__}[t]: t must be a type\"\n         params = tuple(_type_check(p, msg) for p in params)\n \n         if isinstance(self.type.param, _GenericAlias):\n@@ -303,7 +303,7 @@ def _getitem_(self, params):\n                                        '__type_class__': True})\n \n         if len(params) > 1:\n-            raise TypeError('Too many parameters for {} actual {}, expected 1'.format(self, len(params)))\n+            raise TypeError(f'Too many parameters for {self} actual {len(params)}, expected 1')\n \n         t = _DataPipeType(params[0])\n \ndiff --git a/torch/utils/data/datapipes/dataframe/dataframes.py b/torch/utils/data/datapipes/dataframe/dataframes.py\nindex 06029e07851685..72d93cde66c3cb 100644\n--- a/torch/utils/data/datapipes/dataframe/dataframes.py\n+++ b/torch/utils/data/datapipes/dataframe/dataframes.py\n@@ -36,7 +36,7 @@ def disable_capture():\n     CaptureControl.disabled = True\n \n \n-class CaptureControl():\n+class CaptureControl:\n     disabled = False\n \n \n@@ -184,7 +184,7 @@ def execute(self):\n         return value\n \n \n-class CaptureLikeMock():\n+class CaptureLikeMock:\n     def __init__(self, name):\n         import unittest.mock as mock\n         # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n@@ -232,7 +232,7 @@ class CaptureVariableAssign(CaptureF):\n     def __str__(self):\n         variable = self.kwargs['variable']\n         value = self.kwargs['value']\n-        return \"{variable} = {value}\".format(variable=variable, value=value)\n+        return f\"{variable} = {value}\"\n \n     def execute(self):\n         self.kwargs['variable'].calculated_value = self.kwargs['value'].execute()\n@@ -272,7 +272,7 @@ def __init__(self, left, key, ctx):\n         self.key = key\n \n     def __str__(self):\n-        return \"%s[%s]\" % (self.left, get_val(self.key))\n+        return f\"{self.left}[{get_val(self.key)}]\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -287,7 +287,7 @@ def __init__(self, left, key, value, ctx):\n         self.value = value\n \n     def __str__(self):\n-        return \"%s[%s] = %s\" % (self.left, get_val(self.key), self.value)\n+        return f\"{self.left}[{get_val(self.key)}] = {self.value}\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -302,7 +302,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s + %s\" % (self.left, self.right)\n+        return f\"{self.left} + {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) + get_val(self.right)\n@@ -315,7 +315,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s * %s\" % (self.left, self.right)\n+        return f\"{self.left} * {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) * get_val(self.right)\n@@ -328,7 +328,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s - %s\" % (self.left, self.right)\n+        return f\"{self.left} - {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) - get_val(self.right)\n@@ -341,7 +341,7 @@ def __init__(self, src, name, ctx):\n         self.name = name\n \n     def __str__(self):\n-        return \"%s.%s\" % (self.src, self.name)\n+        return f\"{self.src}.{self.name}\"\n \n     def execute(self):\n         val = get_val(self.src)\ndiff --git a/torch/utils/data/datapipes/datapipe.py b/torch/utils/data/datapipes/datapipe.py\nindex 445400ecb59c32..1017b52af0fbce 100644\n--- a/torch/utils/data/datapipes/datapipe.py\n+++ b/torch/utils/data/datapipes/datapipe.py\n@@ -126,7 +126,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -135,7 +135,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register, enable_df_api_tracing=False):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, enable_df_api_tracing, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -265,7 +265,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -274,7 +274,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -363,7 +363,7 @@ def __len__(self):\n             return len(self._datapipe)\n         except Exception as e:\n             raise TypeError(\n-                \"{} instance doesn't have valid length\".format(type(self).__name__)\n+                f\"{type(self).__name__} instance doesn't have valid length\"\n             ) from e\n \n \ndiff --git a/torch/utils/data/datapipes/gen_pyi.py b/torch/utils/data/datapipes/gen_pyi.py\nindex 1b77fbfecf0290..ed3e75bc5da12e 100644\n--- a/torch/utils/data/datapipes/gen_pyi.py\n+++ b/torch/utils/data/datapipes/gen_pyi.py\n@@ -19,7 +19,7 @@ def gen_from_template(dir: str, template_name: str, output_name: str, replacemen\n     template_path = os.path.join(dir, template_name)\n     output_path = os.path.join(dir, output_name)\n \n-    with open(template_path, \"r\") as f:\n+    with open(template_path) as f:\n         content = f.read()\n     for placeholder, lines, indentation in replacements:\n         with open(output_path, \"w\") as f:\ndiff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py\nindex 4e3dce4b82d1dd..9916b094e408d1 100644\n--- a/torch/utils/data/datapipes/iter/callable.py\n+++ b/torch/utils/data/datapipes/iter/callable.py\n@@ -126,7 +126,7 @@ def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n         raise TypeError(\n-            \"{} instance doesn't have valid length\".format(type(self).__name__)\n+            f\"{type(self).__name__} instance doesn't have valid length\"\n         )\n \n \ndiff --git a/torch/utils/data/datapipes/iter/combinatorics.py b/torch/utils/data/datapipes/iter/combinatorics.py\nindex 30b569e329b654..4d2973bbc5a2e9 100644\n--- a/torch/utils/data/datapipes/iter/combinatorics.py\n+++ b/torch/utils/data/datapipes/iter/combinatorics.py\n@@ -48,7 +48,7 @@ def __len__(self) -> int:\n         # Dataset has been tested as `Sized`\n         if isinstance(self.sampler, Sized):\n             return len(self.sampler)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('shuffle')\n@@ -137,7 +137,7 @@ def __iter__(self) -> Iterator[T_co]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self._buffer = []\ndiff --git a/torch/utils/data/datapipes/iter/combining.py b/torch/utils/data/datapipes/iter/combining.py\nindex 7c76e986b230d4..4fe05ea717cf16 100644\n--- a/torch/utils/data/datapipes/iter/combining.py\n+++ b/torch/utils/data/datapipes/iter/combining.py\n@@ -56,7 +56,7 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return sum(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('fork')\n@@ -567,7 +567,7 @@ def __len__(self):\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes) * len(self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self.buffer = []\n@@ -627,4 +627,4 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/filelister.py b/torch/utils/data/datapipes/iter/filelister.py\nindex b2ecd71b5ce9c9..22e2cd432d6a3a 100644\n--- a/torch/utils/data/datapipes/iter/filelister.py\n+++ b/torch/utils/data/datapipes/iter/filelister.py\n@@ -61,5 +61,5 @@ def __iter__(self) -> Iterator[str] :\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/fileopener.py b/torch/utils/data/datapipes/iter/fileopener.py\nindex 03d5761a9f164c..50737d9b587b25 100644\n--- a/torch/utils/data/datapipes/iter/fileopener.py\n+++ b/torch/utils/data/datapipes/iter/fileopener.py\n@@ -51,7 +51,7 @@ def __init__(\n         self.encoding: Optional[str] = encoding\n \n         if self.mode not in ('b', 't', 'rb', 'rt', 'r'):\n-            raise ValueError(\"Invalid mode {}\".format(mode))\n+            raise ValueError(f\"Invalid mode {mode}\")\n         # TODO: enforce typing for each instance based on mode, otherwise\n         #       `argument_validation` with this DataPipe may be potentially broken\n \n@@ -68,5 +68,5 @@ def __iter__(self):\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/grouping.py b/torch/utils/data/datapipes/iter/grouping.py\nindex c83bd2748b78fb..b26847d7319740 100644\n--- a/torch/utils/data/datapipes/iter/grouping.py\n+++ b/torch/utils/data/datapipes/iter/grouping.py\n@@ -83,7 +83,7 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('unbatch')\ndiff --git a/torch/utils/data/datapipes/iter/routeddecoder.py b/torch/utils/data/datapipes/iter/routeddecoder.py\nindex 8bfbe1442180ab..5e68ae133e05ab 100644\n--- a/torch/utils/data/datapipes/iter/routeddecoder.py\n+++ b/torch/utils/data/datapipes/iter/routeddecoder.py\n@@ -62,4 +62,4 @@ def __iter__(self) -> Iterator[Tuple[str, Any]]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/sharding.py b/torch/utils/data/datapipes/iter/sharding.py\nindex 730caeaf7d4da3..1f4a3a291bd11f 100644\n--- a/torch/utils/data/datapipes/iter/sharding.py\n+++ b/torch/utils/data/datapipes/iter/sharding.py\n@@ -80,4 +80,4 @@ def __len__(self):\n         if isinstance(self.source_datapipe, Sized):\n             return len(self.source_datapipe) // self.num_of_instances +\\\n                 (1 if (self.instance_id < len(self.source_datapipe) % self.num_of_instances) else 0)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/map/combining.py b/torch/utils/data/datapipes/map/combining.py\nindex 85146f8345cbdc..4a4a785eff78e5 100644\n--- a/torch/utils/data/datapipes/map/combining.py\n+++ b/torch/utils/data/datapipes/map/combining.py\n@@ -47,7 +47,7 @@ def __getitem__(self, index) -> T_co:  # type: ignore[type-var]\n                 return dp[index - offset]\n             else:\n                 offset += len(dp)\n-        raise IndexError(\"Index {} is out of range.\".format(index))\n+        raise IndexError(f\"Index {index} is out of range.\")\n \n     def __len__(self) -> int:\n         return sum(len(dp) for dp in self.datapipes)\ndiff --git a/torch/utils/data/datapipes/map/grouping.py b/torch/utils/data/datapipes/map/grouping.py\nindex da3cf5688a1bb0..65b30d8eba1f40 100644\n--- a/torch/utils/data/datapipes/map/grouping.py\n+++ b/torch/utils/data/datapipes/map/grouping.py\n@@ -64,4 +64,4 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/utils/common.py b/torch/utils/data/datapipes/utils/common.py\nindex e39d67ee6c81b9..99ae0cb4cbd024 100644\n--- a/torch/utils/data/datapipes/utils/common.py\n+++ b/torch/utils/data/datapipes/utils/common.py\n@@ -305,7 +305,7 @@ def __init__(self, file_obj, parent_stream=None, name=None):\n         self.closed = False\n         if parent_stream is not None:\n             if not isinstance(parent_stream, StreamWrapper):\n-                raise RuntimeError('Parent stream should be StreamWrapper, {} was given'.format(type(parent_stream)))\n+                raise RuntimeError(f'Parent stream should be StreamWrapper, {type(parent_stream)} was given')\n             parent_stream.child_counter += 1\n             self.parent_stream = parent_stream\n         if StreamWrapper.debug_unclosed_streams:\ndiff --git a/torch/utils/data/datapipes/utils/decoder.py b/torch/utils/data/datapipes/utils/decoder.py\nindex 4da810c3276684..8a7cb71b619de4 100644\n--- a/torch/utils/data/datapipes/utils/decoder.py\n+++ b/torch/utils/data/datapipes/utils/decoder.py\n@@ -137,7 +137,7 @@ class ImageHandler:\n     - pilrgba: pil None rgba\n     \"\"\"\n     def __init__(self, imagespec):\n-        assert imagespec in list(imagespecs.keys()), \"unknown image specification: {}\".format(imagespec)\n+        assert imagespec in list(imagespecs.keys()), f\"unknown image specification: {imagespec}\"\n         self.imagespec = imagespec.lower()\n \n     def __call__(self, extension, data):\n@@ -167,14 +167,14 @@ def __call__(self, extension, data):\n                 return img\n             elif atype == \"numpy\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n                 if etype == \"uint8\":\n                     return result\n                 else:\n                     return result.astype(\"f\") / 255.0\n             elif atype == \"torch\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n \n                 if etype == \"uint8\":\n                     result = np.array(result.transpose(2, 0, 1))\ndiff --git a/torch/utils/data/graph.py b/torch/utils/data/graph.py\nindex 2769e326c03e3b..7fc95d58fa2198 100644\n--- a/torch/utils/data/graph.py\n+++ b/torch/utils/data/graph.py\n@@ -130,7 +130,7 @@ def traverse(datapipe: DataPipe, only_datapipe: Optional[bool] = None) -> DataPi\n # Add cache here to prevent infinite recursion on DataPipe\n def _traverse_helper(datapipe: DataPipe, only_datapipe: bool, cache: Set[int]) -> DataPipeGraph:\n     if not isinstance(datapipe, (IterDataPipe, MapDataPipe)):\n-        raise RuntimeError(\"Expected `IterDataPipe` or `MapDataPipe`, but {} is found\".format(type(datapipe)))\n+        raise RuntimeError(f\"Expected `IterDataPipe` or `MapDataPipe`, but {type(datapipe)} is found\")\n \n     dp_id = id(datapipe)\n     if dp_id in cache:\ndiff --git a/torch/utils/dlpack.py b/torch/utils/dlpack.py\nindex f903de94eb67b2..a987bca6dcd51b 100644\n--- a/torch/utils/dlpack.py\n+++ b/torch/utils/dlpack.py\n@@ -102,7 +102,7 @@ def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n         # device is either CUDA or ROCm, we need to pass the current\n         # stream\n         if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n-            stream = torch.cuda.current_stream('cuda:{}'.format(device[1]))\n+            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n             # cuda_stream is the pointer to the stream and it is a public\n             # attribute, but it is not documented\n             # The array API specify that the default legacy stream must be passed\ndiff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py\nindex 3b583dbf790109..163f3649c41279 100644\n--- a/torch/utils/hipify/cuda_to_hip_mappings.py\n+++ b/torch/utils/hipify/cuda_to_hip_mappings.py\n@@ -46,7 +46,7 @@\n     RE_MINOR = re.compile(r\"#define\\s+ROCM_VERSION_MINOR\\s+(\\d+)\")\n     RE_PATCH = re.compile(r\"#define\\s+ROCM_VERSION_PATCH\\s+(\\d+)\")\n     major, minor, patch = 0, 0, 0\n-    for line in open(rocm_version_h, \"r\"):\n+    for line in open(rocm_version_h):\n         match = RE_MAJOR.search(line)\n         if match:\n             major = int(match.group(1))\ndiff --git a/torch/utils/hipify/hipify_python.py b/torch/utils/hipify/hipify_python.py\nindex 34a066750e1cdc..fa800659595bd7 100755\n--- a/torch/utils/hipify/hipify_python.py\n+++ b/torch/utils/hipify/hipify_python.py\n@@ -219,13 +219,13 @@ def compute_stats(stats):\n     unsupported_calls = {cuda_call for (cuda_call, _filepath) in stats[\"unsupported_calls\"]}\n \n     # Print the number of unsupported calls\n-    print(\"Total number of unsupported CUDA function calls: {0:d}\".format(len(unsupported_calls)))\n+    print(f\"Total number of unsupported CUDA function calls: {len(unsupported_calls):d}\")\n \n     # Print the list of unsupported calls\n     print(\", \".join(unsupported_calls))\n \n     # Print the number of kernel launches\n-    print(\"\\nTotal number of replaced kernel launches: {0:d}\".format(len(stats[\"kernel_launches\"])))\n+    print(\"\\nTotal number of replaced kernel launches: {:d}\".format(len(stats[\"kernel_launches\"])))\n \n \n def add_dim3(kernel_string, cuda_kernel):\n@@ -254,8 +254,8 @@ def add_dim3(kernel_string, cuda_kernel):\n     first_arg_clean = kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace(\"\\n\", \"\").strip(\" \")\n     second_arg_clean = kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace(\"\\n\", \"\").strip(\" \")\n \n-    first_arg_dim3 = \"dim3({})\".format(first_arg_clean)\n-    second_arg_dim3 = \"dim3({})\".format(second_arg_clean)\n+    first_arg_dim3 = f\"dim3({first_arg_clean})\"\n+    second_arg_dim3 = f\"dim3({second_arg_clean})\"\n \n     first_arg_raw_dim3 = first_arg_raw.replace(first_arg_clean, first_arg_dim3)\n     second_arg_raw_dim3 = second_arg_raw.replace(second_arg_clean, second_arg_dim3)\n@@ -269,7 +269,7 @@ def add_dim3(kernel_string, cuda_kernel):\n def processKernelLaunches(string, stats):\n     \"\"\" Replace the CUDA style Kernel launches with the HIP style kernel launches.\"\"\"\n     # Concat the namespace with the kernel names. (Find cleaner way of doing this later).\n-    string = RE_KERNEL_LAUNCH.sub(lambda inp: \"{0}{1}::\".format(inp.group(1), inp.group(2)), string)\n+    string = RE_KERNEL_LAUNCH.sub(lambda inp: f\"{inp.group(1)}{inp.group(2)}::\", string)\n \n     def grab_method_and_template(in_kernel):\n         # The positions for relevant kernel components.\n@@ -482,7 +482,7 @@ def replace_math_functions(input_string):\n     \"\"\"\n     output_string = input_string\n     for func in MATH_TRANSPILATIONS:\n-        output_string = output_string.replace(r'{}('.format(func), '{}('.format(MATH_TRANSPILATIONS[func]))\n+        output_string = output_string.replace(fr'{func}(', f'{MATH_TRANSPILATIONS[func]}(')\n \n     return output_string\n \n@@ -531,7 +531,7 @@ def replace_extern_shared(input_string):\n     \"\"\"\n     output_string = input_string\n     output_string = RE_EXTERN_SHARED.sub(\n-        lambda inp: \"HIP_DYNAMIC_SHARED({0} {1}, {2})\".format(\n+        lambda inp: \"HIP_DYNAMIC_SHARED({} {}, {})\".format(\n             inp.group(1) or \"\", inp.group(2), inp.group(3)), output_string)\n \n     return output_string\n@@ -657,7 +657,7 @@ def is_caffe2_gpu_file(rel_filepath):\n \n \n # Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n-class Trie():\n+class Trie:\n     \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n     The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n \n@@ -750,7 +750,7 @@ def pattern(self):\n             CAFFE2_TRIE.add(src)\n             CAFFE2_MAP[src] = dst\n RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.pattern())\n-RE_PYTORCH_PREPROCESSOR = re.compile(r'(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))\n+RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\\W)({PYTORCH_TRIE.pattern()})(?=\\W)')\n \n RE_QUOTE_HEADER = re.compile(r'#include \"([^\"]+)\"')\n RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')\n@@ -789,7 +789,7 @@ def preprocessor(\n \n     rel_filepath = os.path.relpath(filepath, output_directory)\n \n-    with open(fin_path, 'r', encoding='utf-8') as fin:\n+    with open(fin_path, encoding='utf-8') as fin:\n         if fin.readline() == HIPIFY_C_BREADCRUMB:\n             hipify_result.hipified_path = None\n             hipify_result.status = \"[ignored, input is hipified output]\"\n@@ -929,7 +929,7 @@ def repl(m):\n \n     do_write = True\n     if os.path.exists(fout_path):\n-        with open(fout_path, 'r', encoding='utf-8') as fout_old:\n+        with open(fout_path, encoding='utf-8') as fout_old:\n             do_write = fout_old.read() != output_source\n     if do_write:\n         try:\n@@ -956,7 +956,7 @@ def file_specific_replacement(filepath, search_string, replace_string, strict=Fa\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if strict:\n-            contents = re.sub(r'\\b({0})\\b'.format(re.escape(search_string)), lambda x: replace_string, contents)\n+            contents = re.sub(fr'\\b({re.escape(search_string)})\\b', lambda x: replace_string, contents)\n         else:\n             contents = contents.replace(search_string, replace_string)\n         f.seek(0)\n@@ -968,8 +968,8 @@ def file_add_header(filepath, header):\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if header[0] != \"<\" and header[-1] != \">\":\n-            header = '\"{0}\"'.format(header)\n-        contents = ('#include {0} \\n'.format(header)) + contents\n+            header = f'\"{header}\"'\n+        contents = (f'#include {header} \\n') + contents\n         f.seek(0)\n         f.write(contents)\n         f.truncate()\ndiff --git a/torch/utils/jit/log_extract.py b/torch/utils/jit/log_extract.py\nindex d9d0e442c1dbf6..2e89a769eff0c8 100644\n--- a/torch/utils/jit/log_extract.py\n+++ b/torch/utils/jit/log_extract.py\n@@ -11,7 +11,7 @@ def extract_ir(filename: str) -> List[str]:\n     pfx = None\n     current = \"\"\n     graphs = []\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         split_strs = f.read().split(BEGIN)\n         for i, split_str in enumerate(split_strs):\n             if i == 0:\ndiff --git a/torch/utils/mobile_optimizer.py b/torch/utils/mobile_optimizer.py\nindex ec200423e10c5b..66d57a2372baf9 100644\n--- a/torch/utils/mobile_optimizer.py\n+++ b/torch/utils/mobile_optimizer.py\n@@ -31,7 +31,7 @@ def optimize_for_mobile(\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     if optimization_blocklist is None:\n         optimization_blocklist = set()\n@@ -86,7 +86,7 @@ def generate_mobile_module_lints(script_module: torch.jit.ScriptModule):\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     lint_list = []\n \ndiff --git a/torch/utils/tensorboard/_caffe2_graph.py b/torch/utils/tensorboard/_caffe2_graph.py\nindex 8bba2aeffddef2..2aa162af7ad5c3 100644\n--- a/torch/utils/tensorboard/_caffe2_graph.py\n+++ b/torch/utils/tensorboard/_caffe2_graph.py\n@@ -232,7 +232,7 @@ def _add_gradient_scope(shapes, blob_name_tracker, ops):\n \n     def f(name):\n         if \"_grad\" in name:\n-            return \"GRADIENTS/{}\".format(name)\n+            return f\"GRADIENTS/{name}\"\n         else:\n             return name\n \n@@ -317,7 +317,7 @@ def _tf_device(device_option):\n     ):\n         return \"/cpu:*\"\n     if device_option.device_type == caffe2_pb2.CUDA:\n-        return \"/gpu:{}\".format(device_option.device_id)\n+        return f\"/gpu:{device_option.device_id}\"\n     raise Exception(\"Unhandled device\", device_option)\n \n \ndiff --git a/torch/utils/tensorboard/_embedding.py b/torch/utils/tensorboard/_embedding.py\nindex f172e092608337..afbe68191aa98f 100644\n--- a/torch/utils/tensorboard/_embedding.py\n+++ b/torch/utils/tensorboard/_embedding.py\n@@ -62,7 +62,7 @@ def make_sprite(label_img, save_path):\n \n def get_embedding_info(metadata, label_img, subdir, global_step, tag):\n     info = EmbeddingInfo()\n-    info.tensor_name = \"{}:{}\".format(tag, str(global_step).zfill(5))\n+    info.tensor_name = f\"{tag}:{str(global_step).zfill(5)}\"\n     info.tensor_path = _gfile_join(subdir, \"tensors.tsv\")\n     if metadata is not None:\n         info.metadata_path = _gfile_join(subdir, \"metadata.tsv\")\ndiff --git a/torch/utils/tensorboard/_pytorch_graph.py b/torch/utils/tensorboard/_pytorch_graph.py\nindex f03812b603e1c0..280b503c515c0b 100644\n--- a/torch/utils/tensorboard/_pytorch_graph.py\n+++ b/torch/utils/tensorboard/_pytorch_graph.py\n@@ -275,7 +275,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n                     parent_scope, attr_scope, attr_name\n                 )\n             else:\n-                attr_to_scope[attr_key] = \"__module.{}\".format(attr_name)\n+                attr_to_scope[attr_key] = f\"__module.{attr_name}\"\n             # We don't need classtype nodes; scope will provide this information\n             if node.output().type().kind() != CLASSTYPE_KIND:\n                 node_py = NodePyOP(node)\n@@ -286,7 +286,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n \n     for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n         node_pyio = NodePyIO(node, \"output\")\n-        node_pyio.debugName = \"output.{}\".format(i + 1)\n+        node_pyio.debugName = f\"output.{i + 1}\"\n         node_pyio.inputs = [node.debugName()]\n         nodes_py.append(node_pyio)\n \n@@ -302,7 +302,7 @@ def parse_traced_name(module):\n     for name, module in trace.named_modules(prefix=\"__module\"):\n         mod_name = parse_traced_name(module)\n         attr_name = name.split(\".\")[-1]\n-        alias_to_name[name] = \"{}[{}]\".format(mod_name, attr_name)\n+        alias_to_name[name] = f\"{mod_name}[{attr_name}]\"\n \n     for node in nodes_py.nodes_op:\n         module_aliases = node.scopeName.split(\"/\")\ndiff --git a/torch/utils/tensorboard/writer.py b/torch/utils/tensorboard/writer.py\nindex 2fa34d05e727d9..b592707df2c1e1 100644\n--- a/torch/utils/tensorboard/writer.py\n+++ b/torch/utils/tensorboard/writer.py\n@@ -953,7 +953,7 @@ def add_embedding(\n \n         # Maybe we should encode the tag so slashes don't trip us up?\n         # I don't think this will mess us up, but better safe than sorry.\n-        subdir = \"%s/%s\" % (str(global_step).zfill(5), self._encode(tag))\n+        subdir = f\"{str(global_step).zfill(5)}/{self._encode(tag)}\"\n         save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n \n         fs = tf.io.gfile\ndiff --git a/torch/utils/throughput_benchmark.py b/torch/utils/throughput_benchmark.py\nindex 8b2fd1a76ca8a4..2dc3ce8543a9b6 100644\n--- a/torch/utils/throughput_benchmark.py\n+++ b/torch/utils/throughput_benchmark.py\n@@ -18,10 +18,10 @@ def format_time(time_us=None, time_ms=None, time_s=None):\n             raise AssertionError(\"Shouldn't reach here :)\")\n \n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n \n class ExecutionStats:\n@@ -52,8 +52,8 @@ def total_time_seconds(self):\n     def __str__(self):\n         return '\\n'.join([\n             \"Average latency per example: \" + format_time(time_ms=self.latency_avg_ms),\n-            \"Total number of iterations: {}\".format(self.num_iters),\n-            \"Total number of iterations per second (across all threads): {:.2f}\".format(self.iters_per_second),\n+            f\"Total number of iterations: {self.num_iters}\",\n+            f\"Total number of iterations per second (across all threads): {self.iters_per_second:.2f}\",\n             \"Total time: \" + format_time(time_s=self.total_time_seconds)\n         ])\n \ndiff --git a/torch/utils/viz/_cycles.py b/torch/utils/viz/_cycles.py\nindex a64d5e9c35830a..13a425cd1b8285 100644\n--- a/torch/utils/viz/_cycles.py\n+++ b/torch/utils/viz/_cycles.py\n@@ -220,29 +220,29 @@ def format_sequence(obj):\n     if isinstance(obj, BASE_TYPES):\n         return repr(obj)\n     if type(obj).__name__ == 'function':\n-        return \"function\\n{}\".format(obj.__name__)\n+        return f\"function\\n{obj.__name__}\"\n     elif isinstance(obj, types.MethodType):\n         try:\n             func_name = obj.__func__.__qualname__\n         except AttributeError:\n             func_name = \"<anonymous>\"\n-        return \"instancemethod\\n{}\".format(func_name)\n+        return f\"instancemethod\\n{func_name}\"\n     elif isinstance(obj, list):\n         return f\"[{format_sequence(obj)}]\"\n     elif isinstance(obj, tuple):\n         return f\"({format_sequence(obj)})\"\n     elif isinstance(obj, dict):\n-        return \"dict[{}]\".format(len(obj))\n+        return f\"dict[{len(obj)}]\"\n     elif isinstance(obj, types.ModuleType):\n-        return \"module\\n{}\".format(obj.__name__)\n+        return f\"module\\n{obj.__name__}\"\n     elif isinstance(obj, type):\n-        return \"type\\n{}\".format(obj.__name__)\n+        return f\"type\\n{obj.__name__}\"\n     elif isinstance(obj, weakref.ref):\n         referent = obj()\n         if referent is None:\n             return \"weakref (dead referent)\"\n         else:\n-            return \"weakref to id 0x{:x}\".format(id(referent))\n+            return f\"weakref to id 0x{id(referent):x}\"\n     elif isinstance(obj, types.FrameType):\n         filename = obj.f_code.co_filename\n         if len(filename) > FRAME_FILENAME_LIMIT:\ndiff --git a/torch/utils/weak.py b/torch/utils/weak.py\nindex 2a7d597c4f2a06..bcd3025bc68e3a 100644\n--- a/torch/utils/weak.py\n+++ b/torch/utils/weak.py\n@@ -4,7 +4,6 @@\n from weakref import ref\n from _weakrefset import _IterationGuard  # type: ignore[attr-defined]\n from collections.abc import MutableMapping, Mapping\n-from typing import Dict\n from torch import Tensor\n import collections.abc as _collections_abc\n \n@@ -83,7 +82,7 @@ def __eq__(self, other):\n \n # This is directly adapted from cpython/Lib/weakref.py\n class WeakIdKeyDictionary(MutableMapping):\n-    data: Dict[WeakIdRef, object]\n+    data: dict[WeakIdRef, object]\n \n     def __init__(self, dict=None):\n         self.data = {}\n@@ -144,7 +143,7 @@ def __len__(self):\n         return len(self.data) - len(self._pending_removals)\n \n     def __repr__(self):\n-        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n+        return f\"<{self.__class__.__name__} at {id(self):#x}>\"\n \n     def __setitem__(self, key, value):\n         self.data[WeakIdRef(key, self._remove)] = value  # CHANGED\n\nFrom 36882a7de3270913949d7e9be765c57c9b150ac9 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:25:33 +0000\nSubject: [PATCH 2/3] Update on \"[BE] Enable ruff's UP rules and autoformat\n utils/\"\n\n[ghstack-poisoned]\n---\n torch/utils/collect_env.py | 26 +++++++++++++-------------\n 1 file changed, 13 insertions(+), 13 deletions(-)\n\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex 2266d64c1944d5..de03564a2b75d1 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, f\"{conda} list\")\n+    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n+        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return f'Probably one of the following:\\n{result}'\n+    return 'Probably one of the following:\\n{}'.format(result)\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = f'\"{candidate_smi}\"'\n+                smi = '\"{}\"'.format(candidate_smi)\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n+    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return f'macOS {version} ({machine()})'\n+        return 'macOS {} ({})'.format(version, machine())\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return f'{desc} ({machine()})'\n+            return '{} ({})'.format(desc, machine())\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return f'{desc} ({machine()})'\n+            return '{} ({})'.format(desc, machine())\n \n-        return f'{platform} ({machine()})'\n+        return '{} ({})'.format(platform, machine())\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n+        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return f'\\n{string}\\n'\n+            return '\\n{}\\n'.format(string)\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               f'[{envinfo.pip_version}] ')\n+                                               '[{}] '.format(envinfo.pip_version))\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n+            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \n\nFrom ca3e195fab108b3b8f8ddf1137d1ecd8e659f5cf Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 16:21:14 +0000\nSubject: [PATCH 3/3] Update on \"[BE] Enable ruff's UP rules and autoformat\n utils/\"\n\n[ghstack-poisoned]\n---\n .../benchmark/utils/valgrind_wrapper/timer_interface.py     | 3 ++-\n torch/utils/cpp_extension.py                                | 6 +++---\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 11ce6d90fc47f3..61e43488e41c33 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -29,7 +29,8 @@\n \n \n class FunctionCount(NamedTuple):\n-    count: int\n+    # TODO(#105471): Rename the count field\n+    count: int  # type: ignore[assignment]\n     function: str\n \n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex e847f4e30915b9..323a391a684ed8 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -151,10 +151,10 @@ def _join_rocm_home(*paths) -> str:\n     '''\n     if ROCM_HOME is None:\n         raise OSError('ROCM_HOME environment variable is not set. '\n-                               'Please set it to your ROCm install root.')\n+                      'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n         raise OSError('Building PyTorch extensions using '\n-                               'ROCm and Windows is not supported.')\n+                      'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n \n@@ -2248,7 +2248,7 @@ def _join_cuda_home(*paths) -> str:\n     '''\n     if CUDA_HOME is None:\n         raise OSError('CUDA_HOME environment variable is not set. '\n-                               'Please set it to your CUDA install root.')\n+                      'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \n \n"
  },
  {
    "number": 105423,
    "title": "[BE] Enable ruff's UP rules and autoformat torchgen/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #105437\n* #105436\n* #105435\n* #105434\n* #105433\n* #105432\n* #105431\n* #105430\n* #105429\n* #105428\n* #105427\n* #105426\n* #105425\n* #105424\n* __->__ #105423\n\n",
    "merge_commit_sha": "46c57f92106631a4c7567d86f9ccba7922f464b5",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105423",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105423/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105423.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105423.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105423/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105423/comments",
    "labels": [
      "Merged",
      "open source",
      "topic: not user facing",
      "ciflow/trunk"
    ],
    "_event_time": "2023-07-18T01:20:34.759117Z",
    "state": "closed",
    "patch": "From efb916409c6f7cb67ce6aec5ea614012c3716188 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:20:28 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat torchgen/\n\n[ghstack-poisoned]\n---\n torchgen/api/python.py               | 16 ++++++++--------\n torchgen/code_template.py            |  2 +-\n torchgen/executorch/parse.py         |  2 +-\n torchgen/gen.py                      |  4 ++--\n torchgen/gen_backend_stubs.py        |  6 +++---\n torchgen/gen_executorch.py           |  6 +++---\n torchgen/gen_lazy_tensor.py          |  6 +++---\n torchgen/model.py                    |  2 +-\n torchgen/selective_build/operator.py |  2 +-\n torchgen/selective_build/selector.py |  4 ++--\n torchgen/utils.py                    | 10 +++++-----\n 11 files changed, 30 insertions(+), 30 deletions(-)\n\ndiff --git a/torchgen/api/python.py b/torchgen/api/python.py\nindex b4da5d1113dce6..96aa43be1060b5 100644\n--- a/torchgen/api/python.py\n+++ b/torchgen/api/python.py\n@@ -315,7 +315,7 @@ def from_outputs(\n                 outputs=outputs,\n             )\n         elif size > 1:\n-            if any((not a.type.is_tensor_like() for a in outputs)):\n+            if any(not a.type.is_tensor_like() for a in outputs):\n                 raise RuntimeError(f\"Unsupported output type: {outputs}\")\n             return PythonOutArgument(\n                 name=\"out\",\n@@ -882,10 +882,10 @@ def topt_default_init(name: str) -> Optional[str]:\n \n \n def namedtuple_fieldnames(returns: Tuple[Return, ...]) -> List[str]:\n-    if len(returns) <= 1 or all((r.name is None for r in returns)):\n+    if len(returns) <= 1 or all(r.name is None for r in returns):\n         return []\n     else:\n-        if any((r.name is None for r in returns)):\n+        if any(r.name is None for r in returns):\n             # When building on Windows, `PyStructSequence_UnnamedField` could not be\n             # resolved by the linker for some reason, which cause error in building:\n             #\n@@ -1163,7 +1163,7 @@ def dispatch_lambda_return_str(f: NativeFunction) -> str:\n     # mutable reference to temporary.  Maybe we could assign it to a\n     # variable itself.)\n     returns_without_annotation = tuple(\n-        (Return(r.name, r.type, None) for r in f.func.returns)\n+        Return(r.name, r.type, None) for r in f.func.returns\n     )\n     return_str = cpp.returns_type(returns_without_annotation, symint=True).cpp_type()\n     if return_str not in SUPPORTED_RETURN_TYPES:\n@@ -1195,7 +1195,7 @@ def cpp_dispatch_exprs(\n     exprs: Tuple[str, ...] = tuple()\n     if not isinstance(python_signature, PythonSignatureDeprecated):\n         # By default the exprs are consistent with the C++ signature.\n-        exprs = tuple((a.name for a in cpp_args))\n+        exprs = tuple(a.name for a in cpp_args)\n     else:\n         # For deprecated python signature we may need fill in some constants.\n         exprs = tuple(\n@@ -1426,7 +1426,7 @@ def dispatch_lambda_exprs(\n                     f\"{f.func}: unrecognized type '{str(a.type)}' for tensor options field '{a.name}'\"\n                 )\n         if not all(\n-            (a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys())\n+            a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys()\n         ):\n             raise RuntimeError(\n                 f\"{f.func}: incomplete tensor options args: {tensor_options_args_names}\"\n@@ -1454,7 +1454,7 @@ def dispatch_lambda_exprs(\n                 raise RuntimeError(\n                     f\"{f.func}: dtype in tensor_options_args without output arg\"\n                 )\n-            if not all((a in tensor_options_args_names for a in (\"layout\", \"device\"))):\n+            if not all(a in tensor_options_args_names for a in (\"layout\", \"device\")):\n                 raise RuntimeError(\n                     f\"{f.func}: incomplete tensor options for output check\"\n                 )\n@@ -1473,6 +1473,6 @@ def dispatch_lambda_exprs(\n             )\n \n     return DispatchLambdaArgumentExprs(\n-        exprs=tuple((lambda_args_exprs[a.name] for a in lambda_args)),\n+        exprs=tuple(lambda_args_exprs[a.name] for a in lambda_args),\n         inits=inits,\n     )\ndiff --git a/torchgen/code_template.py b/torchgen/code_template.py\nindex 9f877771afe9be..b932a94ecc9192 100644\n--- a/torchgen/code_template.py\n+++ b/torchgen/code_template.py\n@@ -20,7 +20,7 @@ class CodeTemplate:\n \n     @staticmethod\n     def from_file(filename: str) -> \"CodeTemplate\":\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             return CodeTemplate(f.read(), filename)\n \n     def __init__(self, pattern: str, filename: str = \"\") -> None:\ndiff --git a/torchgen/executorch/parse.py b/torchgen/executorch/parse.py\nindex f6f30b4554aafb..89b4b93558a6a2 100644\n--- a/torchgen/executorch/parse.py\n+++ b/torchgen/executorch/parse.py\n@@ -124,7 +124,7 @@ def parse_et_yaml(\n     \"\"\"Parse native_functions.yaml into NativeFunctions and an Operator Indexed Dict\n     of fields to persist from native_functions.yaml to functions.yaml\n     \"\"\"\n-    with open(path, \"r\") as f:\n+    with open(path) as f:\n         es = yaml.load(f, Loader=LineLoader)\n \n     et_kernel = extract_kernel_fields(es)\ndiff --git a/torchgen/gen.py b/torchgen/gen.py\nindex dcdd0945dff019..9766c8af5bc0f5 100644\n--- a/torchgen/gen.py\n+++ b/torchgen/gen.py\n@@ -212,7 +212,7 @@ def parse_tags_yaml_struct(es: object, path: str = \"<stdin>\") -> Set[str]:\n def parse_tags_yaml(path: str) -> Set[str]:\n     global _GLOBAL_PARSE_TAGS_YAML_CACHE\n     if path not in _GLOBAL_PARSE_TAGS_YAML_CACHE:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n             _GLOBAL_PARSE_TAGS_YAML_CACHE[path] = parse_tags_yaml_struct(es, path=path)\n \n@@ -233,7 +233,7 @@ def parse_native_yaml(\n \n         # if a loaded yaml is provided, use that instead of reading from path\n         if loaded_yaml is None:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 es = yaml.load(f, Loader=LineLoader)\n         else:\n             es = loaded_yaml\ndiff --git a/torchgen/gen_backend_stubs.py b/torchgen/gen_backend_stubs.py\nindex 7322daa5dc7602..ff23aa9be39713 100644\n--- a/torchgen/gen_backend_stubs.py\n+++ b/torchgen/gen_backend_stubs.py\n@@ -47,7 +47,7 @@ def parse_backend_yaml(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -253,9 +253,9 @@ def error_on_missing_kernels(\n     full_codegen: Optional[List[OperatorName]] = None,\n ) -> None:\n     try:\n-        with open(kernel_defn_file_path, \"r\") as f:\n+        with open(kernel_defn_file_path) as f:\n             backend_defns = f.read()\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified impl_path file: {kernel_defn_file_path}\"\n         ) from e\ndiff --git a/torchgen/gen_executorch.py b/torchgen/gen_executorch.py\nindex bfd42a7985e49d..6f5df46944f0f6 100644\n--- a/torchgen/gen_executorch.py\n+++ b/torchgen/gen_executorch.py\n@@ -575,7 +575,7 @@ def translate_native_yaml(\n         None\n     \"\"\"\n     if use_aten_lib:\n-        with open(aten_yaml_path, \"r\") as aten_yaml:\n+        with open(aten_yaml_path) as aten_yaml:\n             out_file.writelines(aten_yaml.readlines())\n         return\n \n@@ -604,7 +604,7 @@ def translate_native_yaml(\n         or os.stat(native_yaml_path).st_size == 0\n     ):\n         return\n-    with open(native_yaml_path, \"r\") as native_yaml:\n+    with open(native_yaml_path) as native_yaml:\n         native_es = yaml.load(native_yaml, Loader=LineLoader)\n         if not native_es:\n             return\n@@ -641,7 +641,7 @@ def parse_yaml(\n     Union[Dict[DispatchKey, Dict[OperatorName, BackendMetadata]], ETKernelIndex],\n ]:\n     if path and os.path.exists(path) and os.stat(path).st_size > 0:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n \n         # Check for kernel index structure\ndiff --git a/torchgen/gen_lazy_tensor.py b/torchgen/gen_lazy_tensor.py\nindex f995bdb2619838..3e4e4b0414277c 100644\n--- a/torchgen/gen_lazy_tensor.py\n+++ b/torchgen/gen_lazy_tensor.py\n@@ -115,7 +115,7 @@ def parse_native_functions_keys(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -134,10 +134,10 @@ def validate_shape_inference_header(\n     shape_inference_hdr: str, expected_shape_infr_decls: List[str]\n ) -> None:\n     try:\n-        with open(shape_inference_hdr, \"r\") as f:\n+        with open(shape_inference_hdr) as f:\n             shape_infr_decls = f.read()\n             shape_infr_decl_lines = set(shape_infr_decls.split(\"\\n\"))\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified shape_inference_hdr file: {shape_inference_hdr}\"\n         ) from e\ndiff --git a/torchgen/model.py b/torchgen/model.py\nindex 151fb02bb2c908..0b44732455ea2e 100644\n--- a/torchgen/model.py\n+++ b/torchgen/model.py\n@@ -40,7 +40,7 @@ class Location:\n     line: int\n \n     def __str__(self) -> str:\n-        return \"{}:{}\".format(self.file, self.line)\n+        return f\"{self.file}:{self.line}\"\n \n \n # Valid values of the 'variants' field in native_functions.yaml\ndiff --git a/torchgen/selective_build/operator.py b/torchgen/selective_build/operator.py\nindex 52fdcb74fca84b..d7f5c56f63a60d 100644\n--- a/torchgen/selective_build/operator.py\n+++ b/torchgen/selective_build/operator.py\n@@ -83,7 +83,7 @@ def from_yaml_dict(\n         if \"debug_info\" in op_info:\n             di_list = op_info[\"debug_info\"]\n             assert isinstance(di_list, list)\n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         return SelectiveBuildOperator(\n             name=op_name,\ndiff --git a/torchgen/selective_build/selector.py b/torchgen/selective_build/selector.py\nindex 1d4a00e5968950..4fdc513534444d 100644\n--- a/torchgen/selective_build/selector.py\n+++ b/torchgen/selective_build/selector.py\n@@ -93,7 +93,7 @@ def from_yaml_dict(data: Dict[str, object]) -> \"SelectiveBuilder\":\n             di_list = data[\"debug_info\"]\n             assert isinstance(di_list, list)\n \n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         operators = {}\n         operators_dict = data.get(\"operators\", {})\n@@ -141,7 +141,7 @@ def from_yaml_str(config_contents: str) -> \"SelectiveBuilder\":\n \n     @staticmethod\n     def from_yaml_path(config_path: str) -> \"SelectiveBuilder\":\n-        with open(config_path, \"r\") as f:\n+        with open(config_path) as f:\n             contents = yaml.safe_load(f)\n             return SelectiveBuilder.from_yaml_dict(contents)\n \ndiff --git a/torchgen/utils.py b/torchgen/utils.py\nindex dd187c737c93ed..0729645ef10b92 100644\n--- a/torchgen/utils.py\n+++ b/torchgen/utils.py\n@@ -105,7 +105,7 @@ def context(msg_fn: Callable[[], str]) -> Iterator[None]:\n # for getting mypy to do exhaustiveness checking\n # TODO: put this somewhere else, maybe\n def assert_never(x: NoReturn) -> NoReturn:\n-    raise AssertionError(\"Unhandled type: {}\".format(type(x).__name__))\n+    raise AssertionError(f\"Unhandled type: {type(x).__name__}\")\n \n \n @functools.lru_cache(maxsize=None)\n@@ -137,9 +137,9 @@ def __init__(self, install_dir: str, template_dir: str, dry_run: bool) -> None:\n     def _write_if_changed(self, filename: str, contents: str) -> None:\n         old_contents: Optional[str]\n         try:\n-            with open(filename, \"r\") as f:\n+            with open(filename) as f:\n                 old_contents = f.read()\n-        except IOError:\n+        except OSError:\n             old_contents = None\n         if contents != old_contents:\n             # Create output directory if it doesn't exist\n@@ -157,7 +157,7 @@ def substitute_with_template(\n             # TODO: Update the comment reference to the correct location\n             if \"generated_comment\" not in env:\n                 comment = \"@\" + \"generated by torchgen/gen.py\"\n-                comment += \" from {}\".format(os.path.basename(template_path))\n+                comment += f\" from {os.path.basename(template_path)}\"\n                 env[\"generated_comment\"] = comment\n             template = _read_template(template_path)\n             return template.substitute(env)\n@@ -172,7 +172,7 @@ def write_with_template(\n         template_fn: str,\n         env_callable: Callable[[], Union[str, Dict[str, Any]]],\n     ) -> None:\n-        filename = \"{}/{}\".format(self.install_dir, filename)\n+        filename = f\"{self.install_dir}/{filename}\"\n         assert filename not in self.filenames, \"duplicate file write {filename}\"\n         self.filenames.add(filename)\n         if not self.dry_run:\n"
  },
  {
    "number": 105422,
    "title": "[BE] Enable ruff's UP rules and autoformat dynamo / functorch and refs",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "8e17252aac6dd8395a5e1fefbe7f6648b02aef59",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105422",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105422/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105422.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105422.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105422/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105422/comments",
    "labels": [
      "ciflow/inductor",
      "release notes: fx",
      "module: export",
      "module: dynamo"
    ],
    "_event_time": "2023-07-18T01:17:48.864688Z",
    "state": "closed",
    "patch": "From b7df66079ab30b28088cc2bc70ece1d4d75b82c4 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:42 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat dynamo / functorch\n and refs\n\n[ghstack-poisoned]\n---\n functorch/benchmarks/operator_authoring.py    |  2 +-\n functorch/einops/rearrange.py                 |  4 +-\n .../examples/compilation/linear_train.py      |  2 +-\n .../maml_omniglot/support/omniglot_loaders.py |  4 +-\n functorch/op_analysis/gen_data.py             | 10 +--\n test/dynamo/test_autograd_function.py         |  4 +-\n test/dynamo/test_compile.py                   |  2 +-\n test/dynamo/test_logging.py                   |  2 +-\n test/dynamo/test_misc.py                      | 34 +++++-----\n test/dynamo/test_modules.py                   | 12 ++--\n test/dynamo/test_profiler.py                  |  4 +-\n test/dynamo/test_repros.py                    |  2 +-\n test/error_messages/storage.py                |  2 +-\n test/export/test_db.py                        |  6 +-\n test/export/test_serialize.py                 |  2 +-\n .../check_forward_backward_compatibility.py   |  2 +-\n test/functorch/test_aotdispatch.py            |  4 +-\n test/functorch/test_vmap.py                   |  2 +-\n test/fx/test_future.py                        |  4 +-\n test/fx/test_gradual_type.py                  |  8 +--\n torch/_decomp/decompositions.py               | 14 ++---\n torch/_dynamo/debug_utils.py                  | 16 ++---\n torch/_dynamo/eval_frame.py                   |  4 +-\n torch/_dynamo/output_graph.py                 |  2 +-\n torch/_dynamo/resume_execution.py             |  6 +-\n torch/_dynamo/symbolic_convert.py             |  4 +-\n torch/_dynamo/test_minifier_common.py         |  6 +-\n torch/_dynamo/variables/builder.py            | 10 ++-\n torch/_dynamo/variables/misc.py               |  2 +-\n torch/_export/verifier.py                     |  8 +--\n torch/_functorch/eager_transforms.py          |  2 +-\n torch/_functorch/pytree_hacks.py              |  2 +-\n torch/_prims/__init__.py                      | 62 +++++++++----------\n torch/_prims/executor.py                      |  2 +-\n torch/_prims/nvfuser_executor.py              |  4 +-\n torch/_prims_common/__init__.py               | 28 ++++-----\n torch/_prims_common/wrappers.py               |  8 +--\n torch/_refs/__init__.py                       | 46 +++++++-------\n torch/_refs/nn/functional/__init__.py         | 16 ++---\n .../migrate_gradual_types/constraint.py       |  1 -\n .../constraint_transformation.py              |  4 +-\n .../migrate_gradual_types/operation.py        |  1 -\n torch/fx/experimental/symbolic_shapes.py      |  4 +-\n .../multipledispatch/dispatcher.py            | 15 ++---\n torch/fx/interpreter.py                       |  2 +-\n torch/fx/passes/utils/matcher_utils.py        |  2 +-\n torch/fx/passes/utils/source_matcher_utils.py |  2 +-\n 47 files changed, 181 insertions(+), 204 deletions(-)\n\ndiff --git a/functorch/benchmarks/operator_authoring.py b/functorch/benchmarks/operator_authoring.py\nindex cbd816e2ad1324..456f5040d759f2 100644\n--- a/functorch/benchmarks/operator_authoring.py\n+++ b/functorch/benchmarks/operator_authoring.py\n@@ -113,7 +113,7 @@ def out_setup(n):\n def test_backwards(make_args, nnc=nnc_add, aten=torch.add):\n     def backwards_setup(n):\n         args = make_args(n)\n-        (grad_var,) = [a for a in args if a.requires_grad]\n+        (grad_var,) = (a for a in args if a.requires_grad)\n         aten(*args).sum().backward()\n         correct = grad_var.grad.clone()\n         grad_var.grad.zero_()\ndiff --git a/functorch/einops/rearrange.py b/functorch/einops/rearrange.py\nindex c45d2063c7114a..f8f60c4917b766 100644\n--- a/functorch/einops/rearrange.py\n+++ b/functorch/einops/rearrange.py\n@@ -108,7 +108,7 @@ class dims.\"\"\"\n \n     custom_rearrange_callable_name = \"do_rearrange\"\n     custom_rearrange_callable_code = (\n-        (\n+\n             f\"def {custom_rearrange_callable_name}(tensor):\\n\"\n             f\"    {comma_separate(first_class_dims)} = dims({n_dims})\\n\"\n             + (\n@@ -120,7 +120,7 @@ class dims.\"\"\"\n                 f\"    return tensor.sum({comma_separate([anon_dims])}, keepdim=False)\\n\"\n                 if anon_dims else \"    return tensor\\n\"\n             )\n-        )\n+\n     )\n \n     exec(custom_rearrange_callable_code)\ndiff --git a/functorch/examples/compilation/linear_train.py b/functorch/examples/compilation/linear_train.py\nindex 2d5f9d7dd37b44..ee84347470835b 100644\n--- a/functorch/examples/compilation/linear_train.py\n+++ b/functorch/examples/compilation/linear_train.py\n@@ -18,7 +18,7 @@ def bench(f, iters=100, warmup=10):\n     begin = time.time()\n     for _ in range(iters):\n         f()\n-    print((time.time() - begin))\n+    print(time.time() - begin)\n \n \n class Foo(nn.Module):\ndiff --git a/functorch/examples/maml_omniglot/support/omniglot_loaders.py b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\nindex ce636ecca0b1b2..6a4369ba4b208f 100644\n--- a/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n+++ b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n@@ -276,10 +276,10 @@ def load_data_cache(self, data_pack):\n             x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n             y_qrys = np.array(y_qrys).astype(int).reshape(self.batchsz, querysz)\n \n-            x_spts, y_spts, x_qrys, y_qrys = [\n+            x_spts, y_spts, x_qrys, y_qrys = (\n                 torch.from_numpy(z).to(self.device) for z in\n                 [x_spts, y_spts, x_qrys, y_qrys]\n-            ]\n+            )\n \n             data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n \ndiff --git a/functorch/op_analysis/gen_data.py b/functorch/op_analysis/gen_data.py\nindex a9cc84e6f9362c..ab1f3a79125c20 100644\n--- a/functorch/op_analysis/gen_data.py\n+++ b/functorch/op_analysis/gen_data.py\n@@ -23,7 +23,7 @@ def gen_data(special_op_lists, analysis_name):\n     composite_ops = get_ops_for_key('CompositeImplicitAutograd')\n     noncomposite_ops = all_ops - composite_ops\n \n-    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)\n+    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml').read(), Loader=yaml.CLoader)\n \n     annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}\n     from collections import defaultdict\n@@ -132,19 +132,19 @@ def remove_prefix(input_string, prefix):\n \n \n if True:\n-    with open('run_ops.txt', 'r') as f:\n+    with open('run_ops.txt') as f:\n         opinfo_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n-    with open('count_ops.txt', 'r') as f:\n+    with open('count_ops.txt') as f:\n         opinfo_counts = [i.strip() for i in f.readlines()]\n         opinfo_counts = defaultdict(int, dict(zip(opinfo_ops, opinfo_counts)))\n \n     def count_fn(x):\n         return opinfo_counts[x['full_name']]\n \n-    with open('run_decompositions.txt', 'r') as f:\n+    with open('run_decompositions.txt') as f:\n         decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n \n-    with open('public_api', 'r') as f:\n+    with open('public_api') as f:\n         ref_api = [i.strip() for i in f.readlines()]\n \n     def has_ref_impl(x):\ndiff --git a/test/dynamo/test_autograd_function.py b/test/dynamo/test_autograd_function.py\nindex 55165edd61a41d..7de264e5051743 100644\n--- a/test/dynamo/test_autograd_function.py\n+++ b/test/dynamo/test_autograd_function.py\n@@ -207,7 +207,7 @@ def backward(ctx, grad_output):\n \n class ModuleWithGradFunc(torch.nn.Module):\n     def __init__(self, func):\n-        super(ModuleWithGradFunc, self).__init__()\n+        super().__init__()\n         self.f = func.apply\n \n     def forward(self, x):\n@@ -336,7 +336,7 @@ def backward(ctx, grad_output):\n \n         class MyMod(torch.nn.Module):\n             def __init__(self):\n-                super(MyMod, self).__init__()\n+                super().__init__()\n                 self.gamma = torch.nn.Parameter(torch.rand([4, 128, 32, 32]))\n \n             def forward(self, x):\ndiff --git a/test/dynamo/test_compile.py b/test/dynamo/test_compile.py\nindex e3847cbb2ae121..5b2de2b7b3867f 100644\n--- a/test/dynamo/test_compile.py\n+++ b/test/dynamo/test_compile.py\n@@ -11,7 +11,7 @@\n \n class ToyModel(torch.nn.Module):\n     def __init__(self):\n-        super(ToyModel, self).__init__()\n+        super().__init__()\n         self.linear = torch.nn.Linear(10, 10)\n         self.relu = torch.nn.ReLU()\n \ndiff --git a/test/dynamo/test_logging.py b/test/dynamo/test_logging.py\nindex 183910f6db3510..eed99681e2c04e 100644\n--- a/test/dynamo/test_logging.py\n+++ b/test/dynamo/test_logging.py\n@@ -157,7 +157,7 @@ def throw(x):\n     def test_ddp_graphs(self, records):\n         class ToyModel(torch.nn.Module):\n             def __init__(self):\n-                super(ToyModel, self).__init__()\n+                super().__init__()\n                 self.layers = torch.nn.Sequential(\n                     torch.nn.Linear(1024, 1024),\n                     torch.nn.Linear(1024, 1024),\ndiff --git a/test/dynamo/test_misc.py b/test/dynamo/test_misc.py\nindex 6b020adc0d8007..6ec0acf8054fd3 100644\n--- a/test/dynamo/test_misc.py\n+++ b/test/dynamo/test_misc.py\n@@ -823,7 +823,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 3)\n@@ -837,7 +837,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 2)\n@@ -2202,7 +2202,7 @@ def fn():\n def fn():\n     foo.bar(1, 2, 3)\n {str(chr(10)).join(' ' * 4 + 'x' + str(i) + ' = 1' for i in range(1 << 9))}\n-    l = [{str(' ').join('x' + str(i) + ',' for i in range(1 << 9))}]\n+    l = [{' '.join('x' + str(i) + ',' for i in range(1 << 9))}]\n         \"\"\"\n         locals = {}\n         exec(fn_str, {}, locals)\n@@ -3087,7 +3087,7 @@ def foo(self, memo=None, prefix=\"\", remove_duplicate=False):\n                     memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n                 ):\n                     for pn, p in self.named_parameters():\n-                        fpn = \"%s.%s\" % (mn, pn) if mn else pn\n+                        fpn = f\"{mn}.{pn}\" if mn else pn\n                         self.names.append(fpn)\n \n         # Test plain recurse\n@@ -5032,11 +5032,11 @@ def test_compute_exception_table_nested(self):\n             (15, 16, 7),\n             (17, 17, 6),\n         ]\n-        self.assertEquals(len(tab), len(expected))\n+        self.assertEqual(len(tab), len(expected))\n         for entry, exp in zip(tab, expected):\n-            self.assertEquals(entry.start, exp[0] * 2)\n-            self.assertEquals(entry.end, exp[1] * 2)\n-            self.assertEquals(entry.target, exp[2] * 2)\n+            self.assertEqual(entry.start, exp[0] * 2)\n+            self.assertEqual(entry.end, exp[1] * 2)\n+            self.assertEqual(entry.target, exp[2] * 2)\n \n     @skipIfNotPy311\n     def test_remove_dead_code_with_exn_table_entries(self):\n@@ -5060,17 +5060,17 @@ def test_remove_dead_code_with_exn_table_entries(self):\n         )\n         bytecode_transformation.propagate_inst_exn_table_entries(insts)\n         insts = bytecode_analysis.remove_dead_code(insts)\n-        self.assertEquals(len(insts), 5)\n+        self.assertEqual(len(insts), 5)\n         self.assertNotIn(exn_start, insts)\n         self.assertNotIn(exn_end, insts)\n         self.assertIn(target2, insts)\n         self.assertIn(target3, insts)\n         bytecode_transformation.update_offsets(insts)\n         tab = bytecode_transformation.compute_exception_table(insts)\n-        self.assertEquals(len(tab), 1)\n-        self.assertEquals(tab[0].start, 2)\n-        self.assertEquals(tab[0].end, 4)\n-        self.assertEquals(tab[0].target, 6)\n+        self.assertEqual(len(tab), 1)\n+        self.assertEqual(tab[0].start, 2)\n+        self.assertEqual(tab[0].end, 4)\n+        self.assertEqual(tab[0].target, 6)\n \n     def test_unhandled_exception_in_dynamo(self):\n         # traceback.format_exc() approximates an unhandled exception\n@@ -5757,7 +5757,7 @@ def guard(L):\n     def test_dynamo_compiling_fake_tensor_to_vararg_int(self):\n         class MyModule(torch.nn.Module):\n             def __init__(self):\n-                super(MyModule, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 # use numpy int so it's wrapped as fake tensor in dynamo\n@@ -5776,7 +5776,7 @@ def forward(self, x):\n     def test_scalar_tensor_is_equivalent_to_symint_argument(self):\n         class GumbelTopKSampler(torch.nn.Module):\n             def __init__(self, T, k):\n-                super(GumbelTopKSampler, self).__init__()\n+                super().__init__()\n                 self.T = torch.nn.Parameter(\n                     torch.tensor(T, dtype=torch.float32), requires_grad=False\n                 )\n@@ -5803,7 +5803,7 @@ def forward(self, logits):\n     def test_scalar_tensor_is_equivalent_to_symint_list_argument(self):\n         class Jitter(torch.nn.Module):\n             def __init__(self, jitter_val):\n-                super(Jitter, self).__init__()\n+                super().__init__()\n                 self.jitter_val = jitter_val\n \n             def roll_tensor(self, input):\n@@ -5986,7 +5986,7 @@ def _prepare_for_translation_validator(self):\n \n         # Z3 symbols.\n         [validator.add_var(s, int) for s in (s0, s1, s2)]\n-        z0, z1, z2 = [validator.z3var(s) for s in (s0, s1, s2)]\n+        z0, z1, z2 = (validator.z3var(s) for s in (s0, s1, s2))\n \n         return (s0, s1, s2), (z0, z1, z2), validator\n \ndiff --git a/test/dynamo/test_modules.py b/test/dynamo/test_modules.py\nindex 03ef4f07305454..5a881d0053ec1d 100644\n--- a/test/dynamo/test_modules.py\n+++ b/test/dynamo/test_modules.py\n@@ -762,7 +762,7 @@ def forward(self, x):\n \n class ConvCallSuperForwardDirectly(torch.nn.Conv1d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -770,13 +770,13 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n         )\n \n     def forward(self, inputs, mask=None):\n-        outputs = super(ConvCallSuperForwardDirectly, self).forward(inputs)\n+        outputs = super().forward(inputs)\n         return outputs\n \n \n class ConvTransposeCallSuperForwardDirectly(torch.nn.ConvTranspose2d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvTransposeCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -785,7 +785,7 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n \n     def forward(self, x):\n         if x.numel() > 0:\n-            return super(ConvTransposeCallSuperForwardDirectly, self).forward(x)\n+            return super().forward(x)\n         output_shape = [\n             ((i - 1) * d - 2 * p + (di * (k - 1) + 1) + op)\n             for i, p, di, k, d, op in zip(\n@@ -923,7 +923,7 @@ def forward(self, x):\n class SequentialWithDuplicatedModule(torch.nn.Module):\n     # Sequential module(self.layer) contains three duplicated ReLU module.\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             torch.nn.Linear(10, 20),\n@@ -940,7 +940,7 @@ def forward(self, x):\n \n class SequentialWithDuplicatedModule2(torch.nn.Module):\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule2, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             collections.OrderedDict(\ndiff --git a/test/dynamo/test_profiler.py b/test/dynamo/test_profiler.py\nindex 7f58d99863d093..bec7adb33eda98 100644\n--- a/test/dynamo/test_profiler.py\n+++ b/test/dynamo/test_profiler.py\n@@ -20,7 +20,7 @@ def inner_fn(x):\n         def outer_fn(x, y):\n             return inner_fn(x) * y\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             outer_fn(x, y)\n@@ -40,7 +40,7 @@ def test_dynamo_timed_profiling_backend_compile(self):\n         def fn(x, y):\n             return x.sin() * y.cos()\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             torch._dynamo.optimize(\"aot_eager\")(fn)(x, y)\ndiff --git a/test/dynamo/test_repros.py b/test/dynamo/test_repros.py\nindex 2e84776ea76580..77d8859541472c 100644\n--- a/test/dynamo/test_repros.py\n+++ b/test/dynamo/test_repros.py\n@@ -2632,7 +2632,7 @@ def test_error_return_without_exception_set(self):\n         # https://github.com/pytorch/pytorch/issues/93781\n         @torch.compile\n         def f():\n-            _generator_type = type((_ for _ in ()))\n+            _generator_type = type(_ for _ in ())\n \n         self.assertNoUnraisable(f)\n \ndiff --git a/test/error_messages/storage.py b/test/error_messages/storage.py\nindex f3053d862a220c..b33b86e0908a95 100644\n--- a/test/error_messages/storage.py\n+++ b/test/error_messages/storage.py\n@@ -14,7 +14,7 @@ def check_error(desc, fn, *required_substrings):\n         for sub in required_substrings:\n             assert sub in error_message\n         return\n-    raise AssertionError(\"given function ({}) didn't raise an error\".format(desc))\n+    raise AssertionError(f\"given function ({desc}) didn't raise an error\")\n \n check_error(\n     'Wrong argument types',\ndiff --git a/test/export/test_db.py b/test/export/test_db.py\nindex 10d149e096be9d..bfa57baf214c8f 100644\n--- a/test/export/test_db.py\n+++ b/test/export/test_db.py\n@@ -23,7 +23,7 @@ class ExampleTests(TestCase):\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.SUPPORTED).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -51,7 +51,7 @@ def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.NOT_SUPPORTED_YET).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -73,7 +73,7 @@ def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n             ).items()\n             for rewrite_case in get_rewrite_cases(case)\n         ],\n-        name_fn=lambda name, case: \"case_{}_{}\".format(name, case.name),\n+        name_fn=lambda name, case: f\"case_{name}_{case.name}\",\n     )\n     def test_exportdb_not_supported_rewrite(\n         self, name: str, rewrite_case: ExportCase\ndiff --git a/test/export/test_serialize.py b/test/export/test_serialize.py\nindex 01bb32ad2c791b..c3942936e2bc55 100644\n--- a/test/export/test_serialize.py\n+++ b/test/export/test_serialize.py\n@@ -361,7 +361,7 @@ def f(x, y):\n     @parametrize(\n         \"name,case\",\n         get_filtered_export_db_tests(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\ndiff --git a/test/forward_backward_compatibility/check_forward_backward_compatibility.py b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\nindex 886ad32a24bafb..cf4ce8def1adb7 100644\n--- a/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n+++ b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n@@ -501,7 +501,7 @@ def check_fc(existing_schemas):\n     args = parser.parse_args()\n     existing_schema_dict = {}\n     slist = []\n-    with open(args.existing_schemas, \"r\") as f:\n+    with open(args.existing_schemas) as f:\n         while True:\n             line = f.readline()\n             if not line:\ndiff --git a/test/functorch/test_aotdispatch.py b/test/functorch/test_aotdispatch.py\nindex e4357ce0bcafd8..ca92201fedc5ea 100644\n--- a/test/functorch/test_aotdispatch.py\n+++ b/test/functorch/test_aotdispatch.py\n@@ -1805,8 +1805,8 @@ def test_batch_norm_amp(self):\n         device = \"cuda\"\n         input_dtype = torch.float16\n         param_dtype = torch.float32\n-        weight, bias = [torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2)]\n-        running_mean, running_var = [torch.ones(64, device=device, dtype=param_dtype) for _ in range(2)]\n+        weight, bias = (torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2))\n+        running_mean, running_var = (torch.ones(64, device=device, dtype=param_dtype) for _ in range(2))\n \n         def bn(x):\n             return torch.ops.aten.cudnn_batch_norm(\ndiff --git a/test/functorch/test_vmap.py b/test/functorch/test_vmap.py\nindex a0f6e077004344..81b980edd6d006 100644\n--- a/test/functorch/test_vmap.py\n+++ b/test/functorch/test_vmap.py\n@@ -3438,7 +3438,7 @@ def test():\n             check_shape_only = op.name in ('empty_like', 'new_empty')\n             for sample_input in sample_inputs_itr:\n                 args = (sample_input.input,) + sample_input.args\n-                if not any((isinstance(arg, torch.Tensor) for arg in args)):\n+                if not any(isinstance(arg, torch.Tensor) for arg in args):\n                     # Atleast one tensor required for vmap.\n                     continue\n                 kwargs = sample_input.kwargs\ndiff --git a/test/fx/test_future.py b/test/fx/test_future.py\nindex 4f093de54b4f84..4525f678eaeb6c 100644\n--- a/test/fx/test_future.py\n+++ b/test/fx/test_future.py\n@@ -16,7 +16,7 @@ def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n \n # Forward references\n class M2(torch.nn.Module):\n-    def forward(self, x: 'torch.Tensor', a: 'A') -> 'torch.Tensor':\n+    def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n         return a(x)\n \n # Non-torch annotation with no internal forward references\n@@ -26,7 +26,7 @@ def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n \n # Non-torch annotation with internal forward references\n class M4(torch.nn.Module):\n-    def forward(self, x: typing.List['torch.Tensor'], a: A) -> 'torch.Tensor':\n+    def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n         return a(x[0])\n \n x = torch.rand(2, 3)\ndiff --git a/test/fx/test_gradual_type.py b/test/fx/test_gradual_type.py\nindex 23c6496b3a294f..e3f83756eb2668 100644\n--- a/test/fx/test_gradual_type.py\n+++ b/test/fx/test_gradual_type.py\n@@ -990,12 +990,12 @@ def forward(self, x : TensorType((4, 3, Dyn, Dyn))):\n \n         for n in traced.graph.nodes:\n             if n.target == 'conv1':\n-                assert n.type == TensorType((4, 6, sympy.floor((sympy.symbols('~0') - 4)),\n-                                             sympy.floor((sympy.symbols('~1') - 4))))\n+                assert n.type == TensorType((4, 6, sympy.floor(sympy.symbols('~0') - 4),\n+                                             sympy.floor(sympy.symbols('~1') - 4)))\n \n             elif n.target == 'conv2':\n-                assert n.type == TensorType((4, 16, sympy.floor((sympy.symbols('~4') - 4)),\n-                                             sympy.floor((sympy.symbols('~5') - 4))))\n+                assert n.type == TensorType((4, 16, sympy.floor(sympy.symbols('~4') - 4),\n+                                             sympy.floor(sympy.symbols('~5') - 4)))\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py\nindex 70c69ff5cef47a..f6b268abc72a3f 100644\n--- a/torch/_decomp/decompositions.py\n+++ b/torch/_decomp/decompositions.py\n@@ -1331,10 +1331,10 @@ def native_layer_norm_backward(\n     input_shape = input.shape\n     input_ndim = input.dim()\n     computation_dtype = utils.get_computation_dtype(input.dtype)\n-    grad_out_cast, input_cast, weight_cast, bias_cast = [\n+    grad_out_cast, input_cast, weight_cast, bias_cast = (\n         x.to(computation_dtype).contiguous() if x is not None else x\n         for x in (grad_out, input, weight, bias)\n-    ]\n+    )\n     assert grad_out_cast is not None\n \n     axis = input_ndim - len(normalized_shape)\n@@ -1745,7 +1745,7 @@ def native_batch_norm_backward(\n         running_var_cast,\n         save_mean_cast,\n         save_invstd_cast,\n-    ) = [\n+    ) = (\n         x.to(computation_dtype) if x is not None else x\n         for x in (\n             grad_out,\n@@ -1756,7 +1756,7 @@ def native_batch_norm_backward(\n             save_mean,\n             save_invstd,\n         )\n-    ]\n+    )\n     input_shape = input.shape\n     input_rank = input.dim()\n     assert input_rank >= 2, \"rank of the input must be at least 2\"\n@@ -3123,7 +3123,7 @@ def get_coeff(ofs: int) -> Tensor:\n             )\n             return _upsample_cubic_interp1d(cs, tx.unsqueeze(1))\n \n-        coeffs = tuple((get_coeff(ofs) for ofs in range(4)))\n+        coeffs = tuple(get_coeff(ofs) for ofs in range(4))\n         return _upsample_cubic_interp1d(coeffs, ty.unsqueeze(1))\n \n \n@@ -3371,10 +3371,10 @@ def load_bounded(ys, xs):\n         return aten._unsafe_index(a, [N_idx, C_idx, y_idx, x_idx])\n \n     def get_x_interp(y):\n-        coeffs_x = tuple((load_bounded(y, x_ofs) for x_ofs in ixs_ofs))\n+        coeffs_x = tuple(load_bounded(y, x_ofs) for x_ofs in ixs_ofs)\n         return _upsample_cubic_interp1d(coeffs_x, t_x)\n \n-    coeffs_y = tuple((get_x_interp(y_ofs) for y_ofs in iys_ofs))\n+    coeffs_y = tuple(get_x_interp(y_ofs) for y_ofs in iys_ofs)\n     result = _upsample_cubic_interp1d(coeffs_y, t_y)\n \n     # convert output to correct memory format, if necessary\ndiff --git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py\nindex 4d7c98aa222536..bf7a61b0793654 100644\n--- a/torch/_dynamo/debug_utils.py\n+++ b/torch/_dynamo/debug_utils.py\n@@ -385,11 +385,9 @@ def same_two_models(\n         # This means that the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return True.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph.\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph.\"\n         )\n         return True\n \n@@ -465,11 +463,9 @@ def backend_accuracy_fails(\n         # This means that the the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return False.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph\"\n         )\n         return False\n \ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..0bb1dbf7342a44 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -750,9 +750,7 @@ def __init__(\n \n         self.new_args = []\n         for i in range(0, len(flat_args)):\n-            arg = super(FlattenInputOutputSignature, self).placeholder(\n-                f\"arg{i}\", (), {}\n-            )\n+            arg = super().placeholder(f\"arg{i}\", (), {})\n             if i in matched_input_elements_to_fake:\n                 arg.node.meta[\"val\"] = matched_input_elements_to_fake[i]\n             else:\ndiff --git a/torch/_dynamo/output_graph.py b/torch/_dynamo/output_graph.py\nindex f6527f9b2de356..667b2363cff694 100644\n--- a/torch/_dynamo/output_graph.py\n+++ b/torch/_dynamo/output_graph.py\n@@ -1073,7 +1073,7 @@ class SubgraphTracer(fx.Tracer):\n     \"\"\"\n \n     def __init__(self, output_graph, parent=None):\n-        super(SubgraphTracer, self).__init__()\n+        super().__init__()\n         self.output_graph = weakref.proxy(output_graph)\n         self.graph = torch.fx.Graph()\n         # Map from graph input name to its placeholder proxy object, where the\ndiff --git a/torch/_dynamo/resume_execution.py b/torch/_dynamo/resume_execution.py\nindex a3344f3d69bca8..f3eafba1979470 100644\n--- a/torch/_dynamo/resume_execution.py\n+++ b/torch/_dynamo/resume_execution.py\n@@ -490,13 +490,13 @@ def find_new_offset(\n             instructions: List[Instruction], code_options: Dict[str, Any]\n         ):\n             nonlocal new_offset\n-            (target,) = [i for i in instructions if i.offset == offset]\n+            (target,) = (i for i in instructions if i.offset == offset)\n             # match the functions starting at the last instruction as we have added a prefix\n-            (new_target,) = [\n+            (new_target,) = (\n                 i2\n                 for i1, i2 in zip(reversed(instructions), reversed(meta.instructions))\n                 if i1 is target\n-            ]\n+            )\n             assert target.opcode == new_target.opcode\n             new_offset = new_target.offset\n \ndiff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py\nindex 75a44965b63a3e..46d7526ebbc57d 100644\n--- a/torch/_dynamo/symbolic_convert.py\n+++ b/torch/_dynamo/symbolic_convert.py\n@@ -888,7 +888,7 @@ def resolve_name(self, name, package, level):\n         if len(bits) < level:\n             raise ImportError(\"attempted relative import beyond top-level package\")\n         base = bits[0]\n-        return \"{}.{}\".format(base, name) if name else base\n+        return f\"{base}.{name}\" if name else base\n \n     def calc_package(self):\n         \"\"\"\n@@ -1840,7 +1840,7 @@ def format_frame_summary(self, additional_stack_frames=None):\n             additional_stack_frames = []\n         return \"\".join(\n             traceback.format_list(\n-                ([self.frame_summary()] + list(reversed(additional_stack_frames)))\n+                [self.frame_summary()] + list(reversed(additional_stack_frames))\n             )\n         )\n \ndiff --git a/torch/_dynamo/test_minifier_common.py b/torch/_dynamo/test_minifier_common.py\nindex 757e92d2f23b51..e1eadd6da8a595 100644\n--- a/torch/_dynamo/test_minifier_common.py\n+++ b/torch/_dynamo/test_minifier_common.py\n@@ -86,7 +86,7 @@ def _maybe_subprocess_run(self, args, *, isolate, cwd=None):\n                 args = [\"-c\"]\n             else:\n                 assert len(args) >= 2, args\n-                with open(args[1], \"r\") as f:\n+                with open(args[1]) as f:\n                     code = f.read()\n                 args = args[1:]\n \n@@ -156,7 +156,7 @@ def _run_test_code(self, code, *, isolate):\n     def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n         self.assertIsNotNone(repro_dir)\n         launch_file = os.path.join(repro_dir, \"minifier_launcher.py\")\n-        with open(launch_file, \"r\") as f:\n+        with open(launch_file) as f:\n             launch_code = f.read()\n         self.assertTrue(os.path.exists(launch_file))\n \n@@ -175,7 +175,7 @@ def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n     def _run_repro(self, repro_dir, *, isolate=True):\n         self.assertIsNotNone(repro_dir)\n         repro_file = os.path.join(repro_dir, \"repro.py\")\n-        with open(repro_file, \"r\") as f:\n+        with open(repro_file) as f:\n             repro_code = f.read()\n         self.assertTrue(os.path.exists(repro_file))\n \ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex d2aab5a65bd4d4..c720a0e637c478 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -368,12 +368,10 @@ def _wrap(self, value):\n         elif istype(\n             value, (dict, collections.defaultdict, collections.OrderedDict)\n         ) and all(\n-            (\n-                ConstantVariable.is_literal(k)\n-                or self.tensor_can_be_dict_key(k)\n-                or isinstance(k, enum.Enum)\n-                for k in value.keys()\n-            )\n+            ConstantVariable.is_literal(k)\n+            or self.tensor_can_be_dict_key(k)\n+            or isinstance(k, enum.Enum)\n+            for k in value.keys()\n         ):\n             if not value and self.get_source().is_nn_module():\n                 # It is faster to guard on 'false' property than to guard\ndiff --git a/torch/_dynamo/variables/misc.py b/torch/_dynamo/variables/misc.py\nindex b260eab0af0fdb..e049ccfb25269d 100644\n--- a/torch/_dynamo/variables/misc.py\n+++ b/torch/_dynamo/variables/misc.py\n@@ -880,7 +880,7 @@ def as_proxy(self):\n # Used to keep track of NULLs pushed on the stack for Python 3.11 function calls\n class NullVariable(VariableTracker):\n     def __init__(self, **kwargs):\n-        super(NullVariable, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n     def __str__(self):\n         return \"NullVariable\"\ndiff --git a/torch/_export/verifier.py b/torch/_export/verifier.py\nindex 41888230e02242..73906bb6a0d7a8 100644\n--- a/torch/_export/verifier.py\n+++ b/torch/_export/verifier.py\n@@ -38,7 +38,7 @@ def _check_is_fake_tensor(val):\n \n     val = node.meta.get(\"val\", None)\n     if val is None or not _check_is_fake_tensor(val):\n-        raise SpecViolationError(\"Node.meta {} is missing val field.\".format(node.name))\n+        raise SpecViolationError(f\"Node.meta {node.name} is missing val field.\")\n \n \n @compatibility(is_backward_compatible=False)\n@@ -71,7 +71,7 @@ def check_valid_op(self, op):\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         # All ops functional\n@@ -87,7 +87,7 @@ def check_valid(self, gm: GraphModule) -> None:  # noqa: C901\n             # TODO(T140410192): should have fake tensor for all dialects\n             if node.op in {\"call_module\", \"call_method\"}:\n                 raise SpecViolationError(\n-                    \"call_module is not valid: got a class '{}' \".format(node.target),\n+                    f\"call_module is not valid: got a class '{node.target}' \",\n                 )\n \n             if node.op == \"call_function\":\n@@ -122,7 +122,7 @@ def check_valid_op(self, op) -> None:\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         if (\ndiff --git a/torch/_functorch/eager_transforms.py b/torch/_functorch/eager_transforms.py\nindex a25a7bc456bec1..4ba72eb2152c6b 100644\n--- a/torch/_functorch/eager_transforms.py\n+++ b/torch/_functorch/eager_transforms.py\n@@ -548,7 +548,7 @@ def compute_jacobian_stacked():\n             # Iterate and concat the jacobians of different\n             # inputs.\n             for idx in range(len(flat_primals)):\n-                r = tuple((r_[idx] for r_ in chunked_results))\n+                r = tuple(r_[idx] for r_ in chunked_results)\n                 flat_results.append(torch.cat(r, 0))\n \n             return flat_results\ndiff --git a/torch/_functorch/pytree_hacks.py b/torch/_functorch/pytree_hacks.py\nindex 3694a53d7debb0..61bcdfbbf38b16 100644\n--- a/torch/_functorch/pytree_hacks.py\n+++ b/torch/_functorch/pytree_hacks.py\n@@ -13,7 +13,7 @@ def tree_map_(fn_, pytree):\n     return pytree\n \n \n-class PlaceHolder():\n+class PlaceHolder:\n     def __repr__(self):\n         return '*'\n \ndiff --git a/torch/_prims/__init__.py b/torch/_prims/__init__.py\nindex 7e8a37da76b06d..ac447dab410a07 100644\n--- a/torch/_prims/__init__.py\n+++ b/torch/_prims/__init__.py\n@@ -1437,7 +1437,7 @@ def expand_dims(\n     else:\n         dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]\n     if len(set(dims)) != len(dims):\n-        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))\n+        msg = f\"Received duplicate dimensions to expand in {str(dimensions)}\"\n         raise ValueError(msg)\n \n     new_shape = list(a.shape)\n@@ -1463,35 +1463,33 @@ def _slice_meta(\n     _strides = strides if strides is not None else [1] * len(start_indices)\n \n     if a.ndim != len(start_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with start_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with start_indices of length {}!\".format(\n             a.ndim, len(start_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(limit_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with limit_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with limit_indices of length {}!\".format(\n             a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(_strides):\n-        msg = (\n-            \"Attempting to slice tensor of rank {0} with strides of length {1}!\".format(\n-                a.ndim, len(limit_indices)\n-            )\n+        msg = \"Attempting to slice tensor of rank {} with strides of length {}!\".format(\n+            a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     for x, y in zip(start_indices, a.shape):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative start index of {0}!\".format(\n+            msg = \"Attempting to slice a tensor with a negative start index of {}!\".format(\n                 x\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than\"\n-                \" the length of its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a start index in {} is greater than\"\n+                \" the length of its corresponding dimension in shape {}\".format(\n                     start_indices, a.shape\n                 )\n             )\n@@ -1499,30 +1497,30 @@ def _slice_meta(\n \n     for x, y, z in zip(limit_indices, a.shape, start_indices):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative stop index of {0}!\".format(\n-                x\n+            msg = (\n+                \"Attempting to slice a tensor with a negative stop index of {}!\".format(\n+                    x\n+                )\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a stop index in {0} is greater than the length of \"\n-                \" its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a stop index in {} is greater than the length of \"\n+                \" its corresponding dimension in shape {}\".format(\n                     limit_indices, a.shape\n                 )\n             )\n             raise ValueError(msg)\n         if x < z:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than \"\n-                \" its corresponding stop index {1}\".format(x, z)\n+                \"Attempting to slice a tensor but a start index in {} is greater than \"\n+                \" its corresponding stop index {}\".format(x, z)\n             )\n \n     for x in _strides:\n         if x <= 0:\n-            msg = (\n-                \"Attempting to slice a tensor with a non-positive step of {0}!\".format(\n-                    x\n-                )\n+            msg = \"Attempting to slice a tensor with a non-positive step of {}!\".format(\n+                x\n             )\n             raise ValueError(msg)\n \n@@ -1581,38 +1579,38 @@ def _slice_in_dim_meta(\n     axis: int = 0,\n ) -> TensorLikeType:\n     if axis < 0:\n-        msg = \"slice_in_dim: received a negative axis {0}\".format(axis)\n+        msg = f\"slice_in_dim: received a negative axis {axis}\"\n         raise ValueError(msg)\n     if axis >= a.ndim:\n-        msg = \"slice_in_dim: axis {0} is greater or equal to the rank {1} of the tensor\".format(\n+        msg = \"slice_in_dim: axis {} is greater or equal to the rank {} of the tensor\".format(\n             axis, a.ndim\n         )\n         raise ValueError(msg)\n \n     if start_index < 0:\n-        msg = \"slice_in_dim: received a negative start_index {0}\".format(start_index)\n+        msg = f\"slice_in_dim: received a negative start_index {start_index}\"\n         raise ValueError(msg)\n \n     if start_index > a.shape[axis]:\n-        msg = \"slice_in_dim: start_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: start_index is greater than the length {} of dimension {}\".format(\n             start_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index > a.shape[axis]:\n-        msg = \"slice_in_dim: limit_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: limit_index is greater than the length {} of dimension {}\".format(\n             limit_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index < start_index:\n-        msg = \"slice_in_dim: received a limit_index {0} less than the start_index {1}\".format(\n+        msg = \"slice_in_dim: received a limit_index {} less than the start_index {}\".format(\n             limit_index, start_index\n         )\n         raise ValueError(msg)\n \n     if stride < 0:\n-        msg = \"slice_in_dim: received a non-positive stride of {0}!\".format(stride)\n+        msg = f\"slice_in_dim: received a non-positive stride of {stride}!\"\n         raise ValueError(msg)\n \n     start_indices = [0] * a.ndim\n@@ -1667,7 +1665,7 @@ def _split_dim_meta(a: TensorLikeType, dim: int, outer_length: int) -> TensorLik\n     inner_length = a.shape[dim] // outer_length\n \n     if (a.shape[dim] % outer_length) != 0:\n-        msg = \"Attempting to split dimension of length {0}, but outer length of {1} divides it with a remainder!\".format(\n+        msg = \"Attempting to split dimension of length {}, but outer length of {} divides it with a remainder!\".format(\n             a.shape[dim], outer_length\n         )\n         raise ValueError(msg)\n@@ -1746,13 +1744,13 @@ def _squeeze_meta(a: TensorLikeType, dimensions: Sequence) -> TensorLikeType:\n \n def _transpose_meta(a: TensorLikeType, permutation: DimsSequenceType) -> TensorLikeType:\n     if a.ndim != len(permutation):\n-        msg = \"Attempting to permute a tensor of rank {0}, but received a permutation of length {1}!\".format(\n+        msg = \"Attempting to permute a tensor of rank {}, but received a permutation of length {}!\".format(\n             a.ndim, len(permutation)\n         )\n         raise ValueError(msg)\n \n     if not utils.is_valid_permutation(a.ndim, permutation):\n-        msg = \"Received an invalid permutation, {0}!\".format(permutation)\n+        msg = f\"Received an invalid permutation, {permutation}!\"\n         raise ValueError(msg)\n \n     new_shape = [0] * a.ndim\n@@ -1938,7 +1936,7 @@ def _reshape_meta(a: TensorLikeType, shape: ShapeType):\n     # same number of elements\n     numel = reduce(operator.mul, shape)\n     if numel != a.numel():\n-        msg = \"Attempting to reshape a tensor with {0} elements to a shape with {1} elements!\".format(\n+        msg = \"Attempting to reshape a tensor with {} elements to a shape with {} elements!\".format(\n             a.numel(), numel\n         )\n         raise ValueError(msg)\n@@ -2190,7 +2188,7 @@ def _copy_to_meta(a: TensorLikeType, b: TensorLikeType):\n \n     # Validates the tensors have the same number of elements\n     if a.numel() != b.numel():\n-        msg = \"Attempting to copy {0} elements to a tensor with {1} elements!\".format(\n+        msg = \"Attempting to copy {} elements to a tensor with {} elements!\".format(\n             b.numel(), a.numel()\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_prims/executor.py b/torch/_prims/executor.py\nindex 2d8d815f063809..325ac67a665cc3 100644\n--- a/torch/_prims/executor.py\n+++ b/torch/_prims/executor.py\n@@ -28,7 +28,7 @@ def execute(\n     elif executor == \"strictly_nvfuser\":\n         return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)\n \n-    msg = \"Received unexpected value for 'executor': {0}. Allowed values are: aten, nvfuser.\".format(\n+    msg = \"Received unexpected value for 'executor': {}. Allowed values are: aten, nvfuser.\".format(\n         executor\n     )\n     raise ValueError(msg)\ndiff --git a/torch/_prims/nvfuser_executor.py b/torch/_prims/nvfuser_executor.py\nindex d0f51e928650c4..c1e61c1bb72f1f 100644\n--- a/torch/_prims/nvfuser_executor.py\n+++ b/torch/_prims/nvfuser_executor.py\n@@ -282,7 +282,7 @@ def nvfuser_execute(gm: GraphModule, *args, executor_parameters=None):\n \n         if get_nvprim_dump_nvtx():\n             torch.cuda.nvtx.range_push(\n-                \"fusion: {0}, graph: {1}\".format(\n+                \"fusion: {}, graph: {}\".format(\n                     fusion.id(),\n                     str(\n                         [\n@@ -475,7 +475,7 @@ def maybe_partition_graph(\n class NVTXInterpreter(torch.fx.Interpreter):\n     def run_node(self, n):\n         torch.cuda.nvtx.range_push(\n-            \"name: {0}, args: {1}, op: {2}, kwargs: {3}\".format(\n+            \"name: {}, args: {}, op: {}, kwargs: {}\".format(\n                 n.name, n.args, n.op, n.kwargs\n             )\n         )\ndiff --git a/torch/_prims_common/__init__.py b/torch/_prims_common/__init__.py\nindex f8033bb5780f74..4800966f3e2ff5 100644\n--- a/torch/_prims_common/__init__.py\n+++ b/torch/_prims_common/__init__.py\n@@ -120,11 +120,11 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n     assert isinstance(b, TensorLike)\n \n     if not same_shape(a.shape, b.shape):\n-        msg = \"Shapes {0} and {1} are not equal!\".format(a.shape, b.shape)\n+        msg = f\"Shapes {a.shape} and {b.shape} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.dtype != b.dtype:\n-        msg = \"Dtypes {0} and {1} are not equal!\".format(a.dtype, b.dtype)\n+        msg = f\"Dtypes {a.dtype} and {b.dtype} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.device != b.device:\n@@ -135,7 +135,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         ):\n             pass\n         else:\n-            msg = \"Devices {0} and {1} are not equal!\".format(a.device, b.device)\n+            msg = f\"Devices {a.device} and {b.device} are not equal!\"\n             raise AssertionError(msg)\n \n     # Stride checking is currently disabled, see https://github.com/pytorch/pytorch/issues/78050\n@@ -143,7 +143,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         same_strides, idx = check_significant_strides(a, b)\n         if not same_strides:\n             msg = (\n-                \"Stride mismatch! Strides are {0} and {1} (mismatched at {2})!\".format(\n+                \"Stride mismatch! Strides are {} and {} (mismatched at {})!\".format(\n                     a.stride(), b.stride(), idx\n                 )\n             )\n@@ -151,7 +151,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n \n         if a.storage_offset() != b.storage_offset():\n             msg = (\n-                \"Storage offset mismatch! Storage offsets are {0} and {1}!\".format(\n+                \"Storage offset mismatch! Storage offsets are {} and {}!\".format(\n                     a.storage_offset(), b.storage_offset()\n                 )\n             )\n@@ -584,7 +584,7 @@ def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int:\n \n     if _idx < 0 or _idx >= rank:\n         # Same error message as in aten/src/ATen/WrapDimUtils.h:49\n-        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(\n+        msg = \"Dimension out of range (expected to be in range of [{}, {}], but got {})\".format(\n             -rank, rank - 1, idx\n         )\n         raise IndexError(msg)\n@@ -710,7 +710,7 @@ def check_same_shape(*args, allow_cpu_scalar_tensors: bool):\n                 shape = arg.shape\n \n             if not is_same_shape(shape, arg.shape):\n-                msg = \"Shape {0} is not the expected shape {1}!\".format(\n+                msg = \"Shape {} is not the expected shape {}!\".format(\n                     arg.shape, shape\n                 )\n                 raise RuntimeError(msg)\n@@ -1102,7 +1102,7 @@ def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool:\n         if fn(cast_from):\n             return False\n \n-    raise ValueError(\"Received unknown dtypes {0}, {1}!\".format(cast_to, cast_from))\n+    raise ValueError(f\"Received unknown dtypes {cast_to}, {cast_from}!\")\n \n \n def check_same_dtype(*args):\n@@ -1340,7 +1340,7 @@ def elementwise_dtypes(\n     for x in args:\n         if not isinstance(x, (Number, TensorLike, sympy.Symbol)):\n             msg = (\n-                \"Unexpected type {0} when computing elementwise type promotion!\".format(\n+                \"Unexpected type {} when computing elementwise type promotion!\".format(\n                     str(type(x))\n                 )\n             )\n@@ -1424,7 +1424,7 @@ def _find_highest_dtype_filtered(\n         return get_computation_dtype(result_dtype), torch.bool\n     else:\n         raise ValueError(\n-            \"Unknown type promotion kind {0}\".format(str(type_promotion_kind))\n+            f\"Unknown type promotion kind {str(type_promotion_kind)}\"\n         )\n \n \n@@ -1648,8 +1648,8 @@ def check_in_bounds_for_storage(\n     required_length = compute_required_storage_length(shape, strides, storage_offset)\n     if a.size() < required_length:\n         msg = (\n-            \"Can't view a storage of size {0} with an offset of {1}, shape of {2}, and strides of {3}, \"\n-            \"which requires a storage of size {4}\".format(\n+            \"Can't view a storage of size {} with an offset of {}, shape of {}, and strides of {}, \"\n+            \"which requires a storage of size {}\".format(\n                 a.size(), storage_offset, str(shape), str(strides), required_length\n             )\n         )\n@@ -1671,9 +1671,9 @@ def check(\n     .. note:: This function is planned for removal in the future. Please use\n         `torch._check*` functions instead.\n     \"\"\"\n-    warnings.warn(DeprecationWarning((\n+    warnings.warn(DeprecationWarning(\n         \"'torch._prims_common.check' will be removed in the future. Please use \"\n-        \"'torch._check*' functions instead\")))\n+        \"'torch._check*' functions instead\"))\n     torch._check_with(exc_type, b, s)\n \n \ndiff --git a/torch/_prims_common/wrappers.py b/torch/_prims_common/wrappers.py\nindex 938465cac36318..c9755de3e0da63 100644\n--- a/torch/_prims_common/wrappers.py\n+++ b/torch/_prims_common/wrappers.py\n@@ -48,16 +48,16 @@ def _maybe_convert_to_dtype(a, dtype):\n         return None\n \n     raise ValueError(\n-        \"Received type {0} that is neither a tensor or a number!\".format(type(a))\n+        f\"Received type {type(a)} that is neither a tensor or a number!\"\n     )\n \n \n def _maybe_convert_to_type(a: NumberType, typ: type) -> NumberType:\n     if not isinstance(a, Number):\n-        msg = \"Found unknown type {0} when trying to convert scalars!\".format(type(a))\n+        msg = f\"Found unknown type {type(a)} when trying to convert scalars!\"\n         raise ValueError(msg)\n     if not utils.is_weakly_lesser_type(type(a), typ):\n-        msg = \"Scalar {0} of type {1} cannot be safely cast to type {2}!\".format(\n+        msg = \"Scalar {} of type {} cannot be safely cast to type {}!\".format(\n             a, type(a), typ\n         )\n         raise ValueError(msg)\n@@ -169,7 +169,7 @@ def _safe_copy_out(\n ):\n     # Checks same device\n     if copy_from.device != copy_to.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             copy_from.device, copy_to.device\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_refs/__init__.py b/torch/_refs/__init__.py\nindex 611c6e3b9a625a..53851d62a5eb44 100644\n--- a/torch/_refs/__init__.py\n+++ b/torch/_refs/__init__.py\n@@ -597,7 +597,7 @@ def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType:\n \n     python_type = utils.dtype_to_type(a.dtype)\n     if not utils.is_weakly_lesser_type(type(value), python_type):\n-        msg = \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+        msg = \"value argument of type {} cannot be safely cast to type {}!\".format(\n             type(value), python_type\n         )\n         raise ValueError(msg)\n@@ -997,10 +997,8 @@ def add(\n         if python_type != bool and not utils.is_weakly_lesser_type(\n             type(alpha), python_type\n         ):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         b = prims.mul(b, alpha)\n@@ -1069,7 +1067,7 @@ def copysign(\n     if isinstance(b, Number) and isinstance(a, Tensor):\n         b = scalar_tensor(b, dtype=a.dtype, device=a.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n-        msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+        msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n             a.device, b.device\n         )\n         raise RuntimeError(msg)\n@@ -1100,7 +1098,7 @@ def div(\n     else:\n         msg = (\n             \"div expected rounding_mode to be one of None, 'trunc', or 'floor' \"\n-            \"but found {0}.\".format(rounding_mode)\n+            \"but found {}.\".format(rounding_mode)\n         )\n         raise ValueError(msg)\n \n@@ -1218,7 +1216,7 @@ def floor_divide(\n         a = scalar_tensor(a, dtype=b.dtype, device=b.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n         if a.device == torch.device(\"cpu\"):\n-            msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+            msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n                 a.device, b.device\n             )\n             raise RuntimeError(msg)\n@@ -1378,19 +1376,19 @@ def _check_close_args(\n ) -> None:\n     torch._check_value(\n         a.dtype == b.dtype,\n-        lambda: \"{0}: Attempting to compare tensors of different dtypes {1} and {2}!\".format(\n+        lambda: \"{}: Attempting to compare tensors of different dtypes {} and {}!\".format(\n             name, a.dtype, b.dtype\n         ),\n     )\n     torch._check(\n         rtol >= 0,\n-        lambda: \"{0}: rtol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: rtol must be greater than or equal to zero, but got {}!\".format(\n             name, rtol\n         ),\n     )\n     torch._check(\n         atol >= 0,\n-        lambda: \"{0}: atol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: atol must be greater than or equal to zero, but got {}!\".format(\n             name, atol\n         ),\n     )\n@@ -1664,10 +1662,8 @@ def sub(\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         if isinstance(b, torch.Tensor):\n@@ -1759,7 +1755,7 @@ def addcdiv(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1788,7 +1784,7 @@ def addcmul(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1892,7 +1888,7 @@ def clone(\n \n def copy_to(a: Tensor, b: Tensor, *, allow_cross_device=True):\n     if not allow_cross_device and a.device != b.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             b.device, a.device\n         )\n         raise RuntimeError(msg)\n@@ -2098,7 +2094,7 @@ def _reduction(\n     assert isinstance(a, TensorLike)\n     if a.ndim > 64:\n         raise RuntimeError(\n-            \"Received a tensor with {0} dimensions, but only tensors with up to 64 dims are supported!\".format(\n+            \"Received a tensor with {} dimensions, but only tensors with up to 64 dims are supported!\".format(\n                 a.ndim\n             )\n         )\n@@ -2864,7 +2860,7 @@ def expand_as(a: Tensor, b: Tensor) -> Tensor:\n \n def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> Tuple[TensorLikeType, ...]:\n     if chunks <= 0:\n-        msg = \"Expected at least one chunk, but got {0}!\".format(chunks)\n+        msg = f\"Expected at least one chunk, but got {chunks}!\"\n         raise ValueError(msg)\n \n     dim = utils.canonicalize_dim(a.ndim, dim)\n@@ -3346,7 +3342,7 @@ def _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorL\n                 if allow_copy:\n                     return prims.reshape(a, shape)\n \n-                msg = \"Cannot view a tensor with shape {0} and strides {1} as a tensor with shape {2}!\".format(\n+                msg = \"Cannot view a tensor with shape {} and strides {} as a tensor with shape {}!\".format(\n                     a.shape, a.stride(), shape\n                 )\n                 raise ValueError(msg)\n@@ -3704,13 +3700,13 @@ def tensor_split(\n     # If indices_or_sections is a tensor, it must be a CPU Long tensor\n     if isinstance(indices_or_sections, TensorLike):\n         if not indices_or_sections.device.type == \"cpu\":\n-            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {0}\".format(\n+            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {}\".format(\n                 indices_or_sections.device\n             )\n             raise ValueError(msg)\n         if indices_or_sections.dtype != torch.long:\n             msg = \"tensor_split: if indices_or_sections is a tensor it must have long dtype, \"\n-            \" but received one with dtype {0}\".format(indices_or_sections.dtype)\n+            f\" but received one with dtype {indices_or_sections.dtype}\"\n             raise ValueError(msg)\n \n     # Case 0 -- indices_or_sections is an integer or a scalar tensor n and a is split along dim into n parts of equal-ish length\n@@ -3724,7 +3720,7 @@ def tensor_split(\n         )\n \n         if sections <= 0:\n-            msg = \"tensor_split: number of sections must be greater than 0, but was {0}\".format(\n+            msg = \"tensor_split: number of sections must be greater than 0, but was {}\".format(\n                 sections\n             )\n             raise ValueError(msg)\n@@ -3751,7 +3747,7 @@ def tensor_split(\n         if isinstance(indices_or_sections, TensorLike):\n             if indices_or_sections.ndim != 1:\n                 msg = \"tensor_split: non-scalar indices_or_sections tensors must have only one dimension, \"\n-                \"but received a tensor with {0} dimensions\".format(\n+                \"but received a tensor with {} dimensions\".format(\n                     indices_or_sections.ndim\n                 )\n                 raise ValueError(msg)\ndiff --git a/torch/_refs/nn/functional/__init__.py b/torch/_refs/nn/functional/__init__.py\nindex eaa6618379f356..ba00179c4b2d6f 100644\n--- a/torch/_refs/nn/functional/__init__.py\n+++ b/torch/_refs/nn/functional/__init__.py\n@@ -167,10 +167,8 @@ def celu(\n     if alpha is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         rhs = alpha * torch.expm1(torch.true_divide(a, alpha))  # type: ignore[arg-type]\n@@ -437,7 +435,7 @@ def softplus(\n     if beta is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(beta), python_type):\n-            msg = \"beta argument of type {0} cannot be safely cast to type {1}!\".format(\n+            msg = \"beta argument of type {} cannot be safely cast to type {}!\".format(\n                 type(beta), python_type\n             )\n             raise ValueError(msg)\n@@ -610,11 +608,9 @@ def margin_ranking_loss(\n     # loss_without_reduction = max(0, \u2212target * (input1 \u2212 input2) + margin)\n     if input1.ndim != input2.ndim or input1.ndim != target.ndim:\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(\n-                    input1.shape, input2.shape, target.shape\n-                )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(\n+                input1.shape, input2.shape, target.shape\n             )\n         )\n     _check_reduction_value(reduction)\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint.py b/torch/fx/experimental/migrate_gradual_types/constraint.py\nindex bab7c62347bbd9..0f0d23d0187490 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from torch.fx.experimental.migrate_gradual_types.operation import op_add, op_sub, op_mul, op_div, \\\n     op_mod, op_gt, op_lt, op_neq, op_eq\n from torch.fx.tensor_type import TensorType, Dyn\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\nindex fc1fae790d8300..153a8407fc4113 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n@@ -75,7 +75,7 @@ def transform_index_select(constraint, counter):\n     # if the index is valid then replace the input dimension with the new dimension\n     # otherwise the dimension will not be replaced and the clause will contain False\n     if is_valid_index == T():\n-        new_dims = copy.deepcopy((dims))\n+        new_dims = copy.deepcopy(dims)\n         new_dims[constraint.index] = constraint.dim_replace\n \n     transformed_constraint = Conj([BinConstraintT(constraint.input_var, TensorType(dims), op_eq),\n@@ -803,7 +803,7 @@ def apply_padding(e1_var: TVar,\n         broadcast_padding = []\n \n         # for every padding size, we also consider broadcasting\n-        for j in range((len(d2) - i)):\n+        for j in range(len(d2) - i):\n             broadcast_padding.append(broadcast_dim(simulate_padding, d2, d11, d12, j, True))\n \n         # we consider the possibilities for broadcasting for every dimension. Since we already\ndiff --git a/torch/fx/experimental/migrate_gradual_types/operation.py b/torch/fx/experimental/migrate_gradual_types/operation.py\nindex 68bba2d59a7608..ec2cb91bbcc179 100644\n--- a/torch/fx/experimental/migrate_gradual_types/operation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/operation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n op_add = '+'\n op_sub = '-'\n op_mul = '*'\ndiff --git a/torch/fx/experimental/symbolic_shapes.py b/torch/fx/experimental/symbolic_shapes.py\nindex 90a3c519b3c44b..70762cf1f81b8c 100644\n--- a/torch/fx/experimental/symbolic_shapes.py\n+++ b/torch/fx/experimental/symbolic_shapes.py\n@@ -1488,7 +1488,7 @@ def _print_Symbol(self, expr) -> str:\n         return self.print_source(self.symbol_to_source[expr][0])\n \n     def _print_Relational(self, expr):\n-        return '%s %s %s' % (\n+        return '{} {} {}'.format(\n             self.parenthesize(expr.lhs, precedence(expr)),\n             expr.rel_op,\n             self.parenthesize(expr.rhs, precedence(expr))\n@@ -1887,7 +1887,7 @@ def print_results(grouped, indent, result_fn):\n class ShapeEnvLoggerAdapter(logging.LoggerAdapter):\n     def process(self, msg, kwargs):\n         # TODO: Maybe suppress the envid if not DEBUG?\n-        return '%s: %s' % (self.extra['envid'], msg), kwargs\n+        return '{}: {}'.format(self.extra['envid'], msg), kwargs\n \n \n ENV_COUNTER = collections.Counter()\ndiff --git a/torch/fx/experimental/unification/multipledispatch/dispatcher.py b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\nindex c76d8c60b097c7..ac8bc7d8dd159c 100644\n--- a/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n+++ b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n@@ -205,10 +205,9 @@ def add(self, signature, func):\n             if not isinstance(typ, (type, list)):\n                 str_sig = ', '.join(c.__name__ if isinstance(c, type)\n                                     else str(c) for c in signature)\n-                raise TypeError(\"Tried to dispatch on non-type: %s\\n\"\n-                                \"In signature: <%s>\\n\"\n-                                \"In function: %s\" %\n-                                (typ, str_sig, self.name))\n+                raise TypeError(\"Tried to dispatch on non-type: {}\\n\"\n+                                \"In signature: <{}>\\n\"\n+                                \"In function: {}\".format(typ, str_sig, self.name))\n \n             # handle variadic signatures\n             if isinstance(typ, list):\n@@ -257,8 +256,7 @@ def __call__(self, *args, **kwargs):\n             func = self.dispatch(*types)\n             if not func:\n                 raise NotImplementedError(\n-                    'Could not find signature for %s: <%s>' %\n-                    (self.name, str_signature(types))) from e\n+                    f'Could not find signature for {self.name}: <{str_signature(types)}>') from e\n             self._cache[types] = func\n         try:\n             return func(*args, **kwargs)\n@@ -274,7 +272,7 @@ def __call__(self, *args, **kwargs):\n \n             raise NotImplementedError(\n                 \"Matching functions for \"\n-                \"%s: <%s> found, but none completed successfully\" % (\n+                \"{}: <{}> found, but none completed successfully\".format(\n                     self.name, str_signature(types),),) from e\n \n     def __str__(self):\n@@ -408,8 +406,7 @@ def __call__(self, *args, **kwargs):\n         types = tuple([type(arg) for arg in args])\n         func = self.dispatch(*types)\n         if not func:\n-            raise NotImplementedError('Could not find signature for %s: <%s>' %\n-                                      (self.name, str_signature(types)))\n+            raise NotImplementedError(f'Could not find signature for {self.name}: <{str_signature(types)}>')\n         return func(self.obj, *args, **kwargs)\n \n \ndiff --git a/torch/fx/interpreter.py b/torch/fx/interpreter.py\nindex 7bc5e55288b03c..6ee5706f92ee34 100644\n--- a/torch/fx/interpreter.py\n+++ b/torch/fx/interpreter.py\n@@ -139,7 +139,7 @@ def run(self, *args, initial_env : Optional[Dict[Node, Any]] = None, enable_io_p\n             except Exception as e:\n                 if self.extra_traceback:\n                     msg = f\"While executing {node.format_node()}\"\n-                    msg = '{}\\n\\n{}'.format(e.args[0], msg) if e.args else str(msg)\n+                    msg = f'{e.args[0]}\\n\\n{msg}' if e.args else str(msg)\n                     msg += f\"\\nOriginal traceback:\\n{node.stack_trace}\"\n                     e.args = (msg,) + e.args[1:]\n                     if isinstance(e, KeyError):\ndiff --git a/torch/fx/passes/utils/matcher_utils.py b/torch/fx/passes/utils/matcher_utils.py\nindex 1037b48d8eb61d..8b66561a6e280d 100644\n--- a/torch/fx/passes/utils/matcher_utils.py\n+++ b/torch/fx/passes/utils/matcher_utils.py\n@@ -30,7 +30,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class InternalMatch():\n+class InternalMatch:\n     # Nodes from which the match was found\n     anchors: List[Node]\n     # Maps nodes in the pattern subgraph to nodes in the larger graph\ndiff --git a/torch/fx/passes/utils/source_matcher_utils.py b/torch/fx/passes/utils/source_matcher_utils.py\nindex e00b9695742e36..da8cf9f0f168c4 100644\n--- a/torch/fx/passes/utils/source_matcher_utils.py\n+++ b/torch/fx/passes/utils/source_matcher_utils.py\n@@ -29,7 +29,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class SourcePartition():\n+class SourcePartition:\n     # Nodes in a particular partition\n     nodes: List[Node]\n \n"
  },
  {
    "number": 105421,
    "title": "[BE] Enable ruff's UP rules and autoformat inductor/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "98ac0fc92d7ba08c8c6d37ead83c12345cc91fb1",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105421",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105421/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105421.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105421.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105421/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105421/comments",
    "labels": [
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T01:17:44.375165Z",
    "state": "closed",
    "patch": "From f6bbcccba5d658470b72c464d9fa5f67617b0eec Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:37 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat inductor/\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py                    |  2 +-\n test/inductor/test_cuda_repro.py                   |  2 +-\n test/inductor/test_cudagraph_trees.py              |  2 +-\n test/inductor/test_fused_attention.py              |  4 ++--\n test/inductor/test_mkldnn_pattern_matcher.py       |  8 ++++----\n test/inductor/test_profiler.py                     |  4 ++--\n test/inductor/test_standalone_compile.py           |  2 +-\n test/inductor/test_torchinductor.py                |  6 +++---\n .../test_torchinductor_codegen_dynamic_shapes.py   | 14 +++++++-------\n torch/_inductor/codecache.py                       |  6 +++---\n torch/_inductor/codegen/cpp.py                     |  2 +-\n torch/_inductor/ir.py                              | 10 +++++-----\n torch/_inductor/lowering.py                        |  6 +++---\n torch/_inductor/triton_heuristics.py               |  6 ++----\n torch/_inductor/utils.py                           |  2 +-\n 15 files changed, 37 insertions(+), 39 deletions(-)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 89ea4315b1de6c..2074d8451be9cc 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -93,7 +93,7 @@ def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n     def test_conv2d_bn_mixed_dtype(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     3,\n                     16,\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..7df54f0d35a146 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -783,7 +783,7 @@ def forward(inductor_seeds, mul_4, view_15):\n     def test_issue100806(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.linear1 = torch.nn.Linear(10, 20)\n                 self.linear2 = torch.nn.Linear(20, 30)\n                 self.relu = torch.nn.ReLU()\ndiff --git a/test/inductor/test_cudagraph_trees.py b/test/inductor/test_cudagraph_trees.py\nindex 85e44c1e272f9d..8181484c419b58 100644\n--- a/test/inductor/test_cudagraph_trees.py\n+++ b/test/inductor/test_cudagraph_trees.py\n@@ -137,7 +137,7 @@ def tearDown(self):\n \n         def get_manager(self, device_index=None):\n             return torch._inductor.cudagraph_trees.get_container(\n-                (self.device_idx if not device_index else device_index)\n+                self.device_idx if not device_index else device_index\n             ).tree_manager\n \n         def get_roots(self):\ndiff --git a/test/inductor/test_fused_attention.py b/test/inductor/test_fused_attention.py\nindex e509c97b0cb04a..7163ac24de5cf3 100644\n--- a/test/inductor/test_fused_attention.py\n+++ b/test/inductor/test_fused_attention.py\n@@ -297,7 +297,7 @@ def test_pattern_fails_with_tensor_factor(self):\n         # https://github.com/pytorch/pytorch/issues/99124\n         class Model(torch.nn.Module):\n             def __init__(self, is_inv_factor):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.is_inv_factor = is_inv_factor\n \n             def forward(self, query, key, value, scale_factor) -> torch.Tensor:\n@@ -328,7 +328,7 @@ class Model(torch.nn.Module):\n             def __init__(\n                 self,\n             ):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, query, key, value, attn_mask) -> torch.Tensor:\n                 attn_weight = torch.softmax(\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex 3e07c0181994cf..9337a70b2f857e 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -374,7 +374,7 @@ def forward(self, x, negative_slope):\n     def test_conv2d_add_scalar(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n                 )\n@@ -476,7 +476,7 @@ def forward(self, x, other, alpha):\n         # we can't do the fusion when add's inputs are same tensor.\n         class Model2(torch.nn.Module):\n             def __init__(self):\n-                super(Model2, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -490,7 +490,7 @@ def forward(self, x):\n         # we can't do the fusion when add's inputs are mixed dtype.\n         class Model3(torch.nn.Module):\n             def __init__(self):\n-                super(Model3, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -526,7 +526,7 @@ def forward(self, x):\n     def test_reproduce_99842_issue(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n \n             def forward(self, input_tensor):\ndiff --git a/test/inductor/test_profiler.py b/test/inductor/test_profiler.py\nindex eebe884da46b33..68f44df8b9f052 100644\n--- a/test/inductor/test_profiler.py\n+++ b/test/inductor/test_profiler.py\n@@ -21,14 +21,14 @@ def test_inductor_profiling_triton_launch(self):\n         def fn(x, y):\n             return (x + y).sin().cos()\n \n-        x, y = [torch.rand((4, 4), device=\"cuda\") for _ in range(2)]\n+        x, y = (torch.rand((4, 4), device=\"cuda\") for _ in range(2))\n \n         with torch.profiler.profile() as prof:\n             fn(x, y)\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with open(fname, \"r\") as f:\n+            with open(fname) as f:\n                 trace_json = json.load(f)\n \n         self.assertTrue(\"traceEvents\" in trace_json)\ndiff --git a/test/inductor/test_standalone_compile.py b/test/inductor/test_standalone_compile.py\nindex c424c76244cc0e..88c528c891a4fc 100644\n--- a/test/inductor/test_standalone_compile.py\n+++ b/test/inductor/test_standalone_compile.py\n@@ -100,7 +100,7 @@ def test_inductor_via_export2(self):\n     def test_inductor_via_op_with_multiple_outputs(self):\n         x1 = torch.randn((2, 512, 128))\n         x2 = [128]\n-        x3 = torch.randn((128))\n+        x3 = torch.randn(128)\n         x4 = torch.randn((128,))\n         x5 = 1e-6\n         mod, inp = gen_gm_and_inputs(\ndiff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py\nindex 520837d3865a8d..45ee70cd8067ee 100644\n--- a/test/inductor/test_torchinductor.py\n+++ b/test/inductor/test_torchinductor.py\n@@ -2354,7 +2354,7 @@ def fn(x):\n     def test_adaptive_avg_pool2d_low_prec(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n \n             def forward(self, x):\n@@ -5995,7 +5995,7 @@ def fn(x, y):\n \n         self.common(\n             fn,\n-            [torch.randn((4, 2)), torch.randn((4))],\n+            [torch.randn((4, 2)), torch.randn(4)],\n         )\n \n     # Shape padding causes the inputs to all get specialized, so the codegen\n@@ -6047,7 +6047,7 @@ def test_sqrt_dynamic_shapes(self):\n \n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 B, N, C = x.shape\ndiff --git a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\nindex 8f4086af099e64..0cdfbd54ffbe3b 100644\n--- a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n+++ b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n@@ -124,14 +124,14 @@ def run(*ex, **kwargs):\n     \"test_expand_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_glu_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_isinf2_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_layer_norm_dynamic_shapes\": TestFailure((\"cuda\")),\n+    \"test_layer_norm_dynamic_shapes\": TestFailure(\"cuda\"),\n     \"test_linspace1_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_stack_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor2_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor3_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_to_device_constant_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_to_device_constant_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_upsample_nearest2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views3_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views4_dynamic_shapes\": TestFailure((\"cpu\",)),\n@@ -161,9 +161,9 @@ def run(*ex, **kwargs):\n     \"test_empty2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_empty_strided_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_index3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_inductor_bucketize_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_inductor_bucketize_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_like_rands_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n@@ -194,7 +194,7 @@ def run(*ex, **kwargs):\n     \"test_views6_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_view_detach_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_view_on_aliased_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_linear_float64_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_linear_float64_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_adaptive_avg_pool_with_output_size_0_dynamic_shapes\": TestFailure(\n         (\"cpu\", \"cuda\")\n     ),\n@@ -288,7 +288,7 @@ def run(*ex, **kwargs):\n \n if TEST_WITH_ROCM:\n     # aten.miopen_batch_norm is not registered for lowering\n-    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure((\"cuda\"))\n+    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure(\"cuda\")\n \n DynamicShapesCodegenCommonTemplate = make_dynamic_cls(\n     CommonTemplate, xfail_prop=\"_expected_failure_codegen_dynamic\"\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex fd1f4228c9d29a..8ac73569fe1c2a 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -158,7 +158,7 @@ def __init__(self):\n     def get_local_cache(self):\n         if not self.local_cache_path.is_file():\n             return {}\n-        with open(self.local_cache_path, \"r\") as local_cache_fp:\n+        with open(self.local_cache_path) as local_cache_fp:\n             local_cache = json.load(local_cache_fp)\n         return local_cache[\"cache\"]\n \n@@ -201,7 +201,7 @@ class PersistentCache(CacheBase):\n     def get_global_cache(self):\n         if self.global_cache_path is None or not self.global_cache_path.is_file():\n             return {}\n-        with open(self.global_cache_path, \"r\") as global_cache_fp:\n+        with open(self.global_cache_path) as global_cache_fp:\n             global_cache = json.load(global_cache_fp)\n         return global_cache[\"cache\"]\n \n@@ -844,7 +844,7 @@ def wrapper_call(*args):\n # - valid_vec_isa_list()\n # - VecISA.__bool__() <-- takes out a lock\n # - compile_file() <-- imports cpp_prefix_path from cpp, which causes us to try to take out the same lock.\n-@functools.lru_cache()\n+@functools.lru_cache\n def cpp_prefix_path():\n     path = Path(__file__).parent / \"codegen/cpp_prefix.h\"\n     with path.open() as f:\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex bb2469ebffd33d..f844160b7255c0 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -278,7 +278,7 @@ def parallel_num_threads():\n     return threads\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def stride_at(var: sympy.Symbol, index: sympy.Expr):\n     replacement = {var: var + 1}\n     new_index = sympy_subs(index, replacement)\ndiff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py\nindex 869a0fb60861f6..c8172ff36b3dcf 100644\n--- a/torch/_inductor/ir.py\n+++ b/torch/_inductor/ir.py\n@@ -3042,7 +3042,7 @@ class InplaceBernoulliFallback(ExternKernel):\n     kernel = \"aten.bernoulli_\"\n \n     def codegen(self, wrapper):\n-        (x,) = [t.codegen_reference() for t in self.inputs]\n+        (x,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(\n             f\"{self.kernel}({x}, {', '.join(map(repr, self.constant_args))})\"\n         )\n@@ -3073,9 +3073,9 @@ class ScatterFallback(ExternKernel):\n \n     def codegen(self, wrapper):\n         if self.src_is_tensor:\n-            (x, index, src) = [t.codegen_reference() for t in self.inputs]\n+            (x, index, src) = (t.codegen_reference() for t in self.inputs)\n         else:\n-            (x, index) = [t.codegen_reference() for t in self.inputs]\n+            (x, index) = (t.codegen_reference() for t in self.inputs)\n             src = self.constant_args[1]\n         wrapper.generate_scatter_fallback(\n             x,\n@@ -3156,7 +3156,7 @@ class IndexPutFallback(ExternKernel):\n     \"\"\"\n \n     def codegen(self, wrapper):\n-        (x, values, *valid_indices) = [t.codegen_reference() for t in self.inputs]\n+        (x, values, *valid_indices) = (t.codegen_reference() for t in self.inputs)\n         indices = []\n         iter_valid_indices = iter(valid_indices)\n         for i, _ in enumerate(self.indices):\n@@ -4694,7 +4694,7 @@ def codegen(self, wrapper):\n         wrapper.add_import_once(\n             \"from torch.distributed._functional_collectives_impl import _wait_tensor\"\n         )\n-        (input_collective,) = [t.codegen_reference() for t in self.inputs]\n+        (input_collective,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(f\"{input_collective} = _wait_tensor({input_collective})\")\n \n         # wait op still needs to produce a 'buffer' that represents the tensor output.\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 3f17db1e5fed3a..14430ace100534 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -2871,11 +2871,11 @@ def load_bounded(fy, fx):\n \n         iy = ops.to_dtype(in_y, get_int_dtype(iH + 1))\n         ix = ops.to_dtype(in_x, get_int_dtype(iW + 1))\n-        iys_ofs = tuple((ops.add(iy, ofs) for ofs in (-1, 0, 1, 2)))\n-        ixs_ofs = tuple((ops.add(ix, ofs) for ofs in (-1, 0, 1, 2)))\n+        iys_ofs = tuple(ops.add(iy, ofs) for ofs in (-1, 0, 1, 2))\n+        ixs_ofs = tuple(ops.add(ix, ofs) for ofs in (-1, 0, 1, 2))\n \n         def get_x_interp(y):\n-            coeffs_x = tuple((load_bounded(y, x) for x in ixs_ofs))\n+            coeffs_x = tuple(load_bounded(y, x) for x in ixs_ofs)\n             return cubic_interp1d(coeffs_x, t_x)\n \n         coeffs_y = tuple(get_x_interp(y) for y in iys_ofs)\ndiff --git a/torch/_inductor/triton_heuristics.py b/torch/_inductor/triton_heuristics.py\nindex 61027661111e5a..88fa275f8b0102 100644\n--- a/torch/_inductor/triton_heuristics.py\n+++ b/torch/_inductor/triton_heuristics.py\n@@ -482,9 +482,7 @@ def hash_configs(configs: List[Config]):\n     hasher = hashlib.sha256()\n     for cfg in configs:\n         hasher.update(\n-            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode(\n-                \"utf-8\"\n-            )\n+            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode()\n         )\n     return hasher.hexdigest()\n \n@@ -498,7 +496,7 @@ def load_cached_autotuning(\n     if not os.path.exists(cache_filename):\n         return None\n \n-    with open(cache_filename, \"r\") as fd:\n+    with open(cache_filename) as fd:\n         best_config = json.loads(fd.read())\n     if best_config.pop(\"configs_hash\", None) != configs_hash:\n         return None\ndiff --git a/torch/_inductor/utils.py b/torch/_inductor/utils.py\nindex 538d0a2040fb93..c604c45d53d32a 100644\n--- a/torch/_inductor/utils.py\n+++ b/torch/_inductor/utils.py\n@@ -688,7 +688,7 @@ def run_and_get_code(fn, *args, **kwargs):\n \n     def patched_compile_to_module(self):\n         mod = compile_to_module(self)\n-        with open(mod.__file__, \"r\") as f:\n+        with open(mod.__file__) as f:\n             source_codes.append(f.read())\n         return mod\n \n"
  },
  {
    "number": 105420,
    "title": "[BE] Enable ruff's UP rules and autoformat ao/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "82d3db03f5ae3251320ec6890312f9b316e7eb70",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105420",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105420/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105420.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105420.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105420/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105420/comments",
    "labels": [
      "release notes: AO frontend",
      "release notes: quantization"
    ],
    "_event_time": "2023-07-18T01:17:39.736755Z",
    "state": "closed",
    "patch": "From 8f26a323aa7dc5c2abab72ac8316035409576ac6 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:33 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat ao/\n\n[ghstack-poisoned]\n---\n .../ao/sparsity/test_activation_sparsifier.py |  1 -\n test/ao/sparsity/test_composability.py        |  1 -\n test/ao/sparsity/test_data_scheduler.py       |  1 -\n test/ao/sparsity/test_data_sparsifier.py      |  1 -\n test/ao/sparsity/test_kernels.py              |  1 -\n test/ao/sparsity/test_parametrization.py      |  1 -\n test/ao/sparsity/test_scheduler.py            |  1 -\n test/ao/sparsity/test_sparsifier.py           |  1 -\n test/ao/sparsity/test_sparsity_utils.py       |  1 -\n .../ao/sparsity/test_structured_sparsifier.py |  1 -\n torch/ao/nn/intrinsic/modules/fused.py        |  2 +-\n .../ao/nn/intrinsic/qat/modules/conv_fused.py | 12 +++----\n .../nn/intrinsic/qat/modules/linear_relu.py   |  2 +-\n .../quantized/dynamic/modules/linear_relu.py  |  2 +-\n .../nn/intrinsic/quantized/modules/bn_relu.py |  4 +--\n .../intrinsic/quantized/modules/conv_relu.py  |  6 ++--\n .../quantized/modules/linear_relu.py          |  2 +-\n torch/ao/nn/quantizable/modules/activation.py |  6 ++--\n torch/ao/nn/quantized/dynamic/modules/conv.py |  1 -\n .../ao/nn/quantized/dynamic/modules/linear.py |  2 +-\n torch/ao/nn/quantized/dynamic/modules/rnn.py  | 32 +++++++++----------\n torch/ao/nn/quantized/modules/__init__.py     |  2 +-\n torch/ao/nn/quantized/modules/conv.py         |  5 ++-\n torch/ao/nn/quantized/modules/linear.py       |  2 +-\n .../ao/nn/quantized/reference/modules/rnn.py  |  2 +-\n .../ao/nn/sparse/quantized/dynamic/linear.py  |  2 +-\n torch/ao/ns/fx/ns_types.py                    |  8 ++---\n .../data_scheduler/base_data_scheduler.py     |  4 +--\n torch/ao/pruning/scheduler/base_scheduler.py  |  4 +--\n torch/ao/pruning/scheduler/cubic_scheduler.py |  1 -\n .../quantization/_learnable_fake_quantize.py  |  4 +--\n .../backend_config/backend_config.py          |  3 +-\n .../ao/quantization/backend_config/onednn.py  | 12 +++----\n .../quantization/experimental/APoT_tensor.py  |  2 +-\n .../ao/quantization/experimental/quantizer.py |  2 +-\n torch/ao/quantization/fake_quantize.py        |  2 +-\n torch/ao/quantization/fuse_modules.py         |  2 +-\n .../ao/quantization/fuser_method_mappings.py  | 10 +++---\n torch/ao/quantization/fx/_equalize.py         |  2 +-\n .../fx/_lower_to_native_backend.py            |  2 +-\n .../quantization/fx/_model_report/detector.py |  8 ++---\n .../_model_report/model_report_visualizer.py  |  2 +-\n torch/ao/quantization/fx/convert.py           |  2 +-\n torch/ao/quantization/fx/custom_config.py     |  9 ++----\n torch/ao/quantization/fx/prepare.py           |  2 +-\n torch/ao/quantization/fx/utils.py             | 24 ++++++--------\n torch/ao/quantization/observer.py             | 18 +++++------\n torch/ao/quantization/pt2e/prepare.py         |  4 +--\n .../quantization/pt2e/quantizer/quantizer.py  | 21 ++----------\n torch/ao/quantization/qconfig.py              |  8 ++---\n .../ao/quantization/quantization_mappings.py  | 10 +++---\n torch/ao/quantization/quantize.py             |  2 +-\n torch/ao/quantization/utils.py                |  2 +-\n 53 files changed, 114 insertions(+), 150 deletions(-)\n\ndiff --git a/test/ao/sparsity/test_activation_sparsifier.py b/test/ao/sparsity/test_activation_sparsifier.py\nindex 573a40762c31cc..01bdfa045da9d1 100644\n--- a/test/ao/sparsity/test_activation_sparsifier.py\n+++ b/test/ao/sparsity/test_activation_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import copy\ndiff --git a/test/ao/sparsity/test_composability.py b/test/ao/sparsity/test_composability.py\nindex 85d78c49ea54ae..cb799f714ca17b 100644\n--- a/test/ao/sparsity/test_composability.py\n+++ b/test/ao/sparsity/test_composability.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_data_scheduler.py b/test/ao/sparsity/test_data_scheduler.py\nindex 9c33a160e76836..ab7c051c21077a 100644\n--- a/test/ao/sparsity/test_data_scheduler.py\n+++ b/test/ao/sparsity/test_data_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_data_sparsifier.py b/test/ao/sparsity/test_data_sparsifier.py\nindex 81a899f6932a0d..9248a371826ebd 100644\n--- a/test/ao/sparsity/test_data_sparsifier.py\n+++ b/test/ao/sparsity/test_data_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_kernels.py b/test/ao/sparsity/test_kernels.py\nindex 4786557ceb3be7..111d51465be109 100644\n--- a/test/ao/sparsity/test_kernels.py\n+++ b/test/ao/sparsity/test_kernels.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.testing._internal.common_utils import run_tests\ndiff --git a/test/ao/sparsity/test_parametrization.py b/test/ao/sparsity/test_parametrization.py\nindex 54b6f778d9fa8f..02f7cc6db7fddf 100644\n--- a/test/ao/sparsity/test_parametrization.py\n+++ b/test/ao/sparsity/test_parametrization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_scheduler.py b/test/ao/sparsity/test_scheduler.py\nindex 52eb54cb9ecb96..835c5143f18bc2 100644\n--- a/test/ao/sparsity/test_scheduler.py\n+++ b/test/ao/sparsity/test_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch import nn\ndiff --git a/test/ao/sparsity/test_sparsifier.py b/test/ao/sparsity/test_sparsifier.py\nindex 4c79416a78dd69..c9309d4b81fe5b 100644\n--- a/test/ao/sparsity/test_sparsifier.py\n+++ b/test/ao/sparsity/test_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import itertools\ndiff --git a/test/ao/sparsity/test_sparsity_utils.py b/test/ao/sparsity/test_sparsity_utils.py\nindex 90aad10ab18db6..9a4fc79e6c454e 100644\n--- a/test/ao/sparsity/test_sparsity_utils.py\n+++ b/test/ao/sparsity/test_sparsity_utils.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_structured_sparsifier.py b/test/ao/sparsity/test_structured_sparsifier.py\nindex f50420c89a199d..13ab245a2efc27 100644\n--- a/test/ao/sparsity/test_structured_sparsifier.py\n+++ b/test/ao/sparsity/test_structured_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n import copy\n import logging\ndiff --git a/torch/ao/nn/intrinsic/modules/fused.py b/torch/ao/nn/intrinsic/modules/fused.py\nindex f70a5430e65c21..7c87154b0e6225 100644\n--- a/torch/ao/nn/intrinsic/modules/fused.py\n+++ b/torch/ao/nn/intrinsic/modules/fused.py\n@@ -125,7 +125,7 @@ class LinearBn1d(_FusedModule):\n     During quantization this will be replaced with the corresponding fused module.\"\"\"\n     def __init__(self, linear, bn):\n         assert type_before_parametrizations(linear) == Linear and type_before_parametrizations(bn) == BatchNorm1d, \\\n-            'Incorrect types for input modules{}{}'.format(type_before_parametrizations(linear), type_before_parametrizations(bn))\n+            f'Incorrect types for input modules{type_before_parametrizations(linear)}{type_before_parametrizations(bn)}'\n         super().__init__(linear, bn)\n \n class LinearLeakyReLU(_FusedModule):\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\nindex 3f457ad5917eba..161280ca079d53 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n@@ -453,7 +453,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU1d(nnqat.Conv1d, nni._FusedModule):\n     r\"\"\"A ConvReLU1d module is a fused module of Conv1d and ReLU, attached with\n@@ -490,7 +490,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn2d(_ConvBnNd, nn.Conv2d):\n     r\"\"\"\n@@ -585,7 +585,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU2d(nnqat.Conv2d, nni._FusedModule):\n     r\"\"\"A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\n@@ -622,7 +622,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn3d(_ConvBnNd, nn.Conv3d):\n     r\"\"\"\n@@ -758,7 +758,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU3d(nnqat.Conv3d, nni._FusedModule):\n     r\"\"\"A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with\n@@ -813,7 +813,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n def update_bn_stats(mod):\n     if type(mod) in {ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvBn1d, ConvBn2d, ConvBn3d}:\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\nindex 93b19537083427..11d11047c2c723 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n@@ -37,7 +37,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     def to_float(self):\n         linear = torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)\ndiff --git a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nindex 9a6502d546641b..a0bccdc0e3d3d4 100644\n--- a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n@@ -48,7 +48,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qlinear_relu):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\nindex 5cd2ed8a757cee..856fa43aac9941 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n@@ -39,7 +39,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU2d\n-        return super(BNReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\n@@ -75,7 +75,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU3d\n-        return super(BNReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\nindex 7a88a7b8f92d3b..30d00474e4a5ad 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n@@ -58,7 +58,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -107,7 +107,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -163,7 +163,7 @@ def from_float(cls, mod):\n                 mod.bn.weight,\n                 mod.bn.bias,\n             )\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\nindex 9c3a7bcd3b4a0c..17cb48f80fda91 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n@@ -41,7 +41,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_linear_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/quantizable/modules/activation.py b/torch/ao/nn/quantizable/modules/activation.py\nindex b7ba9dd8dc72c2..6a25d0f591021d 100644\n--- a/torch/ao/nn/quantizable/modules/activation.py\n+++ b/torch/ao/nn/quantizable/modules/activation.py\n@@ -317,7 +317,7 @@ def _forward_impl(self,\n             raise AssertionError(\"causal mask not supported by AO MHA module\")\n \n         if self.batch_first:\n-            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n+            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n \n         tgt_len, bsz, embed_dim_to_check = query.size()\n         assert self.embed_dim == embed_dim_to_check\n@@ -339,7 +339,7 @@ def _forward_impl(self,\n                 warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n                 attn_mask = attn_mask.to(torch.bool)\n             assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n-                'Only float and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n+                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n \n             if attn_mask.dim() == 2:\n                 attn_mask = attn_mask.unsqueeze(0)\n@@ -349,7 +349,7 @@ def _forward_impl(self,\n                 if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n                     raise RuntimeError('The size of the 3D attn_mask is not correct.')\n             else:\n-                raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n+                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n             # attn_mask's dim is 3 now.\n \n         # convert ByteTensor key_padding_mask to bool\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/conv.py b/torch/ao/nn/quantized/dynamic/modules/conv.py\nindex 125b48edaacde5..f1af7796413655 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/conv.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Dynamically quantized convolution modules.\"\"\"\n \n import torch\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/linear.py b/torch/ao/nn/quantized/dynamic/modules/linear.py\nindex 78e459f9bc63c5..22f483f32fd7a8 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/linear.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/linear.py\n@@ -68,7 +68,7 @@ def extra_repr(self):\n             self.in_features, self.out_features, self._packed_params.dtype\n         )\n         if self._packed_params.dtype == torch.qint8:\n-            extra_repr_str += ', qscheme={}'.format(self.weight().qscheme())\n+            extra_repr_str += f', qscheme={self.weight().qscheme()}'\n         return extra_repr_str\n \n     def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/rnn.py b/torch/ao/nn/quantized/dynamic/modules/rnn.py\nindex 3e78948b5447b2..47c8a9ac2fb43c 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/rnn.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/rnn.py\n@@ -231,8 +231,8 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n     def set_weight_bias(self, weight_bias_dict):\n \n         def weight_bias_name(ihhh, layer, suffix):\n-            weight_name = \"weight_{}_l{}{}\".format(ihhh, layer, suffix)\n-            bias_name = \"bias_{}_l{}{}\".format(ihhh, layer, suffix)\n+            weight_name = f\"weight_{ihhh}_l{layer}{suffix}\"\n+            bias_name = f\"bias_{ihhh}_l{layer}{suffix}\"\n             return weight_name, bias_name\n \n         num_directions = 2 if self.bidirectional else 1\n@@ -286,7 +286,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n         # RNNBase can be either LSTM or GRU\n         qRNNBase: Union[LSTM, GRU]\n         if mod.mode == 'LSTM':\n@@ -308,8 +308,8 @@ def from_float(cls, mod):\n                 suffix = '_reverse' if direction == 1 else ''\n \n                 def retrieve_weight_bias(ihhh):\n-                    weight_name = 'weight_{}_l{}{}'.format(ihhh, layer, suffix)\n-                    bias_name = 'bias_{}_l{}{}'.format(ihhh, layer, suffix)\n+                    weight_name = f'weight_{ihhh}_l{layer}{suffix}'\n+                    bias_name = f'bias_{ihhh}_l{layer}{suffix}'\n                     weight = getattr(mod, weight_name)\n                     bias = getattr(mod, bias_name)\n                     return weight, bias\n@@ -358,15 +358,15 @@ def _weight_bias(self):\n         for layer in range(self.num_layers):\n             for direction in range(num_directions):\n                 suffix = '_reverse' if direction == 1 else ''\n-                key_name1 = 'weight_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'weight_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'weight_ih_l{layer}{suffix}'\n+                key_name2 = f'weight_hh_l{layer}{suffix}'\n                 # packed weights are part of torchbind class, CellParamsSerializationType\n                 # Within the packed weight class, the weight and bias are accessible as Tensors\n                 packed_weight_bias = self._all_weight_values[count].param.__getstate__()[0][4]\n                 weight_bias_dict['weight'][key_name1] = packed_weight_bias[0].__getstate__()[0][0]\n                 weight_bias_dict['weight'][key_name2] = packed_weight_bias[1].__getstate__()[0][0]\n-                key_name1 = 'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'bias_ih_l{layer}{suffix}'\n+                key_name2 = f'bias_hh_l{layer}{suffix}'\n                 weight_bias_dict['bias'][key_name1] = packed_weight_bias[0].__getstate__()[0][1]\n                 weight_bias_dict['bias'][key_name2] = packed_weight_bias[1].__getstate__()[0][1]\n                 count = count + 1\n@@ -494,7 +494,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTM, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -746,7 +746,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -860,7 +860,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n \n         qRNNCellBase: Union[LSTMCell, GRUCell, RNNCell]\n \n@@ -1009,12 +1009,12 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n         return ret\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(RNNCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class LSTMCell(RNNCellBase):\n@@ -1057,7 +1057,7 @@ def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) ->\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTMCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class GRUCell(RNNCellBase):\n@@ -1098,4 +1098,4 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRUCell, cls).from_float(mod)\n+        return super().from_float(mod)\ndiff --git a/torch/ao/nn/quantized/modules/__init__.py b/torch/ao/nn/quantized/modules/__init__.py\nindex 05866f6da4066a..668f765fe3ef0a 100644\n--- a/torch/ao/nn/quantized/modules/__init__.py\n+++ b/torch/ao/nn/quantized/modules/__init__.py\n@@ -104,7 +104,7 @@ def from_float(mod):\n         return Quantize(scale.float().item(), zero_point.long().item(), mod.activation_post_process.dtype)\n \n     def extra_repr(self):\n-        return 'scale={}, zero_point={}, dtype={}'.format(self.scale, self.zero_point, self.dtype)\n+        return f'scale={self.scale}, zero_point={self.zero_point}, dtype={self.dtype}'\n \n \n class DeQuantize(torch.nn.Module):\ndiff --git a/torch/ao/nn/quantized/modules/conv.py b/torch/ao/nn/quantized/modules/conv.py\nindex 727447841ca43c..22a11014375948 100644\n--- a/torch/ao/nn/quantized/modules/conv.py\n+++ b/torch/ao/nn/quantized/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Quantized convolution modules.\"\"\"\n \n from typing import Optional, List, TypeVar\n@@ -64,7 +63,7 @@ def _init(self, in_channels, out_channels, kernel_size, stride,\n         self.output_padding = output_padding\n         self.groups = groups\n         if padding_mode not in _SUPPORTED_PADDING:\n-            raise ValueError(\"'padding_mode' {} is not supported by quantized convolution\".format(padding_mode))\n+            raise ValueError(f\"'padding_mode' {padding_mode} is not supported by quantized convolution\")\n         self.padding_mode = padding_mode\n         # Initialize as NCHW. set_weight will internally transpose to NHWC.\n         if self.transposed:\n@@ -593,7 +592,7 @@ def __init__(self, in_channels, out_channels, kernel_size, stride,\n                  padding, dilation, transposed, output_padding,\n                  groups, bias, padding_mode, device=None, dtype=None):\n         if padding_mode != 'zeros':\n-            raise ValueError('Only \"zeros\" padding mode is supported for {}'.format(self.__class__.__name__))\n+            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n         factory_kwargs = {'device': device, 'dtype': dtype}\n         # Subclasses of _ConvNd need to call _init rather than __init__. See\n         # discussion on PR #49702\ndiff --git a/torch/ao/nn/quantized/modules/linear.py b/torch/ao/nn/quantized/modules/linear.py\nindex e592c5f9b4d015..213934e62962a0 100644\n--- a/torch/ao/nn/quantized/modules/linear.py\n+++ b/torch/ao/nn/quantized/modules/linear.py\n@@ -262,7 +262,7 @@ def from_float(cls, mod):\n             if not isinstance(cls._FLOAT_MODULE, Iterable):\n                 cls._FLOAT_MODULE = [cls._FLOAT_MODULE]  # type: ignore[assignment]\n             supported_modules = ', '.join([float_mod.__name__ for float_mod in cls._FLOAT_MODULE])  # type: ignore[attr-defined]\n-            error_msg = 'nnq.{}.from_float only works for {}, but got: {}'.format(cls.__name__, supported_modules, type(mod))\n+            error_msg = f'nnq.{cls.__name__}.from_float only works for {supported_modules}, but got: {type(mod)}'\n             assert type_before_parametrizations(mod) in cls._FLOAT_MODULE, error_msg.format()  # type: ignore[attr-defined]\n             assert hasattr(mod, 'qconfig'), 'Input float module must have qconfig defined'\n             activation_post_process = mod.activation_post_process\ndiff --git a/torch/ao/nn/quantized/reference/modules/rnn.py b/torch/ao/nn/quantized/reference/modules/rnn.py\nindex 566642832a544d..9f44667c270b56 100644\n--- a/torch/ao/nn/quantized/reference/modules/rnn.py\n+++ b/torch/ao/nn/quantized/reference/modules/rnn.py\n@@ -152,7 +152,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n \n         if not is_batched:\n             ret = ret.squeeze(0)\ndiff --git a/torch/ao/nn/sparse/quantized/dynamic/linear.py b/torch/ao/nn/sparse/quantized/dynamic/linear.py\nindex 87d174db8098ac..4190ebe38c2f93 100644\n--- a/torch/ao/nn/sparse/quantized/dynamic/linear.py\n+++ b/torch/ao/nn/sparse/quantized/dynamic/linear.py\n@@ -60,7 +60,7 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                               missing_keys, unexpected_keys, error_msgs):\n         op_type = int(state_dict[prefix + 'op_type'])\n         assert op_type == 'sparse', \\\n-            \"Cannot load from op_type [{}], expecting [{}]\".format(op_type, self._op_type)\n+            f\"Cannot load from op_type [{op_type}], expecting [{self._op_type}]\"\n         state_dict.pop(prefix + 'op_type')\n \n         version = local_metadata.get('version', None)\ndiff --git a/torch/ao/ns/fx/ns_types.py b/torch/ao/ns/fx/ns_types.py\nindex cf0451a155dd44..5c3c422dd4ae9d 100644\n--- a/torch/ao/ns/fx/ns_types.py\n+++ b/torch/ao/ns/fx/ns_types.py\n@@ -10,10 +10,10 @@ class NSSingleResultValuesType(str, enum.Enum):\n     NODE_OUTPUT = 'node_output'\n     NODE_INPUT = 'node_input'\n \n-NSSubgraph = NamedTuple(\n-    'NSSubgraph',\n-    [('start_node', Node), ('end_node', Node), ('base_op_node', Node)]\n-)\n+class NSSubgraph(NamedTuple):\n+    start_node: Node\n+    end_node: Node\n+    base_op_node: Node\n \n # TODO(future PR): see if we can use typing_extensions's TypedDict instead\n # to properly type the various keys\ndiff --git a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\nindex 26da2146952ffd..0e4060f95435b6 100644\n--- a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n+++ b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n@@ -103,8 +103,8 @@ def get_schedule_param(self):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Data Sparsifier {0}\\n'.format(self.data_sparsifier)\n-        format_string += '    {0}: {1}\\n'.format(self.schedule_param, self.base_param)\n+        format_string += f'Data Sparsifier {self.data_sparsifier}\\n'\n+        format_string += f'    {self.schedule_param}: {self.base_param}\\n'\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/base_scheduler.py b/torch/ao/pruning/scheduler/base_scheduler.py\nindex 8986d5bbdf630f..66863b31c5f8d8 100644\n--- a/torch/ao/pruning/scheduler/base_scheduler.py\n+++ b/torch/ao/pruning/scheduler/base_scheduler.py\n@@ -106,8 +106,8 @@ def print_sl(self, is_verbose, group, sl, epoch=None):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Sparsifier {0}\\n'.format(self.sparsifier)\n-        format_string += '    {0}: {1}\\n'.format('base_sl', self.base_sl)\n+        format_string += f'Sparsifier {self.sparsifier}\\n'\n+        format_string += '    {}: {}\\n'.format('base_sl', self.base_sl)\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/cubic_scheduler.py b/torch/ao/pruning/scheduler/cubic_scheduler.py\nindex 49ee9f51b42ae6..76fc61daa288a6 100644\n--- a/torch/ao/pruning/scheduler/cubic_scheduler.py\n+++ b/torch/ao/pruning/scheduler/cubic_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import warnings\n \n from .base_scheduler import BaseScheduler\ndiff --git a/torch/ao/quantization/_learnable_fake_quantize.py b/torch/ao/quantization/_learnable_fake_quantize.py\nindex df86cd50a2a775..f21fedeb3bc28f 100644\n--- a/torch/ao/quantization/_learnable_fake_quantize.py\n+++ b/torch/ao/quantization/_learnable_fake_quantize.py\n@@ -114,8 +114,8 @@ def toggle_fake_quant(self, enabled=True):\n \n     @torch.jit.export\n     def observe_quant_params(self):\n-        print('_LearnableFakeQuantize Scale: {}'.format(self.scale.detach()))\n-        print('_LearnableFakeQuantize Zero Point: {}'.format(self.zero_point.detach()))\n+        print(f'_LearnableFakeQuantize Scale: {self.scale.detach()}')\n+        print(f'_LearnableFakeQuantize Zero Point: {self.zero_point.detach()}')\n \n     @torch.jit.export\n     def calculate_qparams(self):\ndiff --git a/torch/ao/quantization/backend_config/backend_config.py b/torch/ao/quantization/backend_config/backend_config.py\nindex ef31166b5cdab1..32abcc42e402fb 100644\n--- a/torch/ao/quantization/backend_config/backend_config.py\n+++ b/torch/ao/quantization/backend_config/backend_config.py\n@@ -599,8 +599,7 @@ def _get_dtype_config(obj: Any) -> DTypeConfig:\n                 return obj\n             if isinstance(obj, Dict):\n                 return DTypeConfig.from_dict(obj)\n-            raise ValueError(\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (DTYPE_CONFIGS_DICT_KEY, type(obj)))\n+            raise ValueError(f\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"{DTYPE_CONFIGS_DICT_KEY}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         if PATTERN_DICT_KEY in backend_pattern_config_dict:\ndiff --git a/torch/ao/quantization/backend_config/onednn.py b/torch/ao/quantization/backend_config/onednn.py\nindex 6a896608c9b5a8..8c14637ae3d3f7 100644\n--- a/torch/ao/quantization/backend_config/onednn.py\n+++ b/torch/ao/quantization/backend_config/onednn.py\n@@ -89,7 +89,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n         \"Linear, BN and LeakyReLU all must be in the same mode (train or eval).\"\n \n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((linear, bn, leaky_relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(linear, bn, leaky_relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Linear: nni.LinearLeakyReLU,\n@@ -100,7 +100,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n             fm = fused_module(fused_linear, leaky_relu)\n             return fm\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((linear, bn, leaky_relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(linear, bn, leaky_relu)}\")\n \n # ======================\n # |  CONFIGS FOR CONV  |\n@@ -144,7 +144,7 @@ def _conv_add_extra_inputs_getter_left(pattern):\n def _fuse_conv_bn_add_left(is_qat, add, bn_conv, _):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -216,7 +216,7 @@ def _conv_add_extra_inputs_getter_right(pattern):\n def _fuse_conv_bn_add_right(is_qat, add, _, bn_conv):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -305,7 +305,7 @@ def _fuse_conv_bn_add_relu_left(is_qat, relu, add_pattern):\n     add, bn_conv, _ = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\n@@ -387,7 +387,7 @@ def _fuse_conv_bn_add_relu_right(is_qat, relu, add_pattern):\n     add, _, bn_conv = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\ndiff --git a/torch/ao/quantization/experimental/APoT_tensor.py b/torch/ao/quantization/experimental/APoT_tensor.py\nindex f780e204154147..debda7aea8c0d1 100644\n--- a/torch/ao/quantization/experimental/APoT_tensor.py\n+++ b/torch/ao/quantization/experimental/APoT_tensor.py\n@@ -2,7 +2,7 @@\n from torch.ao.quantization.experimental.quantizer import APoTQuantizer\n \n # class to store APoT quantized tensor\n-class TensorAPoT():\n+class TensorAPoT:\n     quantizer: APoTQuantizer\n     data: torch.Tensor\n \ndiff --git a/torch/ao/quantization/experimental/quantizer.py b/torch/ao/quantization/experimental/quantizer.py\nindex e7e6048fb00e08..df9c0f27847e13 100644\n--- a/torch/ao/quantization/experimental/quantizer.py\n+++ b/torch/ao/quantization/experimental/quantizer.py\n@@ -5,7 +5,7 @@\n \n # class to store APoT quantizer and\n # implement quantize and dequantize\n-class APoTQuantizer():\n+class APoTQuantizer:\n     alpha: torch.Tensor\n     gamma: torch.Tensor\n     quantization_levels: torch.Tensor\ndiff --git a/torch/ao/quantization/fake_quantize.py b/torch/ao/quantization/fake_quantize.py\nindex 881d431dcccb18..0da19e9f09b5a8 100644\n--- a/torch/ao/quantization/fake_quantize.py\n+++ b/torch/ao/quantization/fake_quantize.py\n@@ -268,7 +268,7 @@ class FixedQParamsFakeQuantize(FakeQuantize):\n     def __init__(self, observer):\n         super().__init__(observer=observer)\n         assert type(self.activation_post_process) == FixedQParamsObserver,\\\n-            \"%s's observer must be a %s\" % (self.__class__.__name__, FixedQParamsObserver.__name__)\n+            f\"{self.__class__.__name__}'s observer must be a {FixedQParamsObserver.__name__}\"\n         self._observer_ctr = observer\n         self.scale = self.activation_post_process.scale\n         self.zero_point = self.activation_post_process.zero_point\ndiff --git a/torch/ao/quantization/fuse_modules.py b/torch/ao/quantization/fuse_modules.py\nindex 80c3933ddc06a1..7c7ef1a88e83a7 100644\n--- a/torch/ao/quantization/fuse_modules.py\n+++ b/torch/ao/quantization/fuse_modules.py\n@@ -51,7 +51,7 @@ def fuse_known_modules(mod_list, is_qat, additional_fuser_method_mapping=None):\n     types = tuple(type_before_parametrizations(m) for m in mod_list)\n     fuser_method = get_fuser_method(types, additional_fuser_method_mapping)\n     if fuser_method is None:\n-        raise NotImplementedError(\"Cannot fuse modules: {}\".format(types))\n+        raise NotImplementedError(f\"Cannot fuse modules: {types}\")\n     new_mod : List[Optional[nn.Module]] = [None] * len(mod_list)\n     fused = fuser_method(is_qat, *mod_list)\n     # NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\ndiff --git a/torch/ao/quantization/fuser_method_mappings.py b/torch/ao/quantization/fuser_method_mappings.py\nindex 9971326de1d102..3140f13008ac3d 100644\n--- a/torch/ao/quantization/fuser_method_mappings.py\n+++ b/torch/ao/quantization/fuser_method_mappings.py\n@@ -47,7 +47,7 @@ def fuse_conv_bn(is_qat, conv, bn):\n         if fused_module_class is not None:\n             return fused_module_class(conv, bn)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn)}\")\n     else:\n         return nn.utils.fuse_conv_bn_eval(conv, bn)\n \n@@ -84,7 +84,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n         if fused_module is not None:\n             return fused_module(conv, bn, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Conv1d: nni.ConvReLU1d,\n@@ -96,7 +96,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n             fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n             return fused_module(fused_conv, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(conv, bn, relu)}\")\n \n def fuse_linear_bn(is_qat, linear, bn):\n     r\"\"\"Given the linear and bn modules, fuses them and returns the fused module\n@@ -187,7 +187,7 @@ def get_fuser_method(op_list, additional_fuser_method_mapping=None):\n     all_mappings = get_combined_dict(_DEFAULT_OP_LIST_TO_FUSER_METHOD,\n                                      additional_fuser_method_mapping)\n     fuser_method = all_mappings.get(op_list, None)\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_list)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_list} \"\n     return fuser_method\n \n def _reverse2(f):\n@@ -244,5 +244,5 @@ def get_fuser_method_new(\n         fuser_method = fuser_method_mapping.get(op_pattern, None)\n         if fuser_method is not None:\n             break\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_pattern)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_pattern} \"\n     return fuser_method\ndiff --git a/torch/ao/quantization/fx/_equalize.py b/torch/ao/quantization/fx/_equalize.py\nindex 357db6454032e7..883ddf19682f08 100644\n--- a/torch/ao/quantization/fx/_equalize.py\n+++ b/torch/ao/quantization/fx/_equalize.py\n@@ -227,7 +227,7 @@ def __new__(cls, input_activation=torch.nn.Identity, weight=torch.nn.Identity):\n         if isinstance(input_activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"EqualizationQConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        self = super(EqualizationQConfig, cls).__new__(cls, input_activation, weight)\n+        self = super().__new__(cls, input_activation, weight)\n         return self\n \n \ndiff --git a/torch/ao/quantization/fx/_lower_to_native_backend.py b/torch/ao/quantization/fx/_lower_to_native_backend.py\nindex b897a7e80ab7cf..f1e9c81896f6c5 100644\n--- a/torch/ao/quantization/fx/_lower_to_native_backend.py\n+++ b/torch/ao/quantization/fx/_lower_to_native_backend.py\n@@ -514,7 +514,7 @@ def _match_static_pattern(\n     matched_dequantize = False\n     for i in dequantize_node_arg_indices:\n         assert i < len(ref_node.args),\\\n-            \"Dequantize index %s exceeded reference node's arg length %s\" % (i, len(ref_node.args))\n+            f\"Dequantize index {i} exceeded reference node's arg length {len(ref_node.args)}\"\n         arg = ref_node.args[i]\n         if is_dequantize_node(arg):\n             matched_dequantize = True\ndiff --git a/torch/ao/quantization/fx/_model_report/detector.py b/torch/ao/quantization/fx/_model_report/detector.py\nindex 60c9b26ffacecd..2e3cdc0a316611 100644\n--- a/torch/ao/quantization/fx/_model_report/detector.py\n+++ b/torch/ao/quantization/fx/_model_report/detector.py\n@@ -32,7 +32,7 @@\n DETECTOR_OBS_ARGS_KEY = \"observer_args\"\n \n # Mapping related code\n-class DetectorQConfigInfo():\n+class DetectorQConfigInfo:\n     r\"\"\"\n     This class contains the QConfig information for a single module.\n     The list of variables / values this contains can grow depending on the\n@@ -234,7 +234,7 @@ def __init__(self, backend: str = torch.backends.quantized.engine):\n         if self.backend_chosen in self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES:\n             self.supported_modules = self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES[self.backend_chosen]\n         else:\n-            raise ValueError(\"Not configured to work with {}. Try a different default backend\".format(self.backend_chosen))\n+            raise ValueError(f\"Not configured to work with {self.backend_chosen}. Try a different default backend\")\n \n     def get_detector_name(self) -> str:\n         r\"\"\" returns the string name of this detector\"\"\"\n@@ -352,7 +352,7 @@ def generate_detector_report(self, model: nn.Module) -> Tuple[str, Dict[str, Any\n         per_channel_info = self._detect_per_channel_helper(model)\n \n         # String to let the user know of further optimizations\n-        further_optims_str = \"Further Optimizations for backend {}: \\n\".format(self.backend_chosen)\n+        further_optims_str = f\"Further Optimizations for backend {self.backend_chosen}: \\n\"\n \n         optimizations_possible = False\n         for fqn in per_channel_info:\n@@ -1019,7 +1019,7 @@ def _generate_comparison_values(self, input_info: Dict, weight_info: Dict) -> Di\n \n             # raise error if not in weight info\n             if module_fqn not in weight_info:\n-                raise KeyError(\"Unable to find weight range stats for module {}\".format(module_fqn))\n+                raise KeyError(f\"Unable to find weight range stats for module {module_fqn}\")\n \n             # calculate the ratios of the weight info and input info\n             weight_ratio = self._calculate_range_ratio(weight_info[module_fqn], self.WEIGHT_STR, module_fqn)\ndiff --git a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\nindex f1f17e80982e54..8e04338446dab1 100644\n--- a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n+++ b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n@@ -587,7 +587,7 @@ def generate_plot_visualization(self, feature_filter: str, module_fqn_filter: st\n             avg_vals = [sum(y_data[:][index]) / num_channels for index in range(num_modules)]\n \n             # plot the three things we measured\n-            ax.plot(x_data, avg_vals, label=\"Average Value Across {} Channels\".format(num_channels))\n+            ax.plot(x_data, avg_vals, label=f\"Average Value Across {num_channels} Channels\")\n             ax.legend(loc='upper right')\n         else:\n             ax.set_xlabel(\"idx\")\ndiff --git a/torch/ao/quantization/fx/convert.py b/torch/ao/quantization/fx/convert.py\nindex 14e6ec094ede48..687342cc0ed321 100644\n--- a/torch/ao/quantization/fx/convert.py\n+++ b/torch/ao/quantization/fx/convert.py\n@@ -970,7 +970,7 @@ def convert(\n         # all the values either match what was set in prepare node_name_to_qconfig\n         # or are set to None in the convert_node_name_to_qconfig.\n         for k, v in node_name_to_qconfig.items():\n-            assert k in convert_node_name_to_qconfig, 'Expected key {} in convert node_name_to_qconfig'.format(k)\n+            assert k in convert_node_name_to_qconfig, f'Expected key {k} in convert node_name_to_qconfig'\n             if convert_node_name_to_qconfig[k] is not None:\n                 assert qconfig_equals(v, convert_node_name_to_qconfig[k]), \\\n                     \"Expected k {} to have the same value in prepare and convert QConfigMappings, \" \\\ndiff --git a/torch/ao/quantization/fx/custom_config.py b/torch/ao/quantization/fx/custom_config.py\nindex ef29061796d3a3..4fb2c3a28cb0a5 100644\n--- a/torch/ao/quantization/fx/custom_config.py\n+++ b/torch/ao/quantization/fx/custom_config.py\n@@ -197,8 +197,7 @@ def _get_qconfig_mapping(obj: Any, dict_key: str) -> Optional[QConfigMapping]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return QConfigMapping.from_dict(obj)\n-            raise ValueError(\"Expected QConfigMapping in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected QConfigMapping in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCustomConfig]:\n             \"\"\"\n@@ -208,8 +207,7 @@ def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCusto\n                 return obj\n             if isinstance(obj, Dict):\n                 return PrepareCustomConfig.from_dict(obj)\n-            raise ValueError(\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n             \"\"\"\n@@ -219,8 +217,7 @@ def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return BackendConfig.from_dict(obj)\n-            raise ValueError(\"Expected BackendConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected BackendConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         for (module_name, qconfig_dict, example_inputs, _prepare_custom_config_dict, backend_config_dict) in\\\ndiff --git a/torch/ao/quantization/fx/prepare.py b/torch/ao/quantization/fx/prepare.py\nindex d14bf444ccd8c6..aa9f1f7467f932 100644\n--- a/torch/ao/quantization/fx/prepare.py\n+++ b/torch/ao/quantization/fx/prepare.py\n@@ -238,7 +238,7 @@ def _needs_obs_or_fq(\n     # be converted to choose_qparams -> q -> dq in convert step\n     if cur_target_is_dynamic:\n         assert cur_target_dtype in _OBS_DTYPE_LIST, \\\n-            \"Expected cur_target_dtype to be torch.float, but got: {}\".format(cur_target_dtype)\n+            f\"Expected cur_target_dtype to be torch.float, but got: {cur_target_dtype}\"\n         assert prev_output_dtype not in _DO_NOT_OBS_DTYPE_LIST\n         return is_zeroth_arg\n     if reuse_input_obs_or_fq:\ndiff --git a/torch/ao/quantization/fx/utils.py b/torch/ao/quantization/fx/utils.py\nindex 2e0b6bbb130530..0942fd9462b0a1 100644\n--- a/torch/ao/quantization/fx/utils.py\n+++ b/torch/ao/quantization/fx/utils.py\n@@ -149,7 +149,7 @@ def get_qconv_prepack_op(conv_op: Callable) -> Callable:\n         torch.nn.functional.conv_transpose3d: torch.ops.quantized.conv_transpose3d_prepack,\n     }\n     prepack_op = prepack_ops.get(conv_op, None)\n-    assert prepack_op, \"Didn't find prepack op for {}\".format(conv_op)\n+    assert prepack_op, f\"Didn't find prepack op for {conv_op}\"\n     return prepack_op\n \n # Returns a function that can get a new attribute name for module with given\n@@ -811,24 +811,21 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n         # check quantization ranges\n         if backend_quant_min is not None and backend_quant_max is not None:\n             if app_quant_min is None or app_quant_max is None:\n-                warnings.warn(\"QConfig %s must specify 'quant_min' and 'quant_max', ignoring %s\" %\n-                              (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'quant_min' and 'quant_max', ignoring {qconfig}\")\n                 return False\n             elif app_quant_min < backend_quant_min or app_quant_max > backend_quant_max:\n-                warnings.warn((\"QConfig %s quantization range must fall within the backend's:\\n\"\n-                              \"QConfig range = (%s, %s), BackendConfig range = (%s, %s), ignoring %s\") %\n-                              (debug_string, app_quant_min, app_quant_max,\n+                warnings.warn((\"QConfig {} quantization range must fall within the backend's:\\n\"\n+                              \"QConfig range = ({}, {}), BackendConfig range = ({}, {}), ignoring {}\").format(debug_string, app_quant_min, app_quant_max,\n                               backend_quant_min, backend_quant_max, qconfig))\n                 return False\n         # check scale min\n         if backend_scale_min is not None:\n             if app_scale_min is None:\n-                warnings.warn(\"QConfig %s must specify 'eps', ignoring %s\" % (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'eps', ignoring {qconfig}\")\n                 return False\n             elif app_scale_min < backend_scale_min:\n-                warnings.warn((\"QConfig %s eps (%s) must be greater than or equal to \"\n-                              \"the backend's min scale value (%s), ignoring %s\") %\n-                              (debug_string, app_scale_min, backend_scale_min, qconfig))\n+                warnings.warn((\"QConfig {} eps ({}) must be greater than or equal to \"\n+                              \"the backend's min scale value ({}), ignoring {}\").format(debug_string, app_scale_min, backend_scale_min, qconfig))\n                 return False\n         # check fixed scale and zero point\n         if backend_scale_exact_match is not None and backend_zero_point_exact_match is not None:\n@@ -846,12 +843,11 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n             if not isinstance(activation_post_process, FixedQParamsObserver) and \\\n                     not isinstance(activation_post_process, FixedQParamsFakeQuantize):\n                 warnings.warn((\"QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize \"\n-                              \"for fixed qparams ops, ignoring %s.\\n%s\") % (qconfig, suggestion_str))\n+                              \"for fixed qparams ops, ignoring {}.\\n{}\").format(qconfig, suggestion_str))\n                 return False\n             if observer.scale != backend_scale_exact_match or observer.zero_point != backend_zero_point_exact_match:\n-                warnings.warn((\"QConfig fixed scale (%s) and zero point (%s) do not match the backend's \"\n-                              \"(%s and %s), ignoring %s.\\n%s\") %\n-                              (observer.scale, observer.zero_point, backend_scale_exact_match,\n+                warnings.warn((\"QConfig fixed scale ({}) and zero point ({}) do not match the backend's \"\n+                              \"({} and {}), ignoring {}.\\n{}\").format(observer.scale, observer.zero_point, backend_scale_exact_match,\n                               backend_zero_point_exact_match, qconfig, suggestion_str))\n                 return False\n         return True\ndiff --git a/torch/ao/quantization/observer.py b/torch/ao/quantization/observer.py\nindex 3263ae11564129..f5d24c45787265 100644\n--- a/torch/ao/quantization/observer.py\n+++ b/torch/ao/quantization/observer.py\n@@ -118,7 +118,7 @@ def _with_callable_args(cls_or_self, **kwargs):\n     return r.with_callable_args(**kwargs)\n \n \n-ABC: Any = ABCMeta(str(\"ABC\"), (object,), {})  # compatible with Python 2 *and* 3:\n+ABC: Any = ABCMeta(\"ABC\", (object,), {})  # compatible with Python 2 *and* 3:\n \n \n class ObserverBase(ABC, nn.Module):\n@@ -509,7 +509,7 @@ def calculate_qparams(self):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     @torch.jit.export\n     def reset_min_max_vals(self):\n@@ -712,7 +712,7 @@ def calculate_qparams(self):\n         return self._calculate_qparams(self.min_val, self.max_val)\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     def _load_from_state_dict(\n         self,\n@@ -746,7 +746,7 @@ def _load_from_state_dict(\n                 elif name == expected_max_name:\n                     self.max_val.resize_(val.shape)\n                 else:\n-                    warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                    warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n                 # For torchscript module we need to update the attributes here since we do not\n                 # call the `_load_from_state_dict` function defined module.py\n                 if torch.jit.is_scripting():\n@@ -755,7 +755,7 @@ def _load_from_state_dict(\n                     elif name == expected_max_name:\n                         self.max_val.copy_(val)\n                     else:\n-                        warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                        warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n             elif strict:\n                 missing_keys.append(key)\n \n@@ -1265,7 +1265,7 @@ def _load_from_state_dict(\n         )\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n \n class FixedQParamsObserver(ObserverBase):\n@@ -1363,7 +1363,7 @@ def forward(self, x):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"dtype={}, is_dynamic={}\".format(self.dtype, self.is_dynamic)\n+        return f\"dtype={self.dtype}, is_dynamic={self.is_dynamic}\"\n \n     @torch.jit.export\n     def calculate_qparams(self):\n@@ -1518,10 +1518,10 @@ def load_observer_state_dict(mod, obs_dict):\n                 )\n     for k in missing_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Missing keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Missing keys for observer {k} in state_dict\")\n     for k in unexpected_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Unexpected keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Unexpected keys for observer {k} in state_dict\")\n \n \n # Restrict activations to be in the range (0,127)\ndiff --git a/torch/ao/quantization/pt2e/prepare.py b/torch/ao/quantization/pt2e/prepare.py\nindex 2a29a92a986ced..13f73acca354b4 100644\n--- a/torch/ao/quantization/pt2e/prepare.py\n+++ b/torch/ao/quantization/pt2e/prepare.py\n@@ -68,9 +68,9 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n             assert _is_activation_post_process_node(arg, named_modules)\n             assert arg_as_input_act_obs_or_fq is not None\n             observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), \"expect observed argument to be a Node, but got: {}\".format(type(observed_arg))\n+            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n             assert observed_arg in obs_or_fq_map, \\\n-                \"can't refer to a node that does not have observer/fake_quant inserted yet: {}\".format(observed_arg)\n+                f\"can't refer to a node that does not have observer/fake_quant inserted yet: {observed_arg}\"\n             arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n             new_arg = arg\n             obs_or_fq_map[(observed_arg, node)] = arg_as_input_act_obs_or_fq\ndiff --git a/torch/ao/quantization/pt2e/quantizer/quantizer.py b/torch/ao/quantization/pt2e/quantizer/quantizer.py\nindex 3f7531c33a4e8b..46d7286756d17c 100644\n--- a/torch/ao/quantization/pt2e/quantizer/quantizer.py\n+++ b/torch/ao/quantization/pt2e/quantizer/quantizer.py\n@@ -128,24 +128,9 @@ class QuantizationConfig:\n OperatorPatternType = List[Callable]\n OperatorPatternType.__module__ = \"torch.ao.quantization.pt2e.quantizer.quantizer\"\n \n-OperatorConfig = NamedTuple(\n-    \"OperatorConfig\",\n-    # fix List[str] with List[List[Union[nn.Module, FunctionType, BuiltinFunctionType]]]\n-    # Basically we are mapping a quantization config to some list of patterns.\n-    # a pattern is defined as a list of nn module, function or builtin function names\n-    # e.g. [nn.Conv2d, torch.relu, torch.add]\n-    # We have not resolved whether fusion can be considered internal details of the\n-    # quantizer hence it does not need communication to user.\n-    # Note this pattern is not really informative since it does not really\n-    # tell us the graph structure resulting from the list of ops.\n-    [\n-        (\"config\", QuantizationConfig),\n-        (\n-            \"operators\",\n-            List[OperatorPatternType],\n-        ),\n-    ],\n-)\n+class OperatorConfig(NamedTuple):\n+    config: QuantizationConfig\n+    operators: List[OperatorPatternType]\n \n @dataclass\n class QuantizationAnnotation:\ndiff --git a/torch/ao/quantization/qconfig.py b/torch/ao/quantization/qconfig.py\nindex f7489c1ed4d05a..dc8353d6172990 100644\n--- a/torch/ao/quantization/qconfig.py\n+++ b/torch/ao/quantization/qconfig.py\n@@ -103,7 +103,7 @@ def __new__(cls, activation, weight):\n         if isinstance(activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"QConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        return super(QConfig, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n class QConfigDynamic(namedtuple('QConfigDynamic', ['activation', 'weight'])):\n@@ -128,7 +128,7 @@ def __new__(cls, activation=torch.nn.Identity, weight=torch.nn.Identity):\n             raise ValueError(\"QConfigDynamic received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n         warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n-        return super(QConfigDynamic, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n default_qconfig = QConfig(activation=default_observer,\n@@ -236,7 +236,7 @@ def get_default_qconfig(backend='x86', version=0):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     if version == 0:\n@@ -326,7 +326,7 @@ def get_default_qat_qconfig(backend='x86', version=1):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     # Histogram observer is too slow for quantization aware training\ndiff --git a/torch/ao/quantization/quantization_mappings.py b/torch/ao/quantization/quantization_mappings.py\nindex 96db52624acd34..4b3f4d26b2ac09 100644\n--- a/torch/ao/quantization/quantization_mappings.py\n+++ b/torch/ao/quantization/quantization_mappings.py\n@@ -251,7 +251,7 @@ def get_static_quant_module_class(\n         else DEFAULT_STATIC_QUANT_MODULE_MAPPINGS, additional_static_quant_mapping)\n     static_quant_module_class = all_mappings.get(float_module_class, None)\n     assert static_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(static_quant_module_class)\n \n@@ -266,7 +266,7 @@ def get_dynamic_quant_module_class(\n     all_mappings = get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping)\n     dynamic_quant_module_class = all_mappings.get(float_module_class, None)\n     assert dynamic_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(dynamic_quant_module_class)\n \n@@ -300,10 +300,10 @@ def get_default_qconfig_propagation_list() -> Set[Callable]:\n     attribute to in prepare\n     '''\n     QCONFIG_PROPAGATE_MODULE_CLASS_LIST = (\n-        (set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         _INCLUDE_QCONFIG_PROPAGATE_LIST)\n+         _INCLUDE_QCONFIG_PROPAGATE_LIST\n     )\n     return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n \n@@ -332,7 +332,7 @@ def get_quantized_operator(float_op: Union[Callable, str]) -> Callable:\n     '''\n     quantized_op = DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS.get(float_op, None)\n     assert quantized_op is not None, \\\n-        'Operator {} does not have corresponding quantized op'.format(str(float_op))\n+        f'Operator {str(float_op)} does not have corresponding quantized op'\n     return quantized_op\n \n def _get_special_act_post_process(module: torch.nn.Module) -> Optional[Callable]:\ndiff --git a/torch/ao/quantization/quantize.py b/torch/ao/quantization/quantize.py\nindex ca60475fc95a79..23c234c3f35103 100644\n--- a/torch/ao/quantization/quantize.py\n+++ b/torch/ao/quantization/quantize.py\n@@ -442,7 +442,7 @@ def quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8,\n             }\n         else:\n             raise ValueError(\n-                \"Don't know how to quantize with default settings for {}. Provide full qconfig please\".format(dtype))\n+                f\"Don't know how to quantize with default settings for {dtype}. Provide full qconfig please\")\n     elif isinstance(qconfig_spec, set):\n         if dtype is torch.qint8:\n             default_qconfig = default_dynamic_qconfig\ndiff --git a/torch/ao/quantization/utils.py b/torch/ao/quantization/utils.py\nindex 1a82c0fcf432ae..d97b76fb253389 100644\n--- a/torch/ao/quantization/utils.py\n+++ b/torch/ao/quantization/utils.py\n@@ -327,7 +327,7 @@ def check_min_max_valid(min_val: torch.Tensor, max_val: torch.Tensor) -> bool:\n     else:\n         assert torch.all(\n             min_val <= max_val\n-        ), \"min {} should be less than max {}\".format(min_val, max_val)\n+        ), f\"min {min_val} should be less than max {max_val}\"\n \n     return True\n \n"
  },
  {
    "number": 105419,
    "title": "[BE] Enable ruff's UP rules and autoformat benchmarks/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "75325028dd5879beaf293438a81c81dbe4c93258",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105419",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105419/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105419.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105419.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105419/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105419/comments",
    "labels": [
      "module: dynamo",
      "ciflow/inductor",
      "release notes: distributed (ddp)"
    ],
    "_event_time": "2023-07-18T01:17:35.268067Z",
    "state": "closed",
    "patch": "From 53a7fb206358b94495dd3ff116bb423e48c84c1b Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:28 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat benchmarks/\n\n[ghstack-poisoned]\n---\n benchmarks/compare-fastrnn-results.py              |  4 ++--\n benchmarks/cpp/tensorexpr/bench_ops.py             |  4 ++--\n benchmarks/distributed/ddp/benchmark.py            | 14 +++++++-------\n benchmarks/distributed/ddp/diff.py                 | 10 +++++-----\n benchmarks/distributed/pipeline/pipe.py            |  4 ++--\n .../distributed/rpc/parameter_server/launcher.py   |  2 +-\n benchmarks/distributed/rpc/rl/coordinator.py       |  2 +-\n benchmarks/dynamo/_onnx/reporter.py                | 12 ++++++------\n benchmarks/dynamo/benchmarks.py                    |  2 +-\n benchmarks/dynamo/combine_csv.py                   |  2 +-\n benchmarks/dynamo/common.py                        |  4 ++--\n .../dynamo/microbenchmarks/operator_inp_utils.py   |  2 +-\n benchmarks/dynamo/parse_logs.py                    |  2 +-\n benchmarks/dynamo/runner.py                        | 10 +++++-----\n benchmarks/dynamo/timm_models.py                   |  4 ++--\n benchmarks/fastrnns/bench.py                       |  2 +-\n benchmarks/fastrnns/profile.py                     |  8 ++++----\n benchmarks/fastrnns/runner.py                      |  6 +++---\n benchmarks/fastrnns/test.py                        |  4 ++--\n .../framework_overhead_benchmark/C2Module.py       |  4 ++--\n .../framework_overhead_benchmark.py                |  6 +++---\n .../pt_wrapper_module.py                           |  4 ++--\n benchmarks/framework_overhead_benchmark/utils.py   |  2 +-\n .../functional_autograd_benchmark/compare.py       |  4 ++--\n .../functional_autograd_benchmark.py               |  6 +++---\n benchmarks/functional_autograd_benchmark/utils.py  |  2 +-\n benchmarks/instruction_counts/applications/ci.py   |  2 +-\n benchmarks/instruction_counts/core/expand.py       |  2 +-\n benchmarks/operator_benchmark/benchmark_caffe2.py  |  6 +++---\n benchmarks/operator_benchmark/benchmark_core.py    |  8 ++++----\n benchmarks/operator_benchmark/benchmark_pytorch.py |  2 +-\n .../operator_benchmark/common/repeat_benchmark.py  |  2 +-\n benchmarks/overrides_benchmark/bench.py            |  4 ++--\n benchmarks/sparse/dlmc/matmul_bench.py             |  4 ++--\n benchmarks/sparse/dlmc/utils.py                    |  8 ++++----\n benchmarks/tensorexpr/__main__.py                  |  9 ++++-----\n benchmarks/tensorexpr/benchmark.py                 |  4 ++--\n benchmarks/tensorexpr/reduction.py                 |  2 +-\n benchmarks/upload_scribe.py                        |  2 +-\n 39 files changed, 90 insertions(+), 91 deletions(-)\n\ndiff --git a/benchmarks/compare-fastrnn-results.py b/benchmarks/compare-fastrnn-results.py\nindex bc4a55688b17ed..45961ca78de53b 100644\n--- a/benchmarks/compare-fastrnn-results.py\n+++ b/benchmarks/compare-fastrnn-results.py\n@@ -23,9 +23,9 @@ def get_times(json_data):\n parser.add_argument('--format', default='md', type=str, help='output format (csv, md, json, table)')\n args = parser.parse_args()\n \n-with open(args.base, \"r\") as base:\n+with open(args.base) as base:\n     base_times = get_times(json.load(base))\n-with open(args.diff, \"r\") as diff:\n+with open(args.diff) as diff:\n     diff_times = get_times(json.load(diff))\n \n all_keys = set(base_times.keys()).union(diff_times.keys())\ndiff --git a/benchmarks/cpp/tensorexpr/bench_ops.py b/benchmarks/cpp/tensorexpr/bench_ops.py\nindex 12d766ae74862c..fef18f912e75c0 100644\n--- a/benchmarks/cpp/tensorexpr/bench_ops.py\n+++ b/benchmarks/cpp/tensorexpr/bench_ops.py\n@@ -83,8 +83,8 @@ def test_batch_norm():\n         [5, 512, 7, 7]]\n     for n, c, h, w in batch_norm_shapes:\n         x = torch.rand((n, c, h, w))\n-        y = torch.rand((c))\n-        z = torch.rand((c))\n+        y = torch.rand(c)\n+        z = torch.rand(c)\n         traced = torch.jit.trace(lambda x, y, z: op(x, y, z), (x, y, z))\n \n         # Warmup.\ndiff --git a/benchmarks/distributed/ddp/benchmark.py b/benchmarks/distributed/ddp/benchmark.py\nindex c72e3e6a27d961..db592cf6bd32d6 100644\n--- a/benchmarks/distributed/ddp/benchmark.py\n+++ b/benchmarks/distributed/ddp/benchmark.py\n@@ -181,7 +181,7 @@ def __init__(self, device, distributed_backend, bucket_size, model):\n         self.model = model\n \n     def __str__(self):\n-        return \"{} with batch size {}\".format(self.model, self.batch_size)\n+        return f\"{self.model} with batch size {self.batch_size}\"\n \n     def create_model(self):\n         return torchvision.models.__dict__[self.model]().to(self.device)\n@@ -212,7 +212,7 @@ def main():\n     # metadata, like measurements. Not for benchmarking itself.\n     dist.init_process_group(\n         backend=\"gloo\",\n-        init_method=\"tcp://{}:{}\".format(args.master_addr, args.master_port),\n+        init_method=f\"tcp://{args.master_addr}:{args.master_port}\",\n         rank=args.rank,\n         world_size=args.world_size,\n     )\n@@ -227,10 +227,10 @@ def main():\n         print(\"PyTorch distributed benchmark suite\")\n         print(\"-----------------------------------\")\n         print(\"\")\n-        print(\"* PyTorch version: {}\".format(torch.__version__))\n-        print(\"* CUDA version: {}\".format(torch.version.cuda))\n-        print(\"* Distributed backend: {}\".format(args.distributed_backend))\n-        print(\"* Maximum bucket size: {}MB\".format(args.bucket_size))\n+        print(f\"* PyTorch version: {torch.__version__}\")\n+        print(f\"* CUDA version: {torch.version.cuda}\")\n+        print(f\"* Distributed backend: {args.distributed_backend}\")\n+        print(f\"* Maximum bucket size: {args.bucket_size}MB\")\n         print(\"\")\n         print(\"--- nvidia-smi topo -m ---\")\n         print(\"\")\n@@ -261,7 +261,7 @@ def main():\n     benchmark_results = []\n     for benchmark in benchmarks:\n         if args.rank == 0:\n-            print(\"\\nBenchmark: {}\".format(str(benchmark)))\n+            print(f\"\\nBenchmark: {str(benchmark)}\")\n         result = sweep(benchmark)\n         benchmark_results.append({\n             \"model\": benchmark.model,\ndiff --git a/benchmarks/distributed/ddp/diff.py b/benchmarks/distributed/ddp/diff.py\nindex d427a5b29d9199..bce7a8db56c13e 100644\n--- a/benchmarks/distributed/ddp/diff.py\n+++ b/benchmarks/distributed/ddp/diff.py\n@@ -10,7 +10,7 @@\n \n \n def load(path):\n-    with open(path, 'r') as f:\n+    with open(path) as f:\n         return json.load(f)\n \n \n@@ -44,8 +44,8 @@ def main():\n \n         model = ra[\"model\"]\n         batch_size = int(ra[\"batch_size\"])\n-        name = \"{} with batch size {}\".format(model, batch_size)\n-        print(\"Benchmark: {}\".format(name))\n+        name = f\"{model} with batch size {batch_size}\"\n+        print(f\"Benchmark: {name}\")\n \n         # Print header\n         print(\"\")\n@@ -66,13 +66,13 @@ def main():\n             ngpus = len(xa[\"ranks\"])\n             ma = sorted(xa[\"measurements\"])\n             mb = sorted(xb[\"measurements\"])\n-            print(\"{:>4d} GPUs:\".format(ngpus), end='')  # noqa: E999\n+            print(f\"{ngpus:>4d} GPUs:\", end='')  # noqa: E999\n             for p in [75, 95]:\n                 va = np.percentile(ma, p)\n                 vb = np.percentile(mb, p)\n                 # We're measuring time, so lower is better (hence the negation)\n                 delta = -100 * ((vb - va) / va)\n-                print(\"  p{:02d}: {:8.3f}s {:7d}/s {:+8.1f}%\".format(p, vb, int(batch_size / vb), delta), end='')  # noqa: E999\n+                print(f\"  p{p:02d}: {vb:8.3f}s {int(batch_size / vb):7d}/s {delta:+8.1f}%\", end='')  # noqa: E999\n             print(\"\")\n         print(\"\")\n \ndiff --git a/benchmarks/distributed/pipeline/pipe.py b/benchmarks/distributed/pipeline/pipe.py\nindex 8a08d25ca4c940..58f2850e34868e 100644\n--- a/benchmarks/distributed/pipeline/pipe.py\n+++ b/benchmarks/distributed/pipeline/pipe.py\n@@ -16,7 +16,7 @@\n def sizeof_fmt(num, suffix='B'):\n     for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti']:\n         if abs(num) < 1024.0:\n-            return \"%3.2f%sB\" % (num, unit)\n+            return f\"{num:3.2f}{unit}B\"\n         num /= 1024.0\n \n \n@@ -146,7 +146,7 @@ def get_last_device(model):\n             return torch.cuda.current_device()\n \n \n-    print('Number of parameters for model: {}'.format(sum(p.numel() for p in model.parameters())))\n+    print(f'Number of parameters for model: {sum(p.numel() for p in model.parameters())}')\n     for i, batch in enumerate(lm_dataloader):\n         bi = batch[\"input\"]\n         if args.max_batch and i > args.max_batch:\ndiff --git a/benchmarks/distributed/rpc/parameter_server/launcher.py b/benchmarks/distributed/rpc/parameter_server/launcher.py\nindex a4c13cdb29b696..ec6559c8508f9c 100644\n--- a/benchmarks/distributed/rpc/parameter_server/launcher.py\n+++ b/benchmarks/distributed/rpc/parameter_server/launcher.py\n@@ -362,7 +362,7 @@ def get_json_config(file_name, id):\n         file_name (str): name of configuration file to load\n         id (str): configuration that will be loaded\n     \"\"\"\n-    with open(os.path.join(Path(__file__).parent, file_name), \"r\") as f:\n+    with open(os.path.join(Path(__file__).parent, file_name)) as f:\n         json_config = json.load(f)[id]\n     return json_config\n \ndiff --git a/benchmarks/distributed/rpc/rl/coordinator.py b/benchmarks/distributed/rpc/rl/coordinator.py\nindex b488378d5aee58..8b6d246f0861f6 100644\n--- a/benchmarks/distributed/rpc/rl/coordinator.py\n+++ b/benchmarks/distributed/rpc/rl/coordinator.py\n@@ -102,7 +102,7 @@ def run_coordinator(self, episodes, episode_steps, queue):\n                              'observer throughput': {}}\n \n \n-        print(\"For batch size {0}\".format(self.batch_size))\n+        print(f\"For batch size {self.batch_size}\")\n         print(\"\\nAgent Latency - \", len(agent_latency_final))\n         agent_latency_final = sorted(agent_latency_final)\n         for p in [50, 75, 90, 95]:\ndiff --git a/benchmarks/dynamo/_onnx/reporter.py b/benchmarks/dynamo/_onnx/reporter.py\nindex f10b50c4e323df..9318b7a6dc2e47 100644\n--- a/benchmarks/dynamo/_onnx/reporter.py\n+++ b/benchmarks/dynamo/_onnx/reporter.py\n@@ -22,7 +22,7 @@\n _COMPACT_ERROR_GROUP = False\n \n \n-class ErrorAggregator(object):\n+class ErrorAggregator:\n     \"\"\"\n     Collect and group error messages for report at the end.\n \n@@ -47,7 +47,7 @@ class ErrorAggregator(object):\n     ]\n \n     def __init__(self, log: Optional[logging.Logger] = None):\n-        super(ErrorAggregator, self).__init__()\n+        super().__init__()\n         self.error_groups = []\n         self.bigram_to_group_ids = collections.defaultdict(list)\n         self.log = log or logging.getLogger(__name__)\n@@ -141,7 +141,7 @@ def __len__(self):\n         return sum(map(len, self.error_groups))\n \n \n-class ErrorAggregatorDict(object):\n+class ErrorAggregatorDict:\n     \"\"\"\n     Collect error types and individually group their error messages for a debug report at the end.\n \n@@ -152,7 +152,7 @@ class ErrorAggregatorDict(object):\n     \"\"\"\n \n     def __init__(self):\n-        super(ErrorAggregatorDict, self).__init__()\n+        super().__init__()\n         self.aggregator: Dict[str, ErrorAggregator] = dict()\n \n     def __getitem__(self, item: str):\n@@ -179,7 +179,7 @@ def record(self, error_type: str, error: str, module: str):\n             log.exception(\"%s error from %s\", error_type, module)\n \n \n-class ExportErrorCsvParser(object):\n+class ExportErrorCsvParser:\n     \"\"\"Parses `*_export_error.csv` produced by onnxbench, aggregates errors and produces report.\n \n     Two types of aggregations are performed.\n@@ -310,7 +310,7 @@ def row(self) -> List[str]:\n         return [getattr(self, field.name) for field in dataclasses.fields(self)]\n \n \n-class ExportErrorParser(object):\n+class ExportErrorParser:\n     def __init__(self, device: str, model_name: str, batch_size: int):\n         self.device = device\n         self.model_name = model_name\ndiff --git a/benchmarks/dynamo/benchmarks.py b/benchmarks/dynamo/benchmarks.py\nindex 36aaf33df96b1f..cb4cc84867ca9b 100755\n--- a/benchmarks/dynamo/benchmarks.py\n+++ b/benchmarks/dynamo/benchmarks.py\n@@ -9,7 +9,7 @@\n # TOOD(voz): Someday, consolidate all the files into one runner instead of a shim like this...\n def model_names(filename: str) -> Set[str]:\n     names = set()\n-    with open(filename, \"r\") as fh:\n+    with open(filename) as fh:\n         lines = fh.readlines()\n         lines = [line.rstrip() for line in lines]\n         for line in lines:\ndiff --git a/benchmarks/dynamo/combine_csv.py b/benchmarks/dynamo/combine_csv.py\nindex b579e0a1bbbd5c..560b8a3cf2405a 100644\n--- a/benchmarks/dynamo/combine_csv.py\n+++ b/benchmarks/dynamo/combine_csv.py\n@@ -11,7 +11,7 @@\n RESULTS = defaultdict(dict)\n \n for side, f in zip([\"static\", \"dynamic\"], sys.argv[1:]):\n-    with open(f, \"r\") as f:\n+    with open(f) as f:\n         reader = csv.DictReader(f)\n         for row in reader:\n             RESULTS[(row[\"bench\"], row[\"name\"])][side] = row\ndiff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py\nindex cabc18f35697a7..0e1ce36461a122 100644\n--- a/benchmarks/dynamo/common.py\n+++ b/benchmarks/dynamo/common.py\n@@ -341,7 +341,7 @@ def load_model_from_path(path_and_class_str):\n \n def output_csv(filename, headers, row):\n     if os.path.exists(filename):\n-        with open(filename, \"r\") as fd:\n+        with open(filename) as fd:\n             lines = list(csv.reader(fd)) or [[]]\n             if headers and len(headers) > len(lines[0]):\n                 # if prior results failed the header might not be filled in yet\n@@ -1417,7 +1417,7 @@ def read_batch_size_from_file(args, filename, model_name):\n     if os.path.exists(\"benchmarks\"):\n         filename = os.path.join(\"benchmarks\", filename)\n     assert os.path.exists(filename), filename\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n         lines = [i.split(\",\") for i in lines if len(i.strip()) > 0]\n         for val in lines:\ndiff --git a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\nindex 997624f583b4c8..f83568c4db6cc6 100644\n--- a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n+++ b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n@@ -240,7 +240,7 @@ class OperatorInputsLoader:\n     def __init__(self, json_file_path):\n         self.operator_db = defaultdict(Counter)\n \n-        with open(json_file_path, \"r\") as f:\n+        with open(json_file_path) as f:\n             lines = f.readlines()\n \n         i = 0\ndiff --git a/benchmarks/dynamo/parse_logs.py b/benchmarks/dynamo/parse_logs.py\nindex aeb72c231c5462..8ae272897903f9 100644\n--- a/benchmarks/dynamo/parse_logs.py\n+++ b/benchmarks/dynamo/parse_logs.py\n@@ -15,7 +15,7 @@\n \n assert len(sys.argv) == 2\n \n-full_log = open(sys.argv[1], \"r\").read()\n+full_log = open(sys.argv[1]).read()\n \n # If the log contains a gist URL, extract it so we can include it in the CSV\n gist_url = \"\"\ndiff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py\nindex c3c3e8bc046c8e..5a544914ca74bd 100755\n--- a/benchmarks/dynamo/runner.py\n+++ b/benchmarks/dynamo/runner.py\n@@ -608,7 +608,7 @@ def __init__(\n \n     def has_header(self, output_filename):\n         header_present = False\n-        with open(output_filename, \"r\") as f:\n+        with open(output_filename) as f:\n             line = f.readline()\n             if \"dev\" in line:\n                 header_present = True\n@@ -1026,7 +1026,7 @@ def __init__(self, args):\n         assert os.path.exists(self.lookup_file)\n \n     def generate_diff(self, last2, filename, caption):\n-        df_cur, df_prev = [pd.read_csv(os.path.join(path, filename)) for path in last2]\n+        df_cur, df_prev = (pd.read_csv(os.path.join(path, filename)) for path in last2)\n         df_merge = df_cur.merge(df_prev, on=\"Compiler\", suffixes=(\"_cur\", \"_prev\"))\n         data = {col: [] for col in (\"compiler\", \"suite\", \"prev_value\", \"cur_value\")}\n         for _, row in df_merge.iterrows():\n@@ -1145,10 +1145,10 @@ def generate_comment(self):\n                     if last2[compiler] is None:\n                         continue\n \n-                    df_cur, df_prev = [\n+                    df_cur, df_prev = (\n                         last2[compiler][i].untouched_parsed_frames[suite][metric]\n                         for i in (0, 1)\n-                    ]\n+                    )\n                     df_merge = df_cur.merge(\n                         df_prev, on=\"name\", suffixes=(\"_cur\", \"_prev\")\n                     )\n@@ -1367,7 +1367,7 @@ def gen_comment(self):\n         all_lines = []\n         for f in files:\n             try:\n-                with open(os.path.join(self.output_dir, f), \"r\") as fh:\n+                with open(os.path.join(self.output_dir, f)) as fh:\n                     all_lines.extend(fh.readlines())\n             except FileNotFoundError:\n                 pass\ndiff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py\nindex 75769f7cb6c50a..587dbb93683f3f 100755\n--- a/benchmarks/dynamo/timm_models.py\n+++ b/benchmarks/dynamo/timm_models.py\n@@ -31,7 +31,7 @@ def pip_install(package):\n TIMM_MODELS = dict()\n filename = os.path.join(os.path.dirname(__file__), \"timm_models_list.txt\")\n \n-with open(filename, \"r\") as fh:\n+with open(filename) as fh:\n     lines = fh.readlines()\n     lines = [line.rstrip() for line in lines]\n     for line in lines:\n@@ -92,7 +92,7 @@ def read_models_from_docs():\n         models = set()\n         # TODO - set the path to pytorch-image-models repo\n         for fn in glob.glob(\"../pytorch-image-models/docs/models/*.md\"):\n-            with open(fn, \"r\") as f:\n+            with open(fn) as f:\n                 while True:\n                     line = f.readline()\n                     if not line:\ndiff --git a/benchmarks/fastrnns/bench.py b/benchmarks/fastrnns/bench.py\nindex d4b70ff78b7a72..f0e9679b80f275 100644\n--- a/benchmarks/fastrnns/bench.py\n+++ b/benchmarks/fastrnns/bench.py\n@@ -187,7 +187,7 @@ def bench(rnn_runners, group_name, print_json=False, sep=' ', **params):\n \n \n def bench_group(model_list, bench_name, bench_group, bench_args):\n-    print_stderr('Benchmarking {}s...'.format(bench_name))\n+    print_stderr(f'Benchmarking {bench_name}s...')\n     nn_results = bench(get_nn_runners(*model_list), bench_group, **bench_args)\n     print_stderr('')\n     return nn_results\ndiff --git a/benchmarks/fastrnns/profile.py b/benchmarks/fastrnns/profile.py\nindex 7f3de61ef9c39f..10707fab986bc7 100644\n--- a/benchmarks/fastrnns/profile.py\n+++ b/benchmarks/fastrnns/profile.py\n@@ -54,7 +54,7 @@ def profile(rnns, sleep_between_seconds=1, nloops=5,\n \n def system(command):\n     \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n-    print('[system] {}'.format(command))\n+    print(f'[system] {command}')\n     p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, shell=True)\n     output, err = p.communicate()\n@@ -87,13 +87,13 @@ def nvprof_output_filename(rnns, **params):\n \n \n def nvprof(cmd, outpath):\n-    return system('nvprof -o {} {}'.format(outpath, cmd))\n+    return system(f'nvprof -o {outpath} {cmd}')\n \n \n def full_profile(rnns, **args):\n     profile_args = []\n     for k, v in args.items():\n-        profile_args.append('--{}={}'.format(k, v))\n+        profile_args.append(f'--{k}={v}')\n     profile_args.append('--rnns {}'.format(' '.join(rnns)))\n     profile_args.append('--internal-run')\n \n@@ -103,7 +103,7 @@ def full_profile(rnns, **args):\n         sys.executable, ' '.join(profile_args))\n     rc, stdout, stderr = nvprof(cmd, outpath)\n     if rc != 0:\n-        raise RuntimeError('stderr: {}\\nstdout: {}'.format(stderr, stdout))\n+        raise RuntimeError(f'stderr: {stderr}\\nstdout: {stdout}')\n \n \n if __name__ == '__main__':\ndiff --git a/benchmarks/fastrnns/runner.py b/benchmarks/fastrnns/runner.py\nindex c6bf3727f38071..c33f3c92ad0f04 100644\n--- a/benchmarks/fastrnns/runner.py\n+++ b/benchmarks/fastrnns/runner.py\n@@ -11,7 +11,7 @@\n                       varlen_lstm_creator, varlen_pytorch_lstm_creator)\n \n \n-class DisableCuDNN():\n+class DisableCuDNN:\n     def __enter__(self):\n         self.saved = torch.backends.cudnn.enabled\n         torch.backends.cudnn.enabled = False\n@@ -20,7 +20,7 @@ def __exit__(self, *args, **kwargs):\n         torch.backends.cudnn.enabled = self.saved\n \n \n-class DummyContext():\n+class DummyContext:\n     def __enter__(self):\n         pass\n \n@@ -28,7 +28,7 @@ def __exit__(self, *args, **kwargs):\n         pass\n \n \n-class AssertNoJIT():\n+class AssertNoJIT:\n     def __enter__(self):\n         import os\n         enabled = os.environ.get('PYTORCH_JIT', 1)\ndiff --git a/benchmarks/fastrnns/test.py b/benchmarks/fastrnns/test.py\nindex a56cf928fd7add..640af10b95c042 100644\n--- a/benchmarks/fastrnns/test.py\n+++ b/benchmarks/fastrnns/test.py\n@@ -71,7 +71,7 @@ def test_vl_py(**test_args):\n     control_creator = varlen_pytorch_lstm_creator\n     name, experim_creator, context = get_nn_runners('vl_py')[0]\n     with context():\n-        print('testing {}...'.format(name))\n+        print(f'testing {name}...')\n         creator_keys = [\n             'seqLength', 'numLayers', 'inputSize',\n             'hiddenSize', 'miniBatch', 'device', 'seed'\n@@ -154,5 +154,5 @@ def test_vl_py(**test_args):\n \n     for name, creator, context in rnn_runners:\n         with context():\n-            print('testing {}...'.format(name))\n+            print(f'testing {name}...')\n             test_rnns(creator, pytorch_lstm_creator, **test_args)\ndiff --git a/benchmarks/framework_overhead_benchmark/C2Module.py b/benchmarks/framework_overhead_benchmark/C2Module.py\nindex dfc5e6e79098a6..b6b80e83db07d0 100644\n--- a/benchmarks/framework_overhead_benchmark/C2Module.py\n+++ b/benchmarks/framework_overhead_benchmark/C2Module.py\n@@ -20,14 +20,14 @@ class C2SimpleNet:\n     def __init__(self, op_name, num_inputs=1, debug=False):\n         self.input_names = []\n         self.net = core.Net(\"framework_benchmark_net\")\n-        self.input_names = [\"in_{}\".format(i) for i in range(num_inputs)]\n+        self.input_names = [f\"in_{i}\" for i in range(num_inputs)]\n         for i in range(num_inputs):\n             add_blob(workspace, self.input_names[i], [1])\n         self.net.AddExternalInputs(self.input_names)\n         op_constructor = getattr(self.net, op_name)\n         op_constructor(self.input_names)\n         self.output_name = self.net._net.op[-1].output\n-        print(\"Benchmarking op {}:\".format(op_name))\n+        print(f\"Benchmarking op {op_name}:\")\n         for _ in range(NUM_LOOP_ITERS):\n             output_name = self.net._net.op[-1].output\n             self.input_names[-1] = output_name[0]\ndiff --git a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\nindex 727b78197b39bc..fd02a00c43655d 100644\n--- a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n+++ b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n@@ -31,7 +31,7 @@ def parse_op_args(op):\n def print_results(result):\n     print(\"===================================\")\n     for key, value in result.items():\n-        print(\"{}, latency per iter (us):{}\".format(key, ms_to_us(value)))\n+        print(f\"{key}, latency per iter (us):{ms_to_us(value)}\")\n     print(\"===================================\")\n \n def benchmark_simple_fn(args, config, module_config, module_type, result):\n@@ -46,7 +46,7 @@ def benchmark_simple_fn(args, config, module_config, module_type, result):\n         result:         dictionary instance to be populated with the benchmark result (latency per iter).\n     \"\"\"\n     benchmark_c2_net = args.benchmark_c2_net\n-    print(\"Benchmarking {}\".format(module_type.__name__))\n+    print(f\"Benchmarking {module_type.__name__}\")\n     if benchmark_c2_net:\n         op_name = module_config.c2_op\n         num_inputs = module_config.num_params\n@@ -86,7 +86,7 @@ def main():\n     args = parser.parse_args()\n \n     if args.op not in SUPPORTED_OPS:\n-        print(\"Op {} is not supported: Supported ops are:{}\".format(args.op, SUPPORTED_OPS))\n+        print(f\"Op {args.op} is not supported: Supported ops are:{SUPPORTED_OPS}\")\n         return\n     assert not (args.benchmark_c2_net and args.use_throughput_benchmark), \\\n         \"Benchmarking of C2 net via throughput benchmarking is not yet supported\"\ndiff --git a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\nindex 154564f1c6d799..19f2471cbbfaaa 100644\n--- a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n+++ b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n@@ -31,8 +31,8 @@ def __init__(self, wrapped_type, module_config, debug, save=False):\n             if save:\n                 file_name = self.module_name + \"_\" + pt_fn.__name__ + \".pt\"\n                 torch.jit.save(self.module, file_name)\n-                print(\"Generated graph is saved in {}\".format(file_name))\n-        print(\"Benchmarking module {} with fn {}: Graph mode:{}\".format(self.module_name, pt_fn.__name__, module_config.graph_mode))\n+                print(f\"Generated graph is saved in {file_name}\")\n+        print(f\"Benchmarking module {self.module_name} with fn {pt_fn.__name__}: Graph mode:{module_config.graph_mode}\")\n         if (debug and isinstance(self.module, torch.jit.ScriptModule)):\n             print(self.module.graph)\n             print(self.module.code)\ndiff --git a/benchmarks/framework_overhead_benchmark/utils.py b/benchmarks/framework_overhead_benchmark/utils.py\nindex 9e760d404339ed..2efb67a51f7887 100644\n--- a/benchmarks/framework_overhead_benchmark/utils.py\n+++ b/benchmarks/framework_overhead_benchmark/utils.py\n@@ -26,7 +26,7 @@ def benchmark_module(config, module, use_throughput_benchmark=False):\n     if use_throughput_benchmark:\n         return benchmark_using_throughput_benchmark(config, module)\n     module.forward(config.num_warmup_iters)\n-    print(\"Running module for {} iterations\".format(config.num_iters))\n+    print(f\"Running module for {config.num_iters} iterations\")\n     start = time.time()\n     module.forward(config.num_iters)\n     end = time.time()\ndiff --git a/benchmarks/functional_autograd_benchmark/compare.py b/benchmarks/functional_autograd_benchmark/compare.py\nindex c2c4ef6c95d5be..65a4a3afcea881 100644\n--- a/benchmarks/functional_autograd_benchmark/compare.py\n+++ b/benchmarks/functional_autograd_benchmark/compare.py\n@@ -10,11 +10,11 @@ def main():\n     parser.add_argument(\"--output\", type=str, default=\"\", help=\"Text file where to write the output\")\n     args = parser.parse_args()\n \n-    with open(args.before, \"r\") as f:\n+    with open(args.before) as f:\n         content = f.read()\n     res_before = from_markdown_table(content)\n \n-    with open(args.after, \"r\") as f:\n+    with open(args.after) as f:\n         content = f.read()\n     res_after = from_markdown_table(content)\n \ndiff --git a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\nindex 1b0ef20902da9b..76c447f04a496f 100644\n--- a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n+++ b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n@@ -198,7 +198,7 @@ def noop():\n             pass\n         do_sync = noop\n     else:\n-        device = torch.device(\"cuda:{}\".format(args.gpu))\n+        device = torch.device(f\"cuda:{args.gpu}\")\n         do_sync = torch.cuda.synchronize\n \n     model, inp = model_getter(device)\n@@ -257,7 +257,7 @@ def main():\n             runtimes = torch.tensor(runtimes)\n             mean, var = runtimes.mean(), runtimes.var()\n             results[name][task] = (mean.item(), var.item())\n-            print(\"Results for model {} on task {}: {}s (var: {})\".format(name, task, mean, var))\n+            print(f\"Results for model {name} on task {task}: {mean}s (var: {var})\")\n \n             if has_functorch:\n                 try:\n@@ -269,7 +269,7 @@ def main():\n                 runtimes = torch.tensor(runtimes)\n                 mean, var = runtimes.mean(), runtimes.var()\n                 results[name][f\"functorch {task}\"] = (mean.item(), var.item())\n-                print(\"Results for model {} on task {} using Functorch: {}s (var: {})\".format(name, task, mean, var))\n+                print(f\"Results for model {name} on task {task} using Functorch: {mean}s (var: {var})\")\n \n     if args.output:\n         with open(args.output, \"w\") as f:\ndiff --git a/benchmarks/functional_autograd_benchmark/utils.py b/benchmarks/functional_autograd_benchmark/utils.py\nindex dcf03e7a28d085..23f3481cbde117 100644\n--- a/benchmarks/functional_autograd_benchmark/utils.py\n+++ b/benchmarks/functional_autograd_benchmark/utils.py\n@@ -97,7 +97,7 @@ def from_markdown_table(data: str) -> TimingResultType:\n     res = defaultdict(defaultdict)\n \n     for line in out:\n-        model, task, mean, var = [f.strip() for f in line.strip().split(\"|\") if f]\n+        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n         res[model][task] = (float(mean), float(var))\n \n     return res\ndiff --git a/benchmarks/instruction_counts/applications/ci.py b/benchmarks/instruction_counts/applications/ci.py\nindex 85ee9881d83b55..c34c60b094a857 100644\n--- a/benchmarks/instruction_counts/applications/ci.py\n+++ b/benchmarks/instruction_counts/applications/ci.py\n@@ -70,7 +70,7 @@ def main(argv: List[str]) -> None:\n     }\n \n     if args.destination:\n-        with open(args.destination, \"wt\") as f:\n+        with open(args.destination, \"w\") as f:\n             json.dump(final_results, f)\n \n     if in_debug_mode:\ndiff --git a/benchmarks/instruction_counts/core/expand.py b/benchmarks/instruction_counts/core/expand.py\nindex f6713ee65cb93c..c60925d9e14e91 100644\n--- a/benchmarks/instruction_counts/core/expand.py\n+++ b/benchmarks/instruction_counts/core/expand.py\n@@ -58,7 +58,7 @@ def _generate_torchscript_file(model_src: str, name: str) -> Optional[str]:\n         # to confirm.\n         raise ValueError(f\"File {module_path} already exists.\")\n \n-    with open(module_path, \"wt\") as f:\n+    with open(module_path, \"w\") as f:\n         f.write(model_src)\n \n     # Import magic to actually load our function.\ndiff --git a/benchmarks/operator_benchmark/benchmark_caffe2.py b/benchmarks/operator_benchmark/benchmark_caffe2.py\nindex d5939030d03c1a..df27a172739bf6 100644\n--- a/benchmarks/operator_benchmark/benchmark_caffe2.py\n+++ b/benchmarks/operator_benchmark/benchmark_caffe2.py\n@@ -122,7 +122,7 @@ def run_forward(self, num_runs, print_per_iter=False, cuda_sync=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.forward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator test case: {self.test_name}\")\n \n     def run_backward(self, num_runs, print_per_iter=False):\n         \"\"\" Run the backward path of an operator in a loop\n@@ -130,7 +130,7 @@ def run_backward(self, num_runs, print_per_iter=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.backward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator gradient test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator gradient test case: {self.test_name}\")\n \n     def _print_per_iter(self):\n         pass\n@@ -140,7 +140,7 @@ def create_caffe2_op_test_case(op_bench, test_config):\n     test_case = Caffe2OperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\n \n \ndiff --git a/benchmarks/operator_benchmark/benchmark_core.py b/benchmarks/operator_benchmark/benchmark_core.py\nindex d6fd00f0522b38..3dfb6e3b00d42c 100644\n--- a/benchmarks/operator_benchmark/benchmark_core.py\n+++ b/benchmarks/operator_benchmark/benchmark_core.py\n@@ -197,7 +197,7 @@ def _print_header(self):\n             print(\"# List of Operators to run:\")\n             self.printed_ops_list = set()\n             if self.args.operators:\n-                print(\"# {}\".format(self.args.operators))\n+                print(f\"# {self.args.operators}\")\n \n     def _print_perf_result(self, reported_run_time_us, test_case):\n         if self.args.report_aibench:\n@@ -206,7 +206,7 @@ def _print_perf_result(self, reported_run_time_us, test_case):\n             return\n             test_name = '_'.join([test_case.framework, test_case.test_config.test_name])\n             for run in range(self.num_runs):\n-                print(\"{}Observer \".format(test_case.framework) + json.dumps(\n+                print(f\"{test_case.framework}Observer \" + json.dumps(\n                     {\n                         \"type\": test_name,\n                         \"metric\": \"latency\",\n@@ -349,14 +349,14 @@ def _keep_test(self, test_case):\n     def _print_test_case_info(self, test_case):\n         # Print out the test name and skip the real execution\n         if self.args.list_tests:\n-            print(\"# {}\".format(test_case.test_config.test_name))\n+            print(f\"# {test_case.test_config.test_name}\")\n             return True\n         elif self.args.list_ops:\n             if self.args.operators is None:\n                 op_name = test_case.op_bench.module_name()\n \n                 if op_name not in self.printed_ops_list:\n-                    print(\"# {}\".format(op_name))\n+                    print(f\"# {op_name}\")\n                     self.printed_ops_list.add(op_name)\n             return True\n \ndiff --git a/benchmarks/operator_benchmark/benchmark_pytorch.py b/benchmarks/operator_benchmark/benchmark_pytorch.py\nindex e9a9b3c5de42ad..c4a82dff2ba5c1 100644\n--- a/benchmarks/operator_benchmark/benchmark_pytorch.py\n+++ b/benchmarks/operator_benchmark/benchmark_pytorch.py\n@@ -192,5 +192,5 @@ def create_pytorch_op_test_case(op_bench, test_config):\n     test_case = PyTorchOperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\ndiff --git a/benchmarks/operator_benchmark/common/repeat_benchmark.py b/benchmarks/operator_benchmark/common/repeat_benchmark.py\nindex b744a95d217363..337bf525b29caf 100644\n--- a/benchmarks/operator_benchmark/common/repeat_benchmark.py\n+++ b/benchmarks/operator_benchmark/common/repeat_benchmark.py\n@@ -54,4 +54,4 @@ def pt_repeat_n_times(niters):\n     total_time_s = (time.time() - s)\n     total_time_per_iter_s = total_time_s / NUM_BENCHMARK_ITERS\n     achieved_bandwidth = (total_bytes * BYTES_TO_MB) / total_time_per_iter_s\n-    print(\"Time:{} Achieved Bandwidth:{} MB/s\".format(total_time_per_iter_s, achieved_bandwidth))\n+    print(f\"Time:{total_time_per_iter_s} Achieved Bandwidth:{achieved_bandwidth} MB/s\")\ndiff --git a/benchmarks/overrides_benchmark/bench.py b/benchmarks/overrides_benchmark/bench.py\nindex b6dbd0c2f8d64c..2c591a6e569793 100644\n--- a/benchmarks/overrides_benchmark/bench.py\n+++ b/benchmarks/overrides_benchmark/bench.py\n@@ -56,8 +56,8 @@ def main():\n \n         bench_min, bench_std = bench(tensor_1, tensor_2)\n         print(\n-            \"Type {0} had a minimum time of {1} us\"\n-            \" and a standard deviation of {2} us.\".format(\n+            \"Type {} had a minimum time of {} us\"\n+            \" and a standard deviation of {} us.\".format(\n                 t.__name__, (10 ** 6 * bench_min), (10 ** 6) * bench_std\n             )\n         )\ndiff --git a/benchmarks/sparse/dlmc/matmul_bench.py b/benchmarks/sparse/dlmc/matmul_bench.py\nindex 6b896ddf34a635..8d37d3242dd1bf 100644\n--- a/benchmarks/sparse/dlmc/matmul_bench.py\n+++ b/benchmarks/sparse/dlmc/matmul_bench.py\n@@ -62,8 +62,8 @@ def filter_ops(operation):\n             test_name = device + \":matmul-forward\"\n             return list(filter(None, [\n                 (test_name, device, \"torch:\" + operation.replace(\"sparse\", \"dense\"),\n-                 \"{}(dx, dy)\".format(OPS_MAP[operation])),\n-                (test_name, device, \"torch:\" + operation, \"{}(x, y)\".format(OPS_MAP[operation])),\n+                 f\"{OPS_MAP[operation]}(dx, dy)\"),\n+                (test_name, device, \"torch:\" + operation, f\"{OPS_MAP[operation]}(x, y)\"),\n                 (test_name, device, \"scipy:\" + operation, \"scipy_matmul(sx, sy)\") if device == \"cpu\" else None\n             ]))\n \ndiff --git a/benchmarks/sparse/dlmc/utils.py b/benchmarks/sparse/dlmc/utils.py\nindex 3079abf6e1dff2..8fad391f63f83d 100644\n--- a/benchmarks/sparse/dlmc/utils.py\n+++ b/benchmarks/sparse/dlmc/utils.py\n@@ -21,7 +21,7 @@ def sparse_grad_output(a, b):\n \n \n def read_matrix_params(path):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         line = file.readline()\n         nrows, ncols, nnz = (int(el) for el in line.split(', '))\n         return (nrows, ncols), nnz\n@@ -38,7 +38,7 @@ def csr_to_coo(indices, indptr, shape):\n \n \n def load_sparse_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -51,7 +51,7 @@ def load_sparse_matrix(path, device):\n \n \n def gen_vector(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -59,7 +59,7 @@ def gen_vector(path, device):\n \n \n def gen_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\ndiff --git a/benchmarks/tensorexpr/__main__.py b/benchmarks/tensorexpr/__main__.py\nindex ed632e966b2cb4..fa81ea6bb1c611 100644\n--- a/benchmarks/tensorexpr/__main__.py\n+++ b/benchmarks/tensorexpr/__main__.py\n@@ -157,7 +157,7 @@ def main():\n         torch._C._jit_set_nvfuser_enabled(True)\n         torch._C._get_graph_executor_optimize(True)\n     else :\n-        raise ValueError(\"Undefined fuser: {}\".format(args.cuda_fuser))\n+        raise ValueError(f\"Undefined fuser: {args.cuda_fuser}\")\n \n     if args.cpu_fusion:\n         import torch\n@@ -207,7 +207,7 @@ def set_global_threads(num_threads):\n     for index, dtype in enumerate(datatypes):\n         datatypes[index] = getattr(torch, dtype)\n         if not datatypes[index] :\n-            raise AttributeError(\"DataType: {} is not valid!\".format(dtype))\n+            raise AttributeError(f\"DataType: {dtype} is not valid!\")\n \n     tensor_engine.set_engine_mode(args.engine)\n \n@@ -282,7 +282,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                         run_with_input_iter(bench_cls, args.input_iter, allow_skip=True)\n                     else :\n                         if args.input_iter is not None :\n-                            print(\"WARNING: Incompatible benchmark class called with input_iter arg: {}\".format(name))\n+                            print(f\"WARNING: Incompatible benchmark class called with input_iter arg: {name}\")\n                         run_default_configs(bench_cls, allow_skip=True)\n \n             if match_class_name:\n@@ -321,8 +321,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                     [bench_cls.module() for bench_cls in benchmark_classes]\n                 )\n                 raise ValueError(\n-                    \"invalid name: %s\\nAvailable benchmark classes:\\n%s\"\n-                    % (name, available_classes)\n+                    f\"invalid name: {name}\\nAvailable benchmark classes:\\n{available_classes}\"\n                 )\n \n \ndiff --git a/benchmarks/tensorexpr/benchmark.py b/benchmarks/tensorexpr/benchmark.py\nindex 7a5b255da904aa..2730a1be24784b 100644\n--- a/benchmarks/tensorexpr/benchmark.py\n+++ b/benchmarks/tensorexpr/benchmark.py\n@@ -66,7 +66,7 @@ def desc(self):\n         if \"NNC_NUM_THREADS\" in os.environ:\n             num_threads_str = os.environ[\"NNC_NUM_THREADS\"]\n             device += num_threads_str\n-        return \"%s: %s_%s_%s_%s\" % (\n+        return \"{}: {}_{}_{}_{}\".format(\n             self.engine.mode,\n             self.module(),\n             self.mode,\n@@ -203,7 +203,7 @@ def dump_result(self, result_dict):\n         if self.output_type == \"json\":\n             print(json.dumps(result_dict))\n         elif self.output_type == \"stdout\":\n-            msg = \"%s: %.2f us, SOL %.2f GB/s, algorithmic %.2f GB/s\" % (\n+            msg = \"{}: {:.2f} us, SOL {:.2f} GB/s, algorithmic {:.2f} GB/s\".format(\n                 result_dict[\"desc\"],\n                 result_dict[\"us\"],\n                 result_dict[\"sol\"],\ndiff --git a/benchmarks/tensorexpr/reduction.py b/benchmarks/tensorexpr/reduction.py\nindex 77d64074eb81d1..3613001667746d 100644\n--- a/benchmarks/tensorexpr/reduction.py\n+++ b/benchmarks/tensorexpr/reduction.py\n@@ -139,7 +139,7 @@ def __init__(self, mode, device, dtype, red_dim, dim0, dim1):\n         )]\n \n         if red_dim != 0 and red_dim != 1 :\n-            raise ValueError(\"invalid reduction dimension: {}\".format(red_dim))\n+            raise ValueError(f\"invalid reduction dimension: {red_dim}\")\n \n     def forward(self, inputs):\n         x = self.add(inputs, 0.001)\ndiff --git a/benchmarks/upload_scribe.py b/benchmarks/upload_scribe.py\nindex d476ade1b8df4d..551544b2d288ae 100644\n--- a/benchmarks/upload_scribe.py\n+++ b/benchmarks/upload_scribe.py\n@@ -95,7 +95,7 @@ def post_pytest_benchmarks(self, pytest_json):\n         for b in pytest_json['benchmarks']:\n             test = b['name'].split('[')[0]\n             net_name = b['params']['net_name']\n-            benchmark_name = '{}[{}]'.format(test, net_name)\n+            benchmark_name = f'{test}[{net_name}]'\n             executor = b['params']['executor']\n             fuser = b['params']['fuser']\n             m = self.format_message({\n"
  },
  {
    "number": 105418,
    "title": "[BE] Enable ruff's UP rules and autoformat tools and scripts",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "031166ea20be70fe47200abe61db38e1366114c2",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105418",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105418/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105418.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105418.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105418/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105418/comments",
    "labels": [
      "release notes: releng"
    ],
    "_event_time": "2023-07-18T01:17:30.053719Z",
    "state": "closed",
    "patch": "From 43948ba74f6d822cec4b6eca659ad96bfe78b526 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:23 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat tools and scripts\n\n[ghstack-poisoned]\n---\n .ci/pytorch/create_test_cert.py               |  4 +-\n .../perf_test/compare_with_baseline.py        |  4 +-\n .../cimodel/data/binary_build_definitions.py  |  2 +-\n .../cimodel/data/pytorch_build_definitions.py |  4 +-\n .circleci/generate_config_yml.py              |  4 +-\n .github/scripts/ensure_actions_will_cancel.py |  4 +-\n .github/scripts/file_io_utils.py              |  2 +-\n .github/scripts/filter_test_configs.py        |  2 +-\n .github/scripts/generate_pytorch_version.py   |  2 +-\n .github/scripts/label_utils.py                |  2 +-\n .github/scripts/lint_native_functions.py      |  2 +-\n .github/scripts/run_torchbench.py             |  4 +-\n .github/scripts/test_check_labels.py          |  2 +-\n .github/scripts/test_trymerge.py              |  2 +-\n .github/scripts/trymerge.py                   |  6 +--\n .github/scripts/trymerge_explainer.py         |  2 +-\n docs/caffe2/process.py                        |  4 +-\n docs/cpp/source/conf.py                       |  3 +-\n docs/source/conf.py                           |  3 +-\n .../scripts/exportdb/generate_example_rst.py  |  4 +-\n setup.py                                      | 38 +++++++++----------\n tools/amd_build/build_amd.py                  |  8 ++--\n tools/autograd/gen_python_functions.py        |  8 ++--\n tools/autograd/load_derivatives.py            |  2 +-\n .../gen_op_registration_allowlist.py          |  4 +-\n tools/code_analyzer/gen_oplist.py             |  2 +-\n tools/coverage_plugins_package/setup.py       |  2 +-\n tools/download_mnist.py                       | 10 ++---\n tools/gen_vulkan_spv.py                       | 20 +++++-----\n tools/generate_torch_version.py               | 12 +++---\n tools/jit/gen_unboxing.py                     |  2 +-\n tools/linter/adapters/constexpr_linter.py     |  2 +-\n tools/linter/adapters/grep_linter.py          |  2 +-\n tools/nightly.py                              |  8 ++--\n tools/onnx/gen_diagnostics.py                 |  2 +-\n tools/onnx/update_default_opset_version.py    |  2 +-\n tools/pyi/gen_pyi.py                          | 20 +++++-----\n tools/setup_helpers/cmake.py                  | 18 ++++-----\n tools/setup_helpers/cmake_utils.py            |  2 +-\n tools/setup_helpers/generate_code.py          |  2 +-\n tools/stats/import_test_stats.py              |  2 +-\n tools/stats/upload_stats_lib.py               |  4 +-\n tools/substitute.py                           |  2 +-\n tools/test/test_executorch_gen.py             |  4 +-\n tools/test/test_executorch_signatures.py      |  8 ++--\n tools/test/test_vulkan_codegen.py             |  4 +-\n tools/testing/explicit_ci_jobs.py             |  2 +-\n tools/testing/test_selections.py              |  2 +-\n torch/package/package_exporter.py             | 20 ++++------\n 49 files changed, 134 insertions(+), 142 deletions(-)\n\ndiff --git a/.ci/pytorch/create_test_cert.py b/.ci/pytorch/create_test_cert.py\nindex d3ead7ae259434..4e31f97878f41b 100644\n--- a/.ci/pytorch/create_test_cert.py\n+++ b/.ci/pytorch/create_test_cert.py\n@@ -88,9 +88,9 @@ def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n \n \n ca_key = genrsa(temp_dir + \"/ca.key\")\n-ca_cert = create_cert(temp_dir + \"/ca.pem\", u\"US\", u\"New York\", u\"New York\", u\"Gloo Certificate Authority\", ca_key)\n+ca_cert = create_cert(temp_dir + \"/ca.pem\", \"US\", \"New York\", \"New York\", \"Gloo Certificate Authority\", ca_key)\n \n pkey = genrsa(temp_dir + \"/pkey.key\")\n-csr = create_req(temp_dir + \"/csr.csr\", u\"US\", u\"California\", u\"San Francisco\", u\"Gloo Testing Company\", pkey)\n+csr = create_req(temp_dir + \"/csr.csr\", \"US\", \"California\", \"San Francisco\", \"Gloo Testing Company\", pkey)\n \n cert = sign_certificate_request(temp_dir + \"/cert.pem\", csr, ca_cert, ca_key)\ndiff --git a/.ci/pytorch/perf_test/compare_with_baseline.py b/.ci/pytorch/perf_test/compare_with_baseline.py\nindex 6d2839ac1db412..f7b962632cd79f 100644\n--- a/.ci/pytorch/perf_test/compare_with_baseline.py\n+++ b/.ci/pytorch/perf_test/compare_with_baseline.py\n@@ -19,7 +19,7 @@\n elif 'gpu' in test_name:\n     backend = 'gpu'\n \n-data_file_path = '../{}_runtime.json'.format(backend)\n+data_file_path = f'../{backend}_runtime.json'\n \n with open(data_file_path) as data_file:\n     data = json.load(data_file)\n@@ -69,7 +69,7 @@\n     print(\"z-value < 3, no perf regression detected.\")\n     if args.update:\n         print(\"We will use these numbers as new baseline.\")\n-        new_data_file_path = '../new_{}_runtime.json'.format(backend)\n+        new_data_file_path = f'../new_{backend}_runtime.json'\n         with open(new_data_file_path) as new_data_file:\n             new_data = json.load(new_data_file)\n         new_data[test_name] = {}\ndiff --git a/.circleci/cimodel/data/binary_build_definitions.py b/.circleci/cimodel/data/binary_build_definitions.py\nindex 45981e8e9ea77b..7dccbdc7cbf689 100644\n--- a/.circleci/cimodel/data/binary_build_definitions.py\n+++ b/.circleci/cimodel/data/binary_build_definitions.py\n@@ -5,7 +5,7 @@\n import cimodel.lib.conf_tree as conf_tree\n import cimodel.lib.miniutils as miniutils\n \n-class Conf(object):\n+class Conf:\n     def __init__(self, os, gpu_version, pydistro, parms, smoke, libtorch_variant, gcc_config_variant, libtorch_config_variant):\n \n         self.os = os\ndiff --git a/.circleci/cimodel/data/pytorch_build_definitions.py b/.circleci/cimodel/data/pytorch_build_definitions.py\nindex 76e87b07c1889f..e6e44bd2b5aeb0 100644\n--- a/.circleci/cimodel/data/pytorch_build_definitions.py\n+++ b/.circleci/cimodel/data/pytorch_build_definitions.py\n@@ -143,7 +143,7 @@ def gen_workflow_job(self, phase):\n \n \n # TODO This is a hack to special case some configs just for the workflow list\n-class HiddenConf(object):\n+class HiddenConf:\n     def __init__(self, name, parent_build=None, filters=None):\n         self.name = name\n         self.parent_build = parent_build\n@@ -160,7 +160,7 @@ def gen_workflow_job(self, phase):\n     def gen_build_name(self, _):\n         return self.name\n \n-class DocPushConf(object):\n+class DocPushConf:\n     def __init__(self, name, parent_build=None, branch=\"master\"):\n         self.name = name\n         self.parent_build = parent_build\ndiff --git a/.circleci/generate_config_yml.py b/.circleci/generate_config_yml.py\nindex b3e47eed8b4317..d1ef439941d4b2 100755\n--- a/.circleci/generate_config_yml.py\n+++ b/.circleci/generate_config_yml.py\n@@ -18,7 +18,7 @@\n import cimodel.lib.miniyaml as miniyaml\n \n \n-class File(object):\n+class File:\n     \"\"\"\n     Verbatim copy the contents of a file into config.yml\n     \"\"\"\n@@ -57,7 +57,7 @@ def horizontal_rule():\n     return \"\".join(\"#\" * 78)\n \n \n-class Header(object):\n+class Header:\n     def __init__(self, title, summary=None):\n         self.title = title\n         self.summary_lines = summary or []\ndiff --git a/.github/scripts/ensure_actions_will_cancel.py b/.github/scripts/ensure_actions_will_cancel.py\nindex 92eb3441acd3cf..8d53f2bed5e18b 100755\n--- a/.github/scripts/ensure_actions_will_cancel.py\n+++ b/.github/scripts/ensure_actions_will_cancel.py\n@@ -17,7 +17,7 @@\n \n \n def should_check(filename: Path) -> bool:\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         content = f.read()\n \n     data = yaml.safe_load(content)\n@@ -37,7 +37,7 @@ def should_check(filename: Path) -> bool:\n     files = [f for f in files if should_check(f)]\n     names = set()\n     for filename in files:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             data = yaml.safe_load(f)\n \n         name = data.get(\"name\")\ndiff --git a/.github/scripts/file_io_utils.py b/.github/scripts/file_io_utils.py\nindex 097b092bc904af..faba9f06d2ac65 100644\n--- a/.github/scripts/file_io_utils.py\n+++ b/.github/scripts/file_io_utils.py\n@@ -44,7 +44,7 @@ def load_json_file(file_path: Path) -> Any:\n     \"\"\"\n     Returns the deserialized json object\n     \"\"\"\n-    with open(file_path, \"r\") as f:\n+    with open(file_path) as f:\n         return json.load(f)\n \n \ndiff --git a/.github/scripts/filter_test_configs.py b/.github/scripts/filter_test_configs.py\nindex 9d1f39a833c571..92968179702273 100755\n--- a/.github/scripts/filter_test_configs.py\n+++ b/.github/scripts/filter_test_configs.py\n@@ -319,7 +319,7 @@ def process_jobs(\n     try:\n         # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking\n         # it into its two components first\n-        current_platform, _ = [n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n]\n+        current_platform, _ = (n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n)\n     except ValueError as error:\n         warnings.warn(f\"Invalid job name {job_name}, returning\")\n         return test_matrix\ndiff --git a/.github/scripts/generate_pytorch_version.py b/.github/scripts/generate_pytorch_version.py\nindex f9a6d49505b308..e70b97165e61d7 100755\n--- a/.github/scripts/generate_pytorch_version.py\n+++ b/.github/scripts/generate_pytorch_version.py\n@@ -50,7 +50,7 @@ def get_tag() -> str:\n \n def get_base_version() -> str:\n     root = get_pytorch_root()\n-    dirty_version = open(root / \"version.txt\", \"r\").read().strip()\n+    dirty_version = open(root / \"version.txt\").read().strip()\n     # Strips trailing a0 from version.txt, not too sure why it's there in the\n     # first place\n     return re.sub(LEGACY_BASE_VERSION_SUFFIX_PATTERN, \"\", dirty_version)\ndiff --git a/.github/scripts/label_utils.py b/.github/scripts/label_utils.py\nindex 812c33b426f441..e3ce0f52fe853f 100644\n--- a/.github/scripts/label_utils.py\n+++ b/.github/scripts/label_utils.py\n@@ -51,7 +51,7 @@ def get_last_page_num_from_header(header: Any) -> int:\n     )\n \n \n-@lru_cache()\n+@lru_cache\n def gh_get_labels(org: str, repo: str) -> List[str]:\n     prefix = f\"https://api.github.com/repos/{org}/{repo}/labels?per_page=100\"\n     header, info = request_for_labels(prefix + \"&page=1\")\ndiff --git a/.github/scripts/lint_native_functions.py b/.github/scripts/lint_native_functions.py\nindex 9bde9e8d84e5f9..4dfe9fd63e2e4e 100755\n--- a/.github/scripts/lint_native_functions.py\n+++ b/.github/scripts/lint_native_functions.py\n@@ -26,7 +26,7 @@ def fn(base: str) -> str:\n     return str(base / Path(\"aten/src/ATen/native/native_functions.yaml\"))\n \n \n-with open(Path(__file__).parent.parent.parent / fn(\".\"), \"r\") as f:\n+with open(Path(__file__).parent.parent.parent / fn(\".\")) as f:\n     contents = f.read()\n \n yaml = ruamel.yaml.YAML()  # type: ignore[attr-defined]\ndiff --git a/.github/scripts/run_torchbench.py b/.github/scripts/run_torchbench.py\nindex 3a80ebbeb9970a..e5e3c7a03dea07 100644\n--- a/.github/scripts/run_torchbench.py\n+++ b/.github/scripts/run_torchbench.py\n@@ -129,7 +129,7 @@ def extract_models_from_pr(\n     model_list = []\n     userbenchmark_list = []\n     pr_list = []\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(filter(lambda x: x.startswith(MAGIC_PREFIX), lines))\n         if magic_lines:\n@@ -157,7 +157,7 @@ def extract_models_from_pr(\n \n def find_torchbench_branch(prbody_file: str) -> str:\n     branch_name: str = \"\"\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(\n             filter(lambda x: x.startswith(MAGIC_TORCHBENCH_PREFIX), lines)\ndiff --git a/.github/scripts/test_check_labels.py b/.github/scripts/test_check_labels.py\nindex 17d33158f2efb4..2b2cd7b6c5204b 100644\n--- a/.github/scripts/test_check_labels.py\n+++ b/.github/scripts/test_check_labels.py\n@@ -15,7 +15,7 @@\n \n \n def mock_parse_args() -> object:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.pr_num = 76123\n \ndiff --git a/.github/scripts/test_trymerge.py b/.github/scripts/test_trymerge.py\nindex 7cf70882ed3d40..a46ef9032a6459 100755\n--- a/.github/scripts/test_trymerge.py\n+++ b/.github/scripts/test_trymerge.py\n@@ -114,7 +114,7 @@ def mocked_rockset_results(head_sha: str, merge_base: str, num_retries: int = 3)\n \n \n def mock_parse_args(revert: bool = False, force: bool = False) -> Any:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.revert = revert\n             self.force = force\ndiff --git a/.github/scripts/trymerge.py b/.github/scripts/trymerge.py\nindex 3cffcaa14efaf8..cc253f36cbd1b7 100755\n--- a/.github/scripts/trymerge.py\n+++ b/.github/scripts/trymerge.py\n@@ -1628,10 +1628,8 @@ def validate_revert(\n         allowed_reverters.append(\"CONTRIBUTOR\")\n     if author_association not in allowed_reverters:\n         raise PostCommentError(\n-            (\n-                f\"Will not revert as @{author_login} is not one of \"\n-                f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n-            )\n+            f\"Will not revert as @{author_login} is not one of \"\n+            f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n         )\n     skip_internal_checks = can_skip_internal_checks(pr, comment_id)\n \ndiff --git a/.github/scripts/trymerge_explainer.py b/.github/scripts/trymerge_explainer.py\nindex ebc74cf63eb833..8aa6ab59b94cbc 100644\n--- a/.github/scripts/trymerge_explainer.py\n+++ b/.github/scripts/trymerge_explainer.py\n@@ -17,7 +17,7 @@ def has_label(labels: List[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:\n     return len(list(filter(pattern.match, labels))) > 0\n \n \n-class TryMergeExplainer(object):\n+class TryMergeExplainer:\n     force: bool\n     labels: List[str]\n     pr_num: int\ndiff --git a/docs/caffe2/process.py b/docs/caffe2/process.py\nindex 3b94b9d38502a2..4a59eec388d90b 100644\n--- a/docs/caffe2/process.py\n+++ b/docs/caffe2/process.py\n@@ -8,7 +8,7 @@\n \n # Module caffe2...caffe2.python.control_test\n def insert(originalfile, first_line, description):\n-    with open(originalfile, 'r') as f:\n+    with open(originalfile) as f:\n         f1 = f.readline()\n         if(f1.find(first_line) < 0):\n             docs = first_line + description + f1\n@@ -30,7 +30,7 @@ def insert(originalfile, first_line, description):\n     for file in files:\n         if (file.endswith(\".py\") and not file.endswith(\"_test.py\") and not file.endswith(\"__.py\")):\n             filepath = os.path.join(root, file)\n-            print((\"filepath: \" + filepath))\n+            print(\"filepath: \" + filepath)\n             directory = os.path.dirname(filepath)[2:]\n             directory = directory.replace(\"/\", \".\")\n             print(\"directory: \" + directory)\ndiff --git a/docs/cpp/source/conf.py b/docs/cpp/source/conf.py\nindex 88648787fa8c8e..2b94cfdb5058fb 100644\n--- a/docs/cpp/source/conf.py\n+++ b/docs/cpp/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -99,7 +98,7 @@\n     ############################################################################\n     # Main library page layout example configuration.                          #\n     ############################################################################\n-    \"afterTitleDescription\": textwrap.dedent(u'''\n+    \"afterTitleDescription\": textwrap.dedent('''\n         Welcome to the developer reference for the PyTorch C++ API.\n     '''),\n }\ndiff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 8bbb189853eec7..8fec5f16f9852c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -624,7 +623,7 @@ def visit_reference(self, node):\n                 anchor = ref_anchor[1]\n                 txt = node.parent.astext()\n                 if txt == anchor or txt == anchor.split('.')[-1]:\n-                    self.body.append('<p id=\"{}\"/>'.format(ref_anchor[1]))\n+                    self.body.append(f'<p id=\"{ref_anchor[1]}\"/>')\n         return old_call(self, node)\n     Klass.visit_reference = visit_reference\n \ndiff --git a/docs/source/scripts/exportdb/generate_example_rst.py b/docs/source/scripts/exportdb/generate_example_rst.py\nindex 58dca5f31ea73d..38f71b905245c3 100644\n--- a/docs/source/scripts/exportdb/generate_example_rst.py\n+++ b/docs/source/scripts/exportdb/generate_example_rst.py\n@@ -31,7 +31,7 @@ def generate_example_rst(example_case: ExportCase):\n         if isinstance(model, torch.nn.Module)\n         else inspect.getfile(model)\n     )\n-    with open(source_file, \"r\") as file:\n+    with open(source_file) as file:\n         source_code = file.read()\n     source_code = re.sub(r\"from torch\\._export\\.db\\.case import .*\\n\", \"\", source_code)\n     source_code = re.sub(r\"@export_case\\((.|\\n)*?\\)\\n\", \"\", source_code)\n@@ -114,7 +114,7 @@ def generate_index_rst(example_cases, tag_to_modules, support_level_to_modules):\n \n     tag_names = \"\\n    \".join(t for t in tag_to_modules.keys())\n \n-    with open(os.path.join(PWD, \"blurb.txt\"), \"r\") as file:\n+    with open(os.path.join(PWD, \"blurb.txt\")) as file:\n         blurb = file.read()\n \n     # Generate contents of the .rst file\ndiff --git a/setup.py b/setup.py\nindex 5f0180cb0c8ac6..d454b2e62f33b4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -323,7 +323,7 @@ def report(*args):\n package_name = os.getenv('TORCH_PACKAGE_NAME', 'torch')\n package_type = os.getenv('PACKAGE_TYPE', 'wheel')\n version = get_torch_version()\n-report(\"Building wheel {}-{}\".format(package_name, version))\n+report(f\"Building wheel {package_name}-{version}\")\n \n cmake = CMake()\n \n@@ -361,7 +361,7 @@ def not_exists_or_empty(folder):\n             start = time.time()\n             subprocess.check_call([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd)\n             end = time.time()\n-            print(' --- Submodule initialization took {:.2f} sec'.format(end - start))\n+            print(f' --- Submodule initialization took {end - start:.2f} sec')\n         except Exception:\n             print(' --- Submodule initalization failed')\n             print('Please run:\\n\\tgit submodule update --init --recursive')\n@@ -616,16 +616,16 @@ def build_extensions(self):\n                 continue\n             fullname = self.get_ext_fullname(ext.name)\n             filename = self.get_ext_filename(fullname)\n-            report(\"\\nCopying extension {}\".format(ext.name))\n+            report(f\"\\nCopying extension {ext.name}\")\n \n             relative_site_packages = sysconfig.get_path('purelib').replace(sysconfig.get_path('data'), '').lstrip(os.path.sep)\n             src = os.path.join(\"torch\", relative_site_packages, filename)\n             if not os.path.exists(src):\n-                report(\"{} does not exist\".format(src))\n+                report(f\"{src} does not exist\")\n                 del self.extensions[i]\n             else:\n                 dst = os.path.join(os.path.realpath(self.build_lib), filename)\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -642,7 +642,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -658,7 +658,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"nvfuser\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -670,7 +670,7 @@ def build_extensions(self):\n     def get_outputs(self):\n         outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n         outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n-        report(\"setup.py::get_outputs returning {}\".format(outputs))\n+        report(f\"setup.py::get_outputs returning {outputs}\")\n         return outputs\n \n     def create_compile_commands(self):\n@@ -694,13 +694,13 @@ def load(filename):\n         new_contents = json.dumps(all_commands, indent=2)\n         contents = ''\n         if os.path.exists('compile_commands.json'):\n-            with open('compile_commands.json', 'r') as f:\n+            with open('compile_commands.json') as f:\n                 contents = f.read()\n         if contents != new_contents:\n             with open('compile_commands.json', 'w') as f:\n                 f.write(new_contents)\n \n-class concat_license_files():\n+class concat_license_files:\n     \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n \n     LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n@@ -723,7 +723,7 @@ def __enter__(self):\n         finally:\n             sys.path = old_path\n \n-        with open(self.f1, 'r') as f1:\n+        with open(self.f1) as f1:\n             self.bsd_text = f1.read()\n \n         with open(self.f1, 'a') as f1:\n@@ -771,7 +771,7 @@ def finalize_options(self):\n     def run(self):\n         import glob\n         import re\n-        with open('.gitignore', 'r') as f:\n+        with open('.gitignore') as f:\n             ignores = f.read()\n             pat = re.compile(r'^#( BEGIN NOT-CLEAN-FILES )?')\n             for wildcard in filter(None, ignores.split('\\n')):\n@@ -934,31 +934,31 @@ def make_relative_rpath_args(path):\n     if cmake_cache_vars['BUILD_CAFFE2']:\n         extensions.append(\n             Extension(\n-                name=str('caffe2.python.caffe2_pybind11_state'),\n+                name='caffe2.python.caffe2_pybind11_state',\n                 sources=[]),\n         )\n         if cmake_cache_vars['USE_CUDA']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_gpu'),\n+                    name='caffe2.python.caffe2_pybind11_state_gpu',\n                     sources=[]),\n             )\n         if cmake_cache_vars['USE_ROCM']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_hip'),\n+                    name='caffe2.python.caffe2_pybind11_state_hip',\n                     sources=[]),\n             )\n     if cmake_cache_vars['BUILD_FUNCTORCH']:\n         extensions.append(\n             Extension(\n-                name=str('functorch._C'),\n+                name='functorch._C',\n                 sources=[]),\n         )\n     if cmake_cache_vars['BUILD_NVFUSER']:\n         extensions.append(\n             Extension(\n-                name=str('nvfuser._C'),\n+                name='nvfuser._C',\n                 sources=[]),\n         )\n \n@@ -1272,7 +1272,7 @@ def main():\n         download_url='https://github.com/pytorch/pytorch/tags',\n         author='PyTorch Team',\n         author_email='packages@pytorch.org',\n-        python_requires='>={}'.format(python_min_version_str),\n+        python_requires=f'>={python_min_version_str}',\n         # PyPI package information.\n         classifiers=[\n             'Development Status :: 5 - Production/Stable',\n@@ -1288,7 +1288,7 @@ def main():\n             'Topic :: Software Development :: Libraries :: Python Modules',\n             'Programming Language :: C++',\n             'Programming Language :: Python :: 3',\n-        ] + ['Programming Language :: Python :: 3.{}'.format(i) for i in range(python_min_version[1], version_range_max)],\n+        ] + [f'Programming Language :: Python :: 3.{i}' for i in range(python_min_version[1], version_range_max)],\n         license='BSD-3',\n         keywords='pytorch, machine learning',\n     )\ndiff --git a/tools/amd_build/build_amd.py b/tools/amd_build/build_amd.py\nindex 59f806b361102e..5d14e9266f3b4a 100755\n--- a/tools/amd_build/build_amd.py\n+++ b/tools/amd_build/build_amd.py\n@@ -140,7 +140,7 @@ def is_hip_clang() -> bool:\n         hip_path = os.getenv(\"HIP_PATH\", \"/opt/rocm/hip\")\n         with open(hip_path + \"/lib/.hipInfo\") as f:\n             return \"HIP_COMPILER=clang\" in f.read()\n-    except IOError:\n+    except OSError:\n         return False\n \n \n@@ -149,7 +149,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Hip.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\" hip_hcc \", \" amdhip64 \") for line in lines]\n         if lines == newlines:\n@@ -163,7 +163,7 @@ def is_hip_clang() -> bool:\n gloo_cmake_file = \"third_party/gloo/cmake/Modules/Findrccl.cmake\"\n if os.path.exists(gloo_cmake_file):\n     do_write = False\n-    with open(gloo_cmake_file, \"r\") as sources:\n+    with open(gloo_cmake_file) as sources:\n         lines = sources.readlines()\n     newlines = [line.replace(\"RCCL_LIBRARY\", \"RCCL_LIB_PATH\") for line in lines]\n     if lines == newlines:\n@@ -179,7 +179,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Dependencies.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\"HIP_HCC_FLAGS\", \"HIP_CLANG_FLAGS\") for line in lines]\n         if lines == newlines:\ndiff --git a/tools/autograd/gen_python_functions.py b/tools/autograd/gen_python_functions.py\nindex 211aacafaa8728..d1e9d60737defc 100644\n--- a/tools/autograd/gen_python_functions.py\n+++ b/tools/autograd/gen_python_functions.py\n@@ -553,7 +553,7 @@ def load_deprecated_signatures(\n     # find matching original signatures for each deprecated signature\n     results: List[PythonSignatureNativeFunctionPair] = []\n \n-    with open(deprecated_yaml_path, \"r\") as f:\n+    with open(deprecated_yaml_path) as f:\n         deprecated_defs = yaml.load(f, Loader=YamlLoader)\n \n     for deprecated in deprecated_defs:\n@@ -873,7 +873,7 @@ def method_impl(\n         name=name,\n         pycname=pycname,\n         method_header=method_header,\n-        max_args=max((o.signature.arguments_count() for o in overloads)),\n+        max_args=max(o.signature.arguments_count() for o in overloads),\n         signatures=signatures,\n         traceable=traceable,\n         check_has_torch_function=gen_has_torch_function_check(\n@@ -1255,10 +1255,10 @@ def go(f: NativeFunction) -> str:\n         # dispatch lambda signature\n         name = cpp.name(f.func)\n         lambda_formals = \", \".join(\n-            (\n+\n                 f\"{a.type_str} {a.name}\"\n                 for a in dispatch_lambda_args(ps, f, symint=symint)\n-            )\n+\n         )\n         lambda_return = dispatch_lambda_return_str(f)\n \ndiff --git a/tools/autograd/load_derivatives.py b/tools/autograd/load_derivatives.py\nindex b51b625b2ea28f..b846892b0e3ed4 100644\n--- a/tools/autograd/load_derivatives.py\n+++ b/tools/autograd/load_derivatives.py\n@@ -98,7 +98,7 @@ def load_derivatives(\n     global _GLOBAL_LOAD_DERIVATIVE_CACHE\n     key = (derivatives_yaml_path, native_yaml_path)\n     if key not in _GLOBAL_LOAD_DERIVATIVE_CACHE:\n-        with open(derivatives_yaml_path, \"r\") as f:\n+        with open(derivatives_yaml_path) as f:\n             definitions = yaml.load(f, Loader=YamlLoader)\n \n         funcs = parse_native_yaml(native_yaml_path, tags_yaml_path).native_functions\ndiff --git a/tools/code_analyzer/gen_op_registration_allowlist.py b/tools/code_analyzer/gen_op_registration_allowlist.py\nindex b01142c872f1c2..b5d15ca1ae8417 100644\n--- a/tools/code_analyzer/gen_op_registration_allowlist.py\n+++ b/tools/code_analyzer/gen_op_registration_allowlist.py\n@@ -24,7 +24,7 @@ def canonical_name(opname: str) -> str:\n \n \n def load_op_dep_graph(fname: str) -> DepGraph:\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         result = defaultdict(set)\n         for op in yaml.safe_load(stream):\n             op_name = canonical_name(op[\"name\"])\n@@ -36,7 +36,7 @@ def load_op_dep_graph(fname: str) -> DepGraph:\n \n def load_root_ops(fname: str) -> List[str]:\n     result = []\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         for op in yaml.safe_load(stream):\n             result.append(canonical_name(op))\n     return result\ndiff --git a/tools/code_analyzer/gen_oplist.py b/tools/code_analyzer/gen_oplist.py\nindex 7c1764deda5b2f..0a9e2a1539b6a7 100644\n--- a/tools/code_analyzer/gen_oplist.py\n+++ b/tools/code_analyzer/gen_oplist.py\n@@ -79,7 +79,7 @@ def gen_supported_mobile_models(model_dicts: List[Any], output_dir: str) -> None\n \n     supported_hashes = \"\"\n     for md5 in md5_hashes:\n-        supported_hashes += '\"{}\",\\n'.format(md5)\n+        supported_hashes += f'\"{md5}\",\\n'\n     with open(\n         os.path.join(output_dir, \"SupportedMobileModelsRegistration.cpp\"), \"wb\"\n     ) as out_file:\ndiff --git a/tools/coverage_plugins_package/setup.py b/tools/coverage_plugins_package/setup.py\nindex 01250694550423..e3e88067cb08db 100644\n--- a/tools/coverage_plugins_package/setup.py\n+++ b/tools/coverage_plugins_package/setup.py\n@@ -1,6 +1,6 @@\n import setuptools  # type: ignore[import]\n \n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n+with open(\"README.md\", encoding=\"utf-8\") as fh:\n     long_description = fh.read()\n \n setuptools.setup(\ndiff --git a/tools/download_mnist.py b/tools/download_mnist.py\nindex 52fa411eda9f88..ac9c049bdeedb6 100644\n--- a/tools/download_mnist.py\n+++ b/tools/download_mnist.py\n@@ -32,16 +32,16 @@ def report_download_progress(\n def download(destination_path: str, resource: str, quiet: bool) -> None:\n     if os.path.exists(destination_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ...\".format(destination_path))\n+            print(f\"{destination_path} already exists, skipping ...\")\n     else:\n         for mirror in MIRRORS:\n             url = mirror + resource\n-            print(\"Downloading {} ...\".format(url))\n+            print(f\"Downloading {url} ...\")\n             try:\n                 hook = None if quiet else report_download_progress\n                 urlretrieve(url, destination_path, reporthook=hook)\n             except (URLError, ConnectionError) as e:\n-                print(\"Failed to download (trying next):\\n{}\".format(e))\n+                print(f\"Failed to download (trying next):\\n{e}\")\n                 continue\n             finally:\n                 if not quiet:\n@@ -56,13 +56,13 @@ def unzip(zipped_path: str, quiet: bool) -> None:\n     unzipped_path = os.path.splitext(zipped_path)[0]\n     if os.path.exists(unzipped_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ... \".format(unzipped_path))\n+            print(f\"{unzipped_path} already exists, skipping ... \")\n         return\n     with gzip.open(zipped_path, \"rb\") as zipped_file:\n         with open(unzipped_path, \"wb\") as unzipped_file:\n             unzipped_file.write(zipped_file.read())\n             if not quiet:\n-                print(\"Unzipped {} ...\".format(zipped_path))\n+                print(f\"Unzipped {zipped_path} ...\")\n \n \n def main() -> None:\ndiff --git a/tools/gen_vulkan_spv.py b/tools/gen_vulkan_spv.py\nindex 02c9c39270b107..8d38dfa0e82bcb 100644\n--- a/tools/gen_vulkan_spv.py\n+++ b/tools/gen_vulkan_spv.py\n@@ -74,7 +74,7 @@ def __init__(self: \"VulkanShaderGenerator\") -> None:\n \n     def add_params_yaml(self, parameters_yaml_file):  # type: ignore[no-untyped-def]\n         all_template_params = OrderedDict()\n-        with open(parameters_yaml_file, \"r\") as f:\n+        with open(parameters_yaml_file) as f:\n             contents = yaml.load(f, Loader=UniqueKeyLoader)\n             for key in contents:\n                 all_template_params[key] = contents[key]\n@@ -205,7 +205,7 @@ def determineDescriptorType(lineStr: str) -> str:\n \n def getShaderInfo(srcFilePath: str) -> ShaderInfo:\n     shader_info = ShaderInfo([], [], \"\")\n-    with open(srcFilePath, 'r') as srcFile:\n+    with open(srcFilePath) as srcFile:\n         for line in srcFile:\n             if isDescriptorLine(line):\n                 shader_info.layouts.append(determineDescriptorType(line))\n@@ -272,13 +272,13 @@ def genCppH(\n         if len(f) > 1:\n             templateSrcPaths.append(f)\n             templateSrcPaths.sort()\n-    print(\"templateSrcPaths:{}\".format(templateSrcPaths))\n+    print(f\"templateSrcPaths:{templateSrcPaths}\")\n \n     spvPaths = {}\n     for templateSrcPath in templateSrcPaths:\n-        print(\"templateSrcPath {}\".format(templateSrcPath))\n+        print(f\"templateSrcPath {templateSrcPath}\")\n         name = getName(templateSrcPath).replace(\"_glsl\", \"\")\n-        print(\"name {}\".format(name))\n+        print(f\"name {name}\")\n \n         codeTemplate = CodeTemplate.from_file(templateSrcPath)\n         srcPath = tmpDirPath + \"/\" + name + \".glsl\"\n@@ -287,7 +287,7 @@ def genCppH(\n             fw.write(content)\n \n         spvPath = tmpDirPath + \"/\" + name + \".spv\"\n-        print(\"spvPath {}\".format(spvPath))\n+        print(f\"spvPath {spvPath}\")\n \n         cmd = [\n             glslcPath, \"-fshader-stage=compute\",\n@@ -328,7 +328,7 @@ def genCppH(\n     h += nsend\n \n     cpp = \"#include <ATen/native/vulkan/api/Shader.h>\\n\"\n-    cpp += \"#include <ATen/native/vulkan/{}>\\n\".format(H_NAME)\n+    cpp += f\"#include <ATen/native/vulkan/{H_NAME}>\\n\"\n     cpp += \"#include <stdint.h>\\n\"\n     cpp += \"#include <vector>\\n\"\n     cpp += nsbegin\n@@ -340,7 +340,7 @@ def genCppH(\n     for spvPath, srcPath in spvPaths.items():\n         name = getName(spvPath).replace(\"_spv\", \"\")\n \n-        print(\"spvPath:{}\".format(spvPath))\n+        print(f\"spvPath:{spvPath}\")\n         with open(spvPath, 'rb') as fr:\n             next_bin = array.array('I', fr.read())\n             sizeBytes = 4 * len(next_bin)\n@@ -362,8 +362,8 @@ def genCppH(\n         shader_info_layouts = \"{{{}}}\".format(\",\\n \".join(shader_info.layouts))\n \n         shader_info_args = [\n-            \"\\\"vulkan.{}\\\"\".format(name),\n-            \"{}_bin\".format(name),\n+            f\"\\\"vulkan.{name}\\\"\",\n+            f\"{name}_bin\",\n             str(sizeBytes),\n             shader_info_layouts,\n             tile_size,\ndiff --git a/tools/generate_torch_version.py b/tools/generate_torch_version.py\nindex 9e9f73b031f810..d90d3646ab1910 100644\n--- a/tools/generate_torch_version.py\n+++ b/tools/generate_torch_version.py\n@@ -41,7 +41,7 @@ def get_tag(pytorch_root: Union[str, Path]) -> str:\n \n def get_torch_version(sha: Optional[str] = None) -> str:\n     pytorch_root = Path(__file__).parent.parent\n-    version = open(pytorch_root / \"version.txt\", \"r\").read().strip()\n+    version = open(pytorch_root / \"version.txt\").read().strip()\n \n     if os.getenv(\"PYTORCH_BUILD_VERSION\"):\n         assert os.getenv(\"PYTORCH_BUILD_NUMBER\") is not None\n@@ -86,11 +86,11 @@ def get_torch_version(sha: Optional[str] = None) -> str:\n         version = tagged_version\n \n     with open(version_path, \"w\") as f:\n-        f.write(\"__version__ = '{}'\\n\".format(version))\n+        f.write(f\"__version__ = '{version}'\\n\")\n         # NB: This is not 100% accurate, because you could have built the\n         # library code with DEBUG, but csrc without DEBUG (in which case\n         # this would claim to be a release build when it's not.)\n-        f.write(\"debug = {}\\n\".format(repr(bool(args.is_debug))))\n-        f.write(\"cuda = {}\\n\".format(repr(args.cuda_version)))\n-        f.write(\"git_version = {}\\n\".format(repr(sha)))\n-        f.write(\"hip = {}\\n\".format(repr(args.hip_version)))\n+        f.write(f\"debug = {repr(bool(args.is_debug))}\\n\")\n+        f.write(f\"cuda = {repr(args.cuda_version)}\\n\")\n+        f.write(f\"git_version = {repr(sha)}\\n\")\n+        f.write(f\"hip = {repr(args.hip_version)}\\n\")\ndiff --git a/tools/jit/gen_unboxing.py b/tools/jit/gen_unboxing.py\nindex 6179d6afe482ff..ee4e2fc2ddb188 100644\n--- a/tools/jit/gen_unboxing.py\n+++ b/tools/jit/gen_unboxing.py\n@@ -250,7 +250,7 @@ def main(args: List[str]) -> None:\n     if options.op_registration_allowlist:\n         op_registration_allowlist = options.op_registration_allowlist\n     elif options.TEST_ONLY_op_registration_allowlist_yaml_path:\n-        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path, \"r\") as f:\n+        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path) as f:\n             op_registration_allowlist = yaml.safe_load(f)\n     else:\n         op_registration_allowlist = None\ndiff --git a/tools/linter/adapters/constexpr_linter.py b/tools/linter/adapters/constexpr_linter.py\nindex 8992f30ac46b29..24ecc83b238e30 100644\n--- a/tools/linter/adapters/constexpr_linter.py\n+++ b/tools/linter/adapters/constexpr_linter.py\n@@ -35,7 +35,7 @@ class LintMessage(NamedTuple):\n def check_file(filename: str) -> Optional[LintMessage]:\n     logging.debug(\"Checking file %s\", filename)\n \n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n \n     for idx, line in enumerate(lines):\ndiff --git a/tools/linter/adapters/grep_linter.py b/tools/linter/adapters/grep_linter.py\nindex 21c8a210b2b697..64dac4cdc079cd 100644\n--- a/tools/linter/adapters/grep_linter.py\n+++ b/tools/linter/adapters/grep_linter.py\n@@ -108,7 +108,7 @@ def lint_file(\n     original = None\n     replacement = None\n     if replace_pattern:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             original = f.read()\n \n         try:\ndiff --git a/tools/nightly.py b/tools/nightly.py\nindex 1544eb0692b661..28a8c6eb2331e7 100755\n--- a/tools/nightly.py\n+++ b/tools/nightly.py\n@@ -105,7 +105,7 @@ def redact(self, needle: str, replace: str = \"<REDACTED>\") -> None:\n         self.redactions[needle] = replace\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_base_dir() -> str:\n     meta_dir = os.getcwd()\n     base_dir = os.path.join(meta_dir, \"nightly\", \"log\")\n@@ -113,17 +113,17 @@ def logging_base_dir() -> str:\n     return base_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_run_dir() -> str:\n     cur_dir = os.path.join(\n         logging_base_dir(),\n-        \"{}_{}\".format(datetime.datetime.now().strftime(DATETIME_FORMAT), uuid.uuid1()),\n+        f\"{datetime.datetime.now().strftime(DATETIME_FORMAT)}_{uuid.uuid1()}\",\n     )\n     os.makedirs(cur_dir, exist_ok=True)\n     return cur_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_record_argv() -> None:\n     s = subprocess.list2cmdline(sys.argv)\n     with open(os.path.join(logging_run_dir(), \"argv\"), \"w\") as f:\ndiff --git a/tools/onnx/gen_diagnostics.py b/tools/onnx/gen_diagnostics.py\nindex 2aeb61a06318d7..4cf70289296050 100644\n--- a/tools/onnx/gen_diagnostics.py\n+++ b/tools/onnx/gen_diagnostics.py\n@@ -205,7 +205,7 @@ def gen_diagnostics(\n     out_cpp_dir: str,\n     out_docs_dir: str,\n ) -> None:\n-    with open(rules_path, \"r\") as f:\n+    with open(rules_path) as f:\n         rules = yaml.load(f, Loader=YamlLoader)\n \n     template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"templates\")\ndiff --git a/tools/onnx/update_default_opset_version.py b/tools/onnx/update_default_opset_version.py\nindex 6dc6ffbd2890f4..6463c6271b6ee6 100755\n--- a/tools/onnx/update_default_opset_version.py\n+++ b/tools/onnx/update_default_opset_version.py\n@@ -23,7 +23,7 @@\n def read_sub_write(path: str, prefix_pat: str, new_default: int) -> None:\n     with open(path, encoding=\"utf-8\") as f:\n         content_str = f.read()\n-    content_str = re.sub(prefix_pat, r\"\\g<1>{}\".format(new_default), content_str)\n+    content_str = re.sub(prefix_pat, fr\"\\g<1>{new_default}\", content_str)\n     with open(path, \"w\", encoding=\"utf-8\") as f:\n         f.write(content_str)\n     print(\"modified\", path)\ndiff --git a/tools/pyi/gen_pyi.py b/tools/pyi/gen_pyi.py\nindex 24ee8ad1bfe580..c74d737416870a 100644\n--- a/tools/pyi/gen_pyi.py\n+++ b/tools/pyi/gen_pyi.py\n@@ -191,15 +191,15 @@ def sig_for_ops(opname: str) -> List[str]:\n \n     name = opname[2:-2]\n     if name in binary_ops:\n-        return [\"def {}(self, other: Any) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self, other: Any) -> Tensor: ...\"]\n     elif name in comparison_ops:\n-        sig = \"def {}(self, other: Any) -> Tensor: ...\".format(opname)\n+        sig = f\"def {opname}(self, other: Any) -> Tensor: ...\"\n         if name in symmetric_comparison_ops:\n             # unsafe override https://github.com/python/mypy/issues/5704\n             sig += \"  # type: ignore[override]\"\n         return [sig]\n     elif name in unary_ops:\n-        return [\"def {}(self) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self) -> Tensor: ...\"]\n     elif name in to_py_type_ops:\n         if name in {\"bool\", \"float\", \"complex\"}:\n             tname = name\n@@ -209,7 +209,7 @@ def sig_for_ops(opname: str) -> List[str]:\n             tname = \"int\"\n         if tname in {\"float\", \"int\", \"bool\", \"complex\"}:\n             tname = \"builtins.\" + tname\n-        return [\"def {}(self) -> {}: ...\".format(opname, tname)]\n+        return [f\"def {opname}(self) -> {tname}: ...\"]\n     else:\n         raise Exception(\"unknown op\", opname)\n \n@@ -1120,7 +1120,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     for name in simple_conversions:\n         unsorted_tensor_method_hints[name].append(\n-            \"def {}(self) -> Tensor: ...\".format(name)\n+            f\"def {name}(self) -> Tensor: ...\"\n         )\n \n     # pyi tensor methods don't currently include deprecated signatures for some reason\n@@ -1150,7 +1150,7 @@ def replace_special_case(hint: str) -> str:\n                 namedtuples[tuple_name] = tuple_def\n \n     for op in all_ops:\n-        name = \"__{}__\".format(op)\n+        name = f\"__{op}__\"\n         unsorted_tensor_method_hints[name] += sig_for_ops(name)\n \n     tensor_method_hints = []\n@@ -1164,7 +1164,7 @@ def replace_special_case(hint: str) -> str:\n     # Generate namedtuple definitions\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-    namedtuple_defs = [\"{}\\n\".format(defn) for defn in namedtuples.values()]\n+    namedtuple_defs = [f\"{defn}\\n\" for defn in namedtuples.values()]\n \n     # Generate type signatures for legacy classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1183,7 +1183,7 @@ def replace_special_case(hint: str) -> str:\n         \"ByteTensor\",\n         \"BoolTensor\",\n     ):\n-        legacy_class_hints.append(\"class {}(Tensor): ...\".format(c))\n+        legacy_class_hints.append(f\"class {c}(Tensor): ...\")\n \n     # Generate type signatures for dtype classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1191,7 +1191,7 @@ def replace_special_case(hint: str) -> str:\n     # TODO: don't explicitly list dtypes here; get it from canonical\n     # source\n     dtype_class_hints = [\n-        \"{}: dtype = ...\".format(n)\n+        f\"{n}: dtype = ...\"\n         for n in [\n             \"float32\",\n             \"float\",\n@@ -1232,7 +1232,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     all_symbols = sorted(list(namedtuples.keys()) + hinted_function_names)\n     all_directive = pformat(all_symbols, width=100, compact=True).split(\"\\n\")\n-    all_directive[0] = \"__all__ = {}\".format(all_directive[0])\n+    all_directive[0] = f\"__all__ = {all_directive[0]}\"\n \n     # Dispatch key hints\n     # ~~~~~~~~~~~~~~~~~~\ndiff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py\nindex 0bd6e5d4c2adc9..cf80bd3eb1e62c 100644\n--- a/tools/setup_helpers/cmake.py\n+++ b/tools/setup_helpers/cmake.py\n@@ -61,10 +61,10 @@ def _get_cmake_command() -> str:\n \n         _cmake_min_version = LooseVersion(\"3.13.0\")\n         if all(\n-            (\n+\n                 ver is None or ver < _cmake_min_version\n                 for ver in [cmake_version, cmake3_version]\n-            )\n+\n         ):\n             raise RuntimeError(\"no cmake or cmake3 with version >= 3.13.0 found\")\n \n@@ -108,7 +108,7 @@ def defines(args: List[str], **kwargs: CMakeValue) -> None:\n         \"Adds definitions to a cmake argument list.\"\n         for key, value in sorted(kwargs.items()):\n             if value is not None:\n-                args.append(\"-D{}={}\".format(key, value))\n+                args.append(f\"-D{key}={value}\")\n \n     def get_cmake_cache_variables(self) -> Dict[str, CMakeValue]:\n         r\"\"\"Gets values in CMakeCache.txt into a dictionary.\n@@ -173,7 +173,7 @@ def generate(\n                     toolset_dict[\"host\"] = \"x64\"\n             if toolset_dict:\n                 toolset_expr = \",\".join(\n-                    [\"{}={}\".format(k, v) for k, v in toolset_dict.items()]\n+                    [f\"{k}={v}\" for k, v in toolset_dict.items()]\n                 )\n                 args.append(\"-T\" + toolset_expr)\n \n@@ -322,10 +322,10 @@ def generate(\n         expected_wrapper = \"/usr/local/opt/ccache/libexec\"\n         if IS_DARWIN and os.path.exists(expected_wrapper):\n             if \"CMAKE_C_COMPILER\" not in build_options and \"CC\" not in os.environ:\n-                CMake.defines(args, CMAKE_C_COMPILER=\"{}/gcc\".format(expected_wrapper))\n+                CMake.defines(args, CMAKE_C_COMPILER=f\"{expected_wrapper}/gcc\")\n             if \"CMAKE_CXX_COMPILER\" not in build_options and \"CXX\" not in os.environ:\n                 CMake.defines(\n-                    args, CMAKE_CXX_COMPILER=\"{}/g++\".format(expected_wrapper)\n+                    args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\"\n                 )\n \n         for env_var_name in my_env:\n@@ -336,10 +336,10 @@ def generate(\n                     my_env[env_var_name] = str(my_env[env_var_name].encode(\"utf-8\"))\n                 except UnicodeDecodeError as e:\n                     shex = \":\".join(\n-                        \"{:02x}\".format(ord(c)) for c in my_env[env_var_name]\n+                        f\"{ord(c):02x}\" for c in my_env[env_var_name]\n                     )\n                     print(\n-                        \"Invalid ENV[{}] = {}\".format(env_var_name, shex),\n+                        f\"Invalid ENV[{env_var_name}] = {shex}\",\n                         file=sys.stderr,\n                     )\n                     print(e, file=sys.stderr)\n@@ -396,7 +396,7 @@ def build(self, my_env: Dict[str, str]) -> None:\n             build_args += [\"--\"]\n             if IS_WINDOWS and not USE_NINJA:\n                 # We are likely using msbuild here\n-                build_args += [\"/p:CL_MPCount={}\".format(max_jobs)]\n+                build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n             else:\n                 build_args += [\"-j\", max_jobs]\n         self.run(build_args, my_env)\ndiff --git a/tools/setup_helpers/cmake_utils.py b/tools/setup_helpers/cmake_utils.py\nindex dabd66a4e838bb..c15b6f7592c015 100644\n--- a/tools/setup_helpers/cmake_utils.py\n+++ b/tools/setup_helpers/cmake_utils.py\n@@ -72,7 +72,7 @@ def get_cmake_cache_variables_from_file(\n         )\n         if matched is None:  # Illegal line\n             raise ValueError(\n-                \"Unexpected line {} in {}: {}\".format(i, repr(cmake_cache_file), line)\n+                f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\"\n             )\n         _, variable, type_, value = matched.groups()\n         if type_ is None:\ndiff --git a/tools/setup_helpers/generate_code.py b/tools/setup_helpers/generate_code.py\nindex c03fd87f25b6aa..afdd168d179fd6 100644\n--- a/tools/setup_helpers/generate_code.py\n+++ b/tools/setup_helpers/generate_code.py\n@@ -75,7 +75,7 @@ def generate_code(\n def get_selector_from_legacy_operator_selection_list(\n     selected_op_list_path: str,\n ) -> Any:\n-    with open(selected_op_list_path, \"r\") as f:\n+    with open(selected_op_list_path) as f:\n         # strip out the overload part\n         # It's only for legacy config - do NOT copy this code!\n         selected_op_list = {\ndiff --git a/tools/stats/import_test_stats.py b/tools/stats/import_test_stats.py\nindex b0719fc56d97b6..28d8ee0961bd9e 100644\n--- a/tools/stats/import_test_stats.py\n+++ b/tools/stats/import_test_stats.py\n@@ -46,7 +46,7 @@ def is_cached_file_valid() -> bool:\n \n     if os.path.exists(path) and is_cached_file_valid():\n         # Another test process already download the file, so don't re-do it\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             return cast(Dict[str, Any], json.load(f))\n \n     for _ in range(3):\ndiff --git a/tools/stats/upload_stats_lib.py b/tools/stats/upload_stats_lib.py\nindex c9223c528fe163..c7e90286d00a2b 100644\n--- a/tools/stats/upload_stats_lib.py\n+++ b/tools/stats/upload_stats_lib.py\n@@ -249,10 +249,10 @@ def value(self) -> Any:\n         value = os.environ.get(self.env_var)\n         if value is None and self.required:\n             raise ValueError(\n-                (\n+\n                     f\"Missing {self.name}. Please set the {self.env_var}\"\n                     \"environment variable to pass in this value.\"\n-                )\n+\n             )\n         if self.type_conversion_fn:\n             return self.type_conversion_fn(value)\ndiff --git a/tools/substitute.py b/tools/substitute.py\nindex c3b353bf740115..e9c05990c75f9a 100644\n--- a/tools/substitute.py\n+++ b/tools/substitute.py\n@@ -11,7 +11,7 @@\n     parser.add_argument(\"--replace\", action=\"append\", nargs=2)\n     options = parser.parse_args()\n \n-    with open(options.input_file, \"r\") as f:\n+    with open(options.input_file) as f:\n         contents = f.read()\n \n     output_file = os.path.join(options.install_dir, options.output_file)\ndiff --git a/tools/test/test_executorch_gen.py b/tools/test/test_executorch_gen.py\nindex b2a0f6768271bf..c9d6c79b85c1ea 100644\n--- a/tools/test/test_executorch_gen.py\n+++ b/tools/test/test_executorch_gen.py\n@@ -181,7 +181,7 @@ def test_translate_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\n@@ -268,7 +268,7 @@ def test_translate_kernel_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\ndiff --git a/tools/test/test_executorch_signatures.py b/tools/test/test_executorch_signatures.py\nindex c137f6982ec2b7..543926d4c31ef0 100644\n--- a/tools/test/test_executorch_signatures.py\n+++ b/tools/test/test_executorch_signatures.py\n@@ -21,7 +21,7 @@ def test_runtime_signature_contains_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=True)\n-            self.assertEquals(len(args), 3)\n+            self.assertEqual(len(args), 3)\n             self.assertTrue(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n@@ -30,7 +30,7 @@ def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=False)\n-            self.assertEquals(len(args), 2)\n+            self.assertEqual(len(args), 2)\n             self.assertFalse(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_declaration_correct(self) -> None:\n@@ -38,7 +38,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             decl = self.sig.decl(include_context=True)\n-            self.assertEquals(\n+            self.assertEqual(\n                 decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\n@@ -48,7 +48,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n                 ),\n             )\n             no_context_decl = self.sig.decl(include_context=False)\n-            self.assertEquals(\n+            self.assertEqual(\n                 no_context_decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\ndiff --git a/tools/test/test_vulkan_codegen.py b/tools/test/test_vulkan_codegen.py\nindex ae87c27e7aeb8e..196be229b348d2 100644\n--- a/tools/test/test_vulkan_codegen.py\n+++ b/tools/test/test_vulkan_codegen.py\n@@ -92,9 +92,9 @@ def test_missing_key_default_val(self) -> None:\n                     file_name_2 = os.path.join(tmp_dir, \"conv2d_pw_1x2.glsl\")\n                     self.assertTrue(os.path.exists(file_name_1))\n                     self.assertTrue(os.path.exists(file_name_2))\n-                    with open(file_name_1, \"r\") as f:\n+                    with open(file_name_1) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 1\" in contents)\n-                    with open(file_name_2, \"r\") as f:\n+                    with open(file_name_2) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 2\" in contents)\ndiff --git a/tools/testing/explicit_ci_jobs.py b/tools/testing/explicit_ci_jobs.py\nindex daff3cce8956ff..594e00d437f9f7 100755\n--- a/tools/testing/explicit_ci_jobs.py\n+++ b/tools/testing/explicit_ci_jobs.py\n@@ -127,7 +127,7 @@ def commit_ci(files: List[str], message: str) -> None:\n     args = parser.parse_args()\n \n     touched_files = [CONFIG_YML]\n-    with open(CONFIG_YML, \"r\") as f:\n+    with open(CONFIG_YML) as f:\n         config_yml = yaml.safe_load(f.read())\n \n     config_yml[\"workflows\"] = get_filtered_circleci_config(\ndiff --git a/tools/testing/test_selections.py b/tools/testing/test_selections.py\nindex 24fb7278d206f9..76f841b8902bdd 100644\n--- a/tools/testing/test_selections.py\n+++ b/tools/testing/test_selections.py\n@@ -163,7 +163,7 @@ def _get_previously_failing_tests() -> Set[str]:\n         )\n         return set()\n \n-    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH, \"r\") as f:\n+    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH) as f:\n         last_failed_tests = json.load(f)\n \n     prioritized_tests = _parse_prev_failing_test_files(last_failed_tests)\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex 053ce0c0a89552..ebd24383e0b53f 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -156,14 +156,12 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-\n-                            \"      Note: While we usually use modules in the python standard library \"\n-                            f\"from the local environment, `{module_name}` has a lot of system \"\n-                            \"level access and therefore can pose a security risk. We heavily \"\n-                            f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n-                            \"is not possible, add it to the extern list by calling \"\n-                            f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-\n+                        \"      Note: While we usually use modules in the python standard library \"\n+                        f\"from the local environment, `{module_name}` has a lot of system \"\n+                        \"level access and therefore can pose a security risk. We heavily \"\n+                        f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n+                        \"is not possible, add it to the extern list by calling \"\n+                        f'PackageExporter.extern(\"`{module_name}`\")\\n'\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +171,8 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-\n-                    \"Set debug=True when invoking PackageExporter for a visualization of where \"\n-                    \"broken modules are coming from!\\n\"\n-\n+                \"Set debug=True when invoking PackageExporter for a visualization of where \"\n+                \"broken modules are coming from!\\n\"\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\n"
  },
  {
    "number": 105417,
    "title": "[BE] Enable ruff's UP rules and autoformat onnx/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "16f74dcabbace01f11a25dd2b5d77a8e4fddb5d6",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105417",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105417/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105417.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105417.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105417/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105417/comments",
    "labels": [
      "release notes: onnx"
    ],
    "_event_time": "2023-07-18T01:17:25.935312Z",
    "state": "closed",
    "patch": "From 803164fcc9085c7f884448321a42d40d55ce1217 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:19 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat onnx/\n\n[ghstack-poisoned]\n---\n torch/onnx/_internal/diagnostics/infra/utils.py    | 2 +-\n torch/onnx/_internal/exporter.py                   | 4 +---\n torch/onnx/_internal/fx/onnxfunction_dispatcher.py | 6 +++---\n torch/onnx/_internal/fx/passes/type_promotion.py   | 2 +-\n torch/onnx/_internal/fx/registration.py            | 4 ++--\n torch/onnx/_internal/io_adapter.py                 | 6 +++---\n torch/onnx/_internal/jit_utils.py                  | 2 +-\n torch/onnx/symbolic_opset11.py                     | 2 +-\n torch/onnx/symbolic_opset17.py                     | 8 ++++----\n torch/onnx/verification.py                         | 4 ++--\n 10 files changed, 19 insertions(+), 21 deletions(-)\n\ndiff --git a/torch/onnx/_internal/diagnostics/infra/utils.py b/torch/onnx/_internal/diagnostics/infra/utils.py\nindex f287268df5727b..4648b477515025 100644\n--- a/torch/onnx/_internal/diagnostics/infra/utils.py\n+++ b/torch/onnx/_internal/diagnostics/infra/utils.py\n@@ -43,7 +43,7 @@ def python_call_stack(frames_to_skip: int = 0, frames_to_log: int = 16) -> _infr\n     return stack\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def _function_source_info(fn: Callable) -> Tuple[Sequence[str], int, Optional[str]]:\n     \"\"\"Returns the source lines, line number, and source file path for the given function.\n \ndiff --git a/torch/onnx/_internal/exporter.py b/torch/onnx/_internal/exporter.py\nindex aae7eb6f8e3804..33bf8c7afe44f5 100644\n--- a/torch/onnx/_internal/exporter.py\n+++ b/torch/onnx/_internal/exporter.py\n@@ -145,9 +145,7 @@ class ResolvedExportOptions(ExportOptions):\n     logging diagnostics, and generating the SARIF log.\"\"\"\n \n     @_beartype.beartype\n-    def __init__(\n-        self, options: Optional[Union[ExportOptions, \"ResolvedExportOptions\"]]\n-    ):\n+    def __init__(self, options: Optional[Union[ExportOptions, ResolvedExportOptions]]):\n         if options is None:\n             options = ExportOptions()\n         if isinstance(options, ResolvedExportOptions):\ndiff --git a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\nindex a312cf1aed3d80..2b489ca076b430 100644\n--- a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n+++ b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n@@ -105,7 +105,7 @@ def dispatch(\n         ],\n         onnx_kwargs: Dict[str, fx_type_utils.Argument],\n         diagnostic_context: diagnostics.DiagnosticContext,\n-    ) -> Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]:\n+    ) -> Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]:\n         \"\"\"Dispatches an ONNX function based on the given FX node, arguments, and keyword arguments.\n         Args:\n             node: The TorchFX node to dispatch the function for.\n@@ -405,7 +405,7 @@ def aten_new_full_dtype(self: TTensor, size: INT64, fill_value: TTensor, dtype:\n \n     \"\"\"\n \n-    def __init__(self, onnxfunction: \"onnxscript.OnnxFunction\"):\n+    def __init__(self, onnxfunction: onnxscript.OnnxFunction):\n         \"\"\"Initialize the OnnxSchemaChecker .\n \n         Args:\n@@ -579,7 +579,7 @@ def _record_matching_score(\n     @_beartype.beartype\n     def _separate_input_attributes_from_arguments(\n         self,\n-        param_schemas: Sequence[\"onnxscript.values.ParamSchema\"],\n+        param_schemas: Sequence[onnxscript.values.ParamSchema],\n         args: Sequence[\n             Optional[Union[fx_type_utils.TensorLike, str, int, float, bool, list]]\n         ],\ndiff --git a/torch/onnx/_internal/fx/passes/type_promotion.py b/torch/onnx/_internal/fx/passes/type_promotion.py\nindex e100afefe7814a..c8a10cc322fe90 100644\n--- a/torch/onnx/_internal/fx/passes/type_promotion.py\n+++ b/torch/onnx/_internal/fx/passes/type_promotion.py\n@@ -1217,7 +1217,7 @@ def add_rule(self, rule: TypePromotionRule) -> None:\n             ValueError: If the rule is invalid.\n         \"\"\"\n         if not rule.is_valid():\n-            raise ValueError(\"Invalid type promotion rule: {}\".format(rule))\n+            raise ValueError(f\"Invalid type promotion rule: {rule}\")\n         self._rule_table[f\"{rule.namespace}.{rule.op_name}\"] = rule\n \n     @_beartype.beartype\ndiff --git a/torch/onnx/_internal/fx/registration.py b/torch/onnx/_internal/fx/registration.py\nindex 135c9afe9cdd57..b7c8c3521e55f5 100644\n--- a/torch/onnx/_internal/fx/registration.py\n+++ b/torch/onnx/_internal/fx/registration.py\n@@ -29,7 +29,7 @@ class SymbolicFunction:\n \n     \"\"\"\n \n-    onnx_function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]\n+    onnx_function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]\n     op_full_name: str\n     is_custom: bool = False\n \n@@ -99,7 +99,7 @@ def _register(\n     @_beartype.beartype\n     def register_custom_op(\n         self,\n-        function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"],\n+        function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction],\n         namespace: str,\n         op_name: str,\n         overload: Optional[str] = None,\ndiff --git a/torch/onnx/_internal/io_adapter.py b/torch/onnx/_internal/io_adapter.py\nindex 2654a1ade32ac4..1a80e179ac0b67 100644\n--- a/torch/onnx/_internal/io_adapter.py\n+++ b/torch/onnx/_internal/io_adapter.py\n@@ -60,7 +60,7 @@ def append_step(self, step: InputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, *model_args, **model_kwargs\n-    ) -> Sequence[Union[int, float, bool, str, \"torch.Tensor\", None]]:\n+    ) -> Sequence[Union[int, float, bool, str, torch.Tensor, None]]:\n         \"\"\"Converts the PyTorch model inputs to exported ONNX model inputs format.\n \n         Args:\n@@ -113,7 +113,7 @@ def append_step(self, step: OutputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, model_outputs: Any\n-    ) -> Sequence[Union[\"torch.Tensor\", int, float, bool, str]]:\n+    ) -> Sequence[Union[torch.Tensor, int, float, bool, str]]:\n         \"\"\"Converts the PyTorch model outputs to exported ONNX model outputs format.\n \n         Args:\n@@ -228,7 +228,7 @@ def apply(\n class LiftParametersAndBuffersIntoArgsStep:\n     \"\"\"Append parameters and buffers to model's positional argument list.\"\"\"\n \n-    def __init__(self, inputs: Tuple[\"torch.Tensor\", ...]) -> None:\n+    def __init__(self, inputs: Tuple[torch.Tensor, ...]) -> None:\n         self.inputs = inputs\n \n     def apply(\ndiff --git a/torch/onnx/_internal/jit_utils.py b/torch/onnx/_internal/jit_utils.py\nindex 9052961fc7a646..c46a82c40dfec8 100644\n--- a/torch/onnx/_internal/jit_utils.py\n+++ b/torch/onnx/_internal/jit_utils.py\n@@ -40,7 +40,7 @@ class GraphContext:\n     block: _C.Block\n     opset: int\n     original_node: _C.Node\n-    params_dict: Dict[str, \"_C.IValue\"]\n+    params_dict: Dict[str, _C.IValue]\n     env: Dict[_C.Value, _C.Value]\n \n     # Relay methods from _C.Graph for compatibility with symbolic functions that expect\ndiff --git a/torch/onnx/symbolic_opset11.py b/torch/onnx/symbolic_opset11.py\nindex b432244c42aaa7..3bb63e0e8fa36b 100644\n--- a/torch/onnx/symbolic_opset11.py\n+++ b/torch/onnx/symbolic_opset11.py\n@@ -888,7 +888,7 @@ def _get_arange_dtype(dtype):\n         dtype = symbolic_helper._maybe_get_const(dtype, \"i\")\n         return dtype\n \n-    if len(args) == 2 and all((isinstance(val, int) for val in args)):\n+    if len(args) == 2 and all(isinstance(val, int) for val in args):\n         # aten::arange(Scalar start, Scalar end)\n         dtype = torch.int64\n         # Start index.\ndiff --git a/torch/onnx/symbolic_opset17.py b/torch/onnx/symbolic_opset17.py\nindex 92151a5e58d977..3fa03ab8e11915 100644\n--- a/torch/onnx/symbolic_opset17.py\n+++ b/torch/onnx/symbolic_opset17.py\n@@ -144,8 +144,8 @@ def stft(\n         # Center window around zeros if needed (required by ONNX's STFT)\n         if n_win < n_fft:\n             left, right = _compute_edge_sizes(n_fft, n_win)\n-            left_win = g.op(\"Constant\", value_t=torch.zeros((left)))\n-            right_win = g.op(\"Constant\", value_t=torch.zeros((right)))\n+            left_win = g.op(\"Constant\", value_t=torch.zeros(left))\n+            right_win = g.op(\"Constant\", value_t=torch.zeros(right))\n             window = g.op(\"Concat\", left_win, window, right_win, axis_i=0)\n \n     # Create window, if needed\n@@ -161,11 +161,11 @@ def stft(\n             # Center window, if needed\n             left, right = _compute_edge_sizes(n_fft, win_length)\n             torch_window = torch.hstack(\n-                (torch.zeros((left)), torch.ones((win_length)), torch.zeros((right)))\n+                (torch.zeros(left), torch.ones(win_length), torch.zeros(right))\n             )\n         else:\n             # Rectangle window\n-            torch_window = torch.ones((n_fft))\n+            torch_window = torch.ones(n_fft)\n         assert torch_window.shape[0] == n_fft\n         window = g.op(\"Constant\", value_t=torch_window)\n     window = g.op(\ndiff --git a/torch/onnx/verification.py b/torch/onnx/verification.py\nindex abfa4677eb21cb..27fe4e28e32cf6 100644\n--- a/torch/onnx/verification.py\n+++ b/torch/onnx/verification.py\n@@ -1310,7 +1310,7 @@ def essential_node_kinds(self) -> Set[str]:\n         }\n \n     @_beartype.beartype\n-    def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n+    def all_mismatch_leaf_graph_info(self) -> List[GraphInfo]:\n         \"\"\"Return a list of all leaf `GraphInfo` objects that have mismatch.\"\"\"\n         if not self.has_mismatch():\n             return []\n@@ -1333,7 +1333,7 @@ def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n         return results\n \n     @_beartype.beartype\n-    def find_partition(self, id: str) -> Optional[\"GraphInfo\"]:\n+    def find_partition(self, id: str) -> Optional[GraphInfo]:\n         \"\"\"Find the `GraphInfo` object with the given id.\"\"\"\n         if id == self.id:\n             return self\n"
  },
  {
    "number": 105416,
    "title": "[BE] Enable ruff's UP rules and autoformat optim/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "06bbac1ecf18bb6c637f433336d36b253c0b4a98",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105416",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105416/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105416.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105416.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105416/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105416/comments",
    "labels": [
      "release notes: optim"
    ],
    "_event_time": "2023-07-18T01:17:20.970200Z",
    "state": "closed",
    "patch": "From cb00747f4a0dc05f277c8765c6630b3f080f6aa2 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:14 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat optim/\n\n[ghstack-poisoned]\n---\n test/distributions/test_constraints.py        |  14 +-\n test/distributions/test_distributions.py      | 238 +++++++++---------\n test/distributions/test_transforms.py         |  20 +-\n test/optim/test_optim.py                      |   4 +-\n torch/distributions/constraints.py            |  16 +-\n torch/distributions/independent.py            |   2 +-\n torch/distributions/kl.py                     |   6 +-\n .../lowrank_multivariate_normal.py            |   2 +-\n torch/distributions/mixture_same_family.py    |   8 +-\n .../distributions/transformed_distribution.py |   2 +-\n torch/distributions/transforms.py             |   6 +-\n torch/optim/adadelta.py                       |   8 +-\n torch/optim/adagrad.py                        |   8 +-\n torch/optim/adam.py                           |  10 +-\n torch/optim/adamax.py                         |  10 +-\n torch/optim/adamw.py                          |  10 +-\n torch/optim/asgd.py                           |   4 +-\n torch/optim/lr_scheduler.py                   |  18 +-\n torch/optim/nadam.py                          |  12 +-\n torch/optim/optimizer.py                      |  12 +-\n torch/optim/radam.py                          |  10 +-\n torch/optim/rmsprop.py                        |  10 +-\n torch/optim/rprop.py                          |   4 +-\n torch/optim/sgd.py                            |   6 +-\n torch/optim/sparse_adam.py                    |   8 +-\n torch/package/_importlib.py                   |   6 +-\n .../package/file_structure_representation.py  |   1 -\n torch/package/package_exporter.py             |  10 +-\n torch/package/package_importer.py             |   6 +-\n torch/profiler/_memory_profiler.py            |   4 +-\n torch/profiler/_pattern_matcher.py            |   2 +-\n torch/signal/windows/windows.py               |   1 -\n torch/sparse/semi_structured.py               |  16 +-\n 33 files changed, 246 insertions(+), 248 deletions(-)\n\ndiff --git a/test/distributions/test_constraints.py b/test/distributions/test_constraints.py\nindex b733cbc021e153..0753b246e37948 100644\n--- a/test/distributions/test_constraints.py\n+++ b/test/distributions/test_constraints.py\n@@ -83,7 +83,7 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         t = biject_to(constraint)\n     except NotImplementedError:\n         pytest.skip('`biject_to` not implemented.')\n-    assert t.bijective, \"biject_to({}) is not bijective\".format(constraint)\n+    assert t.bijective, f\"biject_to({constraint}) is not bijective\"\n     if constraint_fn is constraints.corr_cholesky:\n         # (D * (D-1)) / 2 (where D = 4) = 6 (size of last dim)\n         x = torch.randn(6, 6, dtype=torch.double)\n@@ -93,12 +93,12 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         x = x.cuda()\n     y = t(x)\n     assert constraint.check(y).all(), '\\n'.join([\n-        \"Failed to biject_to({})\".format(constraint),\n-        \"x = {}\".format(x),\n-        \"biject_to(...)(x) = {}\".format(y),\n+        f\"Failed to biject_to({constraint})\",\n+        f\"x = {x}\",\n+        f\"biject_to(...)(x) = {y}\",\n     ])\n     x2 = t.inv(y)\n-    assert torch.allclose(x, x2), \"Error in biject_to({}) inverse\".format(constraint)\n+    assert torch.allclose(x, x2), f\"Error in biject_to({constraint}) inverse\"\n \n     j = t.log_abs_det_jacobian(x, y)\n     assert j.shape == x.shape[:x.dim() - t.domain.event_dim]\n@@ -119,10 +119,10 @@ def test_transform_to(constraint_fn, args, is_cuda):\n     if is_cuda:\n         x = x.cuda()\n     y = t(x)\n-    assert constraint.check(y).all(), \"Failed to transform_to({})\".format(constraint)\n+    assert constraint.check(y).all(), f\"Failed to transform_to({constraint})\"\n     x2 = t.inv(y)\n     y2 = t(x2)\n-    assert torch.allclose(y, y2), \"Error in transform_to({}) pseudoinverse\".format(constraint)\n+    assert torch.allclose(y, y2), f\"Error in transform_to({constraint}) pseudoinverse\"\n \n \n if __name__ == \"__main__\":\ndiff --git a/test/distributions/test_distributions.py b/test/distributions/test_distributions.py\nindex 69591d31c5ed20..2f4d256516c849 100644\n--- a/test/distributions/test_distributions.py\n+++ b/test/distributions/test_distributions.py\n@@ -862,7 +862,7 @@ def _check_sampler_sampler(self, torch_dist, ref_dist, message, multivariate=Fal\n         bins = samples.reshape((num_bins, samples_per_bin)).mean(axis=1)\n         stddev = samples_per_bin ** -0.5\n         threshold = stddev * scipy.special.erfinv(1 - 2 * failure_rate / num_bins)\n-        message = '{}.sample() is biased:\\n{}'.format(message, bins)\n+        message = f'{message}.sample() is biased:\\n{bins}'\n         for bias in bins:\n             self.assertLess(-threshold, bias, message)\n             self.assertLess(bias, threshold, message)\n@@ -971,7 +971,7 @@ def test_has_examples(self):\n             if isinstance(Dist, type) and issubclass(Dist, Distribution) \\\n                     and Dist is not Distribution and Dist is not ExponentialFamily:\n                 self.assertIn(Dist, distributions_with_examples,\n-                              \"Please add {} to the EXAMPLES list in test_distributions.py\".format(Dist.__name__))\n+                              f\"Please add {Dist.__name__} to the EXAMPLES list in test_distributions.py\")\n \n     def test_support_attributes(self):\n         for Dist, params in EXAMPLES:\n@@ -1120,7 +1120,7 @@ def test_geometric_sample(self):\n         for prob in [0.01, 0.18, 0.8]:\n             self._check_sampler_discrete(Geometric(prob),\n                                          scipy.stats.geom(p=prob, loc=-1),\n-                                         'Geometric(prob={})'.format(prob))\n+                                         f'Geometric(prob={prob})')\n \n     def test_binomial(self):\n         p = torch.arange(0.05, 1, 0.1).requires_grad_()\n@@ -1136,7 +1136,7 @@ def test_binomial_sample(self):\n             for count in [2, 10, 100, 500]:\n                 self._check_sampler_discrete(Binomial(total_count=count, probs=prob),\n                                              scipy.stats.binom(count, prob),\n-                                             'Binomial(total_count={}, probs={})'.format(count, prob))\n+                                             f'Binomial(total_count={count}, probs={prob})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_binomial_log_prob_and_entropy(self):\n@@ -1431,7 +1431,7 @@ def test_poisson_sample(self):\n         for rate in [0.1, 1.0, 5.0]:\n             self._check_sampler_discrete(Poisson(rate),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={})'.format(rate),\n+                                         f'Poisson(lambda={rate})',\n                                          failure_rate=1e-3)\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n@@ -1441,7 +1441,7 @@ def test_poisson_gpu_sample(self):\n         for rate in [0.12, 0.9, 4.0]:\n             self._check_sampler_discrete(Poisson(torch.tensor([rate]).cuda()),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={}, cuda)'.format(rate),\n+                                         f'Poisson(lambda={rate}, cuda)',\n                                          failure_rate=1e-3)\n \n     def test_relaxed_bernoulli(self):\n@@ -1476,7 +1476,7 @@ def sample(self, *args, **kwargs):\n         for probs, temp in product([0.1, 0.2, 0.8], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(Rounded(RelaxedBernoulli(temp, probs)),\n                                          scipy.stats.bernoulli(probs),\n-                                         'Rounded(RelaxedBernoulli(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedBernoulli(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [0.001, 0.2, 0.999]:\n@@ -1534,7 +1534,7 @@ def pmf(self, samples):\n         for probs, temp in product([torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(ArgMax(RelaxedOneHotCategorical(temp, probs)),\n                                          ScipyCategorical(scipy.stats.multinomial(1, probs)),\n-                                         'Rounded(RelaxedOneHotCategorical(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedOneHotCategorical(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])]:\n@@ -1588,7 +1588,7 @@ def test_vonmises_sample(self):\n             for concentration in [0.03, 0.3, 1.0, 10.0, 100.0]:\n                 self._check_sampler_sampler(VonMises(loc, concentration),\n                                             scipy.stats.vonmises(loc=loc, kappa=concentration),\n-                                            \"VonMises(loc={}, concentration={})\".format(loc, concentration),\n+                                            f\"VonMises(loc={loc}, concentration={concentration})\",\n                                             num_samples=int(1e5), circular=True)\n \n     def test_vonmises_logprob(self):\n@@ -1694,7 +1694,7 @@ def test_halfnormal_sample(self):\n         for std in [0.1, 1.0, 10.0]:\n             self._check_sampler_sampler(HalfNormal(std),\n                                         scipy.stats.halfnorm(scale=std),\n-                                        'HalfNormal(scale={})'.format(std))\n+                                        f'HalfNormal(scale={std})')\n \n     def test_lognormal(self):\n         mean = torch.randn(5, 5, requires_grad=True)\n@@ -1746,7 +1746,7 @@ def test_lognormal_sample(self):\n         for mean, std in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(LogNormal(mean, std),\n                                         scipy.stats.lognorm(scale=math.exp(mean), s=std),\n-                                        'LogNormal(loc={}, scale={})'.format(mean, std))\n+                                        f'LogNormal(loc={mean}, scale={std})')\n \n     def test_logisticnormal(self):\n         set_rng_seed(1)  # see Note [Randomized statistical tests]\n@@ -1814,7 +1814,7 @@ def test_logisticnormal_sample(self):\n             std_th = torch.tensor(np.sqrt(np.diag(cov)))\n             self._check_sampler_sampler(\n                 LogisticNormal(mean_th, std_th), ref_dist,\n-                'LogisticNormal(loc={}, scale={})'.format(mean_th, std_th),\n+                f'LogisticNormal(loc={mean_th}, scale={std_th})',\n                 multivariate=True)\n \n     def test_mixture_same_family_shape(self):\n@@ -1958,7 +1958,7 @@ def test_normal_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Normal(loc, scale),\n                                         scipy.stats.norm(loc=loc, scale=scale),\n-                                        'Normal(mean={}, std={})'.format(loc, scale))\n+                                        f'Normal(mean={loc}, std={scale})')\n \n     def test_lowrank_multivariate_normal_shape(self):\n         mean = torch.randn(5, 3, requires_grad=True)\n@@ -2191,15 +2191,15 @@ def test_multivariate_normal_sample(self):\n \n         self._check_sampler_sampler(MultivariateNormal(mean, cov),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, cov={})'.format(mean, cov),\n+                                    f'MultivariateNormal(loc={mean}, cov={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, precision_matrix=prec),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, atol={})'.format(mean, prec),\n+                                    f'MultivariateNormal(loc={mean}, atol={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, scale_tril=scale_tril),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, scale_tril={})'.format(mean, scale_tril),\n+                                    f'MultivariateNormal(loc={mean}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_multivariate_normal_properties(self):\n@@ -2352,15 +2352,15 @@ def test_wishart_sample(self):\n \n         self._check_sampler_sampler(Wishart(df, cov),\n                                     ref_dist,\n-                                    'Wishart(df={}, covariance_matrix={})'.format(df, cov),\n+                                    f'Wishart(df={df}, covariance_matrix={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, precision_matrix=prec),\n                                     ref_dist,\n-                                    'Wishart(df={}, precision_matrix={})'.format(df, prec),\n+                                    f'Wishart(df={df}, precision_matrix={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, scale_tril=scale_tril),\n                                     ref_dist,\n-                                    'Wishart(df={}, scale_tril={})'.format(df, scale_tril),\n+                                    f'Wishart(df={df}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_wishart_properties(self):\n@@ -2431,7 +2431,7 @@ def test_exponential_sample(self):\n         for rate in [1e-5, 1.0, 10.]:\n             self._check_sampler_sampler(Exponential(rate),\n                                         scipy.stats.expon(scale=1. / rate),\n-                                        'Exponential(rate={})'.format(rate))\n+                                        f'Exponential(rate={rate})')\n \n     def test_laplace(self):\n         loc = torch.randn(5, 5, requires_grad=True)\n@@ -2482,7 +2482,7 @@ def test_laplace_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Laplace(loc, scale),\n                                         scipy.stats.laplace(loc=loc, scale=scale),\n-                                        'Laplace(loc={}, scale={})'.format(loc, scale))\n+                                        f'Laplace(loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gamma_shape(self):\n@@ -2533,7 +2533,7 @@ def test_gamma_sample(self):\n         for alpha, beta in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gamma(alpha, beta),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(concentration={}, rate={})'.format(alpha, beta))\n+                                        f'Gamma(concentration={alpha}, rate={beta})')\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n@@ -2543,7 +2543,7 @@ def test_gamma_gpu_sample(self):\n             a, b = torch.tensor([alpha]).cuda(), torch.tensor([beta]).cuda()\n             self._check_sampler_sampler(Gamma(a, b),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(alpha={}, beta={})'.format(alpha, beta),\n+                                        f'Gamma(alpha={alpha}, beta={beta})',\n                                         failure_rate=1e-4)\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -2575,7 +2575,7 @@ def test_pareto_sample(self):\n         for scale, alpha in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Pareto(scale, alpha),\n                                         scipy.stats.pareto(alpha, scale=scale),\n-                                        'Pareto(scale={}, alpha={})'.format(scale, alpha))\n+                                        f'Pareto(scale={scale}, alpha={alpha})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gumbel(self):\n@@ -2616,7 +2616,7 @@ def test_gumbel_sample(self):\n         for loc, scale in product([-5.0, -1.0, -0.1, 0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gumbel(loc, scale),\n                                         scipy.stats.gumbel_r(loc=loc, scale=scale),\n-                                        'Gumbel(loc={}, scale={})'.format(loc, scale))\n+                                        f'Gumbel(loc={loc}, scale={scale})')\n \n     def test_kumaraswamy_shape(self):\n         concentration1 = torch.randn(2, 3).abs().requires_grad_()\n@@ -2646,13 +2646,13 @@ def test_kumaraswamy_mean_variance(self):\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .mean\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .mean\")\n             expected = samples.var(0)\n             actual = m.variance\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .variance\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .variance\")\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_fishersnedecor(self):\n@@ -2683,7 +2683,7 @@ def test_fishersnedecor_sample(self):\n         for df1, df2 in product([0.1, 0.5, 1.0, 5.0, 10.0], [0.1, 0.5, 1.0, 5.0, 10.0]):\n             self._check_sampler_sampler(FisherSnedecor(df1, df2),\n                                         scipy.stats.f(df1, df2),\n-                                        'FisherSnedecor(loc={}, scale={})'.format(df1, df2))\n+                                        f'FisherSnedecor(loc={df1}, scale={df2})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_chi2_shape(self):\n@@ -2710,7 +2710,7 @@ def test_chi2_sample(self):\n         for df in [0.1, 1.0, 5.0]:\n             self._check_sampler_sampler(Chi2(df),\n                                         scipy.stats.chi2(df),\n-                                        'Chi2(df={})'.format(df))\n+                                        f'Chi2(df={df})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT(self):\n@@ -2740,7 +2740,7 @@ def test_studentT_sample(self):\n         for df, loc, scale in product([0.1, 1.0, 5.0, 10.0], [-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(StudentT(df=df, loc=loc, scale=scale),\n                                         scipy.stats.t(df=df, loc=loc, scale=scale),\n-                                        'StudentT(df={}, loc={}, scale={})'.format(df, loc, scale))\n+                                        f'StudentT(df={df}, loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT_log_prob(self):\n@@ -2793,7 +2793,7 @@ def test_dirichlet_sample(self):\n         alpha = torch.exp(torch.randn(3))\n         self._check_sampler_sampler(Dirichlet(alpha),\n                                     scipy.stats.dirichlet(alpha.numpy()),\n-                                    'Dirichlet(alpha={})'.format(list(alpha)),\n+                                    f'Dirichlet(alpha={list(alpha)})',\n                                     multivariate=True)\n \n     def test_dirichlet_mode(self):\n@@ -2837,11 +2837,11 @@ def test_beta_sample(self):\n         for con1, con0 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Beta(con1, con0),\n                                         scipy.stats.beta(con1, con0),\n-                                        'Beta(alpha={}, beta={})'.format(con1, con0))\n+                                        f'Beta(alpha={con1}, beta={con0})')\n         # Check that small alphas do not cause NANs.\n         for Tensor in [torch.FloatTensor, torch.DoubleTensor]:\n             x = Beta(Tensor([1e-6]), Tensor([1e-6])).sample()[0]\n-            self.assertTrue(np.isfinite(x) and x > 0, 'Invalid Beta.sample(): {}'.format(x))\n+            self.assertTrue(np.isfinite(x) and x > 0, f'Invalid Beta.sample(): {x}')\n \n     def test_beta_underflow(self):\n         # For low values of (alpha, beta), the gamma samples can underflow\n@@ -2997,10 +2997,10 @@ def test_cdf_icdf_inverse(self):\n                     continue\n                 rel_error = torch.abs(actual - samples) / (1e-10 + torch.abs(samples))\n                 self.assertLess(rel_error.max(), 1e-4, msg='\\n'.join([\n-                    '{} example {}/{}, icdf(cdf(x)) != x'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf(x) = {}'.format(cdf),\n-                    'icdf(cdf(x)) = {}'.format(actual),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, icdf(cdf(x)) != x',\n+                    f'x = {samples}',\n+                    f'cdf(x) = {cdf}',\n+                    f'icdf(cdf(x)) = {actual}',\n                 ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3029,11 +3029,11 @@ def test_cdf_log_prob(self):\n                     continue\n                 cdfs_derivative = grad(cdfs.sum(), [samples])[0]  # this should not be wrapped in torch.abs()\n                 self.assertEqual(cdfs_derivative, pdfs, msg='\\n'.join([\n-                    '{} example {}/{}, d(cdf)/dx != pdf(x)'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf = {}'.format(cdfs),\n-                    'pdf = {}'.format(pdfs),\n-                    'grad(cdf) = {}'.format(cdfs_derivative),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, d(cdf)/dx != pdf(x)',\n+                    f'x = {samples}',\n+                    f'cdf = {cdfs}',\n+                    f'pdf = {pdfs}',\n+                    f'grad(cdf) = {cdfs_derivative}',\n                 ]))\n \n     def test_valid_parameter_broadcasting(self):\n@@ -3144,13 +3144,13 @@ def test_valid_parameter_broadcasting(self):\n         for dist, expected_size in valid_examples:\n             actual_size = dist.sample().size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n             sample_shape = torch.Size((2,))\n             expected_size = sample_shape + expected_size\n             actual_size = dist.sample(sample_shape).size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n     def test_invalid_parameter_broadcasting(self):\n         # invalid broadcasting cases; should throw error\n@@ -3303,13 +3303,13 @@ def test_gamma(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.0005, '\\n'.join([\n-                'Bad gradient dx/alpha for x ~ Gamma({}, 1)'.format(alpha),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at alpha={}, x={}'.format(alpha, x[rel_error.argmax()]),\n+                f'Bad gradient dx/alpha for x ~ Gamma({alpha}, 1)',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at alpha={alpha}, x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3331,12 +3331,12 @@ def test_chi2(self):\n             expected_grad = -cdf_df / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx/ddf for x ~ Chi2({})'.format(df),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n+                f'Bad gradient dx/ddf for x ~ Chi2({df})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3361,13 +3361,13 @@ def test_dirichlet_on_diagonal(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx[0]/dalpha[0] for Dirichlet([{}, {}, {}])'.format(a0, a1, a2),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x={}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx[0]/dalpha[0] for Dirichlet([{a0}, {a1}, {a2}])',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3391,13 +3391,13 @@ def test_beta_wrt_alpha(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon1 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon1 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3421,13 +3421,13 @@ def test_beta_wrt_beta(self):\n             expected_grad = -cdf_beta / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon0 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {!r}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon0 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]!r}',\n             ]))\n \n     def test_dirichlet_multivariate(self):\n@@ -3485,8 +3485,8 @@ def compute_v(x, alpha):\n             # expression in terms of log_prob rather than the less numerically stable log_prob.exp().\n             error = dlogp_da + (dlogp_dx * v).sum(-1) + div_v\n             self.assertLess(torch.abs(error).max(), 0.005, '\\n'.join([\n-                'Dirichlet([{}, {}, {}]) gradient violates continuity equation:'.format(a1, a2, a3),\n-                'error = {}'.format(error),\n+                f'Dirichlet([{a1}, {a2}, {a3}]) gradient violates continuity equation:',\n+                f'error = {error}',\n             ]))\n \n \n@@ -4147,9 +4147,9 @@ def test_kl_monte_carlo(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                'Expected ({} Monte Carlo samples): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                f'Expected ({denominator} Monte Carlo samples): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     # Multivariate normal has a separate Monte Carlo based test due to the requirement of random generation of\n@@ -4174,9 +4174,9 @@ def test_kl_multivariate_normal(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected ({} Monte Carlo sample): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected ({denominator} Monte Carlo sample): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     def test_kl_multivariate_normal_batched(self):\n@@ -4223,23 +4223,23 @@ def test_kl_lowrank_multivariate_normal(self):\n \n             error_lowrank_lowrank = torch.abs(actual_lowrank_lowrank - expected).max()\n             self.assertLess(error_lowrank_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_lowrank),\n+                f'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_lowrank}',\n             ]))\n \n             error_lowrank_full = torch.abs(actual_lowrank_full - expected).max()\n             self.assertLess(error_lowrank_full, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_full),\n+                f'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_full}',\n             ]))\n \n             error_full_lowrank = torch.abs(actual_full_lowrank - expected).max()\n             self.assertLess(error_full_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_full_lowrank),\n+                f'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_full_lowrank}',\n             ]))\n \n     def test_kl_lowrank_multivariate_normal_batched(self):\n@@ -4261,16 +4261,16 @@ def test_kl_exponential_family(self):\n                 actual = kl_divergence(p, q)\n                 expected = _kl_expfamily_expfamily(p, q)\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                    'Expected (using Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                    f'Expected (using Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n     def test_kl_infinite(self):\n         for p, q in self.infinite_examples:\n             self.assertTrue((kl_divergence(p, q) == inf).all(),\n-                            'Incorrect KL({}, {})'.format(type(p).__name__, type(q).__name__))\n+                            f'Incorrect KL({type(p).__name__}, {type(q).__name__})')\n \n     def test_kl_edgecases(self):\n         self.assertEqual(kl_divergence(Bernoulli(0), Bernoulli(0)), 0)\n@@ -4287,9 +4287,9 @@ def test_kl_shape(self):\n                     continue\n                 expected_shape = dist.batch_shape if dist.batch_shape else torch.Size()\n                 self.assertEqual(kl.shape, expected_shape, msg='\\n'.join([\n-                    '{} example {}/{}'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected {}'.format(expected_shape),\n-                    'Actual {}'.format(kl.shape),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}',\n+                    f'Expected {expected_shape}',\n+                    f'Actual {kl.shape}',\n                 ]))\n \n     def test_kl_transformed(self):\n@@ -4316,10 +4316,10 @@ def test_entropy_monte_carlo(self):\n                 ignore = (expected == inf) | (expected == -inf)\n                 expected[ignore] = actual[ignore]\n                 self.assertEqual(actual, expected, atol=0.2, rtol=0, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (monte carlo) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max()),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (monte carlo) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}',\n                 ]))\n \n     def test_entropy_exponential_family(self):\n@@ -4337,10 +4337,10 @@ def test_entropy_exponential_family(self):\n                 except NotImplementedError:\n                     continue\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n \n@@ -4632,7 +4632,7 @@ def test_lazy_logits_initialization(self):\n             dist = Dist(**param)\n             # Create new instance to generate a valid sample\n             dist.log_prob(Dist(**param).sample())\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('probs', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -4649,7 +4649,7 @@ def test_lazy_probs_initialization(self):\n                 continue\n             dist = Dist(**param)\n             dist.sample()\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('logits', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -5161,7 +5161,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_enumerate_support(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5185,7 +5185,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_mean(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5207,7 +5207,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_variance(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5231,7 +5231,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_entropy(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5255,7 +5255,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_cdf(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5276,7 +5276,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n \n if __name__ == '__main__' and torch._C.has_lapack:\ndiff --git a/test/distributions/test_transforms.py b/test/distributions/test_transforms.py\nindex a4a025b83fd36e..6fd4cf818d6b83 100644\n--- a/test/distributions/test_transforms.py\n+++ b/test/distributions/test_transforms.py\n@@ -156,7 +156,7 @@ def generate_data(transform):\n         x /= x.norm(dim=-1, keepdim=True)\n         x.diagonal(dim1=-1).copy_(x.diagonal(dim1=-1).abs())\n         return x\n-    raise ValueError('Unsupported domain: {}'.format(domain))\n+    raise ValueError(f'Unsupported domain: {domain}')\n \n \n TRANSFORMS_CACHE_ACTIVE = get_transforms(cache_size=1)\n@@ -215,19 +215,19 @@ def test_forward_inverse(transform, test_cached):\n     if transform.bijective:\n         # verify function inverse\n         assert torch.allclose(x2, x, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t.inv(t(-)) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n+            f'{transform} t.inv(t(-)) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n         ])\n     else:\n         # verify weaker function pseudo-inverse\n         assert torch.allclose(y2, y, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t(t.inv(t(-))) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n-            'y2 = t(x2) = {}'.format(y2),\n+            f'{transform} t(t.inv(t(-))) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n+            f'y2 = t(x2) = {y2}',\n         ])\n \n \ndiff --git a/test/optim/test_optim.py b/test/optim/test_optim.py\nindex 54307b2417eaf6..2f1f5536fc2fe1 100644\n--- a/test/optim/test_optim.py\n+++ b/test/optim/test_optim.py\n@@ -1701,8 +1701,8 @@ def test_fused_optimizer_does_not_step_if_foundinf(self):\n \n         num_tensors = 5\n         for functional_optim, amsgrad, no_grad_scale in itertools.product((adam.adam, adamw.adamw), (False, True), (False, True)):\n-            params, grads, exp_avgs, exp_avg_sqs = [\n-                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4)]\n+            params, grads, exp_avgs, exp_avg_sqs = (\n+                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4))\n             prev_params = [t.clone().detach() for t in params]\n             max_exp_avg_sqs = [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] if amsgrad else []\n             state_steps = [torch.ones((), dtype=torch.float32, device=\"cuda\") for _ in range(num_tensors)]\ndiff --git a/torch/distributions/constraints.py b/torch/distributions/constraints.py\nindex a4e3c08461cde7..5f284959beb372 100644\n--- a/torch/distributions/constraints.py\n+++ b/torch/distributions/constraints.py\n@@ -258,7 +258,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -277,7 +277,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -296,7 +296,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -321,7 +321,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -338,7 +338,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -355,7 +355,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -373,7 +373,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -391,7 +391,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \ndiff --git a/torch/distributions/independent.py b/torch/distributions/independent.py\nindex 48442650ddcb77..44a01fd62f9130 100644\n--- a/torch/distributions/independent.py\n+++ b/torch/distributions/independent.py\n@@ -109,4 +109,4 @@ def enumerate_support(self, expand=True):\n         return self.base_dist.enumerate_support(expand=expand)\n \n     def __repr__(self):\n-        return self.__class__.__name__ + '({}, {})'.format(self.base_dist, self.reinterpreted_batch_ndims)\n+        return self.__class__.__name__ + f'({self.base_dist}, {self.reinterpreted_batch_ndims})'\ndiff --git a/torch/distributions/kl.py b/torch/distributions/kl.py\nindex 26d7b47d2f51a8..4eda85ef75b68a 100644\n--- a/torch/distributions/kl.py\n+++ b/torch/distributions/kl.py\n@@ -65,9 +65,9 @@ def kl_version2(p, q): ...\n         type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n     \"\"\"\n     if not isinstance(type_p, type) and issubclass(type_p, Distribution):\n-        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n+        raise TypeError(f'Expected type_p to be a Distribution subclass but got {type_p}')\n     if not isinstance(type_q, type) and issubclass(type_q, Distribution):\n-        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n+        raise TypeError(f'Expected type_q to be a Distribution subclass but got {type_q}')\n \n     def decorator(fun):\n         _KL_REGISTRY[type_p, type_q] = fun\n@@ -735,7 +735,7 @@ def _kl_uniform_beta(p, q):\n     common_term = p.high - p.low\n     t1 = torch.log(common_term)\n     t2 = (q.concentration1 - 1) * (_x_log_x(p.high) - _x_log_x(p.low) - common_term) / common_term\n-    t3 = (q.concentration0 - 1) * (_x_log_x((1 - p.high)) - _x_log_x((1 - p.low)) + common_term) / common_term\n+    t3 = (q.concentration0 - 1) * (_x_log_x(1 - p.high) - _x_log_x(1 - p.low) + common_term) / common_term\n     t4 = q.concentration1.lgamma() + q.concentration0.lgamma() - (q.concentration1 + q.concentration0).lgamma()\n     result = t3 + t4 - t1 - t2\n     result[(p.high > q.support.upper_bound) | (p.low < q.support.lower_bound)] = inf\ndiff --git a/torch/distributions/lowrank_multivariate_normal.py b/torch/distributions/lowrank_multivariate_normal.py\nindex f74ea47a7e53a4..5ca125a92dd006 100644\n--- a/torch/distributions/lowrank_multivariate_normal.py\n+++ b/torch/distributions/lowrank_multivariate_normal.py\n@@ -93,7 +93,7 @@ def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n             raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\"\n                              .format(event_shape[0]))\n         if cov_diag.shape[-1:] != event_shape:\n-            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n+            raise ValueError(f\"cov_diag must be a batch of vectors with shape {event_shape}\")\n \n         loc_ = loc.unsqueeze(-1)\n         cov_diag_ = cov_diag.unsqueeze(-1)\ndiff --git a/torch/distributions/mixture_same_family.py b/torch/distributions/mixture_same_family.py\nindex f12bef1da2c54d..a4d7bd6ff4610b 100644\n--- a/torch/distributions/mixture_same_family.py\n+++ b/torch/distributions/mixture_same_family.py\n@@ -71,17 +71,17 @@ def __init__(self,\n         cdbs = self._component_distribution.batch_shape[:-1]\n         for size1, size2 in zip(reversed(mdbs), reversed(cdbs)):\n             if size1 != 1 and size2 != 1 and size1 != size2:\n-                raise ValueError(\"`mixture_distribution.batch_shape` ({0}) is not \"\n+                raise ValueError(\"`mixture_distribution.batch_shape` ({}) is not \"\n                                  \"compatible with `component_distribution.\"\n-                                 \"batch_shape`({1})\".format(mdbs, cdbs))\n+                                 \"batch_shape`({})\".format(mdbs, cdbs))\n \n         # Check that the number of mixture component matches\n         km = self._mixture_distribution.logits.shape[-1]\n         kc = self._component_distribution.batch_shape[-1]\n         if km is not None and kc is not None and km != kc:\n-            raise ValueError(\"`mixture_distribution component` ({0}) does not\"\n+            raise ValueError(\"`mixture_distribution component` ({}) does not\"\n                              \" equal `component_distribution.batch_shape[-1]`\"\n-                             \" ({1})\".format(km, kc))\n+                             \" ({})\".format(km, kc))\n         self._num_component = km\n \n         event_shape = self._component_distribution.event_shape\ndiff --git a/torch/distributions/transformed_distribution.py b/torch/distributions/transformed_distribution.py\nindex d31064210d4ba7..cd7b5f088a99fe 100644\n--- a/torch/distributions/transformed_distribution.py\n+++ b/torch/distributions/transformed_distribution.py\n@@ -51,7 +51,7 @@ def __init__(self, base_distribution, transforms, validate_args=None):\n                 raise ValueError(\"transforms must be a Transform or a list of Transforms\")\n             self.transforms = transforms\n         else:\n-            raise ValueError(\"transforms must be a Transform or list, but was {}\".format(transforms))\n+            raise ValueError(f\"transforms must be a Transform or list, but was {transforms}\")\n \n         # Reshape base_distribution according to transforms.\n         base_shape = base_distribution.batch_shape + base_distribution.event_shape\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 06d21548384e3c..6745d1f6fbd51e 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -135,7 +135,7 @@ def with_cache(self, cache_size=1):\n             return self\n         if type(self).__init__ is Transform.__init__:\n             return type(self)(cache_size=cache_size)\n-        raise NotImplementedError(\"{}.with_cache is not implemented\".format(type(self)))\n+        raise NotImplementedError(f\"{type(self)}.with_cache is not implemented\")\n \n     def __eq__(self, other):\n         return self is other\n@@ -506,7 +506,7 @@ def forward_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.in_shape)\n         if shape[cut:] != self.in_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.in_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.in_shape}\")\n         return shape[:cut] + self.out_shape\n \n     def inverse_shape(self, shape):\n@@ -514,7 +514,7 @@ def inverse_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.out_shape)\n         if shape[cut:] != self.out_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.out_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.out_shape}\")\n         return shape[:cut] + self.in_shape\n \n \ndiff --git a/torch/optim/adadelta.py b/torch/optim/adadelta.py\nindex d4cbd41883af65..a38337426313db 100644\n--- a/torch/optim/adadelta.py\n+++ b/torch/optim/adadelta.py\n@@ -22,13 +22,13 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= rho <= 1.0:\n-            raise ValueError(\"Invalid rho value: {}\".format(rho))\n+            raise ValueError(f\"Invalid rho value: {rho}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adagrad.py b/torch/optim/adagrad.py\nindex 5909818e5bfb6e..1a3e5120004f98 100644\n--- a/torch/optim/adagrad.py\n+++ b/torch/optim/adagrad.py\n@@ -23,11 +23,11 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= lr_decay:\n-            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n+            raise ValueError(f\"Invalid lr_decay value: {lr_decay}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= initial_accumulator_value:\n             raise ValueError(\n                 \"Invalid initial_accumulator_value value: {}\".format(\n@@ -35,7 +35,7 @@ def __init__(\n                 )\n             )\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adam.py b/torch/optim/adam.py\nindex 3c0d550f414b45..c7e4ed45a92156 100644\n--- a/torch/optim/adam.py\n+++ b/torch/optim/adam.py\n@@ -16,15 +16,15 @@ def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                  maximize: bool = False, capturable: bool = False,\n                  differentiable: bool = False, fused: Optional[bool] = None):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, amsgrad=amsgrad,\ndiff --git a/torch/optim/adamax.py b/torch/optim/adamax.py\nindex 9a5bf9131993ad..1ee927274558f1 100644\n--- a/torch/optim/adamax.py\n+++ b/torch/optim/adamax.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adamw.py b/torch/optim/adamw.py\nindex da202d95c2a032..ff8dbef1d46e8a 100644\n--- a/torch/optim/adamw.py\n+++ b/torch/optim/adamw.py\n@@ -26,15 +26,15 @@ def __init__(\n         fused: Optional[bool] = None,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/asgd.py b/torch/optim/asgd.py\nindex 5e5bd759c1d540..e483e1c31fbc7c 100644\n--- a/torch/optim/asgd.py\n+++ b/torch/optim/asgd.py\n@@ -28,9 +28,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/lr_scheduler.py b/torch/optim/lr_scheduler.py\nindex b531f5149d1aff..d0f85a5daea0c8 100644\n--- a/torch/optim/lr_scheduler.py\n+++ b/torch/optim/lr_scheduler.py\n@@ -1366,11 +1366,11 @@ class CosineAnnealingWarmRestarts(LRScheduler):\n \n     def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n         if T_0 <= 0 or not isinstance(T_0, int):\n-            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n+            raise ValueError(f\"Expected positive integer T_0, but got {T_0}\")\n         if T_mult < 1 or not isinstance(T_mult, int):\n-            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n+            raise ValueError(f\"Expected integer T_mult >= 1, but got {T_mult}\")\n         if not isinstance(eta_min, (float, int)):\n-            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))\n+            raise ValueError(f\"Expected float or int eta_min, but got {eta_min} of type {type(eta_min)}\")\n         self.T_0 = T_0\n         self.T_i = T_0\n         self.T_mult = T_mult\n@@ -1425,7 +1425,7 @@ def step(self, epoch=None):\n                 self.T_i = self.T_i * self.T_mult\n         else:\n             if epoch < 0:\n-                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n+                raise ValueError(f\"Expected non-negative epoch, but got {epoch}\")\n             if epoch >= self.T_0:\n                 if self.T_mult == 1:\n                     self.T_cur = epoch % self.T_0\n@@ -1590,13 +1590,13 @@ def __init__(self,\n             raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n         elif total_steps is not None:\n             if total_steps <= 0 or not isinstance(total_steps, int):\n-                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n+                raise ValueError(f\"Expected positive integer total_steps, but got {total_steps}\")\n             self.total_steps = total_steps\n         else:\n             if epochs <= 0 or not isinstance(epochs, int):\n-                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n+                raise ValueError(f\"Expected positive integer epochs, but got {epochs}\")\n             if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n-                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n+                raise ValueError(f\"Expected positive integer steps_per_epoch, but got {steps_per_epoch}\")\n             self.total_steps = epochs * steps_per_epoch\n \n         if three_phase:\n@@ -1643,11 +1643,11 @@ def __init__(self,\n \n         # Validate pct_start\n         if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n-            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n+            raise ValueError(f\"Expected float between 0 and 1 pct_start, but got {pct_start}\")\n \n         # Validate anneal_strategy\n         if anneal_strategy not in ['cos', 'linear']:\n-            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n+            raise ValueError(f\"anneal_strategy must by one of 'cos' or 'linear', instead got {anneal_strategy}\")\n         elif anneal_strategy == 'cos':\n             self.anneal_func = self._annealing_cos\n         elif anneal_strategy == 'linear':\ndiff --git a/torch/optim/nadam.py b/torch/optim/nadam.py\nindex 23fa563f044d0d..aeb3fc8b77dd2c 100644\n--- a/torch/optim/nadam.py\n+++ b/torch/optim/nadam.py\n@@ -11,17 +11,17 @@ def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                  weight_decay=0, momentum_decay=4e-3, *, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= momentum_decay:\n-            raise ValueError(\"Invalid momentum_decay value: {}\".format(momentum_decay))\n+            raise ValueError(f\"Invalid momentum_decay value: {momentum_decay}\")\n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, momentum_decay=momentum_decay,\n                         foreach=foreach, differentiable=differentiable)\ndiff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py\nindex 34d27bdaca6058..2356a073f3719d 100644\n--- a/torch/optim/optimizer.py\n+++ b/torch/optim/optimizer.py\n@@ -246,10 +246,10 @@ def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         for i, group in enumerate(self.param_groups):\n             format_string += '\\n'\n-            format_string += 'Parameter Group {0}\\n'.format(i)\n+            format_string += f'Parameter Group {i}\\n'\n             for key in sorted(group.keys()):\n                 if key != 'params':\n-                    format_string += '    {0}: {1}\\n'.format(key, group[key])\n+                    format_string += f'    {key}: {group[key]}\\n'\n         format_string += ')'\n         return format_string\n \n@@ -304,7 +304,7 @@ def profile_hook_step(func):\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             self, *_ = args\n-            profile_name = \"Optimizer.step#{}.step\".format(self.__class__.__name__)\n+            profile_name = f\"Optimizer.step#{self.__class__.__name__}.step\"\n             with torch.autograd.profiler.record_function(profile_name):\n                 # call optimizer step pre hooks\n                 for pre_hook in chain(_global_optimizer_pre_hooks.values(), self._optimizer_step_pre_hooks.values()):\n@@ -337,7 +337,7 @@ def _group_tensors_by_device_and_dtype(tensorlistlist, with_indices=False):\n             return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)\n \n     def _patch_step_function(self):\n-        self._zero_grad_profile_name = \"Optimizer.zero_grad#{}.zero_grad\".format(self.__class__.__name__)\n+        self._zero_grad_profile_name = f\"Optimizer.zero_grad#{self.__class__.__name__}.zero_grad\"\n         hooked = getattr(self.__class__.step, \"hooked\", None)\n         if not hooked:\n             self.__class__.step = self.profile_hook_step(self.__class__.step)  # type: ignore[method-assign]\n@@ -468,8 +468,8 @@ def load_state_dict(self, state_dict):\n                              \"that doesn't match the size of optimizer's group\")\n \n         # Update the state\n-        id_map = dict(zip(chain.from_iterable((g['params'] for g in saved_groups)),\n-                      chain.from_iterable((g['params'] for g in groups))))\n+        id_map = dict(zip(chain.from_iterable(g['params'] for g in saved_groups),\n+                      chain.from_iterable(g['params'] for g in groups)))\n \n         def cast(param, value, param_id=None, param_groups=None, key=None):\n             r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\ndiff --git a/torch/optim/radam.py b/torch/optim/radam.py\nindex 3078db48cfd2fc..120620ab949cc1 100644\n--- a/torch/optim/radam.py\n+++ b/torch/optim/radam.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/rmsprop.py b/torch/optim/rmsprop.py\nindex 88acf98a1bcbda..cec27d95506840 100644\n--- a/torch/optim/rmsprop.py\n+++ b/torch/optim/rmsprop.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= momentum:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= alpha:\n-            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n+            raise ValueError(f\"Invalid alpha value: {alpha}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/rprop.py b/torch/optim/rprop.py\nindex a0812f5fbc903f..93e7241010500a 100644\n--- a/torch/optim/rprop.py\n+++ b/torch/optim/rprop.py\n@@ -20,9 +20,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < etas[0] < 1.0 < etas[1]:\n-            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n+            raise ValueError(f\"Invalid eta values: {etas[0]}, {etas[1]}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/sgd.py b/torch/optim/sgd.py\nindex c34761d5e48555..d22fb2a697fd41 100644\n--- a/torch/optim/sgd.py\n+++ b/torch/optim/sgd.py\n@@ -11,11 +11,11 @@ def __init__(self, params, lr=required, momentum=0, dampening=0,\n                  weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if lr is not required and lr < 0.0:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if momentum < 0.0:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if weight_decay < 0.0:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                         weight_decay=weight_decay, nesterov=nesterov,\ndiff --git a/torch/optim/sparse_adam.py b/torch/optim/sparse_adam.py\nindex 383b6866e822af..c68441cb389c04 100644\n--- a/torch/optim/sparse_adam.py\n+++ b/torch/optim/sparse_adam.py\n@@ -7,13 +7,13 @@\n class SparseAdam(Optimizer):\n     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, maximize: bool = False):\n         if not 0.0 < lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n \n         params = list(params)\n \ndiff --git a/torch/package/_importlib.py b/torch/package/_importlib.py\nindex 327c79c67ef9d7..011567b89f5d6c 100644\n--- a/torch/package/_importlib.py\n+++ b/torch/package/_importlib.py\n@@ -31,13 +31,13 @@ def _resolve_name(name, package, level):\n     if len(bits) < level:\n         raise ValueError(\"attempted relative import beyond top-level package\")\n     base = bits[0]\n-    return \"{}.{}\".format(base, name) if name else base\n+    return f\"{base}.{name}\" if name else base\n \n \n def _sanity_check(name, package, level):\n     \"\"\"Verify arguments are \"sane\".\"\"\"\n     if not isinstance(name, str):\n-        raise TypeError(\"module name must be str, not {}\".format(type(name)))\n+        raise TypeError(f\"module name must be str, not {type(name)}\")\n     if level < 0:\n         raise ValueError(\"level must be >= 0\")\n     if level > 0:\n@@ -90,6 +90,6 @@ def _normalize_path(path):\n     \"\"\"\n     parent, file_name = os.path.split(path)\n     if parent:\n-        raise ValueError(\"{!r} must be only a file name\".format(path))\n+        raise ValueError(f\"{path!r} must be only a file name\")\n     else:\n         return file_name\ndiff --git a/torch/package/file_structure_representation.py b/torch/package/file_structure_representation.py\nindex 6ea69173ed3f69..cc5f055c1a20ef 100644\n--- a/torch/package/file_structure_representation.py\n+++ b/torch/package/file_structure_representation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Dict, List\n \n from .glob_group import GlobGroup, GlobPattern\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex f9478b66327605..053ce0c0a89552 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -79,7 +79,7 @@ class PackagingErrorReason(Enum):\n     \"\"\"\n \n     def __repr__(self):\n-        return \"<%s.%s>\" % (self.__class__.__name__, self.name)\n+        return f\"<{self.__class__.__name__}.{self.name}>\"\n \n     IS_EXTENSION_MODULE = (\n         \"Module is a C extension module. torch.package supports Python modules only.\"\n@@ -156,14 +156,14 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-                        (\n+\n                             \"      Note: While we usually use modules in the python standard library \"\n                             f\"from the local environment, `{module_name}` has a lot of system \"\n                             \"level access and therefore can pose a security risk. We heavily \"\n                             f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n                             \"is not possible, add it to the extern list by calling \"\n                             f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-                        )\n+\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +173,10 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-                (\n+\n                     \"Set debug=True when invoking PackageExporter for a visualization of where \"\n                     \"broken modules are coming from!\\n\"\n-                )\n+\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\ndiff --git a/torch/package/package_importer.py b/torch/package/package_importer.py\nindex 8369e79e783ad7..2d313c8f14eb45 100644\n--- a/torch/package/package_importer.py\n+++ b/torch/package/package_importer.py\n@@ -539,7 +539,7 @@ def _handle_fromlist(self, module, fromlist, *, recursive=False):\n                     if not recursive and hasattr(module, \"__all__\"):\n                         self._handle_fromlist(module, module.__all__, recursive=True)\n                 elif not hasattr(module, x):\n-                    from_name = \"{}.{}\".format(module_name, x)\n+                    from_name = f\"{module_name}.{x}\"\n                     try:\n                         self._gcd_import(from_name)\n                     except ModuleNotFoundError as exc:\n@@ -587,13 +587,13 @@ def _get_package(self, package):\n         \"\"\"\n         if hasattr(package, \"__spec__\"):\n             if package.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package.__spec__.name))\n+                raise TypeError(f\"{package.__spec__.name!r} is not a package\")\n             else:\n                 return package\n         else:\n             module = self.import_module(package)\n             if module.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package))\n+                raise TypeError(f\"{package!r} is not a package\")\n             else:\n                 return module\n \ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex 7ade85a85caa11..fbbcd4d67b7889 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -738,11 +738,11 @@ def _category_snapshot(self) -> Dict[TensorAndID, Optional[Category]]:\n \n         for node in self._data_flow_graph.flow_nodes:\n             all_tensor_versions.update(((k, v) for k, (_, v) in node.inputs.items()))\n-            all_tensor_versions.update(((key, 0) for key in node.intermediates))\n+            all_tensor_versions.update((key, 0) for key in node.intermediates)\n             all_tensor_versions.update(node.outputs.items())\n \n         for i in self._categories._values.values():\n-            all_tensor_versions.update(((key, 0) for key in i._by_id_keyset))\n+            all_tensor_versions.update((key, 0) for key in i._by_id_keyset)\n \n         return {\n             (key, version): self._categories.get(key, version)\ndiff --git a/torch/profiler/_pattern_matcher.py b/torch/profiler/_pattern_matcher.py\nindex ae95faf0d2bae7..1d85d193ecf894 100644\n--- a/torch/profiler/_pattern_matcher.py\n+++ b/torch/profiler/_pattern_matcher.py\n@@ -642,7 +642,7 @@ def report_all_anti_patterns(prof,\n         json_report_path = os.path.join(json_report_dir,\n                                         \"torchtidy_report.json\")\n         if os.path.exists(json_report_path):\n-            with open(json_report_path, \"r\") as f:\n+            with open(json_report_path) as f:\n                 exisiting_report = json.load(f)\n                 exisiting_report.update(report_dict)\n                 report_dict = exisiting_report\ndiff --git a/torch/signal/windows/windows.py b/torch/signal/windows/windows.py\nindex 1ddfff96228927..d1b8e2529bb97e 100644\n--- a/torch/signal/windows/windows.py\n+++ b/torch/signal/windows/windows.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Optional, Iterable\n \n import torch\ndiff --git a/torch/sparse/semi_structured.py b/torch/sparse/semi_structured.py\nindex 0e4c217a50aafb..d1e4321d66f36b 100644\n--- a/torch/sparse/semi_structured.py\n+++ b/torch/sparse/semi_structured.py\n@@ -136,28 +136,28 @@ def __init__(\n             # check device\n             if not original_tensor.is_cuda:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.device= {original_tensor.device} is not supported! \"\n                         \"Only CUDA tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dim\n             if original_tensor.dim() != 2:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dim = {original_tensor.dim()} is not supported! \"\n                         \"Only 2d tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dtype\n             if original_tensor.dtype not in _DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dtype {original_tensor.dtype} is not a supported dtype! \"\n                         \"dtype must be one of: {_DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG}\"\n-                    )\n+\n                 )\n \n             # check shape\n@@ -167,10 +167,10 @@ def __init__(\n             if m < min_rows or m % min_rows or n < min_cols or n % min_cols:\n                 # TODO in the future we can add in padding to support dimensions that aren't perfect multiples\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.shape {original_tensor.shape} is not supported! \"\n                         \"Both dimensions must be larger or equal than and a multiple of ({min_rows}, {min_cols})\"\n-                    )\n+\n                 )\n \n             # This code calculates the size of the compressed tensor.\n"
  },
  {
    "number": 105415,
    "title": "[BE] Enable ruff's UP rules and autoformat testing/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "089d87a9a06f6d762e531500adc48c9e46f08db0",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105415",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105415/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105415.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105415.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105415/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105415/comments",
    "labels": [
      "release notes: distributed (rpc)"
    ],
    "_event_time": "2023-07-18T01:17:16.588058Z",
    "state": "closed",
    "patch": "From 8ac7475927868c7d7234bf84fe5ab9cb12fd03ac Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:09 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat testing/\n\n[ghstack-poisoned]\n---\n torch/testing/_comparison.py                  | 10 +--\n .../_internal/check_kernel_launches.py        |  2 +-\n .../_internal/codegen/random_topo_test.py     | 16 ++---\n torch/testing/_internal/common_cuda.py        |  2 +-\n torch/testing/_internal/common_device_type.py | 50 +++++++--------\n torch/testing/_internal/common_distributed.py |  4 +-\n torch/testing/_internal/common_fsdp.py        |  2 +-\n .../_internal/common_methods_invocations.py   | 18 +++---\n torch/testing/_internal/common_modules.py     | 64 +++++++++----------\n torch/testing/_internal/common_nn.py          | 12 ++--\n torch/testing/_internal/common_pruning.py     |  1 -\n .../testing/_internal/common_quantization.py  |  5 +-\n torch/testing/_internal/common_utils.py       | 34 +++++-----\n torch/testing/_internal/dist_utils.py         |  4 +-\n .../_internal/distributed/distributed_test.py | 14 ++--\n .../distributed/nn/api/remote_module_test.py  | 20 +++---\n .../distributed/rpc/dist_autograd_test.py     | 22 +++----\n .../reinforcement_learning_rpc_test.py        |  2 +-\n .../distributed/rpc/faulty_agent_rpc_test.py  |  8 +--\n .../rpc/faulty_rpc_agent_test_fixture.py      |  2 +-\n .../_internal/distributed/rpc/jit/rpc_test.py |  2 +-\n .../distributed/rpc/jit/rpc_test_faulty.py    |  8 +--\n .../_internal/distributed/rpc/rpc_test.py     |  4 +-\n .../rpc/tensorpipe_rpc_agent_test_fixture.py  |  2 +-\n .../_internal/jit_metaprogramming_utils.py    | 18 +++---\n torch/testing/_internal/jit_utils.py          | 12 ++--\n torch/testing/_internal/opinfo/core.py        |  9 +--\n .../_internal/opinfo/definitions/linalg.py    |  2 +-\n .../_internal/opinfo/definitions/sparse.py    |  2 +-\n 29 files changed, 172 insertions(+), 179 deletions(-)\n\ndiff --git a/torch/testing/_comparison.py b/torch/testing/_comparison.py\nindex 1ccc7447ed7486..4204a6c0e69441 100644\n--- a/torch/testing/_comparison.py\n+++ b/torch/testing/_comparison.py\n@@ -465,9 +465,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[bool, bool]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_bool(bool_like, id=id) for bool_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_bool(self, bool_like: Any, *, id: Tuple[Any, ...]) -> bool:\n@@ -559,9 +559,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[Union[int, float, complex], Union[int, float, complex]]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_number(number_like, id=id) for number_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_number(\n@@ -675,7 +675,7 @@ def _process_inputs(\n         if not allow_subclasses and type(actual) is not type(expected):\n             self._inputs_not_supported()\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\ndiff --git a/torch/testing/_internal/check_kernel_launches.py b/torch/testing/_internal/check_kernel_launches.py\nindex 667e3412ceaf2c..131ea461ce544a 100644\n--- a/torch/testing/_internal/check_kernel_launches.py\n+++ b/torch/testing/_internal/check_kernel_launches.py\n@@ -111,7 +111,7 @@ def check_file(filename):\n         return 0\n     if should_exclude_file(filename):\n         return 0\n-    with open(filename, \"r\") as fo:\n+    with open(filename) as fo:\n         contents = fo.read()\n         unsafeCount = check_code_for_cuda_kernel_launches(contents, filename)\n     return unsafeCount\ndiff --git a/torch/testing/_internal/codegen/random_topo_test.py b/torch/testing/_internal/codegen/random_topo_test.py\nindex fdb13d4ef139c0..b94f8f60a301d3 100644\n--- a/torch/testing/_internal/codegen/random_topo_test.py\n+++ b/torch/testing/_internal/codegen/random_topo_test.py\n@@ -96,7 +96,7 @@ def get_root(x, dependency_map):\n         out_tensor = None\n \n         if DEBUG_PRINT:\n-            print(\"iteration {0}, num_sets{1}, candidates {2}, tensor_list {3}, lh_index {4}, op_index {5}\".format(\n+            print(\"iteration {}, num_sets{}, candidates {}, tensor_list {}, lh_index {}, op_index {}\".format(\n                 num_operations, num_sets, candidate, len(tensor_list), lh_index, op_index))\n         if num_operations >= 0:\n             num_operations -= 1\n@@ -125,7 +125,7 @@ def get_root(x, dependency_map):\n                     #  right = tensor_list[lh_index]\n                     out_tensor = binary_operations[op_index - u_op_size](left, right)\n                 if DEBUG_PRINT:\n-                    print(\"binary, op_2_index {0}, rh_index ?{1}\".format(op_2_index, rh_index))\n+                    print(f\"binary, op_2_index {op_2_index}, rh_index ?{rh_index}\")\n         else:\n             # binary operation, we just randomly pick two candidates.\n             # this is not the most efficient way to close dependency, as we could have\n@@ -136,7 +136,7 @@ def get_root(x, dependency_map):\n             # [if rh_index: create binary operator output tensor]\n             rh_index = candidate[cand_index]\n             if DEBUG_PRINT:\n-                print(\"binary rh_index ?{0}\".format(rh_index))\n+                print(f\"binary rh_index ?{rh_index}\")\n \n         # update candidate should happen before we remove rh_index\n         candidate[index] = len(tensor_list)\n@@ -185,7 +185,7 @@ def get_root(x, dependency_map):\n             ret_list.append(tensor_list[ind])\n \n     if DEBUG_PRINT:\n-        print(\"ended with tensor_list: {0}\".format(len(tensor_list)))\n+        print(f\"ended with tensor_list: {len(tensor_list)}\")\n \n     return tuple(ret_list)\n \n@@ -248,7 +248,7 @@ def prepareInputTensorsToRandomTopoTest(seed,\n \n \n def reproString(current_seed, args):\n-    repro_str = \"python {0}\".format(__file__)\n+    repro_str = f\"python {__file__}\"\n     if args.cuda_fuser:\n         repro_str += \" --cuda-fuser\"\n     if args.legacy_fuser:\n@@ -259,8 +259,8 @@ def reproString(current_seed, args):\n         repro_str += \" --fp16\"\n     if args.cpu:\n         repro_str += \" --cpu\"\n-    repro_str += \" --max-num-tensor {0} --max-tensor-dim {1} --max-tensor-size {2}\"\\\n-        \" --depth-factor {3} --seed {4} --repro-run\".format(\n+    repro_str += \" --max-num-tensor {} --max-tensor-dim {} --max-tensor-size {}\"\\\n+        \" --depth-factor {} --seed {} --repro-run\".format(\n             args.max_num_tensor, args.max_tensor_dim, args.max_tensor_size,\n             args.depth_factor, current_seed)\n     return repro_str\n@@ -390,7 +390,7 @@ def parse_args():\n         if len(failing_repros) == 0:\n             print(\"test passed\")\n         else:\n-            print(\"{0} out of {1} tests failed;\".format(\n+            print(\"{} out of {} tests failed;\".format(\n                   len(failing_repros), args.iterations))\n             print(\"To repro failing tests, run\\n\")\n             for repro in failing_repros:\ndiff --git a/torch/testing/_internal/common_cuda.py b/torch/testing/_internal/common_cuda.py\nindex c380dd5e6d7250..f427f7b62b9f77 100644\n--- a/torch/testing/_internal/common_cuda.py\n+++ b/torch/testing/_internal/common_cuda.py\n@@ -48,7 +48,7 @@ def initialize_cuda_context_rng():\n     if not __cuda_ctx_rng_initialized:\n         # initialize cuda context and rng for memory tests\n         for i in range(torch.cuda.device_count()):\n-            torch.randn(1, device=\"cuda:{}\".format(i))\n+            torch.randn(1, device=f\"cuda:{i}\")\n         __cuda_ctx_rng_initialized = True\n \n \ndiff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py\nindex d9c362c332479d..891c878cc5f059 100644\n--- a/torch/testing/_internal/common_device_type.py\n+++ b/torch/testing/_internal/common_device_type.py\n@@ -276,9 +276,9 @@ def _dtype_test_suffix(dtypes):\n     if isinstance(dtypes, (list, tuple)):\n         if len(dtypes) == 0:\n             return ''\n-        return '_' + '_'.join((dtype_name(d) for d in dtypes))\n+        return '_' + '_'.join(dtype_name(d) for d in dtypes)\n     elif dtypes:\n-        return '_{}'.format(dtype_name(dtypes))\n+        return f'_{dtype_name(dtypes)}'\n     else:\n         return ''\n \n@@ -286,7 +286,7 @@ def _dtype_test_suffix(dtypes):\n def _update_param_kwargs(param_kwargs, name, value):\n     \"\"\" Adds a kwarg with the specified name and value to the param_kwargs dict. \"\"\"\n     # Make name plural (e.g. devices / dtypes) if the value is composite.\n-    plural_name = '{}s'.format(name)\n+    plural_name = f'{name}s'\n \n     # Clear out old entries of the arg if any.\n     if name in param_kwargs:\n@@ -432,7 +432,7 @@ def instantiated_test(self, param_kwargs=param_kwargs):\n \n                 return result\n \n-            assert not hasattr(cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(cls, name), f\"Redefinition of test {name}\"\n             setattr(cls, name, instantiated_test)\n \n         def default_parametrize_fn(test, generic_cls, device_cls):\n@@ -467,7 +467,7 @@ def dtype_parametrize_fn(test, generic_cls, device_cls, dtypes=dtypes):\n             dtype_kwarg = None\n             if 'dtype' in param_kwargs or 'dtypes' in param_kwargs:\n                 dtype_kwarg = param_kwargs['dtypes'] if 'dtypes' in param_kwargs else param_kwargs['dtype']\n-            test_name = '{}{}{}{}'.format(name, test_suffix, device_suffix, _dtype_test_suffix(dtype_kwarg))\n+            test_name = f'{name}{test_suffix}{device_suffix}{_dtype_test_suffix(dtype_kwarg)}'\n \n             instantiate_test_helper(cls=cls, name=test_name, test=test, param_kwargs=param_kwargs,\n                                     decorator_fn=decorator_fn)\n@@ -523,7 +523,7 @@ def setUpClass(cls):\n         cls.cudnn_version = None if cls.no_cudnn else torch.backends.cudnn.version()\n \n         # Acquires the current device as the primary (test) device\n-        cls.primary_device = 'cuda:{0}'.format(torch.cuda.current_device())\n+        cls.primary_device = f'cuda:{torch.cuda.current_device()}'\n \n # See Note [Lazy Tensor tests in device agnostic testing]\n lazy_ts_backend_init = False\n@@ -589,7 +589,7 @@ def setUpClass(cls):\n         cls.device_mod = getattr(torch, cls.device_type, None)\n         assert cls.device_mod is not None, f'''torch has no module of `{cls.device_type}`, you should register\n                                             a module by `torch._register_device_module`.'''\n-        cls.primary_device = '{device_type}:{id}'.format(device_type=cls.device_type, id=cls.device_mod.current_device())\n+        cls.primary_device = f'{cls.device_type}:{cls.device_mod.current_device()}'\n \n # Adds available device-type-specific test base classes\n def get_device_type_test_bases():\n@@ -744,7 +744,7 @@ def split_if_not_empty(x: str):\n                 else:\n                     device_type_test_class.instantiate_test(name, copy.deepcopy(test))\n             else:  # Ports non-test member\n-                assert name not in device_type_test_class.__dict__, \"Redefinition of directly defined member {0}\".format(name)\n+                assert name not in device_type_test_class.__dict__, f\"Redefinition of directly defined member {name}\"\n                 nontest = getattr(generic_test_class, name)\n                 setattr(device_type_test_class, name, nontest)\n \n@@ -913,7 +913,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for op {1}!\".format(test_name, op.name))\n+                    print(f\"Failed to instantiate {test_name} for op {op.name}!\")\n                     raise ex\n         if op is check_exhausted_iterator:\n             raise ValueError('An empty op_list was passed to @ops. '\n@@ -1034,7 +1034,7 @@ def dep_fn(self, *args, **kwargs):\n             size_bytes = size(self, *args, **kwargs) if callable(size) else size\n             _device = device if device is not None else self.get_primary_device()\n             if not _has_sufficient_memory(_device, size_bytes):\n-                raise unittest.SkipTest('Insufficient {} memory'.format(_device))\n+                raise unittest.SkipTest(f'Insufficient {_device} memory')\n \n             return fn(self, *args, **kwargs)\n         return dep_fn\n@@ -1072,7 +1072,7 @@ def __call__(self, fn):\n         @wraps(fn)\n         def only_fn(slf, *args, **kwargs):\n             if self.device_type != slf.device_type:\n-                reason = \"Only runs on {0}\".format(self.device_type)\n+                reason = f\"Only runs on {self.device_type}\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, *args, **kwargs)\n@@ -1090,13 +1090,13 @@ def __init__(self, num_required_devices):\n         self.num_required_devices = num_required_devices\n \n     def __call__(self, fn):\n-        assert not hasattr(fn, 'num_required_devices'), \"deviceCountAtLeast redefinition for {0}\".format(fn.__name__)\n+        assert not hasattr(fn, 'num_required_devices'), f\"deviceCountAtLeast redefinition for {fn.__name__}\"\n         fn.num_required_devices = self.num_required_devices\n \n         @wraps(fn)\n         def multi_fn(slf, devices, *args, **kwargs):\n             if len(devices) < self.num_required_devices:\n-                reason = \"fewer than {0} devices detected\".format(self.num_required_devices)\n+                reason = f\"fewer than {self.num_required_devices} devices detected\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, devices, *args, **kwargs)\n@@ -1108,7 +1108,7 @@ def onlyNativeDeviceTypes(fn):\n     @wraps(fn)\n     def only_fn(self, *args, **kwargs):\n         if self.device_type not in NATIVE_DEVICES:\n-            reason = \"onlyNativeDeviceTypes: doesn't run on {0}\".format(self.device_type)\n+            reason = f\"onlyNativeDeviceTypes: doesn't run on {self.device_type}\"\n             raise unittest.SkipTest(reason)\n \n         return fn(self, *args, **kwargs)\n@@ -1137,7 +1137,7 @@ class precisionOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"precisionOverride not given a dtype : precision dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"precisionOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"precisionOverride given unknown dtype {dtype}\"\n \n         self.d = d\n \n@@ -1168,7 +1168,7 @@ class toleranceOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"toleranceOverride not given a dtype : tol dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"toleranceOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"toleranceOverride given unknown dtype {dtype}\"\n             assert isinstance(prec, tol), \"toleranceOverride not given a dtype : tol dict!\"\n \n         self.d = d\n@@ -1195,17 +1195,17 @@ def __init__(self, *args, device_type=\"all\"):\n                 assert isinstance(arg, (list, tuple)), \\\n                     \"When one dtype variant is a tuple or list, \" \\\n                     \"all dtype variants must be. \" \\\n-                    \"Received non-list non-tuple dtype {0}\".format(str(arg))\n-                assert all(isinstance(dtype, torch.dtype) for dtype in arg), \"Unknown dtype in {0}\".format(str(arg))\n+                    \"Received non-list non-tuple dtype {}\".format(str(arg))\n+                assert all(isinstance(dtype, torch.dtype) for dtype in arg), f\"Unknown dtype in {str(arg)}\"\n         else:\n-            assert all(isinstance(arg, torch.dtype) for arg in args), \"Unknown dtype in {0}\".format(str(args))\n+            assert all(isinstance(arg, torch.dtype) for arg in args), f\"Unknown dtype in {str(args)}\"\n \n         self.args = args\n         self.device_type = device_type\n \n     def __call__(self, fn):\n         d = getattr(fn, 'dtypes', {})\n-        assert self.device_type not in d, \"dtypes redefinition for {0}\".format(self.device_type)\n+        assert self.device_type not in d, f\"dtypes redefinition for {self.device_type}\"\n         d[self.device_type] = self.args\n         fn.dtypes = d\n         return fn\n@@ -1244,7 +1244,7 @@ def onlyPRIVATEUSE1(fn):\n     device_type = torch._C._get_privateuse1_backend_name()\n     device_mod = getattr(torch, device_type, None)\n     if device_mod is None:\n-        reason = \"Skip as torch has no module of {0}\".format(device_type)\n+        reason = f\"Skip as torch has no module of {device_type}\"\n         return unittest.skip(reason)(fn)\n     return onlyOn(device_type)(fn)\n \n@@ -1358,7 +1358,7 @@ def wrap_fn(self, *args, **kwargs):\n                     raise unittest.SkipTest(reason)\n                 rocm_version_tuple = _get_torch_rocm_version()\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\n@@ -1375,7 +1375,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version in (versions or []):\n-                reason = \"test skipped for CUDA version {0}\".format(version)\n+                reason = f\"test skipped for CUDA version {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1391,7 +1391,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version < versions:\n-                reason = \"test skipped for CUDA versions < {0}\".format(version)\n+                reason = f\"test skipped for CUDA versions < {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1409,7 +1409,7 @@ def wrap_fn(self, *args, **kwargs):\n                     reason = \"cuDNN not available\"\n                     raise unittest.SkipTest(reason)\n                 if self.cudnn_version is None or self.cudnn_version < version:\n-                    reason = \"cuDNN version {0} is available but {1} required\".format(self.cudnn_version, version)\n+                    reason = f\"cuDNN version {self.cudnn_version} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\ndiff --git a/torch/testing/_internal/common_distributed.py b/torch/testing/_internal/common_distributed.py\nindex 8981aa78d06a09..d1cf02749b79ce 100644\n--- a/torch/testing/_internal/common_distributed.py\n+++ b/torch/testing/_internal/common_distributed.py\n@@ -770,7 +770,7 @@ def _check_no_test_errors(self, elapsed_time) -> None:\n         for i, p in enumerate(self.processes):\n             if p.exitcode is None:\n                 raise RuntimeError(\n-                    \"Process {} timed out after {} seconds\".format(i, elapsed_time)\n+                    f\"Process {i} timed out after {elapsed_time} seconds\"\n                 )\n             self.assertNotEqual(self.TEST_ERROR_EXIT_CODE, p.exitcode)\n \n@@ -1102,7 +1102,7 @@ def _check_return_codes(cls, failed_ranks, timeout, fn):\n                     \"Caught exception: \\n%s exiting thread %s\", msg, rank\n                 )\n                 error_msg += (\n-                    \"Thread {} exited with exception:\\n{}\\n\".format(rank, msg)\n+                    f\"Thread {rank} exited with exception:\\n{msg}\\n\"\n                 )\n             elif isinstance(exc, SystemExit):\n                 if type(exc.code) == int and skip_code < 0:\ndiff --git a/torch/testing/_internal/common_fsdp.py b/torch/testing/_internal/common_fsdp.py\nindex 589372eaa8e216..20a35930a7f5f4 100644\n--- a/torch/testing/_internal/common_fsdp.py\n+++ b/torch/testing/_internal/common_fsdp.py\n@@ -881,7 +881,7 @@ def process_group(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     def _check_cpu_offload(self, fsdp_model, cpu_offload):\n         self.assertEqual(cpu_offload, fsdp_model.cpu_offload)\ndiff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py\nindex 3b9d3269853a05..a9f58c76d90f2d 100644\n--- a/torch/testing/_internal/common_methods_invocations.py\n+++ b/torch/testing/_internal/common_methods_invocations.py\n@@ -851,7 +851,7 @@ def error_inputs_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"normal expects std >= 0.0, but found std {}\".format(invalid_std),\n+        error_regex=fr\"normal expects std >= 0.0, but found std {invalid_std}\",\n     )\n \n def sample_inputs_cauchy(op, device, dtype, requires_grad, **kwargs):\n@@ -871,7 +871,7 @@ def error_inputs_cauchy(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_scale,)),\n         error_type=RuntimeError,\n-        error_regex=r\"cauchy_ expects sigma > 0.0, but found sigma={}\".format(invalid_scale),\n+        error_regex=fr\"cauchy_ expects sigma > 0.0, but found sigma={invalid_scale}\",\n     )\n \n \n@@ -893,7 +893,7 @@ def error_inputs_exponential(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(invalid_rate,)),\n         error_type=RuntimeError,\n-        error_regex=r\"exponential_ expects lambda > 0.0, but found lambda={}\".format(invalid_rate),\n+        error_regex=fr\"exponential_ expects lambda > 0.0, but found lambda={invalid_rate}\",\n     )\n \n \n@@ -915,7 +915,7 @@ def error_inputs_geometric(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(neg_prob,)),\n         error_type=RuntimeError,\n-        error_regex=r\"geometric_ expects p to be in \\(0, 1\\), but got p={}\".format(neg_prob),\n+        error_regex=fr\"geometric_ expects p to be in \\(0, 1\\), but got p={neg_prob}\",\n     )\n \n \n@@ -937,7 +937,7 @@ def error_inputs_log_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"log_normal_ expects std > 0.0, but found std={}\".format(invalid_std),\n+        error_regex=fr\"log_normal_ expects std > 0.0, but found std={invalid_std}\",\n     )\n \n \n@@ -1889,9 +1889,9 @@ def sample_inputs_logcumsumexp(self, device, dtype, requires_grad, **kwargs):\n             yield SampleInput(t, dim)\n \n def sample_inputs_trace(self, device, dtype, requires_grad, **kwargs):\n-    yield SampleInput((make_tensor((S, S), dtype=dtype, device=device,\n+    yield SampleInput(make_tensor((S, S), dtype=dtype, device=device,\n                                    low=None, high=None,\n-                                   requires_grad=requires_grad)))\n+                                   requires_grad=requires_grad))\n \n \n def error_inputs_trace(op, device):\n@@ -4020,7 +4020,7 @@ def error_inputs_group_norm(opinfo, device, **kwargs):\n \n     # check that input has minimum number of dimensions\n     err_msg1 = \"Expected at least 2 dimensions for input tensor but received\"\n-    s1 = SampleInput(make_arg((1)), args=(1,))\n+    s1 = SampleInput(make_arg(1), args=(1,))\n     yield ErrorInput(s1, error_regex=err_msg1)\n \n     # check that the channels dimension is compatible with number of groups\n@@ -6950,7 +6950,7 @@ def make_bool_mask(shape):\n \n         if mask_t.sum() == 0:\n             def random_index(shape):\n-                return tuple((random.randrange(0, max_idx) for max_idx in shape))\n+                return tuple(random.randrange(0, max_idx) for max_idx in shape)\n \n             mask_t[random_index(mask_t.shape)] = True\n             return mask_t\ndiff --git a/torch/testing/_internal/common_modules.py b/torch/testing/_internal/common_modules.py\nindex 2119678a33f5ef..c3ac11454ab410 100644\n--- a/torch/testing/_internal/common_modules.py\n+++ b/torch/testing/_internal/common_modules.py\n@@ -123,7 +123,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for module {1}!\".format(test_name, module_info.name))\n+                    print(f\"Failed to instantiate {test_name} for module {module_info.name}!\")\n                     raise ex\n \n \n@@ -252,7 +252,7 @@ def bilinear_reference_fn(m, p, x1, x2, bias=True):\n                     desc='no_bias',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n         ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n-                    forward_input=FunctionInput(make_input((2)), make_input((3))),\n+                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                     desc='no_batch_dim',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n     ]\n@@ -312,9 +312,9 @@ def module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_\n     for desc, constructor_kwargs in cases:\n         module_inputs.append(\n             ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n-                        forward_input=FunctionInput(make_input((3)),\n-                                                    make_target((3)),\n-                                                    make_input((1)).abs()),\n+                        forward_input=FunctionInput(make_input(3),\n+                                                    make_target(3),\n+                                                    make_input(1).abs()),\n                         desc=desc,\n                         reference_fn=no_batch_dim_reference_fn)\n         )\n@@ -454,7 +454,7 @@ def generate_regression_criterion_inputs(make_input):\n             constructor_input=FunctionInput(reduction=reduction),\n             forward_input=FunctionInput(make_input((4, )), make_input(4,)),\n             reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True),\n-            desc='no_batch_dim_{}'.format(reduction)\n+            desc=f'no_batch_dim_{reduction}'\n         ) for reduction in ['none', 'mean', 'sum']]\n \n \n@@ -752,7 +752,7 @@ def module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, tra\n     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n     conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n     kernel_size, C_in, C_out = 3, 4, 5\n-    input_no_batch_shape = (C_in,) + tuple((i + 3 for i in range(N)))\n+    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n     input_batch_shape = (2,) + input_no_batch_shape\n     return [\n         ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n@@ -878,7 +878,7 @@ def module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad,\n         ModuleInput(constructor_input=FunctionInput(),\n                     forward_input=FunctionInput(make_input((3, 2, 5)))),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(0.5),\n@@ -897,10 +897,10 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n \n     return [\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(()))),\n+                    forward_input=FunctionInput(make_input(())),\n                     desc='scalar'),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -908,7 +908,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -916,7 +916,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -924,7 +924,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5, 6)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d_multiparam')]\n \n@@ -1216,11 +1216,11 @@ def module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3, 6, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 6, 5)))),\n+            forward_input=FunctionInput(make_input((4, 6, 5))),\n             desc='1d_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput(3, 12, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 12)))),\n+            forward_input=FunctionInput(make_input((4, 12))),\n             desc='1d_affine_GN'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 6, 1e-3),\n@@ -1334,13 +1334,13 @@ def module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_g\n             constructor_input=(\n                 FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape)))),\n+            forward_input=FunctionInput(make_input(input_batch_shape))),\n         ModuleInput(\n             constructor_input=(\n                 FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                 FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape))),\n+            forward_input=FunctionInput(make_input(input_batch_shape)),\n             desc='tracking_stats'),\n         ModuleInput(\n             constructor_input=(\n@@ -1365,15 +1365,15 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((128, 5, 5)))),\n+            forward_input=FunctionInput(make_input((128, 5, 5))),\n             desc='1d_elementwise_affine_large_batch'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3, False),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_no_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([2, 2, 5], 1e-3),\n@@ -1396,11 +1396,11 @@ def module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, require\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7))),\n             desc='1d'),\n         ModuleInput(\n             constructor_input=FunctionInput(2,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n             desc='2d_uneven_pad'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 1., 0.5, 2.),\n@@ -1415,7 +1415,7 @@ def module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, t\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(1.5, 2),\n-            forward_input=FunctionInput(make_input(((1, 3, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 7))),\n             desc='norm'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, 2, 3),\n@@ -1449,7 +1449,7 @@ def module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(4),\n-            forward_input=FunctionInput(make_input(((2, 10, 4)))),\n+            forward_input=FunctionInput(make_input((2, 10, 4))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput(4, 4),\n@@ -1468,7 +1468,7 @@ def module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n-            forward_input=FunctionInput(make_input(((3, 7, 7)))),\n+            forward_input=FunctionInput(make_input((3, 7, 7))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n@@ -1486,7 +1486,7 @@ def module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2)),\n-            forward_input=FunctionInput(make_input(((2, 3, 5, 5, 5))))),\n+            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n         ModuleInput(\n             constructor_input=FunctionInput(2, (2, 2, 2)),\n             forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n@@ -1511,7 +1511,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n@@ -1521,11 +1521,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((3, 5, 7))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\n@@ -1545,7 +1545,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n@@ -1559,11 +1559,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\ndiff --git a/torch/testing/_internal/common_nn.py b/torch/testing/_internal/common_nn.py\nindex 6bc41ab20f3fcd..85f9e35ac0a6bb 100644\n--- a/torch/testing/_internal/common_nn.py\n+++ b/torch/testing/_internal/common_nn.py\n@@ -2540,7 +2540,7 @@ def unsqueeze_inp(inp):\n         output_size = (2, 3) + tuple(p + 1 for p in padding)  # simplified from `(4 + 2 * p - 3) // 2 + 1`\n         new_module_tests.append(\n             dict(\n-                module_name='Conv{}d'.format(d),\n+                module_name=f'Conv{d}d',\n                 constructor_args=(2, 3, 3, 2, padding, 1, 1, True, padding_mode),\n                 cpp_constructor_args='''torch::nn::Conv{}dOptions(2, 3, 3)\n                                         .stride(2)\n@@ -2552,7 +2552,7 @@ def unsqueeze_inp(inp):\n                 input_size=input_size,\n                 output_size=output_size,\n                 cudnn=True,\n-                desc='{}_stride2_pad2'.format(padding_mode),\n+                desc=f'{padding_mode}_stride2_pad2',\n                 with_tf32=True,\n                 tf32_precision=0.05\n             ),\n@@ -3906,7 +3906,7 @@ def flatten(xs):\n reductions = ['none', 'mean', 'sum']\n for name, reduction in product(regression_criterion_no_batch, reductions):\n     regression_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_size=(3, ),\n         target_size=(3, ),\n@@ -3959,7 +3959,7 @@ def flatten(xs):\n for (name, input_fn, target_fn), reduction in product(classification_criterion_no_batch,\n                                                       reductions):\n     classification_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_fn=lambda f=input_fn: f(),\n         target_fn=lambda f=target_fn: f(),\n@@ -4152,7 +4152,7 @@ def _get_arg(self, name, unpack):\n                 self._arg_cache[name] = self._extra_kwargs[fn_name]()\n             else:\n                 assert size_name in self._extra_kwargs, \\\n-                    \"Missing `{}`, `{}` or `{}` for {}\".format(name, size_name, fn_name, self.get_name())\n+                    f\"Missing `{name}`, `{size_name}` or `{fn_name}` for {self.get_name()}\"\n \n                 def map_tensor_sizes(sizes):\n                     if isinstance(sizes, list):\n@@ -4281,7 +4281,7 @@ def test_cuda(self, test_case):\n         type_map = {torch.double: torch.float}\n         cpu_input_tuple = cpu_input if isinstance(cpu_input, tuple) else (cpu_input,)\n \n-        is_any_input_complex = any((isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple))\n+        is_any_input_complex = any(isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple)\n \n         gpu_input_tuple = to_gpu(cpu_input_tuple, type_map=type_map)\n \ndiff --git a/torch/testing/_internal/common_pruning.py b/torch/testing/_internal/common_pruning.py\nindex 32732818a25b02..b6cbd92105f3f4 100644\n--- a/torch/testing/_internal/common_pruning.py\n+++ b/torch/testing/_internal/common_pruning.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.ao.pruning import BaseSparsifier\ndiff --git a/torch/testing/_internal/common_quantization.py b/torch/testing/_internal/common_quantization.py\nindex 95686be4511264..dcc575c942bc84 100644\n--- a/torch/testing/_internal/common_quantization.py\n+++ b/torch/testing/_internal/common_quantization.py\n@@ -791,8 +791,7 @@ def _get_underlying_op_type(\n                     (exp_type_end_b is act_type_end_b)\n                 self.assertTrue(\n                     types_match,\n-                    'Type mismatch at %s: expected %s, got %s' %\n-                    (k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n+                    'Type mismatch at {}: expected {}, got {}'.format(k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n                         (act_type_start_a, act_type_end_a, act_type_start_b, act_type_end_b))\n                 )\n \n@@ -1601,7 +1600,7 @@ def __init__(self):\n         super().__init__()\n         self.quant = torch.ao.quantization.QuantStub()\n         self.fc1 = torch.nn.Linear(5, 8).to(dtype=torch.float)\n-        self.layer_norm = torch.nn.LayerNorm((8))\n+        self.layer_norm = torch.nn.LayerNorm(8)\n         self.group_norm = torch.nn.GroupNorm(2, 8)\n         self.instance_norm1d = torch.nn.InstanceNorm1d(8)\n         self.instance_norm2d = torch.nn.InstanceNorm2d(8)\ndiff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py\nindex 3ba2331e7cd40e..c96d13bd46ac53 100644\n--- a/torch/testing/_internal/common_utils.py\n+++ b/torch/testing/_internal/common_utils.py\n@@ -203,7 +203,7 @@ def repro_env_var_prefix() -> str:\n \n def maybe_load_json(filename):\n     if os.path.isfile(filename):\n-        with open(filename, 'r') as fp:\n+        with open(filename) as fp:\n             return json.load(fp)\n     log.warning(\"Attempted to load json file '%s' but it does not exist.\", filename)\n     return {}\n@@ -355,12 +355,12 @@ def instantiate_test_helper(cls, name, test, param_kwargs):\n             def instantiated_test(self, param_kwargs=param_kwargs):\n                 test(self, **param_kwargs)\n \n-            assert not hasattr(generic_cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(generic_cls, name), f\"Redefinition of test {name}\"\n             setattr(generic_cls, name, instantiated_test)\n \n         for (test, test_suffix, param_kwargs, decorator_fn) in class_attr.parametrize_fn(\n                 class_attr, generic_cls=generic_cls, device_cls=None):\n-            full_name = '{}_{}'.format(test.__name__, test_suffix)\n+            full_name = f'{test.__name__}_{test_suffix}'\n \n             # Apply decorators based on full param kwargs.\n             for decorator in decorator_fn(param_kwargs):\n@@ -830,7 +830,7 @@ def run_tests(argv=UNITTEST_ARGS):\n     # import test files.\n     if SLOW_TESTS_FILE:\n         if os.path.exists(SLOW_TESTS_FILE):\n-            with open(SLOW_TESTS_FILE, 'r') as fp:\n+            with open(SLOW_TESTS_FILE) as fp:\n                 global slow_tests_dict\n                 slow_tests_dict = json.load(fp)\n                 # use env vars so pytest-xdist subprocesses can still access them\n@@ -839,7 +839,7 @@ def run_tests(argv=UNITTEST_ARGS):\n             warnings.warn(f'slow test file provided but not found: {SLOW_TESTS_FILE}')\n     if DISABLED_TESTS_FILE:\n         if os.path.exists(DISABLED_TESTS_FILE):\n-            with open(DISABLED_TESTS_FILE, 'r') as fp:\n+            with open(DISABLED_TESTS_FILE) as fp:\n                 global disabled_tests_dict\n                 disabled_tests_dict = json.load(fp)\n                 os.environ['DISABLED_TESTS_FILE'] = DISABLED_TESTS_FILE\n@@ -905,7 +905,7 @@ def run_tests(argv=UNITTEST_ARGS):\n         test_batches = chunk_list(get_test_names(test_cases), RUN_PARALLEL)\n         processes = []\n         for i in range(RUN_PARALLEL):\n-            command = [sys.executable] + argv + ['--log-suffix=-shard-{}'.format(i + 1)] + test_batches[i]\n+            command = [sys.executable] + argv + [f'--log-suffix=-shard-{i + 1}'] + test_batches[i]\n             processes.append(subprocess.Popen(command, universal_newlines=True))\n         failed = False\n         for p in processes:\n@@ -1294,7 +1294,7 @@ def wrap_fn(self, *args, **kwargs):\n                 rocm_version = rocm_version.split(\"-\")[0]    # ignore git sha\n                 rocm_version_tuple = tuple(int(x) for x in rocm_version.split(\".\"))\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n         return wrap_fn\n@@ -1672,7 +1672,7 @@ def is_iterable_of_tensors(iterable, include_empty=False):\n     return True\n \n \n-class CudaNonDefaultStream():\n+class CudaNonDefaultStream:\n     def __enter__(self):\n         # Before starting CUDA test save currently active streams on all\n         # CUDA devices and set new non default streams to all CUDA devices\n@@ -1698,7 +1698,7 @@ def __exit__(self, exec_type, exec_value, traceback):\n                                      device_type=self.beforeStreams[d].device_type)\n         torch._C._cuda_setDevice(beforeDevice)\n \n-class CudaMemoryLeakCheck():\n+class CudaMemoryLeakCheck:\n     def __init__(self, testcase, name=None):\n         self.name = testcase.id() if name is None else name\n         self.testcase = testcase\n@@ -2104,7 +2104,7 @@ def __init__(self, actual, expected, *, rtol_override=0.0, atol_override=0.0, **\n     def _process_inputs(self, actual, expected, *, id, allow_subclasses):\n         self._check_inputs_isinstance(actual, expected, cls=(torch.Tensor, np.ndarray))\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\n@@ -2201,7 +2201,7 @@ def set_warn_always_context(new_val: bool):\n         torch.set_warn_always(old_val)\n \n \n-class NoTest():\n+class NoTest:\n     # causes pytest to not recognize this class as a test\n     __test__ = False\n \n@@ -3408,12 +3408,12 @@ def remove_prefix(text, prefix):\n         subname_output = \"\"\n         if subname:\n             expected_file += \"-\" + subname\n-            subname_output = \" ({})\".format(subname)\n+            subname_output = f\" ({subname})\"\n         expected_file += \".expect\"\n         expected = None\n \n         def accept_output(update_type):\n-            print(\"Accepting {} for {}{}:\\n\\n{}\".format(update_type, munged_id, subname_output, s))\n+            print(f\"Accepting {update_type} for {munged_id}{subname_output}:\\n\\n{s}\")\n             with open(expected_file, 'w') as f:\n                 # Adjust for producer_version, leave s unmodified\n                 s_tag = re.sub(r'(producer_version): \"[0-9.]*\"',\n@@ -3423,7 +3423,7 @@ def accept_output(update_type):\n         try:\n             with open(expected_file) as f:\n                 expected = f.read()\n-        except IOError as e:\n+        except OSError as e:\n             if e.errno != errno.ENOENT:\n                 raise\n             elif expecttest.ACCEPT:\n@@ -3442,7 +3442,7 @@ def accept_output(update_type):\n         # Adjust for producer_version\n         expected = expected.replace(\n             'producer_version: \"CURRENT_VERSION\"',\n-            'producer_version: \"{}\"'.format(torch.onnx.producer_version)\n+            f'producer_version: \"{torch.onnx.producer_version}\"'\n         )\n         if expecttest.ACCEPT:\n             if expected != s:\n@@ -3600,7 +3600,7 @@ def download_file(url, binary=True):\n             f.write(data)\n         return path\n     except error.URLError as e:\n-        msg = \"could not download test file '{}'\".format(url)\n+        msg = f\"could not download test file '{url}'\"\n         warnings.warn(msg, RuntimeWarning)\n         raise unittest.SkipTest(msg) from e\n \n@@ -4359,7 +4359,7 @@ def wrap_fn(*args, **kwargs):\n     return wrap_fn\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def get_cycles_per_ms() -> float:\n     \"\"\"Measure and return approximate number of cycles per millisecond for torch.cuda._sleep\n     \"\"\"\ndiff --git a/torch/testing/_internal/dist_utils.py b/torch/testing/_internal/dist_utils.py\nindex 94aafe31773261..daee88e4580cd5 100644\n--- a/torch/testing/_internal/dist_utils.py\n+++ b/torch/testing/_internal/dist_utils.py\n@@ -101,7 +101,7 @@ def wait_until_node_failure(rank: int, expected_error_regex: str = \".*\") -> str:\n     \"\"\"\n     while True:\n         try:\n-            rpc.rpc_sync(\"worker{}\".format(rank), noop, args=())\n+            rpc.rpc_sync(f\"worker{rank}\", noop, args=())\n             time.sleep(0.1)\n         except Exception as e:\n             if re.search(pattern=expected_error_regex, string=str(e)):\n@@ -187,7 +187,7 @@ def initialize_pg(init_method, rank: int, world_size: int) -> None:\n \n \n def worker_name(rank: int) -> str:\n-    return \"worker{}\".format(rank)\n+    return f\"worker{rank}\"\n \n \n def get_function_event(function_events, partial_event_name):\ndiff --git a/torch/testing/_internal/distributed/distributed_test.py b/torch/testing/_internal/distributed/distributed_test.py\nindex 19a0c3f50d1ae4..5fdc796310d44d 100644\n--- a/torch/testing/_internal/distributed/distributed_test.py\n+++ b/torch/testing/_internal/distributed/distributed_test.py\n@@ -517,7 +517,7 @@ def sync(cls, wait_for=None, timeout=10):\n             arrived = 0\n             with _lock():\n                 for f_name in os.listdir(barrier_dir):\n-                    with open(os.path.join(barrier_dir, f_name), \"r\") as f:\n+                    with open(os.path.join(barrier_dir, f_name)) as f:\n                         data = f.read()\n                         if int(data) >= cls.barrier_id:\n                             arrived += 1\n@@ -552,7 +552,7 @@ def tearDown(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     @classmethod\n     def _run(cls, rank, test_name, file_name, pipe):\n@@ -654,7 +654,7 @@ def test_dump_DDP_relevant_env_vars(self):\n                 lines = out.getvalue().splitlines()\n \n             def format_line(var):\n-                return \"env:%s=%s\" % (\n+                return \"env:{}={}\".format(\n                     var,\n                     os.environ[var] if var in os.environ else \"N/A\",\n                 )\n@@ -692,7 +692,7 @@ def test_get_rank(self):\n \n             all_ranks = set()\n             for f_name in os.listdir(test_dir):\n-                with open(os.path.join(test_dir, f_name), \"r\") as f:\n+                with open(os.path.join(test_dir, f_name)) as f:\n                     all_ranks.add(int(f.read()))\n             self.assertEqual(len(all_ranks), num_processes)\n \n@@ -9640,7 +9640,7 @@ def backward(ctx, grad_output):\n \n             class MyModel(nn.Module):\n                 def __init__(self, device):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.error = True\n                     self.fc1 = nn.Linear(10, 10).cuda(device)\n \n@@ -9683,12 +9683,12 @@ def forward(self, inp):\n         def test_ddp_has_finalized(self):\n \n             @dataclass\n-            class MyClass():\n+            class MyClass:\n                 obj: torch.Tensor\n \n             class MyModel(nn.Module):\n                 def __init__(self, rank):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.rank = rank\n                     self.fc1 = nn.Linear(1024, 1024).cuda(rank)\n                     self.fc2 = nn.Linear(1024, 2 * 1024).cuda(rank)\ndiff --git a/torch/testing/_internal/distributed/nn/api/remote_module_test.py b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\nindex f955c0fc1ace1a..4d9f1d9b53ddc4 100644\n--- a/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n+++ b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n@@ -131,7 +131,7 @@ def test_bad_module(self):\n         if self.rank != 0:\n             return\n         dst_worker_name = dist_utils.worker_name((self.rank + 1) % self.world_size)\n-        remote_device = \"{}/cpu\".format(dst_worker_name)\n+        remote_device = f\"{dst_worker_name}/cpu\"\n         args = (1,)\n         kwargs = dict(first_kwarg=2)\n \n@@ -575,7 +575,7 @@ def test_valid_device(self):\n         dst_worker_name = dist_utils.worker_name(dst_rank)\n \n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -585,7 +585,7 @@ def test_valid_device(self):\n \n         # Test rank works as well.\n         for remote_module in self._create_remote_module_iter(\n-            \"rank:{}/cuda:0\".format(dst_rank), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"rank:{dst_rank}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -607,7 +607,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/foo\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/foo\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -618,7 +618,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:100\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:100\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -627,7 +627,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cpu2\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cpu2\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -636,7 +636,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -648,7 +648,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:0/cuda:1\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:0/cuda:1\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -692,7 +692,7 @@ def test_input_moved_to_cuda_device(self):\n \n         # Only test Python nn.Module, because script module methods don't support taking kwargs.\n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             ret_fut = remote_module.forward_async(*args, **kwargs)\n             ret = ret_fut.wait()\n@@ -716,7 +716,7 @@ def test_input_moved_to_cuda_device_script(self):\n \n         scripted_remote_module = next(\n             self._create_remote_module_iter(\n-                \"{}/cuda:0\".format(dst_worker_name),\n+                f\"{dst_worker_name}/cuda:0\",\n                 modes=[ModuleCreationMode.MODULE_CTOR_WITH_INTERFACE],\n             )\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\nindex 8e8c353460e6ff..b08b51c31d9f7d 100644\n--- a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n+++ b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n@@ -226,7 +226,7 @@ def _exec_func_with_dst(self, dst, exec_mode, method, *args):\n             fut = rpc.rpc_async(worker_name(dst), method, args=(args))\n             return fut.wait()\n         else:\n-            raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+            raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n     def _exec_func(self, exec_mode, method, *args):\n         return self._exec_func_with_dst(\n@@ -288,7 +288,7 @@ def _test_graph(self, fn, exec_mode, sparse):\n                     worker_name(dst_rank), fn, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -355,7 +355,7 @@ def _test_graph_for_py_nested_call(self, exec_mode, sparse):\n                     args=(t1, t2, dst_rank, self.world_size, 1),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             # Barrier to ensure all RPCs are done.\n             dist.barrier()\n@@ -449,7 +449,7 @@ def _test_graph_for_py_nested_call_itself(self, exec_mode, sparse):\n                     ),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name((self.rank + 1) % self.world_size),\n@@ -505,7 +505,7 @@ def _test_no_graph_with_tensors_not_require_grad(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.add, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -548,7 +548,7 @@ def _test_rpc_complex_args(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.stack, args=(tensors,)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             self.assertEqual(torch.stack(tensors), ret)\n \n@@ -1292,7 +1292,7 @@ def test_autograd_context(self):\n         for context_id in context_ids:\n             with self.assertRaisesRegex(\n                 RuntimeError,\n-                \"Could not find autograd context with id: {}\".format(context_id),\n+                f\"Could not find autograd context with id: {context_id}\",\n             ):\n                 dist_autograd._retrieve_context(context_id)\n \n@@ -1357,7 +1357,7 @@ def _test_grad_only_on_return_value(self, exec_mode):\n                     worker_name(dst_rank), ret_requires_grad\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             dist_autograd.backward(context_id, [ret.sum()])\n \n@@ -1748,7 +1748,7 @@ def test_backward_without_context(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             res = rpc.rpc_sync(\n                 worker_name(self._next_rank()), torch.add, args=(t1, t2)\n@@ -2031,7 +2031,7 @@ def test_clean_context_during_backward(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             dist_autograd.backward(context_id, [t1.sum()])\n \n@@ -2234,7 +2234,7 @@ def test_multiple_backward_with_errors(self):\n         t2 = torch.rand((3, 3), requires_grad=True)\n         with dist_autograd.context() as context_id:\n             loss = rpc.rpc_sync(\n-                'worker{}'.format(self._next_rank()),\n+                f'worker{self._next_rank()}',\n                 DistAutogradTest._python_udf_with_backward_error,\n                 args=(t1, t2)).sum()\n \ndiff --git a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\nindex 9f8b71911a07cc..98db73d7401845 100644\n--- a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n@@ -225,7 +225,7 @@ def run_agent(agent, n_steps):\n         last_reward = agent.finish_episode()\n \n         if agent.running_reward > agent.reward_threshold:\n-            print(\"Solved! Running reward is now {}!\".format(agent.running_reward))\n+            print(f\"Solved! Running reward is now {agent.running_reward}!\")\n             break\n \n \ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\nindex d050a2138b7922..b7683064dcfd13 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n@@ -70,7 +70,7 @@ def _test_remote_message_dropped_pickle(self, dst=None):\n         if self.rank != 0:\n             return\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -100,7 +100,7 @@ def _test_remote_message_dropped_timeout(self, func, args, dst=None):\n \n         # test the case where rpc.remote() message creation is completely dropped.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -143,7 +143,7 @@ def _test_remote_message_delay_timeout(self, func, args, dst=None):\n         # Test the case where remote message is eventually processed on the owner,\n         # but the future on the creator times out before the response comes back.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, func, args=args, timeout=0.001)\n         # Future corresponding to the remote creation should time out.\n@@ -233,7 +233,7 @@ def test_rref_to_here_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\nindex b08e897ec464af..af73fef4794b06 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n@@ -54,7 +54,7 @@ def get_shutdown_error_regex(self):\n             \"Connection reset by peer\",\n             \"Connection closed by peer\"\n         ]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\nindex fce0b5e8802567..0bb45ddeadb186 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n@@ -310,7 +310,7 @@ def future_return_to_python(\n             dst_rank: int, inputs: Tuple[Tensor, Tensor]\n         ) -> Future[Tensor]:\n             return rpc.rpc_async(\n-                \"worker{}\".format(dst_rank), two_args_two_kwargs, inputs\n+                f\"worker{dst_rank}\", two_args_two_kwargs, inputs\n             )\n \n         fut_res = future_return_to_python(dst_rank, inputs)\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\nindex 96ede7231a9722..2e4eea3a365176 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n@@ -157,7 +157,7 @@ def test_remote_timeout_to_here_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -173,7 +173,7 @@ def test_rref_to_here_timeout_in_jit(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -188,7 +188,7 @@ def test_rref_timeout_pickle_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -205,7 +205,7 @@ def test_rref_timeout_pickle_script_func(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/rpc_test.py b/torch/testing/_internal/distributed/rpc/rpc_test.py\nindex 2d350d06cc6794..47b13a837a0355 100644\n--- a/torch/testing/_internal/distributed/rpc/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/rpc_test.py\n@@ -3153,7 +3153,7 @@ def test_rref_str(self):\n         rref1 = RRef(self.rank)\n         id_class = \"GloballyUniqueId\"\n         self.assertEqual(\n-            \"OwnerRRef({}(created_on={}, local_id=0))\".format(id_class, self.rank), rref1.__str__()\n+            f\"OwnerRRef({id_class}(created_on={self.rank}, local_id=0))\", rref1.__str__()\n         )\n \n         dst_rank = (self.rank + 1) % self.world_size\n@@ -4296,7 +4296,7 @@ def test_rref_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, my_sleep_func, args=(2, ), timeout=0.01)\n         # Future corresponding to the remote creation should time out.\ndiff --git a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\nindex 0f5cb0a4987a8f..191017caad139e 100644\n--- a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n@@ -26,7 +26,7 @@ def get_shutdown_error_regex(self):\n         # FIXME Once we consolidate the error messages returned by the\n         # TensorPipe agent put some more specific regex here.\n         error_regexes = [\".*\"]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/jit_metaprogramming_utils.py b/torch/testing/_internal/jit_metaprogramming_utils.py\nindex 72b7e477c76a49..88137fd1029a18 100644\n--- a/torch/testing/_internal/jit_metaprogramming_utils.py\n+++ b/torch/testing/_internal/jit_metaprogramming_utils.py\n@@ -338,11 +338,11 @@ def get_call(method_name, func_type, args, kwargs):\n     argument_str += kwargs_str\n \n     if func_type == 'functional' or func_type == 'function':\n-        call = 'torch.{}({})'.format(method_name, argument_str)\n+        call = f'torch.{method_name}({argument_str})'\n     elif func_type == 'method':\n-        call = '{}.{}({})'.format(self_arg, method_name, argument_str)\n+        call = f'{self_arg}.{method_name}({argument_str})'\n     elif func_type == 'nn_functional':\n-        call = 'torch.nn.functional.{}({})'.format(method_name, argument_str)\n+        call = f'torch.nn.functional.{method_name}({argument_str})'\n     else:\n         raise TypeError('Unsupported function type')\n \n@@ -361,17 +361,17 @@ def get_script_args(args):\n     actuals: List[str] = []\n     for arg in args:\n         if isinstance(arg, torch.Tensor):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name)\n             actuals.append(name)\n             tensors.append(arg)\n         elif is_iterable_of_tensors(arg):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name + ': List[torch.Tensor]')\n             actuals.append(name)\n             tensors.append(list(arg))\n         elif isinstance(arg, str):\n-            actuals.append(\"'{}'\".format(arg))\n+            actuals.append(f\"'{arg}'\")\n         else:\n             actuals.append(str(get_constant(arg)))\n     return (formals, tensors, actuals)\n@@ -399,7 +399,7 @@ def script_fn(*args, **kwargs):\n         return output\n     return script_fn\n \n-class SplitInputs():\n+class SplitInputs:\n     all_tensors: List[Any]\n     tensor_args: List[Any]\n     nontensor_args: List[Any]\n@@ -584,7 +584,7 @@ def script_module(*args, **kwargs):\n \n         method_args = ', '.join(['self'] + actuals)\n         call_args_str = ', '.join(actuals)\n-        call = \"self.submodule({})\".format(call_args_str)\n+        call = f\"self.submodule({call_args_str})\"\n         script = script_method_template.format(method_args, call)\n \n         submodule_constants = []\n@@ -640,7 +640,7 @@ def get_nn_mod_test_name(**kwargs):\n         test_name = get_nn_module_name_from_kwargs(**kwargs)\n         if 'desc' in kwargs:\n             test_name = \"{}_{}\".format(test_name, kwargs['desc'])\n-    return 'test_nn_{}'.format(test_name)\n+    return f'test_nn_{test_name}'\n \n def get_nn_module_class_from_kwargs(**kwargs):\n     name = get_nn_module_name_from_kwargs(**kwargs)\ndiff --git a/torch/testing/_internal/jit_utils.py b/torch/testing/_internal/jit_utils.py\nindex b72dd5dc1285a6..2f6675234d3e7c 100644\n--- a/torch/testing/_internal/jit_utils.py\n+++ b/torch/testing/_internal/jit_utils.py\n@@ -176,12 +176,12 @@ def get_nodes_and_parents_recursively(block, kind, acc):\n \n         fusion_groups : Dict[torch._C.Block, List[torch._C.Node]] = defaultdict(list)\n         get_nodes_and_parents_recursively(graph, FUSION_GROUP, fusion_groups)\n-        self.assertTrue(len(fusion_groups) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_groups) == 1, f'got {graph}')\n         (graph, fusion_nodes) = list(fusion_groups.items())[0]\n         # the block contains one FUSION_GROUP and the rest of nodes are `allowed_nodes`\n-        self.assertTrue(len(fusion_nodes) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_nodes) == 1, f'got {graph}')\n         self.assertTrue(all(node.kind() in allowed_nodes for node in graph.nodes()),\n-                        'got {}'.format(graph))\n+                        f'got {graph}')\n \n     def _isHookExceptionOk(self, e):\n         se = str(e)\n@@ -294,7 +294,7 @@ def assertGraphContains(self, graph, kind, consider_subgraphs=False):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             self.assertTrue(count > 0)\n             return\n \n@@ -321,7 +321,7 @@ def perform_assert(graph, kind, actual, expected, consider_subgraphs):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             perform_assert(graph, kind, count, num_kind_nodes,\n                            consider_subgraphs)\n             return\n@@ -768,7 +768,7 @@ def _get_py3_code(code, fn_name):\n         fn = getattr(module, fn_name)\n         return fn\n \n-class TensorExprTestOptions():\n+class TensorExprTestOptions:\n     def __init__(self):\n         self.old_profiling_executor = torch._C._jit_set_profiling_executor(True)\n         self.old_profiling_mode = torch._C._get_graph_executor_optimize(True)\ndiff --git a/torch/testing/_internal/opinfo/core.py b/torch/testing/_internal/opinfo/core.py\nindex 8f86cbd06af61a..5e7e1dcd5a49ea 100644\n--- a/torch/testing/_internal/opinfo/core.py\n+++ b/torch/testing/_internal/opinfo/core.py\n@@ -859,9 +859,7 @@ class OpInfo:\n     def __post_init__(self):\n         self._original_opinfo_args = asdict(self).copy()\n \n-        assert self.dtypes is not None, \"OpInfo for {0} has no dtypes!\".format(\n-            self.name\n-        )\n+        assert self.dtypes is not None, \"OpInfo for {} has no dtypes!\".format(self.name)\n \n         dtypes_args = (self.dtypes, self.dtypesIfCUDA, self.dtypesIfROCM)\n \n@@ -874,10 +872,7 @@ def __post_init__(self):\n \n         # Attribute to verify dynamic_dtypes are used.\n         self.dynamic_dtypes = any(\n-            (\n-                isinstance(dtypes, utils._dynamic_dispatch_dtypes)\n-                for dtypes in dtypes_args\n-            )\n+            isinstance(dtypes, utils._dynamic_dispatch_dtypes) for dtypes in dtypes_args\n         )\n \n         if self.dynamic_dtypes:\ndiff --git a/torch/testing/_internal/opinfo/definitions/linalg.py b/torch/testing/_internal/opinfo/definitions/linalg.py\nindex ca84eca5d3d027..a8c29dbf09309d 100644\n--- a/torch/testing/_internal/opinfo/definitions/linalg.py\n+++ b/torch/testing/_internal/opinfo/definitions/linalg.py\n@@ -1007,7 +1007,7 @@ def sample_inputs_linalg_solve(\n         nrhs = [(1,), (3,)]\n \n     for n, batch, rhs in product(ns, batches, nrhs):\n-        yield SampleInput(make_a(*batch, n, n), args=(make_b((batch + (n,) + rhs)),))\n+        yield SampleInput(make_a(*batch, n, n), args=(make_b(batch + (n,) + rhs),))\n \n \n def sample_inputs_linalg_solve_triangular(\ndiff --git a/torch/testing/_internal/opinfo/definitions/sparse.py b/torch/testing/_internal/opinfo/definitions/sparse.py\nindex 6baff3b2f86fea..570b2c546f099a 100644\n--- a/torch/testing/_internal/opinfo/definitions/sparse.py\n+++ b/torch/testing/_internal/opinfo/definitions/sparse.py\n@@ -331,7 +331,7 @@ def _validate_sample_input_sparse_reduction_sum(sample, check_validate=False):\n     }:\n         if (isinstance(dim, int) and (t_inp.dim() != 2 or keepdim)) or (\n             isinstance(dim, (list, tuple))\n-            and (((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim))\n+            and ((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim)\n         ):\n             if layout in {torch.sparse_bsr, torch.sparse_bsc}:\n                 return ErrorInput(\n"
  },
  {
    "number": 105414,
    "title": "[BE] Enable ruff's UP rules and autoformat utils/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "8a2aa1b42141affac2f9d384b0622c720effffe0",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105414",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105414/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105414.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105414.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105414/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105414/comments",
    "labels": [
      "release notes: dataloader"
    ],
    "_event_time": "2023-07-18T01:17:11.052428Z",
    "state": "closed",
    "patch": "From 1e700d0b15b5a2361fd7f5a7758a3e3335605187 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:04 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat utils/\n\n[ghstack-poisoned]\n---\n torch/utils/_freeze.py                        |  2 +-\n torch/utils/benchmark/examples/end_to_end.py  |  5 ++--\n .../utils/benchmark/examples/op_benchmark.py  |  4 +--\n .../benchmark/examples/sparse/op_benchmark.py |  4 +--\n .../examples/spectral_ops_fuzz_test.py        |  2 +-\n torch/utils/benchmark/utils/common.py         |  4 +--\n torch/utils/benchmark/utils/cpp_jit.py        |  6 ++--\n .../utils/valgrind_wrapper/timer_interface.py | 12 ++++----\n torch/utils/bottleneck/__main__.py            |  6 ++--\n torch/utils/bundled_inputs.py                 |  8 +++---\n torch/utils/checkpoint.py                     |  2 +-\n torch/utils/collect_env.py                    | 26 ++++++++---------\n torch/utils/cpp_extension.py                  |  8 +++---\n torch/utils/data/_utils/pin_memory.py         |  2 +-\n torch/utils/data/_utils/worker.py             |  8 +++---\n torch/utils/data/dataloader.py                |  6 ++--\n torch/utils/data/datapipes/_decorator.py      |  2 +-\n torch/utils/data/datapipes/_typing.py         |  8 +++---\n .../data/datapipes/dataframe/dataframes.py    | 18 ++++++------\n torch/utils/data/datapipes/datapipe.py        | 10 +++----\n torch/utils/data/datapipes/gen_pyi.py         |  2 +-\n torch/utils/data/datapipes/iter/callable.py   |  2 +-\n .../data/datapipes/iter/combinatorics.py      |  4 +--\n torch/utils/data/datapipes/iter/combining.py  |  6 ++--\n torch/utils/data/datapipes/iter/filelister.py |  2 +-\n torch/utils/data/datapipes/iter/fileopener.py |  4 +--\n torch/utils/data/datapipes/iter/grouping.py   |  2 +-\n .../data/datapipes/iter/routeddecoder.py      |  2 +-\n torch/utils/data/datapipes/iter/sharding.py   |  2 +-\n torch/utils/data/datapipes/map/combining.py   |  2 +-\n torch/utils/data/datapipes/map/grouping.py    |  2 +-\n torch/utils/data/datapipes/utils/common.py    |  2 +-\n torch/utils/data/datapipes/utils/decoder.py   |  6 ++--\n torch/utils/data/graph.py                     |  2 +-\n torch/utils/dlpack.py                         |  2 +-\n torch/utils/hipify/cuda_to_hip_mappings.py    |  2 +-\n torch/utils/hipify/hipify_python.py           | 28 +++++++++----------\n torch/utils/jit/log_extract.py                |  2 +-\n torch/utils/mobile_optimizer.py               |  4 +--\n torch/utils/tensorboard/_caffe2_graph.py      |  4 +--\n torch/utils/tensorboard/_embedding.py         |  2 +-\n torch/utils/tensorboard/_pytorch_graph.py     |  6 ++--\n torch/utils/tensorboard/writer.py             |  2 +-\n torch/utils/throughput_benchmark.py           | 10 +++----\n torch/utils/viz/_cycles.py                    | 12 ++++----\n torch/utils/weak.py                           |  5 ++--\n 46 files changed, 131 insertions(+), 131 deletions(-)\n\ndiff --git a/torch/utils/_freeze.py b/torch/utils/_freeze.py\nindex 5245ac011e19ac..6590ff4b769e42 100644\n--- a/torch/utils/_freeze.py\n+++ b/torch/utils/_freeze.py\n@@ -237,7 +237,7 @@ def compile_file(self, path: Path, top_package_path: Path):\n         module_mangled_name = \"__\".join(module_qualname)\n         c_name = \"M_\" + module_mangled_name\n \n-        with open(path, \"r\") as src_file:\n+        with open(path) as src_file:\n             co = self.compile_string(src_file.read())\n \n         bytecode = marshal.dumps(co)\ndiff --git a/torch/utils/benchmark/examples/end_to_end.py b/torch/utils/benchmark/examples/end_to_end.py\nindex 5e0f42712d7c7a..a6d05a91c94253 100644\n--- a/torch/utils/benchmark/examples/end_to_end.py\n+++ b/torch/utils/benchmark/examples/end_to_end.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"End-to-end example to test a PR for regressions:\n \n $ python -m examples.end_to_end --pr 39850\n@@ -111,7 +110,7 @@ def parse_args():\n \n def construct_stmt_and_label(pr, params):\n     if pr == \"39850\":\n-        k0, k1, k2, dim = [params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"]]\n+        k0, k1, k2, dim = (params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"])\n         state = np.random.RandomState(params[\"random_value\"])\n         topk_dim = state.randint(low=0, high=dim)\n         dim_size = [k0, k1, k2][topk_dim]\n@@ -291,7 +290,7 @@ def construct_table(results, device_str, test_variance):\n     )\n \n     _, result_log_file = tempfile.mkstemp(suffix=\".log\")\n-    with open(result_log_file, \"wt\") as f:\n+    with open(result_log_file, \"w\") as f:\n         f.write(f\"{device_str}\\n\\n{column_labels}\\n\")\n         print(f\"\\n{column_labels}\\n[First twenty omitted (these tend to be noisy) ]\")\n         for key, (r_ref, r_pr), rel_diff in results:\ndiff --git a/torch/utils/benchmark/examples/op_benchmark.py b/torch/utils/benchmark/examples/op_benchmark.py\nindex 65b69d84b41f44..b7536b9ec26bb8 100644\n--- a/torch/utils/benchmark/examples/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/op_benchmark.py\n@@ -37,13 +37,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/sparse/op_benchmark.py b/torch/utils/benchmark/examples/sparse/op_benchmark.py\nindex f9ee17d5617e08..d7e97d33cc1101 100644\n--- a/torch/utils/benchmark/examples/sparse/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/sparse/op_benchmark.py\n@@ -32,13 +32,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\nindex d8284ee4187c49..c70395573adb2c 100644\n--- a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n+++ b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n@@ -27,7 +27,7 @@ def run_benchmark(name: str, function: object, dtype: torch.dtype, seed: int, de\n     results = []\n     for tensors, tensor_params, params in spectral_fuzzer.take(samples):\n         shape = [params['k0'], params['k1'], params['k2']][:params['ndim']]\n-        str_shape = ' x '.join([\"{:<4}\".format(s) for s in shape])\n+        str_shape = ' x '.join([f\"{s:<4}\" for s in shape])\n         sub_label = f\"{str_shape} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n         for dim in _dim_options(params['ndim']):\n             for nthreads in (1, 4, 16) if not cuda else (1,):\ndiff --git a/torch/utils/benchmark/utils/common.py b/torch/utils/benchmark/utils/common.py\nindex a8bbef3bfbeb4f..c1636ddb78a2bf 100644\n--- a/torch/utils/benchmark/utils/common.py\n+++ b/torch/utils/benchmark/utils/common.py\n@@ -325,7 +325,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n                 if not os.path.exists(owner_file):\n                     continue\n \n-                with open(owner_file, \"rt\") as f:\n+                with open(owner_file) as f:\n                     owner_pid = int(f.read())\n \n                 if owner_pid == os.getpid():\n@@ -349,7 +349,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n     os.makedirs(path, exist_ok=False)\n \n     if use_dev_shm:\n-        with open(os.path.join(path, \"owner.pid\"), \"wt\") as f:\n+        with open(os.path.join(path, \"owner.pid\"), \"w\") as f:\n             f.write(str(os.getpid()))\n \n     return path\ndiff --git a/torch/utils/benchmark/utils/cpp_jit.py b/torch/utils/benchmark/utils/cpp_jit.py\nindex 65b8c70ee43e6c..a09f1a00aace6f 100644\n--- a/torch/utils/benchmark/utils/cpp_jit.py\n+++ b/torch/utils/benchmark/utils/cpp_jit.py\n@@ -137,7 +137,7 @@ def _compile_template(\n         os.makedirs(build_dir, exist_ok=True)\n \n         src_path = os.path.join(build_dir, \"timer_src.cpp\")\n-        with open(src_path, \"wt\") as f:\n+        with open(src_path, \"w\") as f:\n             f.write(src)\n \n     # `cpp_extension` has its own locking scheme, so we don't need our lock.\n@@ -154,7 +154,7 @@ def _compile_template(\n \n def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> TimeitModuleType:\n     template_path: str = os.path.join(SOURCE_ROOT, \"timeit_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     module = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)\n@@ -164,7 +164,7 @@ def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> Time\n \n def compile_callgrind_template(*, stmt: str, setup: str, global_setup: str) -> str:\n     template_path: str = os.path.join(SOURCE_ROOT, \"valgrind_wrapper\", \"timer_callgrind_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     target = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 71753bd59548ae..11ce6d90fc47f3 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -28,7 +28,9 @@\n     CompletedProcessType = subprocess.CompletedProcess\n \n \n-FunctionCount = NamedTuple(\"FunctionCount\", [(\"count\", int), (\"function\", str)])\n+class FunctionCount(NamedTuple):\n+    count: int\n+    function: str\n \n \n @dataclasses.dataclass(repr=False, eq=False, frozen=True)\n@@ -598,7 +600,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     stderr=subprocess.STDOUT,\n                     **kwargs,\n                 )\n-                with open(stdout_stderr_log, \"rt\") as f:\n+                with open(stdout_stderr_log) as f:\n                     return invocation, f.read()\n             finally:\n                 f_stdout_stderr.close()\n@@ -612,7 +614,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     )\n \n                 script_file = os.path.join(working_dir, \"timer_callgrind.py\")\n-                with open(script_file, \"wt\") as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(self._construct_script(\n                         task_spec,\n                         globals=GlobalsBridge(globals, data_dir),\n@@ -652,7 +654,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n             if valgrind_invocation.returncode:\n                 error_report = \"\"\n                 if os.path.exists(error_log):\n-                    with open(error_log, \"rt\") as f:\n+                    with open(error_log) as f:\n                         error_report = f.read()\n                 if not error_report:\n                     error_report = \"Unknown error.\\n\" + valgrind_invocation_output\n@@ -724,7 +726,7 @@ def read_results(i: int) -> Tuple[FunctionCounts, FunctionCounts, Optional[str]]\n                 fpath = f\"{callgrind_out}.{i + 1}\"  # Callgrind one-indexes files.\n                 callgrind_out_contents: Optional[str] = None\n                 if retain_out_file:\n-                    with open(fpath, \"rt\") as f:\n+                    with open(fpath) as f:\n                         callgrind_out_contents = f.read()\n \n                 return (\ndiff --git a/torch/utils/bottleneck/__main__.py b/torch/utils/bottleneck/__main__.py\nindex 86c1af04baa0e6..f7fd209e1438fa 100644\n--- a/torch/utils/bottleneck/__main__.py\n+++ b/torch/utils/bottleneck/__main__.py\n@@ -16,7 +16,7 @@ def redirect_argv(new_argv):\n \n def compiled_with_cuda(sysinfo):\n     if sysinfo.cuda_compiled_version:\n-        return 'compiled w/ CUDA {}'.format(sysinfo.cuda_compiled_version)\n+        return f'compiled w/ CUDA {sysinfo.cuda_compiled_version}'\n     return 'not compiled w/ CUDA'\n \n \n@@ -59,7 +59,7 @@ def run_env_analysis():\n         'debug_str': debug_str,\n         'pytorch_version': info.torch_version,\n         'cuda_compiled': compiled_with_cuda(info),\n-        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'py_version': f'{sys.version_info[0]}.{sys.version_info[1]}',\n         'cuda_runtime': cuda_avail,\n         'pip_version': pip_version,\n         'pip_list_output': pip_list_output,\n@@ -138,7 +138,7 @@ def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\n \n     result = {\n         'mode': mode,\n-        'description': 'top {} events sorted by {}'.format(topk, sortby),\n+        'description': f'top {topk} events sorted by {sortby}',\n         'output': torch.autograd.profiler_util._build_table(topk_events),\n         'cuda_warning': cuda_warning\n     }\ndiff --git a/torch/utils/bundled_inputs.py b/torch/utils/bundled_inputs.py\nindex 4ae39733ff2e4b..ad34e15e6bfa17 100644\n--- a/torch/utils/bundled_inputs.py\n+++ b/torch/utils/bundled_inputs.py\n@@ -261,11 +261,11 @@ def augment_many_model_functions_with_bundled_inputs(\n \n \n         if input_list is not None and not isinstance(input_list, Sequence):\n-            raise TypeError(\"Error inputs for function {0} is not a Sequence\".format(function_name))\n+            raise TypeError(f\"Error inputs for function {function_name} is not a Sequence\")\n \n         function_arg_types = [arg.type for arg in function.schema.arguments[1:]]  # type: ignore[attr-defined]\n         deflated_inputs_type: ListType = ListType(TupleType(function_arg_types))\n-        model._c._register_attribute(\"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs_type, [])\n+        model._c._register_attribute(f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs_type, [])\n \n         if hasattr(model, \"_generate_bundled_inputs_for_\" + function_name):\n             if input_list is not None:\n@@ -290,7 +290,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             for inp_idx, args in enumerate(input_list):\n                 if not isinstance(args, Tuple) and not isinstance(args, List):  # type: ignore[arg-type]\n                     raise TypeError(\n-                        \"Error bundled input for function {0} idx: {1} is not a Tuple or a List\".format(function_name, inp_idx)\n+                        f\"Error bundled input for function {function_name} idx: {inp_idx} is not a Tuple or a List\"\n                     )\n                 deflated_args = []\n                 parts.append(\"(\")\n@@ -314,7 +314,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             # Back-channel return this expr for debugging.\n             if _receive_inflate_expr is not None:\n                 _receive_inflate_expr.append(expr)\n-            setattr(model, \"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs)\n+            setattr(model, f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs)\n             definition = textwrap.dedent(\"\"\"\n                 def _generate_bundled_inputs_for_{name}(self):\n                     deflated = self._bundled_inputs_deflated_{name}\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..4da281d32ac3bd 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -66,7 +66,7 @@ def _get_device_module(device=\"cuda\"):\n     return device_module\n \n \n-class DefaultDeviceType(object):\n+class DefaultDeviceType:\n     r\"\"\"\n     A class that manages the default device type for checkpointing.\n     If no non-CPU tensors are present, the default device type will\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex de03564a2b75d1..2266d64c1944d5 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n+    out = run_and_read_all(run_lambda, f\"{conda} list\")\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n+        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return 'Probably one of the following:\\n{}'.format(result)\n+    return f'Probably one of the following:\\n{result}'\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = '\"{}\"'.format(candidate_smi)\n+                smi = f'\"{candidate_smi}\"'\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n+    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return 'macOS {} ({})'.format(version, machine())\n+        return f'macOS {version} ({machine()})'\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n-        return '{} ({})'.format(platform, machine())\n+        return f'{platform} ({machine()})'\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n+        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return '\\n{}\\n'.format(string)\n+            return f'\\n{string}\\n'\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               '[{}] '.format(envinfo.pip_version))\n+                                               f'[{envinfo.pip_version}] ')\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n+            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..e847f4e30915b9 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -150,10 +150,10 @@ def _join_rocm_home(*paths) -> str:\n     only once we need to get any ROCm-specific path.\n     '''\n     if ROCM_HOME is None:\n-        raise EnvironmentError('ROCM_HOME environment variable is not set. '\n+        raise OSError('ROCM_HOME environment variable is not set. '\n                                'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n-        raise EnvironmentError('Building PyTorch extensions using '\n+        raise OSError('Building PyTorch extensions using '\n                                'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n@@ -264,7 +264,7 @@ def _maybe_write(filename, new_content):\n     if it already had the right content (to avoid triggering recompile).\n     '''\n     if os.path.exists(filename):\n-        with open(filename, 'r') as f:\n+        with open(filename) as f:\n             content = f.read()\n \n         if content == new_content:\n@@ -2247,7 +2247,7 @@ def _join_cuda_home(*paths) -> str:\n     only once we need to get any CUDA-specific path.\n     '''\n     if CUDA_HOME is None:\n-        raise EnvironmentError('CUDA_HOME environment variable is not set. '\n+        raise OSError('CUDA_HOME environment variable is not set. '\n                                'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \ndiff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py\nindex 074b89b624b9d3..cdd53c2d9ea2b1 100644\n--- a/torch/utils/data/_utils/pin_memory.py\n+++ b/torch/utils/data/_utils/pin_memory.py\n@@ -37,7 +37,7 @@ def do_one_step():\n                 data = pin_memory(data, device)\n             except Exception:\n                 data = ExceptionWrapper(\n-                    where=\"in pin memory thread for device {}\".format(device_id))\n+                    where=f\"in pin memory thread for device {device_id}\")\n             r = (idx, data)\n         while not done_event.is_set():\n             try:\ndiff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py\nindex b4fc8e0748f0f1..0d43f63a6a2f20 100644\n--- a/torch/utils/data/_utils/worker.py\n+++ b/torch/utils/data/_utils/worker.py\n@@ -76,13 +76,13 @@ def __init__(self, **kwargs):\n \n     def __setattr__(self, key, val):\n         if self.__initialized:\n-            raise RuntimeError(\"Cannot assign attributes to {} objects\".format(self.__class__.__name__))\n+            raise RuntimeError(f\"Cannot assign attributes to {self.__class__.__name__} objects\")\n         return super().__setattr__(key, val)\n \n     def __repr__(self):\n         items = []\n         for k in self.__keys:\n-            items.append('{}={}'.format(k, getattr(self, k)))\n+            items.append(f'{k}={getattr(self, k)}')\n         return '{}({})'.format(self.__class__.__name__, ', '.join(items))\n \n \n@@ -252,7 +252,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n             fetcher = _DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)\n         except Exception:\n             init_exception = ExceptionWrapper(\n-                where=\"in DataLoader worker process {}\".format(worker_id))\n+                where=f\"in DataLoader worker process {worker_id}\")\n \n         # When using Iterable mode, some worker can exit earlier than others due\n         # to the IterableDataset behaving differently for different workers.\n@@ -318,7 +318,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n                         # `ExceptionWrapper` does the correct thing.\n                         # See NOTE [ Python Traceback Reference Cycle Problem ]\n                         data = ExceptionWrapper(\n-                            where=\"in DataLoader worker process {}\".format(worker_id))\n+                            where=f\"in DataLoader worker process {worker_id}\")\n             data_queue.put((idx, data))\n             del data, idx, index, r  # save memory\n     except KeyboardInterrupt:\ndiff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py\nindex ec86f778023ba6..1c33592f02f146 100644\n--- a/torch/utils/data/dataloader.py\n+++ b/torch/utils/data/dataloader.py\n@@ -604,7 +604,7 @@ def __init__(self, loader: DataLoader) -> None:\n         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n         self._persistent_workers = loader.persistent_workers\n         self._num_yielded = 0\n-        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n+        self._profile_name = f\"enumerate(DataLoader)#{self.__class__.__name__}.__next__\"\n \n     def __iter__(self) -> '_BaseDataLoaderIter':\n         return self\n@@ -1145,7 +1145,7 @@ def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n                     self._mark_worker_as_unavailable(worker_id)\n             if len(failed_workers) > 0:\n                 pids_str = ', '.join(str(w.pid) for w in failed_workers)\n-                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n+                raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n             if isinstance(e, queue.Empty):\n                 return (False, None)\n             import tempfile\n@@ -1281,7 +1281,7 @@ def _get_data(self):\n             if success:\n                 return data\n             else:\n-                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n+                raise RuntimeError(f'DataLoader timed out after {self._timeout} seconds')\n         elif self._pin_memory:\n             while self._pin_memory_thread.is_alive():\n                 success, data = self._try_get_data()\ndiff --git a/torch/utils/data/datapipes/_decorator.py b/torch/utils/data/datapipes/_decorator.py\nindex e4cc9e4e59365d..96b7e00e076f02 100644\n--- a/torch/utils/data/datapipes/_decorator.py\n+++ b/torch/utils/data/datapipes/_decorator.py\n@@ -80,7 +80,7 @@ def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:\n         elif isinstance(arg, Callable):  # type:ignore[arg-type]\n             self.deterministic_fn = arg  # type: ignore[assignment, misc]\n         else:\n-            raise TypeError(\"{} can not be decorated by non_deterministic\".format(arg))\n+            raise TypeError(f\"{arg} can not be decorated by non_deterministic\")\n \n     def __call__(self, *args, **kwargs):\n         global _determinism\ndiff --git a/torch/utils/data/datapipes/_typing.py b/torch/utils/data/datapipes/_typing.py\nindex 6377a2ec940860..68049ba30d9018 100644\n--- a/torch/utils/data/datapipes/_typing.py\n+++ b/torch/utils/data/datapipes/_typing.py\n@@ -234,7 +234,7 @@ def issubtype(self, other):\n             return issubtype(self.param, other.param)\n         if isinstance(other, type):\n             return issubtype(self.param, other)\n-        raise TypeError(\"Expected '_DataPipeType' or 'type', but found {}\".format(type(other)))\n+        raise TypeError(f\"Expected '_DataPipeType' or 'type', but found {type(other)}\")\n \n     def issubtype_of_instance(self, other):\n         return issubinstance(other, self.param)\n@@ -279,13 +279,13 @@ def __init__(self, name, bases, namespace, **kwargs):\n     @_tp_cache\n     def _getitem_(self, params):\n         if params is None:\n-            raise TypeError('{}[t]: t can not be None'.format(self.__name__))\n+            raise TypeError(f'{self.__name__}[t]: t can not be None')\n         if isinstance(params, str):\n             params = ForwardRef(params)\n         if not isinstance(params, tuple):\n             params = (params, )\n \n-        msg = \"{}[t]: t must be a type\".format(self.__name__)\n+        msg = f\"{self.__name__}[t]: t must be a type\"\n         params = tuple(_type_check(p, msg) for p in params)\n \n         if isinstance(self.type.param, _GenericAlias):\n@@ -303,7 +303,7 @@ def _getitem_(self, params):\n                                        '__type_class__': True})\n \n         if len(params) > 1:\n-            raise TypeError('Too many parameters for {} actual {}, expected 1'.format(self, len(params)))\n+            raise TypeError(f'Too many parameters for {self} actual {len(params)}, expected 1')\n \n         t = _DataPipeType(params[0])\n \ndiff --git a/torch/utils/data/datapipes/dataframe/dataframes.py b/torch/utils/data/datapipes/dataframe/dataframes.py\nindex 06029e07851685..72d93cde66c3cb 100644\n--- a/torch/utils/data/datapipes/dataframe/dataframes.py\n+++ b/torch/utils/data/datapipes/dataframe/dataframes.py\n@@ -36,7 +36,7 @@ def disable_capture():\n     CaptureControl.disabled = True\n \n \n-class CaptureControl():\n+class CaptureControl:\n     disabled = False\n \n \n@@ -184,7 +184,7 @@ def execute(self):\n         return value\n \n \n-class CaptureLikeMock():\n+class CaptureLikeMock:\n     def __init__(self, name):\n         import unittest.mock as mock\n         # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n@@ -232,7 +232,7 @@ class CaptureVariableAssign(CaptureF):\n     def __str__(self):\n         variable = self.kwargs['variable']\n         value = self.kwargs['value']\n-        return \"{variable} = {value}\".format(variable=variable, value=value)\n+        return f\"{variable} = {value}\"\n \n     def execute(self):\n         self.kwargs['variable'].calculated_value = self.kwargs['value'].execute()\n@@ -272,7 +272,7 @@ def __init__(self, left, key, ctx):\n         self.key = key\n \n     def __str__(self):\n-        return \"%s[%s]\" % (self.left, get_val(self.key))\n+        return f\"{self.left}[{get_val(self.key)}]\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -287,7 +287,7 @@ def __init__(self, left, key, value, ctx):\n         self.value = value\n \n     def __str__(self):\n-        return \"%s[%s] = %s\" % (self.left, get_val(self.key), self.value)\n+        return f\"{self.left}[{get_val(self.key)}] = {self.value}\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -302,7 +302,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s + %s\" % (self.left, self.right)\n+        return f\"{self.left} + {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) + get_val(self.right)\n@@ -315,7 +315,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s * %s\" % (self.left, self.right)\n+        return f\"{self.left} * {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) * get_val(self.right)\n@@ -328,7 +328,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s - %s\" % (self.left, self.right)\n+        return f\"{self.left} - {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) - get_val(self.right)\n@@ -341,7 +341,7 @@ def __init__(self, src, name, ctx):\n         self.name = name\n \n     def __str__(self):\n-        return \"%s.%s\" % (self.src, self.name)\n+        return f\"{self.src}.{self.name}\"\n \n     def execute(self):\n         val = get_val(self.src)\ndiff --git a/torch/utils/data/datapipes/datapipe.py b/torch/utils/data/datapipes/datapipe.py\nindex 445400ecb59c32..1017b52af0fbce 100644\n--- a/torch/utils/data/datapipes/datapipe.py\n+++ b/torch/utils/data/datapipes/datapipe.py\n@@ -126,7 +126,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -135,7 +135,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register, enable_df_api_tracing=False):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, enable_df_api_tracing, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -265,7 +265,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -274,7 +274,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -363,7 +363,7 @@ def __len__(self):\n             return len(self._datapipe)\n         except Exception as e:\n             raise TypeError(\n-                \"{} instance doesn't have valid length\".format(type(self).__name__)\n+                f\"{type(self).__name__} instance doesn't have valid length\"\n             ) from e\n \n \ndiff --git a/torch/utils/data/datapipes/gen_pyi.py b/torch/utils/data/datapipes/gen_pyi.py\nindex 1b77fbfecf0290..ed3e75bc5da12e 100644\n--- a/torch/utils/data/datapipes/gen_pyi.py\n+++ b/torch/utils/data/datapipes/gen_pyi.py\n@@ -19,7 +19,7 @@ def gen_from_template(dir: str, template_name: str, output_name: str, replacemen\n     template_path = os.path.join(dir, template_name)\n     output_path = os.path.join(dir, output_name)\n \n-    with open(template_path, \"r\") as f:\n+    with open(template_path) as f:\n         content = f.read()\n     for placeholder, lines, indentation in replacements:\n         with open(output_path, \"w\") as f:\ndiff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py\nindex 4e3dce4b82d1dd..9916b094e408d1 100644\n--- a/torch/utils/data/datapipes/iter/callable.py\n+++ b/torch/utils/data/datapipes/iter/callable.py\n@@ -126,7 +126,7 @@ def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n         raise TypeError(\n-            \"{} instance doesn't have valid length\".format(type(self).__name__)\n+            f\"{type(self).__name__} instance doesn't have valid length\"\n         )\n \n \ndiff --git a/torch/utils/data/datapipes/iter/combinatorics.py b/torch/utils/data/datapipes/iter/combinatorics.py\nindex 30b569e329b654..4d2973bbc5a2e9 100644\n--- a/torch/utils/data/datapipes/iter/combinatorics.py\n+++ b/torch/utils/data/datapipes/iter/combinatorics.py\n@@ -48,7 +48,7 @@ def __len__(self) -> int:\n         # Dataset has been tested as `Sized`\n         if isinstance(self.sampler, Sized):\n             return len(self.sampler)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('shuffle')\n@@ -137,7 +137,7 @@ def __iter__(self) -> Iterator[T_co]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self._buffer = []\ndiff --git a/torch/utils/data/datapipes/iter/combining.py b/torch/utils/data/datapipes/iter/combining.py\nindex 7c76e986b230d4..4fe05ea717cf16 100644\n--- a/torch/utils/data/datapipes/iter/combining.py\n+++ b/torch/utils/data/datapipes/iter/combining.py\n@@ -56,7 +56,7 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return sum(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('fork')\n@@ -567,7 +567,7 @@ def __len__(self):\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes) * len(self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self.buffer = []\n@@ -627,4 +627,4 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/filelister.py b/torch/utils/data/datapipes/iter/filelister.py\nindex b2ecd71b5ce9c9..22e2cd432d6a3a 100644\n--- a/torch/utils/data/datapipes/iter/filelister.py\n+++ b/torch/utils/data/datapipes/iter/filelister.py\n@@ -61,5 +61,5 @@ def __iter__(self) -> Iterator[str] :\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/fileopener.py b/torch/utils/data/datapipes/iter/fileopener.py\nindex 03d5761a9f164c..50737d9b587b25 100644\n--- a/torch/utils/data/datapipes/iter/fileopener.py\n+++ b/torch/utils/data/datapipes/iter/fileopener.py\n@@ -51,7 +51,7 @@ def __init__(\n         self.encoding: Optional[str] = encoding\n \n         if self.mode not in ('b', 't', 'rb', 'rt', 'r'):\n-            raise ValueError(\"Invalid mode {}\".format(mode))\n+            raise ValueError(f\"Invalid mode {mode}\")\n         # TODO: enforce typing for each instance based on mode, otherwise\n         #       `argument_validation` with this DataPipe may be potentially broken\n \n@@ -68,5 +68,5 @@ def __iter__(self):\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/grouping.py b/torch/utils/data/datapipes/iter/grouping.py\nindex c83bd2748b78fb..b26847d7319740 100644\n--- a/torch/utils/data/datapipes/iter/grouping.py\n+++ b/torch/utils/data/datapipes/iter/grouping.py\n@@ -83,7 +83,7 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('unbatch')\ndiff --git a/torch/utils/data/datapipes/iter/routeddecoder.py b/torch/utils/data/datapipes/iter/routeddecoder.py\nindex 8bfbe1442180ab..5e68ae133e05ab 100644\n--- a/torch/utils/data/datapipes/iter/routeddecoder.py\n+++ b/torch/utils/data/datapipes/iter/routeddecoder.py\n@@ -62,4 +62,4 @@ def __iter__(self) -> Iterator[Tuple[str, Any]]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/sharding.py b/torch/utils/data/datapipes/iter/sharding.py\nindex 730caeaf7d4da3..1f4a3a291bd11f 100644\n--- a/torch/utils/data/datapipes/iter/sharding.py\n+++ b/torch/utils/data/datapipes/iter/sharding.py\n@@ -80,4 +80,4 @@ def __len__(self):\n         if isinstance(self.source_datapipe, Sized):\n             return len(self.source_datapipe) // self.num_of_instances +\\\n                 (1 if (self.instance_id < len(self.source_datapipe) % self.num_of_instances) else 0)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/map/combining.py b/torch/utils/data/datapipes/map/combining.py\nindex 85146f8345cbdc..4a4a785eff78e5 100644\n--- a/torch/utils/data/datapipes/map/combining.py\n+++ b/torch/utils/data/datapipes/map/combining.py\n@@ -47,7 +47,7 @@ def __getitem__(self, index) -> T_co:  # type: ignore[type-var]\n                 return dp[index - offset]\n             else:\n                 offset += len(dp)\n-        raise IndexError(\"Index {} is out of range.\".format(index))\n+        raise IndexError(f\"Index {index} is out of range.\")\n \n     def __len__(self) -> int:\n         return sum(len(dp) for dp in self.datapipes)\ndiff --git a/torch/utils/data/datapipes/map/grouping.py b/torch/utils/data/datapipes/map/grouping.py\nindex da3cf5688a1bb0..65b30d8eba1f40 100644\n--- a/torch/utils/data/datapipes/map/grouping.py\n+++ b/torch/utils/data/datapipes/map/grouping.py\n@@ -64,4 +64,4 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/utils/common.py b/torch/utils/data/datapipes/utils/common.py\nindex e39d67ee6c81b9..99ae0cb4cbd024 100644\n--- a/torch/utils/data/datapipes/utils/common.py\n+++ b/torch/utils/data/datapipes/utils/common.py\n@@ -305,7 +305,7 @@ def __init__(self, file_obj, parent_stream=None, name=None):\n         self.closed = False\n         if parent_stream is not None:\n             if not isinstance(parent_stream, StreamWrapper):\n-                raise RuntimeError('Parent stream should be StreamWrapper, {} was given'.format(type(parent_stream)))\n+                raise RuntimeError(f'Parent stream should be StreamWrapper, {type(parent_stream)} was given')\n             parent_stream.child_counter += 1\n             self.parent_stream = parent_stream\n         if StreamWrapper.debug_unclosed_streams:\ndiff --git a/torch/utils/data/datapipes/utils/decoder.py b/torch/utils/data/datapipes/utils/decoder.py\nindex 4da810c3276684..8a7cb71b619de4 100644\n--- a/torch/utils/data/datapipes/utils/decoder.py\n+++ b/torch/utils/data/datapipes/utils/decoder.py\n@@ -137,7 +137,7 @@ class ImageHandler:\n     - pilrgba: pil None rgba\n     \"\"\"\n     def __init__(self, imagespec):\n-        assert imagespec in list(imagespecs.keys()), \"unknown image specification: {}\".format(imagespec)\n+        assert imagespec in list(imagespecs.keys()), f\"unknown image specification: {imagespec}\"\n         self.imagespec = imagespec.lower()\n \n     def __call__(self, extension, data):\n@@ -167,14 +167,14 @@ def __call__(self, extension, data):\n                 return img\n             elif atype == \"numpy\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n                 if etype == \"uint8\":\n                     return result\n                 else:\n                     return result.astype(\"f\") / 255.0\n             elif atype == \"torch\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n \n                 if etype == \"uint8\":\n                     result = np.array(result.transpose(2, 0, 1))\ndiff --git a/torch/utils/data/graph.py b/torch/utils/data/graph.py\nindex 2769e326c03e3b..7fc95d58fa2198 100644\n--- a/torch/utils/data/graph.py\n+++ b/torch/utils/data/graph.py\n@@ -130,7 +130,7 @@ def traverse(datapipe: DataPipe, only_datapipe: Optional[bool] = None) -> DataPi\n # Add cache here to prevent infinite recursion on DataPipe\n def _traverse_helper(datapipe: DataPipe, only_datapipe: bool, cache: Set[int]) -> DataPipeGraph:\n     if not isinstance(datapipe, (IterDataPipe, MapDataPipe)):\n-        raise RuntimeError(\"Expected `IterDataPipe` or `MapDataPipe`, but {} is found\".format(type(datapipe)))\n+        raise RuntimeError(f\"Expected `IterDataPipe` or `MapDataPipe`, but {type(datapipe)} is found\")\n \n     dp_id = id(datapipe)\n     if dp_id in cache:\ndiff --git a/torch/utils/dlpack.py b/torch/utils/dlpack.py\nindex f903de94eb67b2..a987bca6dcd51b 100644\n--- a/torch/utils/dlpack.py\n+++ b/torch/utils/dlpack.py\n@@ -102,7 +102,7 @@ def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n         # device is either CUDA or ROCm, we need to pass the current\n         # stream\n         if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n-            stream = torch.cuda.current_stream('cuda:{}'.format(device[1]))\n+            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n             # cuda_stream is the pointer to the stream and it is a public\n             # attribute, but it is not documented\n             # The array API specify that the default legacy stream must be passed\ndiff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py\nindex 3b583dbf790109..163f3649c41279 100644\n--- a/torch/utils/hipify/cuda_to_hip_mappings.py\n+++ b/torch/utils/hipify/cuda_to_hip_mappings.py\n@@ -46,7 +46,7 @@\n     RE_MINOR = re.compile(r\"#define\\s+ROCM_VERSION_MINOR\\s+(\\d+)\")\n     RE_PATCH = re.compile(r\"#define\\s+ROCM_VERSION_PATCH\\s+(\\d+)\")\n     major, minor, patch = 0, 0, 0\n-    for line in open(rocm_version_h, \"r\"):\n+    for line in open(rocm_version_h):\n         match = RE_MAJOR.search(line)\n         if match:\n             major = int(match.group(1))\ndiff --git a/torch/utils/hipify/hipify_python.py b/torch/utils/hipify/hipify_python.py\nindex 34a066750e1cdc..fa800659595bd7 100755\n--- a/torch/utils/hipify/hipify_python.py\n+++ b/torch/utils/hipify/hipify_python.py\n@@ -219,13 +219,13 @@ def compute_stats(stats):\n     unsupported_calls = {cuda_call for (cuda_call, _filepath) in stats[\"unsupported_calls\"]}\n \n     # Print the number of unsupported calls\n-    print(\"Total number of unsupported CUDA function calls: {0:d}\".format(len(unsupported_calls)))\n+    print(f\"Total number of unsupported CUDA function calls: {len(unsupported_calls):d}\")\n \n     # Print the list of unsupported calls\n     print(\", \".join(unsupported_calls))\n \n     # Print the number of kernel launches\n-    print(\"\\nTotal number of replaced kernel launches: {0:d}\".format(len(stats[\"kernel_launches\"])))\n+    print(\"\\nTotal number of replaced kernel launches: {:d}\".format(len(stats[\"kernel_launches\"])))\n \n \n def add_dim3(kernel_string, cuda_kernel):\n@@ -254,8 +254,8 @@ def add_dim3(kernel_string, cuda_kernel):\n     first_arg_clean = kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace(\"\\n\", \"\").strip(\" \")\n     second_arg_clean = kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace(\"\\n\", \"\").strip(\" \")\n \n-    first_arg_dim3 = \"dim3({})\".format(first_arg_clean)\n-    second_arg_dim3 = \"dim3({})\".format(second_arg_clean)\n+    first_arg_dim3 = f\"dim3({first_arg_clean})\"\n+    second_arg_dim3 = f\"dim3({second_arg_clean})\"\n \n     first_arg_raw_dim3 = first_arg_raw.replace(first_arg_clean, first_arg_dim3)\n     second_arg_raw_dim3 = second_arg_raw.replace(second_arg_clean, second_arg_dim3)\n@@ -269,7 +269,7 @@ def add_dim3(kernel_string, cuda_kernel):\n def processKernelLaunches(string, stats):\n     \"\"\" Replace the CUDA style Kernel launches with the HIP style kernel launches.\"\"\"\n     # Concat the namespace with the kernel names. (Find cleaner way of doing this later).\n-    string = RE_KERNEL_LAUNCH.sub(lambda inp: \"{0}{1}::\".format(inp.group(1), inp.group(2)), string)\n+    string = RE_KERNEL_LAUNCH.sub(lambda inp: f\"{inp.group(1)}{inp.group(2)}::\", string)\n \n     def grab_method_and_template(in_kernel):\n         # The positions for relevant kernel components.\n@@ -482,7 +482,7 @@ def replace_math_functions(input_string):\n     \"\"\"\n     output_string = input_string\n     for func in MATH_TRANSPILATIONS:\n-        output_string = output_string.replace(r'{}('.format(func), '{}('.format(MATH_TRANSPILATIONS[func]))\n+        output_string = output_string.replace(fr'{func}(', f'{MATH_TRANSPILATIONS[func]}(')\n \n     return output_string\n \n@@ -531,7 +531,7 @@ def replace_extern_shared(input_string):\n     \"\"\"\n     output_string = input_string\n     output_string = RE_EXTERN_SHARED.sub(\n-        lambda inp: \"HIP_DYNAMIC_SHARED({0} {1}, {2})\".format(\n+        lambda inp: \"HIP_DYNAMIC_SHARED({} {}, {})\".format(\n             inp.group(1) or \"\", inp.group(2), inp.group(3)), output_string)\n \n     return output_string\n@@ -657,7 +657,7 @@ def is_caffe2_gpu_file(rel_filepath):\n \n \n # Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n-class Trie():\n+class Trie:\n     \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n     The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n \n@@ -750,7 +750,7 @@ def pattern(self):\n             CAFFE2_TRIE.add(src)\n             CAFFE2_MAP[src] = dst\n RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.pattern())\n-RE_PYTORCH_PREPROCESSOR = re.compile(r'(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))\n+RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\\W)({PYTORCH_TRIE.pattern()})(?=\\W)')\n \n RE_QUOTE_HEADER = re.compile(r'#include \"([^\"]+)\"')\n RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')\n@@ -789,7 +789,7 @@ def preprocessor(\n \n     rel_filepath = os.path.relpath(filepath, output_directory)\n \n-    with open(fin_path, 'r', encoding='utf-8') as fin:\n+    with open(fin_path, encoding='utf-8') as fin:\n         if fin.readline() == HIPIFY_C_BREADCRUMB:\n             hipify_result.hipified_path = None\n             hipify_result.status = \"[ignored, input is hipified output]\"\n@@ -929,7 +929,7 @@ def repl(m):\n \n     do_write = True\n     if os.path.exists(fout_path):\n-        with open(fout_path, 'r', encoding='utf-8') as fout_old:\n+        with open(fout_path, encoding='utf-8') as fout_old:\n             do_write = fout_old.read() != output_source\n     if do_write:\n         try:\n@@ -956,7 +956,7 @@ def file_specific_replacement(filepath, search_string, replace_string, strict=Fa\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if strict:\n-            contents = re.sub(r'\\b({0})\\b'.format(re.escape(search_string)), lambda x: replace_string, contents)\n+            contents = re.sub(fr'\\b({re.escape(search_string)})\\b', lambda x: replace_string, contents)\n         else:\n             contents = contents.replace(search_string, replace_string)\n         f.seek(0)\n@@ -968,8 +968,8 @@ def file_add_header(filepath, header):\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if header[0] != \"<\" and header[-1] != \">\":\n-            header = '\"{0}\"'.format(header)\n-        contents = ('#include {0} \\n'.format(header)) + contents\n+            header = f'\"{header}\"'\n+        contents = (f'#include {header} \\n') + contents\n         f.seek(0)\n         f.write(contents)\n         f.truncate()\ndiff --git a/torch/utils/jit/log_extract.py b/torch/utils/jit/log_extract.py\nindex d9d0e442c1dbf6..2e89a769eff0c8 100644\n--- a/torch/utils/jit/log_extract.py\n+++ b/torch/utils/jit/log_extract.py\n@@ -11,7 +11,7 @@ def extract_ir(filename: str) -> List[str]:\n     pfx = None\n     current = \"\"\n     graphs = []\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         split_strs = f.read().split(BEGIN)\n         for i, split_str in enumerate(split_strs):\n             if i == 0:\ndiff --git a/torch/utils/mobile_optimizer.py b/torch/utils/mobile_optimizer.py\nindex ec200423e10c5b..66d57a2372baf9 100644\n--- a/torch/utils/mobile_optimizer.py\n+++ b/torch/utils/mobile_optimizer.py\n@@ -31,7 +31,7 @@ def optimize_for_mobile(\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     if optimization_blocklist is None:\n         optimization_blocklist = set()\n@@ -86,7 +86,7 @@ def generate_mobile_module_lints(script_module: torch.jit.ScriptModule):\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     lint_list = []\n \ndiff --git a/torch/utils/tensorboard/_caffe2_graph.py b/torch/utils/tensorboard/_caffe2_graph.py\nindex 8bba2aeffddef2..2aa162af7ad5c3 100644\n--- a/torch/utils/tensorboard/_caffe2_graph.py\n+++ b/torch/utils/tensorboard/_caffe2_graph.py\n@@ -232,7 +232,7 @@ def _add_gradient_scope(shapes, blob_name_tracker, ops):\n \n     def f(name):\n         if \"_grad\" in name:\n-            return \"GRADIENTS/{}\".format(name)\n+            return f\"GRADIENTS/{name}\"\n         else:\n             return name\n \n@@ -317,7 +317,7 @@ def _tf_device(device_option):\n     ):\n         return \"/cpu:*\"\n     if device_option.device_type == caffe2_pb2.CUDA:\n-        return \"/gpu:{}\".format(device_option.device_id)\n+        return f\"/gpu:{device_option.device_id}\"\n     raise Exception(\"Unhandled device\", device_option)\n \n \ndiff --git a/torch/utils/tensorboard/_embedding.py b/torch/utils/tensorboard/_embedding.py\nindex f172e092608337..afbe68191aa98f 100644\n--- a/torch/utils/tensorboard/_embedding.py\n+++ b/torch/utils/tensorboard/_embedding.py\n@@ -62,7 +62,7 @@ def make_sprite(label_img, save_path):\n \n def get_embedding_info(metadata, label_img, subdir, global_step, tag):\n     info = EmbeddingInfo()\n-    info.tensor_name = \"{}:{}\".format(tag, str(global_step).zfill(5))\n+    info.tensor_name = f\"{tag}:{str(global_step).zfill(5)}\"\n     info.tensor_path = _gfile_join(subdir, \"tensors.tsv\")\n     if metadata is not None:\n         info.metadata_path = _gfile_join(subdir, \"metadata.tsv\")\ndiff --git a/torch/utils/tensorboard/_pytorch_graph.py b/torch/utils/tensorboard/_pytorch_graph.py\nindex f03812b603e1c0..280b503c515c0b 100644\n--- a/torch/utils/tensorboard/_pytorch_graph.py\n+++ b/torch/utils/tensorboard/_pytorch_graph.py\n@@ -275,7 +275,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n                     parent_scope, attr_scope, attr_name\n                 )\n             else:\n-                attr_to_scope[attr_key] = \"__module.{}\".format(attr_name)\n+                attr_to_scope[attr_key] = f\"__module.{attr_name}\"\n             # We don't need classtype nodes; scope will provide this information\n             if node.output().type().kind() != CLASSTYPE_KIND:\n                 node_py = NodePyOP(node)\n@@ -286,7 +286,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n \n     for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n         node_pyio = NodePyIO(node, \"output\")\n-        node_pyio.debugName = \"output.{}\".format(i + 1)\n+        node_pyio.debugName = f\"output.{i + 1}\"\n         node_pyio.inputs = [node.debugName()]\n         nodes_py.append(node_pyio)\n \n@@ -302,7 +302,7 @@ def parse_traced_name(module):\n     for name, module in trace.named_modules(prefix=\"__module\"):\n         mod_name = parse_traced_name(module)\n         attr_name = name.split(\".\")[-1]\n-        alias_to_name[name] = \"{}[{}]\".format(mod_name, attr_name)\n+        alias_to_name[name] = f\"{mod_name}[{attr_name}]\"\n \n     for node in nodes_py.nodes_op:\n         module_aliases = node.scopeName.split(\"/\")\ndiff --git a/torch/utils/tensorboard/writer.py b/torch/utils/tensorboard/writer.py\nindex 2fa34d05e727d9..b592707df2c1e1 100644\n--- a/torch/utils/tensorboard/writer.py\n+++ b/torch/utils/tensorboard/writer.py\n@@ -953,7 +953,7 @@ def add_embedding(\n \n         # Maybe we should encode the tag so slashes don't trip us up?\n         # I don't think this will mess us up, but better safe than sorry.\n-        subdir = \"%s/%s\" % (str(global_step).zfill(5), self._encode(tag))\n+        subdir = f\"{str(global_step).zfill(5)}/{self._encode(tag)}\"\n         save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n \n         fs = tf.io.gfile\ndiff --git a/torch/utils/throughput_benchmark.py b/torch/utils/throughput_benchmark.py\nindex 8b2fd1a76ca8a4..2dc3ce8543a9b6 100644\n--- a/torch/utils/throughput_benchmark.py\n+++ b/torch/utils/throughput_benchmark.py\n@@ -18,10 +18,10 @@ def format_time(time_us=None, time_ms=None, time_s=None):\n             raise AssertionError(\"Shouldn't reach here :)\")\n \n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n \n class ExecutionStats:\n@@ -52,8 +52,8 @@ def total_time_seconds(self):\n     def __str__(self):\n         return '\\n'.join([\n             \"Average latency per example: \" + format_time(time_ms=self.latency_avg_ms),\n-            \"Total number of iterations: {}\".format(self.num_iters),\n-            \"Total number of iterations per second (across all threads): {:.2f}\".format(self.iters_per_second),\n+            f\"Total number of iterations: {self.num_iters}\",\n+            f\"Total number of iterations per second (across all threads): {self.iters_per_second:.2f}\",\n             \"Total time: \" + format_time(time_s=self.total_time_seconds)\n         ])\n \ndiff --git a/torch/utils/viz/_cycles.py b/torch/utils/viz/_cycles.py\nindex a64d5e9c35830a..13a425cd1b8285 100644\n--- a/torch/utils/viz/_cycles.py\n+++ b/torch/utils/viz/_cycles.py\n@@ -220,29 +220,29 @@ def format_sequence(obj):\n     if isinstance(obj, BASE_TYPES):\n         return repr(obj)\n     if type(obj).__name__ == 'function':\n-        return \"function\\n{}\".format(obj.__name__)\n+        return f\"function\\n{obj.__name__}\"\n     elif isinstance(obj, types.MethodType):\n         try:\n             func_name = obj.__func__.__qualname__\n         except AttributeError:\n             func_name = \"<anonymous>\"\n-        return \"instancemethod\\n{}\".format(func_name)\n+        return f\"instancemethod\\n{func_name}\"\n     elif isinstance(obj, list):\n         return f\"[{format_sequence(obj)}]\"\n     elif isinstance(obj, tuple):\n         return f\"({format_sequence(obj)})\"\n     elif isinstance(obj, dict):\n-        return \"dict[{}]\".format(len(obj))\n+        return f\"dict[{len(obj)}]\"\n     elif isinstance(obj, types.ModuleType):\n-        return \"module\\n{}\".format(obj.__name__)\n+        return f\"module\\n{obj.__name__}\"\n     elif isinstance(obj, type):\n-        return \"type\\n{}\".format(obj.__name__)\n+        return f\"type\\n{obj.__name__}\"\n     elif isinstance(obj, weakref.ref):\n         referent = obj()\n         if referent is None:\n             return \"weakref (dead referent)\"\n         else:\n-            return \"weakref to id 0x{:x}\".format(id(referent))\n+            return f\"weakref to id 0x{id(referent):x}\"\n     elif isinstance(obj, types.FrameType):\n         filename = obj.f_code.co_filename\n         if len(filename) > FRAME_FILENAME_LIMIT:\ndiff --git a/torch/utils/weak.py b/torch/utils/weak.py\nindex 2a7d597c4f2a06..bcd3025bc68e3a 100644\n--- a/torch/utils/weak.py\n+++ b/torch/utils/weak.py\n@@ -4,7 +4,6 @@\n from weakref import ref\n from _weakrefset import _IterationGuard  # type: ignore[attr-defined]\n from collections.abc import MutableMapping, Mapping\n-from typing import Dict\n from torch import Tensor\n import collections.abc as _collections_abc\n \n@@ -83,7 +82,7 @@ def __eq__(self, other):\n \n # This is directly adapted from cpython/Lib/weakref.py\n class WeakIdKeyDictionary(MutableMapping):\n-    data: Dict[WeakIdRef, object]\n+    data: dict[WeakIdRef, object]\n \n     def __init__(self, dict=None):\n         self.data = {}\n@@ -144,7 +143,7 @@ def __len__(self):\n         return len(self.data) - len(self._pending_removals)\n \n     def __repr__(self):\n-        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n+        return f\"<{self.__class__.__name__} at {id(self):#x}>\"\n \n     def __setitem__(self, key, value):\n         self.data[WeakIdRef(key, self._remove)] = value  # CHANGED\n"
  },
  {
    "number": 105413,
    "title": "[BE] Enable ruff's UP rules and autoformat torchgen/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "3f99961e233df20f7155fc24e2bddecb1c72e34c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105413",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105413/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105413.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105413.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105413/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105413/comments",
    "labels": [
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:17:06.389833Z",
    "state": "closed",
    "patch": "From d99b7570c1c29484f4a031ea30d6f3a9501d6339 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:17:00 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat torchgen/\n\n[ghstack-poisoned]\n---\n torchgen/api/python.py               | 16 ++++++++--------\n torchgen/code_template.py            |  2 +-\n torchgen/executorch/parse.py         |  2 +-\n torchgen/gen.py                      |  4 ++--\n torchgen/gen_backend_stubs.py        |  6 +++---\n torchgen/gen_executorch.py           |  6 +++---\n torchgen/gen_lazy_tensor.py          |  6 +++---\n torchgen/model.py                    |  2 +-\n torchgen/selective_build/operator.py |  2 +-\n torchgen/selective_build/selector.py |  4 ++--\n torchgen/utils.py                    | 10 +++++-----\n 11 files changed, 30 insertions(+), 30 deletions(-)\n\ndiff --git a/torchgen/api/python.py b/torchgen/api/python.py\nindex b4da5d1113dce6..96aa43be1060b5 100644\n--- a/torchgen/api/python.py\n+++ b/torchgen/api/python.py\n@@ -315,7 +315,7 @@ def from_outputs(\n                 outputs=outputs,\n             )\n         elif size > 1:\n-            if any((not a.type.is_tensor_like() for a in outputs)):\n+            if any(not a.type.is_tensor_like() for a in outputs):\n                 raise RuntimeError(f\"Unsupported output type: {outputs}\")\n             return PythonOutArgument(\n                 name=\"out\",\n@@ -882,10 +882,10 @@ def topt_default_init(name: str) -> Optional[str]:\n \n \n def namedtuple_fieldnames(returns: Tuple[Return, ...]) -> List[str]:\n-    if len(returns) <= 1 or all((r.name is None for r in returns)):\n+    if len(returns) <= 1 or all(r.name is None for r in returns):\n         return []\n     else:\n-        if any((r.name is None for r in returns)):\n+        if any(r.name is None for r in returns):\n             # When building on Windows, `PyStructSequence_UnnamedField` could not be\n             # resolved by the linker for some reason, which cause error in building:\n             #\n@@ -1163,7 +1163,7 @@ def dispatch_lambda_return_str(f: NativeFunction) -> str:\n     # mutable reference to temporary.  Maybe we could assign it to a\n     # variable itself.)\n     returns_without_annotation = tuple(\n-        (Return(r.name, r.type, None) for r in f.func.returns)\n+        Return(r.name, r.type, None) for r in f.func.returns\n     )\n     return_str = cpp.returns_type(returns_without_annotation, symint=True).cpp_type()\n     if return_str not in SUPPORTED_RETURN_TYPES:\n@@ -1195,7 +1195,7 @@ def cpp_dispatch_exprs(\n     exprs: Tuple[str, ...] = tuple()\n     if not isinstance(python_signature, PythonSignatureDeprecated):\n         # By default the exprs are consistent with the C++ signature.\n-        exprs = tuple((a.name for a in cpp_args))\n+        exprs = tuple(a.name for a in cpp_args)\n     else:\n         # For deprecated python signature we may need fill in some constants.\n         exprs = tuple(\n@@ -1426,7 +1426,7 @@ def dispatch_lambda_exprs(\n                     f\"{f.func}: unrecognized type '{str(a.type)}' for tensor options field '{a.name}'\"\n                 )\n         if not all(\n-            (a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys())\n+            a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys()\n         ):\n             raise RuntimeError(\n                 f\"{f.func}: incomplete tensor options args: {tensor_options_args_names}\"\n@@ -1454,7 +1454,7 @@ def dispatch_lambda_exprs(\n                 raise RuntimeError(\n                     f\"{f.func}: dtype in tensor_options_args without output arg\"\n                 )\n-            if not all((a in tensor_options_args_names for a in (\"layout\", \"device\"))):\n+            if not all(a in tensor_options_args_names for a in (\"layout\", \"device\")):\n                 raise RuntimeError(\n                     f\"{f.func}: incomplete tensor options for output check\"\n                 )\n@@ -1473,6 +1473,6 @@ def dispatch_lambda_exprs(\n             )\n \n     return DispatchLambdaArgumentExprs(\n-        exprs=tuple((lambda_args_exprs[a.name] for a in lambda_args)),\n+        exprs=tuple(lambda_args_exprs[a.name] for a in lambda_args),\n         inits=inits,\n     )\ndiff --git a/torchgen/code_template.py b/torchgen/code_template.py\nindex 9f877771afe9be..b932a94ecc9192 100644\n--- a/torchgen/code_template.py\n+++ b/torchgen/code_template.py\n@@ -20,7 +20,7 @@ class CodeTemplate:\n \n     @staticmethod\n     def from_file(filename: str) -> \"CodeTemplate\":\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             return CodeTemplate(f.read(), filename)\n \n     def __init__(self, pattern: str, filename: str = \"\") -> None:\ndiff --git a/torchgen/executorch/parse.py b/torchgen/executorch/parse.py\nindex f6f30b4554aafb..89b4b93558a6a2 100644\n--- a/torchgen/executorch/parse.py\n+++ b/torchgen/executorch/parse.py\n@@ -124,7 +124,7 @@ def parse_et_yaml(\n     \"\"\"Parse native_functions.yaml into NativeFunctions and an Operator Indexed Dict\n     of fields to persist from native_functions.yaml to functions.yaml\n     \"\"\"\n-    with open(path, \"r\") as f:\n+    with open(path) as f:\n         es = yaml.load(f, Loader=LineLoader)\n \n     et_kernel = extract_kernel_fields(es)\ndiff --git a/torchgen/gen.py b/torchgen/gen.py\nindex dcdd0945dff019..9766c8af5bc0f5 100644\n--- a/torchgen/gen.py\n+++ b/torchgen/gen.py\n@@ -212,7 +212,7 @@ def parse_tags_yaml_struct(es: object, path: str = \"<stdin>\") -> Set[str]:\n def parse_tags_yaml(path: str) -> Set[str]:\n     global _GLOBAL_PARSE_TAGS_YAML_CACHE\n     if path not in _GLOBAL_PARSE_TAGS_YAML_CACHE:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n             _GLOBAL_PARSE_TAGS_YAML_CACHE[path] = parse_tags_yaml_struct(es, path=path)\n \n@@ -233,7 +233,7 @@ def parse_native_yaml(\n \n         # if a loaded yaml is provided, use that instead of reading from path\n         if loaded_yaml is None:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 es = yaml.load(f, Loader=LineLoader)\n         else:\n             es = loaded_yaml\ndiff --git a/torchgen/gen_backend_stubs.py b/torchgen/gen_backend_stubs.py\nindex 7322daa5dc7602..ff23aa9be39713 100644\n--- a/torchgen/gen_backend_stubs.py\n+++ b/torchgen/gen_backend_stubs.py\n@@ -47,7 +47,7 @@ def parse_backend_yaml(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -253,9 +253,9 @@ def error_on_missing_kernels(\n     full_codegen: Optional[List[OperatorName]] = None,\n ) -> None:\n     try:\n-        with open(kernel_defn_file_path, \"r\") as f:\n+        with open(kernel_defn_file_path) as f:\n             backend_defns = f.read()\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified impl_path file: {kernel_defn_file_path}\"\n         ) from e\ndiff --git a/torchgen/gen_executorch.py b/torchgen/gen_executorch.py\nindex bfd42a7985e49d..6f5df46944f0f6 100644\n--- a/torchgen/gen_executorch.py\n+++ b/torchgen/gen_executorch.py\n@@ -575,7 +575,7 @@ def translate_native_yaml(\n         None\n     \"\"\"\n     if use_aten_lib:\n-        with open(aten_yaml_path, \"r\") as aten_yaml:\n+        with open(aten_yaml_path) as aten_yaml:\n             out_file.writelines(aten_yaml.readlines())\n         return\n \n@@ -604,7 +604,7 @@ def translate_native_yaml(\n         or os.stat(native_yaml_path).st_size == 0\n     ):\n         return\n-    with open(native_yaml_path, \"r\") as native_yaml:\n+    with open(native_yaml_path) as native_yaml:\n         native_es = yaml.load(native_yaml, Loader=LineLoader)\n         if not native_es:\n             return\n@@ -641,7 +641,7 @@ def parse_yaml(\n     Union[Dict[DispatchKey, Dict[OperatorName, BackendMetadata]], ETKernelIndex],\n ]:\n     if path and os.path.exists(path) and os.stat(path).st_size > 0:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n \n         # Check for kernel index structure\ndiff --git a/torchgen/gen_lazy_tensor.py b/torchgen/gen_lazy_tensor.py\nindex f995bdb2619838..3e4e4b0414277c 100644\n--- a/torchgen/gen_lazy_tensor.py\n+++ b/torchgen/gen_lazy_tensor.py\n@@ -115,7 +115,7 @@ def parse_native_functions_keys(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -134,10 +134,10 @@ def validate_shape_inference_header(\n     shape_inference_hdr: str, expected_shape_infr_decls: List[str]\n ) -> None:\n     try:\n-        with open(shape_inference_hdr, \"r\") as f:\n+        with open(shape_inference_hdr) as f:\n             shape_infr_decls = f.read()\n             shape_infr_decl_lines = set(shape_infr_decls.split(\"\\n\"))\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified shape_inference_hdr file: {shape_inference_hdr}\"\n         ) from e\ndiff --git a/torchgen/model.py b/torchgen/model.py\nindex 151fb02bb2c908..0b44732455ea2e 100644\n--- a/torchgen/model.py\n+++ b/torchgen/model.py\n@@ -40,7 +40,7 @@ class Location:\n     line: int\n \n     def __str__(self) -> str:\n-        return \"{}:{}\".format(self.file, self.line)\n+        return f\"{self.file}:{self.line}\"\n \n \n # Valid values of the 'variants' field in native_functions.yaml\ndiff --git a/torchgen/selective_build/operator.py b/torchgen/selective_build/operator.py\nindex 52fdcb74fca84b..d7f5c56f63a60d 100644\n--- a/torchgen/selective_build/operator.py\n+++ b/torchgen/selective_build/operator.py\n@@ -83,7 +83,7 @@ def from_yaml_dict(\n         if \"debug_info\" in op_info:\n             di_list = op_info[\"debug_info\"]\n             assert isinstance(di_list, list)\n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         return SelectiveBuildOperator(\n             name=op_name,\ndiff --git a/torchgen/selective_build/selector.py b/torchgen/selective_build/selector.py\nindex 1d4a00e5968950..4fdc513534444d 100644\n--- a/torchgen/selective_build/selector.py\n+++ b/torchgen/selective_build/selector.py\n@@ -93,7 +93,7 @@ def from_yaml_dict(data: Dict[str, object]) -> \"SelectiveBuilder\":\n             di_list = data[\"debug_info\"]\n             assert isinstance(di_list, list)\n \n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         operators = {}\n         operators_dict = data.get(\"operators\", {})\n@@ -141,7 +141,7 @@ def from_yaml_str(config_contents: str) -> \"SelectiveBuilder\":\n \n     @staticmethod\n     def from_yaml_path(config_path: str) -> \"SelectiveBuilder\":\n-        with open(config_path, \"r\") as f:\n+        with open(config_path) as f:\n             contents = yaml.safe_load(f)\n             return SelectiveBuilder.from_yaml_dict(contents)\n \ndiff --git a/torchgen/utils.py b/torchgen/utils.py\nindex dd187c737c93ed..0729645ef10b92 100644\n--- a/torchgen/utils.py\n+++ b/torchgen/utils.py\n@@ -105,7 +105,7 @@ def context(msg_fn: Callable[[], str]) -> Iterator[None]:\n # for getting mypy to do exhaustiveness checking\n # TODO: put this somewhere else, maybe\n def assert_never(x: NoReturn) -> NoReturn:\n-    raise AssertionError(\"Unhandled type: {}\".format(type(x).__name__))\n+    raise AssertionError(f\"Unhandled type: {type(x).__name__}\")\n \n \n @functools.lru_cache(maxsize=None)\n@@ -137,9 +137,9 @@ def __init__(self, install_dir: str, template_dir: str, dry_run: bool) -> None:\n     def _write_if_changed(self, filename: str, contents: str) -> None:\n         old_contents: Optional[str]\n         try:\n-            with open(filename, \"r\") as f:\n+            with open(filename) as f:\n                 old_contents = f.read()\n-        except IOError:\n+        except OSError:\n             old_contents = None\n         if contents != old_contents:\n             # Create output directory if it doesn't exist\n@@ -157,7 +157,7 @@ def substitute_with_template(\n             # TODO: Update the comment reference to the correct location\n             if \"generated_comment\" not in env:\n                 comment = \"@\" + \"generated by torchgen/gen.py\"\n-                comment += \" from {}\".format(os.path.basename(template_path))\n+                comment += f\" from {os.path.basename(template_path)}\"\n                 env[\"generated_comment\"] = comment\n             template = _read_template(template_path)\n             return template.substitute(env)\n@@ -172,7 +172,7 @@ def write_with_template(\n         template_fn: str,\n         env_callable: Callable[[], Union[str, Dict[str, Any]]],\n     ) -> None:\n-        filename = \"{}/{}\".format(self.install_dir, filename)\n+        filename = f\"{self.install_dir}/{filename}\"\n         assert filename not in self.filenames, \"duplicate file write {filename}\"\n         self.filenames.add(filename)\n         if not self.dry_run:\n"
  },
  {
    "number": 105412,
    "title": "[BE] Enable ruff's UP rules and autoformat dynamo / functorch and refs",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "207e7fad3e8687808e1dbbcb99198e5ce85e2fef",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105412",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105412/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105412.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105412.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105412/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105412/comments",
    "labels": [
      "open source",
      "release notes: fx",
      "ciflow/inductor",
      "module: dynamo",
      "module: export"
    ],
    "_event_time": "2023-07-18T01:16:17.270613Z",
    "state": "closed",
    "patch": "From cc5c54f2b07ab20a4beab91fb5056a1f3affa2c0 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:16:10 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat dynamo / functorch\n and refs\n\n[ghstack-poisoned]\n---\n functorch/benchmarks/operator_authoring.py    |  2 +-\n functorch/einops/rearrange.py                 |  4 +-\n .../examples/compilation/linear_train.py      |  2 +-\n .../maml_omniglot/support/omniglot_loaders.py |  4 +-\n functorch/op_analysis/gen_data.py             | 10 +--\n test/dynamo/test_autograd_function.py         |  4 +-\n test/dynamo/test_compile.py                   |  2 +-\n test/dynamo/test_logging.py                   |  2 +-\n test/dynamo/test_misc.py                      | 34 +++++-----\n test/dynamo/test_modules.py                   | 12 ++--\n test/dynamo/test_profiler.py                  |  4 +-\n test/dynamo/test_repros.py                    |  2 +-\n test/error_messages/storage.py                |  2 +-\n test/export/test_db.py                        |  6 +-\n test/export/test_serialize.py                 |  2 +-\n .../check_forward_backward_compatibility.py   |  2 +-\n test/functorch/test_aotdispatch.py            |  4 +-\n test/functorch/test_vmap.py                   |  2 +-\n test/fx/test_future.py                        |  4 +-\n test/fx/test_gradual_type.py                  |  8 +--\n torch/_decomp/decompositions.py               | 14 ++---\n torch/_dynamo/debug_utils.py                  | 16 ++---\n torch/_dynamo/eval_frame.py                   |  4 +-\n torch/_dynamo/output_graph.py                 |  2 +-\n torch/_dynamo/resume_execution.py             |  6 +-\n torch/_dynamo/symbolic_convert.py             |  4 +-\n torch/_dynamo/test_minifier_common.py         |  6 +-\n torch/_dynamo/variables/builder.py            | 10 ++-\n torch/_dynamo/variables/misc.py               |  2 +-\n torch/_export/verifier.py                     |  8 +--\n torch/_functorch/eager_transforms.py          |  2 +-\n torch/_functorch/pytree_hacks.py              |  2 +-\n torch/_prims/__init__.py                      | 62 +++++++++----------\n torch/_prims/executor.py                      |  2 +-\n torch/_prims/nvfuser_executor.py              |  4 +-\n torch/_prims_common/__init__.py               | 28 ++++-----\n torch/_prims_common/wrappers.py               |  8 +--\n torch/_refs/__init__.py                       | 46 +++++++-------\n torch/_refs/nn/functional/__init__.py         | 16 ++---\n .../migrate_gradual_types/constraint.py       |  1 -\n .../constraint_transformation.py              |  4 +-\n .../migrate_gradual_types/operation.py        |  1 -\n torch/fx/experimental/symbolic_shapes.py      |  4 +-\n .../multipledispatch/dispatcher.py            | 15 ++---\n torch/fx/interpreter.py                       |  2 +-\n torch/fx/passes/utils/matcher_utils.py        |  2 +-\n torch/fx/passes/utils/source_matcher_utils.py |  2 +-\n 47 files changed, 181 insertions(+), 204 deletions(-)\n\ndiff --git a/functorch/benchmarks/operator_authoring.py b/functorch/benchmarks/operator_authoring.py\nindex cbd816e2ad1324..456f5040d759f2 100644\n--- a/functorch/benchmarks/operator_authoring.py\n+++ b/functorch/benchmarks/operator_authoring.py\n@@ -113,7 +113,7 @@ def out_setup(n):\n def test_backwards(make_args, nnc=nnc_add, aten=torch.add):\n     def backwards_setup(n):\n         args = make_args(n)\n-        (grad_var,) = [a for a in args if a.requires_grad]\n+        (grad_var,) = (a for a in args if a.requires_grad)\n         aten(*args).sum().backward()\n         correct = grad_var.grad.clone()\n         grad_var.grad.zero_()\ndiff --git a/functorch/einops/rearrange.py b/functorch/einops/rearrange.py\nindex c45d2063c7114a..f8f60c4917b766 100644\n--- a/functorch/einops/rearrange.py\n+++ b/functorch/einops/rearrange.py\n@@ -108,7 +108,7 @@ class dims.\"\"\"\n \n     custom_rearrange_callable_name = \"do_rearrange\"\n     custom_rearrange_callable_code = (\n-        (\n+\n             f\"def {custom_rearrange_callable_name}(tensor):\\n\"\n             f\"    {comma_separate(first_class_dims)} = dims({n_dims})\\n\"\n             + (\n@@ -120,7 +120,7 @@ class dims.\"\"\"\n                 f\"    return tensor.sum({comma_separate([anon_dims])}, keepdim=False)\\n\"\n                 if anon_dims else \"    return tensor\\n\"\n             )\n-        )\n+\n     )\n \n     exec(custom_rearrange_callable_code)\ndiff --git a/functorch/examples/compilation/linear_train.py b/functorch/examples/compilation/linear_train.py\nindex 2d5f9d7dd37b44..ee84347470835b 100644\n--- a/functorch/examples/compilation/linear_train.py\n+++ b/functorch/examples/compilation/linear_train.py\n@@ -18,7 +18,7 @@ def bench(f, iters=100, warmup=10):\n     begin = time.time()\n     for _ in range(iters):\n         f()\n-    print((time.time() - begin))\n+    print(time.time() - begin)\n \n \n class Foo(nn.Module):\ndiff --git a/functorch/examples/maml_omniglot/support/omniglot_loaders.py b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\nindex ce636ecca0b1b2..6a4369ba4b208f 100644\n--- a/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n+++ b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n@@ -276,10 +276,10 @@ def load_data_cache(self, data_pack):\n             x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n             y_qrys = np.array(y_qrys).astype(int).reshape(self.batchsz, querysz)\n \n-            x_spts, y_spts, x_qrys, y_qrys = [\n+            x_spts, y_spts, x_qrys, y_qrys = (\n                 torch.from_numpy(z).to(self.device) for z in\n                 [x_spts, y_spts, x_qrys, y_qrys]\n-            ]\n+            )\n \n             data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n \ndiff --git a/functorch/op_analysis/gen_data.py b/functorch/op_analysis/gen_data.py\nindex a9cc84e6f9362c..ab1f3a79125c20 100644\n--- a/functorch/op_analysis/gen_data.py\n+++ b/functorch/op_analysis/gen_data.py\n@@ -23,7 +23,7 @@ def gen_data(special_op_lists, analysis_name):\n     composite_ops = get_ops_for_key('CompositeImplicitAutograd')\n     noncomposite_ops = all_ops - composite_ops\n \n-    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)\n+    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml').read(), Loader=yaml.CLoader)\n \n     annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}\n     from collections import defaultdict\n@@ -132,19 +132,19 @@ def remove_prefix(input_string, prefix):\n \n \n if True:\n-    with open('run_ops.txt', 'r') as f:\n+    with open('run_ops.txt') as f:\n         opinfo_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n-    with open('count_ops.txt', 'r') as f:\n+    with open('count_ops.txt') as f:\n         opinfo_counts = [i.strip() for i in f.readlines()]\n         opinfo_counts = defaultdict(int, dict(zip(opinfo_ops, opinfo_counts)))\n \n     def count_fn(x):\n         return opinfo_counts[x['full_name']]\n \n-    with open('run_decompositions.txt', 'r') as f:\n+    with open('run_decompositions.txt') as f:\n         decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n \n-    with open('public_api', 'r') as f:\n+    with open('public_api') as f:\n         ref_api = [i.strip() for i in f.readlines()]\n \n     def has_ref_impl(x):\ndiff --git a/test/dynamo/test_autograd_function.py b/test/dynamo/test_autograd_function.py\nindex 55165edd61a41d..7de264e5051743 100644\n--- a/test/dynamo/test_autograd_function.py\n+++ b/test/dynamo/test_autograd_function.py\n@@ -207,7 +207,7 @@ def backward(ctx, grad_output):\n \n class ModuleWithGradFunc(torch.nn.Module):\n     def __init__(self, func):\n-        super(ModuleWithGradFunc, self).__init__()\n+        super().__init__()\n         self.f = func.apply\n \n     def forward(self, x):\n@@ -336,7 +336,7 @@ def backward(ctx, grad_output):\n \n         class MyMod(torch.nn.Module):\n             def __init__(self):\n-                super(MyMod, self).__init__()\n+                super().__init__()\n                 self.gamma = torch.nn.Parameter(torch.rand([4, 128, 32, 32]))\n \n             def forward(self, x):\ndiff --git a/test/dynamo/test_compile.py b/test/dynamo/test_compile.py\nindex e3847cbb2ae121..5b2de2b7b3867f 100644\n--- a/test/dynamo/test_compile.py\n+++ b/test/dynamo/test_compile.py\n@@ -11,7 +11,7 @@\n \n class ToyModel(torch.nn.Module):\n     def __init__(self):\n-        super(ToyModel, self).__init__()\n+        super().__init__()\n         self.linear = torch.nn.Linear(10, 10)\n         self.relu = torch.nn.ReLU()\n \ndiff --git a/test/dynamo/test_logging.py b/test/dynamo/test_logging.py\nindex 183910f6db3510..eed99681e2c04e 100644\n--- a/test/dynamo/test_logging.py\n+++ b/test/dynamo/test_logging.py\n@@ -157,7 +157,7 @@ def throw(x):\n     def test_ddp_graphs(self, records):\n         class ToyModel(torch.nn.Module):\n             def __init__(self):\n-                super(ToyModel, self).__init__()\n+                super().__init__()\n                 self.layers = torch.nn.Sequential(\n                     torch.nn.Linear(1024, 1024),\n                     torch.nn.Linear(1024, 1024),\ndiff --git a/test/dynamo/test_misc.py b/test/dynamo/test_misc.py\nindex 6b020adc0d8007..6ec0acf8054fd3 100644\n--- a/test/dynamo/test_misc.py\n+++ b/test/dynamo/test_misc.py\n@@ -823,7 +823,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 3)\n@@ -837,7 +837,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 2)\n@@ -2202,7 +2202,7 @@ def fn():\n def fn():\n     foo.bar(1, 2, 3)\n {str(chr(10)).join(' ' * 4 + 'x' + str(i) + ' = 1' for i in range(1 << 9))}\n-    l = [{str(' ').join('x' + str(i) + ',' for i in range(1 << 9))}]\n+    l = [{' '.join('x' + str(i) + ',' for i in range(1 << 9))}]\n         \"\"\"\n         locals = {}\n         exec(fn_str, {}, locals)\n@@ -3087,7 +3087,7 @@ def foo(self, memo=None, prefix=\"\", remove_duplicate=False):\n                     memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n                 ):\n                     for pn, p in self.named_parameters():\n-                        fpn = \"%s.%s\" % (mn, pn) if mn else pn\n+                        fpn = f\"{mn}.{pn}\" if mn else pn\n                         self.names.append(fpn)\n \n         # Test plain recurse\n@@ -5032,11 +5032,11 @@ def test_compute_exception_table_nested(self):\n             (15, 16, 7),\n             (17, 17, 6),\n         ]\n-        self.assertEquals(len(tab), len(expected))\n+        self.assertEqual(len(tab), len(expected))\n         for entry, exp in zip(tab, expected):\n-            self.assertEquals(entry.start, exp[0] * 2)\n-            self.assertEquals(entry.end, exp[1] * 2)\n-            self.assertEquals(entry.target, exp[2] * 2)\n+            self.assertEqual(entry.start, exp[0] * 2)\n+            self.assertEqual(entry.end, exp[1] * 2)\n+            self.assertEqual(entry.target, exp[2] * 2)\n \n     @skipIfNotPy311\n     def test_remove_dead_code_with_exn_table_entries(self):\n@@ -5060,17 +5060,17 @@ def test_remove_dead_code_with_exn_table_entries(self):\n         )\n         bytecode_transformation.propagate_inst_exn_table_entries(insts)\n         insts = bytecode_analysis.remove_dead_code(insts)\n-        self.assertEquals(len(insts), 5)\n+        self.assertEqual(len(insts), 5)\n         self.assertNotIn(exn_start, insts)\n         self.assertNotIn(exn_end, insts)\n         self.assertIn(target2, insts)\n         self.assertIn(target3, insts)\n         bytecode_transformation.update_offsets(insts)\n         tab = bytecode_transformation.compute_exception_table(insts)\n-        self.assertEquals(len(tab), 1)\n-        self.assertEquals(tab[0].start, 2)\n-        self.assertEquals(tab[0].end, 4)\n-        self.assertEquals(tab[0].target, 6)\n+        self.assertEqual(len(tab), 1)\n+        self.assertEqual(tab[0].start, 2)\n+        self.assertEqual(tab[0].end, 4)\n+        self.assertEqual(tab[0].target, 6)\n \n     def test_unhandled_exception_in_dynamo(self):\n         # traceback.format_exc() approximates an unhandled exception\n@@ -5757,7 +5757,7 @@ def guard(L):\n     def test_dynamo_compiling_fake_tensor_to_vararg_int(self):\n         class MyModule(torch.nn.Module):\n             def __init__(self):\n-                super(MyModule, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 # use numpy int so it's wrapped as fake tensor in dynamo\n@@ -5776,7 +5776,7 @@ def forward(self, x):\n     def test_scalar_tensor_is_equivalent_to_symint_argument(self):\n         class GumbelTopKSampler(torch.nn.Module):\n             def __init__(self, T, k):\n-                super(GumbelTopKSampler, self).__init__()\n+                super().__init__()\n                 self.T = torch.nn.Parameter(\n                     torch.tensor(T, dtype=torch.float32), requires_grad=False\n                 )\n@@ -5803,7 +5803,7 @@ def forward(self, logits):\n     def test_scalar_tensor_is_equivalent_to_symint_list_argument(self):\n         class Jitter(torch.nn.Module):\n             def __init__(self, jitter_val):\n-                super(Jitter, self).__init__()\n+                super().__init__()\n                 self.jitter_val = jitter_val\n \n             def roll_tensor(self, input):\n@@ -5986,7 +5986,7 @@ def _prepare_for_translation_validator(self):\n \n         # Z3 symbols.\n         [validator.add_var(s, int) for s in (s0, s1, s2)]\n-        z0, z1, z2 = [validator.z3var(s) for s in (s0, s1, s2)]\n+        z0, z1, z2 = (validator.z3var(s) for s in (s0, s1, s2))\n \n         return (s0, s1, s2), (z0, z1, z2), validator\n \ndiff --git a/test/dynamo/test_modules.py b/test/dynamo/test_modules.py\nindex 03ef4f07305454..5a881d0053ec1d 100644\n--- a/test/dynamo/test_modules.py\n+++ b/test/dynamo/test_modules.py\n@@ -762,7 +762,7 @@ def forward(self, x):\n \n class ConvCallSuperForwardDirectly(torch.nn.Conv1d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -770,13 +770,13 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n         )\n \n     def forward(self, inputs, mask=None):\n-        outputs = super(ConvCallSuperForwardDirectly, self).forward(inputs)\n+        outputs = super().forward(inputs)\n         return outputs\n \n \n class ConvTransposeCallSuperForwardDirectly(torch.nn.ConvTranspose2d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvTransposeCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -785,7 +785,7 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n \n     def forward(self, x):\n         if x.numel() > 0:\n-            return super(ConvTransposeCallSuperForwardDirectly, self).forward(x)\n+            return super().forward(x)\n         output_shape = [\n             ((i - 1) * d - 2 * p + (di * (k - 1) + 1) + op)\n             for i, p, di, k, d, op in zip(\n@@ -923,7 +923,7 @@ def forward(self, x):\n class SequentialWithDuplicatedModule(torch.nn.Module):\n     # Sequential module(self.layer) contains three duplicated ReLU module.\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             torch.nn.Linear(10, 20),\n@@ -940,7 +940,7 @@ def forward(self, x):\n \n class SequentialWithDuplicatedModule2(torch.nn.Module):\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule2, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             collections.OrderedDict(\ndiff --git a/test/dynamo/test_profiler.py b/test/dynamo/test_profiler.py\nindex 7f58d99863d093..bec7adb33eda98 100644\n--- a/test/dynamo/test_profiler.py\n+++ b/test/dynamo/test_profiler.py\n@@ -20,7 +20,7 @@ def inner_fn(x):\n         def outer_fn(x, y):\n             return inner_fn(x) * y\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             outer_fn(x, y)\n@@ -40,7 +40,7 @@ def test_dynamo_timed_profiling_backend_compile(self):\n         def fn(x, y):\n             return x.sin() * y.cos()\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             torch._dynamo.optimize(\"aot_eager\")(fn)(x, y)\ndiff --git a/test/dynamo/test_repros.py b/test/dynamo/test_repros.py\nindex 2e84776ea76580..77d8859541472c 100644\n--- a/test/dynamo/test_repros.py\n+++ b/test/dynamo/test_repros.py\n@@ -2632,7 +2632,7 @@ def test_error_return_without_exception_set(self):\n         # https://github.com/pytorch/pytorch/issues/93781\n         @torch.compile\n         def f():\n-            _generator_type = type((_ for _ in ()))\n+            _generator_type = type(_ for _ in ())\n \n         self.assertNoUnraisable(f)\n \ndiff --git a/test/error_messages/storage.py b/test/error_messages/storage.py\nindex f3053d862a220c..b33b86e0908a95 100644\n--- a/test/error_messages/storage.py\n+++ b/test/error_messages/storage.py\n@@ -14,7 +14,7 @@ def check_error(desc, fn, *required_substrings):\n         for sub in required_substrings:\n             assert sub in error_message\n         return\n-    raise AssertionError(\"given function ({}) didn't raise an error\".format(desc))\n+    raise AssertionError(f\"given function ({desc}) didn't raise an error\")\n \n check_error(\n     'Wrong argument types',\ndiff --git a/test/export/test_db.py b/test/export/test_db.py\nindex 10d149e096be9d..bfa57baf214c8f 100644\n--- a/test/export/test_db.py\n+++ b/test/export/test_db.py\n@@ -23,7 +23,7 @@ class ExampleTests(TestCase):\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.SUPPORTED).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -51,7 +51,7 @@ def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.NOT_SUPPORTED_YET).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -73,7 +73,7 @@ def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n             ).items()\n             for rewrite_case in get_rewrite_cases(case)\n         ],\n-        name_fn=lambda name, case: \"case_{}_{}\".format(name, case.name),\n+        name_fn=lambda name, case: f\"case_{name}_{case.name}\",\n     )\n     def test_exportdb_not_supported_rewrite(\n         self, name: str, rewrite_case: ExportCase\ndiff --git a/test/export/test_serialize.py b/test/export/test_serialize.py\nindex 01bb32ad2c791b..c3942936e2bc55 100644\n--- a/test/export/test_serialize.py\n+++ b/test/export/test_serialize.py\n@@ -361,7 +361,7 @@ def f(x, y):\n     @parametrize(\n         \"name,case\",\n         get_filtered_export_db_tests(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\ndiff --git a/test/forward_backward_compatibility/check_forward_backward_compatibility.py b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\nindex 886ad32a24bafb..cf4ce8def1adb7 100644\n--- a/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n+++ b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n@@ -501,7 +501,7 @@ def check_fc(existing_schemas):\n     args = parser.parse_args()\n     existing_schema_dict = {}\n     slist = []\n-    with open(args.existing_schemas, \"r\") as f:\n+    with open(args.existing_schemas) as f:\n         while True:\n             line = f.readline()\n             if not line:\ndiff --git a/test/functorch/test_aotdispatch.py b/test/functorch/test_aotdispatch.py\nindex e4357ce0bcafd8..ca92201fedc5ea 100644\n--- a/test/functorch/test_aotdispatch.py\n+++ b/test/functorch/test_aotdispatch.py\n@@ -1805,8 +1805,8 @@ def test_batch_norm_amp(self):\n         device = \"cuda\"\n         input_dtype = torch.float16\n         param_dtype = torch.float32\n-        weight, bias = [torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2)]\n-        running_mean, running_var = [torch.ones(64, device=device, dtype=param_dtype) for _ in range(2)]\n+        weight, bias = (torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2))\n+        running_mean, running_var = (torch.ones(64, device=device, dtype=param_dtype) for _ in range(2))\n \n         def bn(x):\n             return torch.ops.aten.cudnn_batch_norm(\ndiff --git a/test/functorch/test_vmap.py b/test/functorch/test_vmap.py\nindex a0f6e077004344..81b980edd6d006 100644\n--- a/test/functorch/test_vmap.py\n+++ b/test/functorch/test_vmap.py\n@@ -3438,7 +3438,7 @@ def test():\n             check_shape_only = op.name in ('empty_like', 'new_empty')\n             for sample_input in sample_inputs_itr:\n                 args = (sample_input.input,) + sample_input.args\n-                if not any((isinstance(arg, torch.Tensor) for arg in args)):\n+                if not any(isinstance(arg, torch.Tensor) for arg in args):\n                     # Atleast one tensor required for vmap.\n                     continue\n                 kwargs = sample_input.kwargs\ndiff --git a/test/fx/test_future.py b/test/fx/test_future.py\nindex 4f093de54b4f84..4525f678eaeb6c 100644\n--- a/test/fx/test_future.py\n+++ b/test/fx/test_future.py\n@@ -16,7 +16,7 @@ def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n \n # Forward references\n class M2(torch.nn.Module):\n-    def forward(self, x: 'torch.Tensor', a: 'A') -> 'torch.Tensor':\n+    def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n         return a(x)\n \n # Non-torch annotation with no internal forward references\n@@ -26,7 +26,7 @@ def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n \n # Non-torch annotation with internal forward references\n class M4(torch.nn.Module):\n-    def forward(self, x: typing.List['torch.Tensor'], a: A) -> 'torch.Tensor':\n+    def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n         return a(x[0])\n \n x = torch.rand(2, 3)\ndiff --git a/test/fx/test_gradual_type.py b/test/fx/test_gradual_type.py\nindex 23c6496b3a294f..e3f83756eb2668 100644\n--- a/test/fx/test_gradual_type.py\n+++ b/test/fx/test_gradual_type.py\n@@ -990,12 +990,12 @@ def forward(self, x : TensorType((4, 3, Dyn, Dyn))):\n \n         for n in traced.graph.nodes:\n             if n.target == 'conv1':\n-                assert n.type == TensorType((4, 6, sympy.floor((sympy.symbols('~0') - 4)),\n-                                             sympy.floor((sympy.symbols('~1') - 4))))\n+                assert n.type == TensorType((4, 6, sympy.floor(sympy.symbols('~0') - 4),\n+                                             sympy.floor(sympy.symbols('~1') - 4)))\n \n             elif n.target == 'conv2':\n-                assert n.type == TensorType((4, 16, sympy.floor((sympy.symbols('~4') - 4)),\n-                                             sympy.floor((sympy.symbols('~5') - 4))))\n+                assert n.type == TensorType((4, 16, sympy.floor(sympy.symbols('~4') - 4),\n+                                             sympy.floor(sympy.symbols('~5') - 4)))\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py\nindex 70c69ff5cef47a..f6b268abc72a3f 100644\n--- a/torch/_decomp/decompositions.py\n+++ b/torch/_decomp/decompositions.py\n@@ -1331,10 +1331,10 @@ def native_layer_norm_backward(\n     input_shape = input.shape\n     input_ndim = input.dim()\n     computation_dtype = utils.get_computation_dtype(input.dtype)\n-    grad_out_cast, input_cast, weight_cast, bias_cast = [\n+    grad_out_cast, input_cast, weight_cast, bias_cast = (\n         x.to(computation_dtype).contiguous() if x is not None else x\n         for x in (grad_out, input, weight, bias)\n-    ]\n+    )\n     assert grad_out_cast is not None\n \n     axis = input_ndim - len(normalized_shape)\n@@ -1745,7 +1745,7 @@ def native_batch_norm_backward(\n         running_var_cast,\n         save_mean_cast,\n         save_invstd_cast,\n-    ) = [\n+    ) = (\n         x.to(computation_dtype) if x is not None else x\n         for x in (\n             grad_out,\n@@ -1756,7 +1756,7 @@ def native_batch_norm_backward(\n             save_mean,\n             save_invstd,\n         )\n-    ]\n+    )\n     input_shape = input.shape\n     input_rank = input.dim()\n     assert input_rank >= 2, \"rank of the input must be at least 2\"\n@@ -3123,7 +3123,7 @@ def get_coeff(ofs: int) -> Tensor:\n             )\n             return _upsample_cubic_interp1d(cs, tx.unsqueeze(1))\n \n-        coeffs = tuple((get_coeff(ofs) for ofs in range(4)))\n+        coeffs = tuple(get_coeff(ofs) for ofs in range(4))\n         return _upsample_cubic_interp1d(coeffs, ty.unsqueeze(1))\n \n \n@@ -3371,10 +3371,10 @@ def load_bounded(ys, xs):\n         return aten._unsafe_index(a, [N_idx, C_idx, y_idx, x_idx])\n \n     def get_x_interp(y):\n-        coeffs_x = tuple((load_bounded(y, x_ofs) for x_ofs in ixs_ofs))\n+        coeffs_x = tuple(load_bounded(y, x_ofs) for x_ofs in ixs_ofs)\n         return _upsample_cubic_interp1d(coeffs_x, t_x)\n \n-    coeffs_y = tuple((get_x_interp(y_ofs) for y_ofs in iys_ofs))\n+    coeffs_y = tuple(get_x_interp(y_ofs) for y_ofs in iys_ofs)\n     result = _upsample_cubic_interp1d(coeffs_y, t_y)\n \n     # convert output to correct memory format, if necessary\ndiff --git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py\nindex 4d7c98aa222536..bf7a61b0793654 100644\n--- a/torch/_dynamo/debug_utils.py\n+++ b/torch/_dynamo/debug_utils.py\n@@ -385,11 +385,9 @@ def same_two_models(\n         # This means that the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return True.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph.\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph.\"\n         )\n         return True\n \n@@ -465,11 +463,9 @@ def backend_accuracy_fails(\n         # This means that the the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return False.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph\"\n         )\n         return False\n \ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..0bb1dbf7342a44 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -750,9 +750,7 @@ def __init__(\n \n         self.new_args = []\n         for i in range(0, len(flat_args)):\n-            arg = super(FlattenInputOutputSignature, self).placeholder(\n-                f\"arg{i}\", (), {}\n-            )\n+            arg = super().placeholder(f\"arg{i}\", (), {})\n             if i in matched_input_elements_to_fake:\n                 arg.node.meta[\"val\"] = matched_input_elements_to_fake[i]\n             else:\ndiff --git a/torch/_dynamo/output_graph.py b/torch/_dynamo/output_graph.py\nindex f6527f9b2de356..667b2363cff694 100644\n--- a/torch/_dynamo/output_graph.py\n+++ b/torch/_dynamo/output_graph.py\n@@ -1073,7 +1073,7 @@ class SubgraphTracer(fx.Tracer):\n     \"\"\"\n \n     def __init__(self, output_graph, parent=None):\n-        super(SubgraphTracer, self).__init__()\n+        super().__init__()\n         self.output_graph = weakref.proxy(output_graph)\n         self.graph = torch.fx.Graph()\n         # Map from graph input name to its placeholder proxy object, where the\ndiff --git a/torch/_dynamo/resume_execution.py b/torch/_dynamo/resume_execution.py\nindex a3344f3d69bca8..f3eafba1979470 100644\n--- a/torch/_dynamo/resume_execution.py\n+++ b/torch/_dynamo/resume_execution.py\n@@ -490,13 +490,13 @@ def find_new_offset(\n             instructions: List[Instruction], code_options: Dict[str, Any]\n         ):\n             nonlocal new_offset\n-            (target,) = [i for i in instructions if i.offset == offset]\n+            (target,) = (i for i in instructions if i.offset == offset)\n             # match the functions starting at the last instruction as we have added a prefix\n-            (new_target,) = [\n+            (new_target,) = (\n                 i2\n                 for i1, i2 in zip(reversed(instructions), reversed(meta.instructions))\n                 if i1 is target\n-            ]\n+            )\n             assert target.opcode == new_target.opcode\n             new_offset = new_target.offset\n \ndiff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py\nindex 75a44965b63a3e..46d7526ebbc57d 100644\n--- a/torch/_dynamo/symbolic_convert.py\n+++ b/torch/_dynamo/symbolic_convert.py\n@@ -888,7 +888,7 @@ def resolve_name(self, name, package, level):\n         if len(bits) < level:\n             raise ImportError(\"attempted relative import beyond top-level package\")\n         base = bits[0]\n-        return \"{}.{}\".format(base, name) if name else base\n+        return f\"{base}.{name}\" if name else base\n \n     def calc_package(self):\n         \"\"\"\n@@ -1840,7 +1840,7 @@ def format_frame_summary(self, additional_stack_frames=None):\n             additional_stack_frames = []\n         return \"\".join(\n             traceback.format_list(\n-                ([self.frame_summary()] + list(reversed(additional_stack_frames)))\n+                [self.frame_summary()] + list(reversed(additional_stack_frames))\n             )\n         )\n \ndiff --git a/torch/_dynamo/test_minifier_common.py b/torch/_dynamo/test_minifier_common.py\nindex 757e92d2f23b51..e1eadd6da8a595 100644\n--- a/torch/_dynamo/test_minifier_common.py\n+++ b/torch/_dynamo/test_minifier_common.py\n@@ -86,7 +86,7 @@ def _maybe_subprocess_run(self, args, *, isolate, cwd=None):\n                 args = [\"-c\"]\n             else:\n                 assert len(args) >= 2, args\n-                with open(args[1], \"r\") as f:\n+                with open(args[1]) as f:\n                     code = f.read()\n                 args = args[1:]\n \n@@ -156,7 +156,7 @@ def _run_test_code(self, code, *, isolate):\n     def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n         self.assertIsNotNone(repro_dir)\n         launch_file = os.path.join(repro_dir, \"minifier_launcher.py\")\n-        with open(launch_file, \"r\") as f:\n+        with open(launch_file) as f:\n             launch_code = f.read()\n         self.assertTrue(os.path.exists(launch_file))\n \n@@ -175,7 +175,7 @@ def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n     def _run_repro(self, repro_dir, *, isolate=True):\n         self.assertIsNotNone(repro_dir)\n         repro_file = os.path.join(repro_dir, \"repro.py\")\n-        with open(repro_file, \"r\") as f:\n+        with open(repro_file) as f:\n             repro_code = f.read()\n         self.assertTrue(os.path.exists(repro_file))\n \ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex d2aab5a65bd4d4..c720a0e637c478 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -368,12 +368,10 @@ def _wrap(self, value):\n         elif istype(\n             value, (dict, collections.defaultdict, collections.OrderedDict)\n         ) and all(\n-            (\n-                ConstantVariable.is_literal(k)\n-                or self.tensor_can_be_dict_key(k)\n-                or isinstance(k, enum.Enum)\n-                for k in value.keys()\n-            )\n+            ConstantVariable.is_literal(k)\n+            or self.tensor_can_be_dict_key(k)\n+            or isinstance(k, enum.Enum)\n+            for k in value.keys()\n         ):\n             if not value and self.get_source().is_nn_module():\n                 # It is faster to guard on 'false' property than to guard\ndiff --git a/torch/_dynamo/variables/misc.py b/torch/_dynamo/variables/misc.py\nindex b260eab0af0fdb..e049ccfb25269d 100644\n--- a/torch/_dynamo/variables/misc.py\n+++ b/torch/_dynamo/variables/misc.py\n@@ -880,7 +880,7 @@ def as_proxy(self):\n # Used to keep track of NULLs pushed on the stack for Python 3.11 function calls\n class NullVariable(VariableTracker):\n     def __init__(self, **kwargs):\n-        super(NullVariable, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n     def __str__(self):\n         return \"NullVariable\"\ndiff --git a/torch/_export/verifier.py b/torch/_export/verifier.py\nindex 41888230e02242..73906bb6a0d7a8 100644\n--- a/torch/_export/verifier.py\n+++ b/torch/_export/verifier.py\n@@ -38,7 +38,7 @@ def _check_is_fake_tensor(val):\n \n     val = node.meta.get(\"val\", None)\n     if val is None or not _check_is_fake_tensor(val):\n-        raise SpecViolationError(\"Node.meta {} is missing val field.\".format(node.name))\n+        raise SpecViolationError(f\"Node.meta {node.name} is missing val field.\")\n \n \n @compatibility(is_backward_compatible=False)\n@@ -71,7 +71,7 @@ def check_valid_op(self, op):\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         # All ops functional\n@@ -87,7 +87,7 @@ def check_valid(self, gm: GraphModule) -> None:  # noqa: C901\n             # TODO(T140410192): should have fake tensor for all dialects\n             if node.op in {\"call_module\", \"call_method\"}:\n                 raise SpecViolationError(\n-                    \"call_module is not valid: got a class '{}' \".format(node.target),\n+                    f\"call_module is not valid: got a class '{node.target}' \",\n                 )\n \n             if node.op == \"call_function\":\n@@ -122,7 +122,7 @@ def check_valid_op(self, op) -> None:\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         if (\ndiff --git a/torch/_functorch/eager_transforms.py b/torch/_functorch/eager_transforms.py\nindex a25a7bc456bec1..4ba72eb2152c6b 100644\n--- a/torch/_functorch/eager_transforms.py\n+++ b/torch/_functorch/eager_transforms.py\n@@ -548,7 +548,7 @@ def compute_jacobian_stacked():\n             # Iterate and concat the jacobians of different\n             # inputs.\n             for idx in range(len(flat_primals)):\n-                r = tuple((r_[idx] for r_ in chunked_results))\n+                r = tuple(r_[idx] for r_ in chunked_results)\n                 flat_results.append(torch.cat(r, 0))\n \n             return flat_results\ndiff --git a/torch/_functorch/pytree_hacks.py b/torch/_functorch/pytree_hacks.py\nindex 3694a53d7debb0..61bcdfbbf38b16 100644\n--- a/torch/_functorch/pytree_hacks.py\n+++ b/torch/_functorch/pytree_hacks.py\n@@ -13,7 +13,7 @@ def tree_map_(fn_, pytree):\n     return pytree\n \n \n-class PlaceHolder():\n+class PlaceHolder:\n     def __repr__(self):\n         return '*'\n \ndiff --git a/torch/_prims/__init__.py b/torch/_prims/__init__.py\nindex 7e8a37da76b06d..ac447dab410a07 100644\n--- a/torch/_prims/__init__.py\n+++ b/torch/_prims/__init__.py\n@@ -1437,7 +1437,7 @@ def expand_dims(\n     else:\n         dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]\n     if len(set(dims)) != len(dims):\n-        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))\n+        msg = f\"Received duplicate dimensions to expand in {str(dimensions)}\"\n         raise ValueError(msg)\n \n     new_shape = list(a.shape)\n@@ -1463,35 +1463,33 @@ def _slice_meta(\n     _strides = strides if strides is not None else [1] * len(start_indices)\n \n     if a.ndim != len(start_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with start_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with start_indices of length {}!\".format(\n             a.ndim, len(start_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(limit_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with limit_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with limit_indices of length {}!\".format(\n             a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(_strides):\n-        msg = (\n-            \"Attempting to slice tensor of rank {0} with strides of length {1}!\".format(\n-                a.ndim, len(limit_indices)\n-            )\n+        msg = \"Attempting to slice tensor of rank {} with strides of length {}!\".format(\n+            a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     for x, y in zip(start_indices, a.shape):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative start index of {0}!\".format(\n+            msg = \"Attempting to slice a tensor with a negative start index of {}!\".format(\n                 x\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than\"\n-                \" the length of its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a start index in {} is greater than\"\n+                \" the length of its corresponding dimension in shape {}\".format(\n                     start_indices, a.shape\n                 )\n             )\n@@ -1499,30 +1497,30 @@ def _slice_meta(\n \n     for x, y, z in zip(limit_indices, a.shape, start_indices):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative stop index of {0}!\".format(\n-                x\n+            msg = (\n+                \"Attempting to slice a tensor with a negative stop index of {}!\".format(\n+                    x\n+                )\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a stop index in {0} is greater than the length of \"\n-                \" its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a stop index in {} is greater than the length of \"\n+                \" its corresponding dimension in shape {}\".format(\n                     limit_indices, a.shape\n                 )\n             )\n             raise ValueError(msg)\n         if x < z:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than \"\n-                \" its corresponding stop index {1}\".format(x, z)\n+                \"Attempting to slice a tensor but a start index in {} is greater than \"\n+                \" its corresponding stop index {}\".format(x, z)\n             )\n \n     for x in _strides:\n         if x <= 0:\n-            msg = (\n-                \"Attempting to slice a tensor with a non-positive step of {0}!\".format(\n-                    x\n-                )\n+            msg = \"Attempting to slice a tensor with a non-positive step of {}!\".format(\n+                x\n             )\n             raise ValueError(msg)\n \n@@ -1581,38 +1579,38 @@ def _slice_in_dim_meta(\n     axis: int = 0,\n ) -> TensorLikeType:\n     if axis < 0:\n-        msg = \"slice_in_dim: received a negative axis {0}\".format(axis)\n+        msg = f\"slice_in_dim: received a negative axis {axis}\"\n         raise ValueError(msg)\n     if axis >= a.ndim:\n-        msg = \"slice_in_dim: axis {0} is greater or equal to the rank {1} of the tensor\".format(\n+        msg = \"slice_in_dim: axis {} is greater or equal to the rank {} of the tensor\".format(\n             axis, a.ndim\n         )\n         raise ValueError(msg)\n \n     if start_index < 0:\n-        msg = \"slice_in_dim: received a negative start_index {0}\".format(start_index)\n+        msg = f\"slice_in_dim: received a negative start_index {start_index}\"\n         raise ValueError(msg)\n \n     if start_index > a.shape[axis]:\n-        msg = \"slice_in_dim: start_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: start_index is greater than the length {} of dimension {}\".format(\n             start_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index > a.shape[axis]:\n-        msg = \"slice_in_dim: limit_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: limit_index is greater than the length {} of dimension {}\".format(\n             limit_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index < start_index:\n-        msg = \"slice_in_dim: received a limit_index {0} less than the start_index {1}\".format(\n+        msg = \"slice_in_dim: received a limit_index {} less than the start_index {}\".format(\n             limit_index, start_index\n         )\n         raise ValueError(msg)\n \n     if stride < 0:\n-        msg = \"slice_in_dim: received a non-positive stride of {0}!\".format(stride)\n+        msg = f\"slice_in_dim: received a non-positive stride of {stride}!\"\n         raise ValueError(msg)\n \n     start_indices = [0] * a.ndim\n@@ -1667,7 +1665,7 @@ def _split_dim_meta(a: TensorLikeType, dim: int, outer_length: int) -> TensorLik\n     inner_length = a.shape[dim] // outer_length\n \n     if (a.shape[dim] % outer_length) != 0:\n-        msg = \"Attempting to split dimension of length {0}, but outer length of {1} divides it with a remainder!\".format(\n+        msg = \"Attempting to split dimension of length {}, but outer length of {} divides it with a remainder!\".format(\n             a.shape[dim], outer_length\n         )\n         raise ValueError(msg)\n@@ -1746,13 +1744,13 @@ def _squeeze_meta(a: TensorLikeType, dimensions: Sequence) -> TensorLikeType:\n \n def _transpose_meta(a: TensorLikeType, permutation: DimsSequenceType) -> TensorLikeType:\n     if a.ndim != len(permutation):\n-        msg = \"Attempting to permute a tensor of rank {0}, but received a permutation of length {1}!\".format(\n+        msg = \"Attempting to permute a tensor of rank {}, but received a permutation of length {}!\".format(\n             a.ndim, len(permutation)\n         )\n         raise ValueError(msg)\n \n     if not utils.is_valid_permutation(a.ndim, permutation):\n-        msg = \"Received an invalid permutation, {0}!\".format(permutation)\n+        msg = f\"Received an invalid permutation, {permutation}!\"\n         raise ValueError(msg)\n \n     new_shape = [0] * a.ndim\n@@ -1938,7 +1936,7 @@ def _reshape_meta(a: TensorLikeType, shape: ShapeType):\n     # same number of elements\n     numel = reduce(operator.mul, shape)\n     if numel != a.numel():\n-        msg = \"Attempting to reshape a tensor with {0} elements to a shape with {1} elements!\".format(\n+        msg = \"Attempting to reshape a tensor with {} elements to a shape with {} elements!\".format(\n             a.numel(), numel\n         )\n         raise ValueError(msg)\n@@ -2190,7 +2188,7 @@ def _copy_to_meta(a: TensorLikeType, b: TensorLikeType):\n \n     # Validates the tensors have the same number of elements\n     if a.numel() != b.numel():\n-        msg = \"Attempting to copy {0} elements to a tensor with {1} elements!\".format(\n+        msg = \"Attempting to copy {} elements to a tensor with {} elements!\".format(\n             b.numel(), a.numel()\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_prims/executor.py b/torch/_prims/executor.py\nindex 2d8d815f063809..325ac67a665cc3 100644\n--- a/torch/_prims/executor.py\n+++ b/torch/_prims/executor.py\n@@ -28,7 +28,7 @@ def execute(\n     elif executor == \"strictly_nvfuser\":\n         return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)\n \n-    msg = \"Received unexpected value for 'executor': {0}. Allowed values are: aten, nvfuser.\".format(\n+    msg = \"Received unexpected value for 'executor': {}. Allowed values are: aten, nvfuser.\".format(\n         executor\n     )\n     raise ValueError(msg)\ndiff --git a/torch/_prims/nvfuser_executor.py b/torch/_prims/nvfuser_executor.py\nindex d0f51e928650c4..c1e61c1bb72f1f 100644\n--- a/torch/_prims/nvfuser_executor.py\n+++ b/torch/_prims/nvfuser_executor.py\n@@ -282,7 +282,7 @@ def nvfuser_execute(gm: GraphModule, *args, executor_parameters=None):\n \n         if get_nvprim_dump_nvtx():\n             torch.cuda.nvtx.range_push(\n-                \"fusion: {0}, graph: {1}\".format(\n+                \"fusion: {}, graph: {}\".format(\n                     fusion.id(),\n                     str(\n                         [\n@@ -475,7 +475,7 @@ def maybe_partition_graph(\n class NVTXInterpreter(torch.fx.Interpreter):\n     def run_node(self, n):\n         torch.cuda.nvtx.range_push(\n-            \"name: {0}, args: {1}, op: {2}, kwargs: {3}\".format(\n+            \"name: {}, args: {}, op: {}, kwargs: {}\".format(\n                 n.name, n.args, n.op, n.kwargs\n             )\n         )\ndiff --git a/torch/_prims_common/__init__.py b/torch/_prims_common/__init__.py\nindex f8033bb5780f74..4800966f3e2ff5 100644\n--- a/torch/_prims_common/__init__.py\n+++ b/torch/_prims_common/__init__.py\n@@ -120,11 +120,11 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n     assert isinstance(b, TensorLike)\n \n     if not same_shape(a.shape, b.shape):\n-        msg = \"Shapes {0} and {1} are not equal!\".format(a.shape, b.shape)\n+        msg = f\"Shapes {a.shape} and {b.shape} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.dtype != b.dtype:\n-        msg = \"Dtypes {0} and {1} are not equal!\".format(a.dtype, b.dtype)\n+        msg = f\"Dtypes {a.dtype} and {b.dtype} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.device != b.device:\n@@ -135,7 +135,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         ):\n             pass\n         else:\n-            msg = \"Devices {0} and {1} are not equal!\".format(a.device, b.device)\n+            msg = f\"Devices {a.device} and {b.device} are not equal!\"\n             raise AssertionError(msg)\n \n     # Stride checking is currently disabled, see https://github.com/pytorch/pytorch/issues/78050\n@@ -143,7 +143,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         same_strides, idx = check_significant_strides(a, b)\n         if not same_strides:\n             msg = (\n-                \"Stride mismatch! Strides are {0} and {1} (mismatched at {2})!\".format(\n+                \"Stride mismatch! Strides are {} and {} (mismatched at {})!\".format(\n                     a.stride(), b.stride(), idx\n                 )\n             )\n@@ -151,7 +151,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n \n         if a.storage_offset() != b.storage_offset():\n             msg = (\n-                \"Storage offset mismatch! Storage offsets are {0} and {1}!\".format(\n+                \"Storage offset mismatch! Storage offsets are {} and {}!\".format(\n                     a.storage_offset(), b.storage_offset()\n                 )\n             )\n@@ -584,7 +584,7 @@ def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int:\n \n     if _idx < 0 or _idx >= rank:\n         # Same error message as in aten/src/ATen/WrapDimUtils.h:49\n-        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(\n+        msg = \"Dimension out of range (expected to be in range of [{}, {}], but got {})\".format(\n             -rank, rank - 1, idx\n         )\n         raise IndexError(msg)\n@@ -710,7 +710,7 @@ def check_same_shape(*args, allow_cpu_scalar_tensors: bool):\n                 shape = arg.shape\n \n             if not is_same_shape(shape, arg.shape):\n-                msg = \"Shape {0} is not the expected shape {1}!\".format(\n+                msg = \"Shape {} is not the expected shape {}!\".format(\n                     arg.shape, shape\n                 )\n                 raise RuntimeError(msg)\n@@ -1102,7 +1102,7 @@ def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool:\n         if fn(cast_from):\n             return False\n \n-    raise ValueError(\"Received unknown dtypes {0}, {1}!\".format(cast_to, cast_from))\n+    raise ValueError(f\"Received unknown dtypes {cast_to}, {cast_from}!\")\n \n \n def check_same_dtype(*args):\n@@ -1340,7 +1340,7 @@ def elementwise_dtypes(\n     for x in args:\n         if not isinstance(x, (Number, TensorLike, sympy.Symbol)):\n             msg = (\n-                \"Unexpected type {0} when computing elementwise type promotion!\".format(\n+                \"Unexpected type {} when computing elementwise type promotion!\".format(\n                     str(type(x))\n                 )\n             )\n@@ -1424,7 +1424,7 @@ def _find_highest_dtype_filtered(\n         return get_computation_dtype(result_dtype), torch.bool\n     else:\n         raise ValueError(\n-            \"Unknown type promotion kind {0}\".format(str(type_promotion_kind))\n+            f\"Unknown type promotion kind {str(type_promotion_kind)}\"\n         )\n \n \n@@ -1648,8 +1648,8 @@ def check_in_bounds_for_storage(\n     required_length = compute_required_storage_length(shape, strides, storage_offset)\n     if a.size() < required_length:\n         msg = (\n-            \"Can't view a storage of size {0} with an offset of {1}, shape of {2}, and strides of {3}, \"\n-            \"which requires a storage of size {4}\".format(\n+            \"Can't view a storage of size {} with an offset of {}, shape of {}, and strides of {}, \"\n+            \"which requires a storage of size {}\".format(\n                 a.size(), storage_offset, str(shape), str(strides), required_length\n             )\n         )\n@@ -1671,9 +1671,9 @@ def check(\n     .. note:: This function is planned for removal in the future. Please use\n         `torch._check*` functions instead.\n     \"\"\"\n-    warnings.warn(DeprecationWarning((\n+    warnings.warn(DeprecationWarning(\n         \"'torch._prims_common.check' will be removed in the future. Please use \"\n-        \"'torch._check*' functions instead\")))\n+        \"'torch._check*' functions instead\"))\n     torch._check_with(exc_type, b, s)\n \n \ndiff --git a/torch/_prims_common/wrappers.py b/torch/_prims_common/wrappers.py\nindex 938465cac36318..c9755de3e0da63 100644\n--- a/torch/_prims_common/wrappers.py\n+++ b/torch/_prims_common/wrappers.py\n@@ -48,16 +48,16 @@ def _maybe_convert_to_dtype(a, dtype):\n         return None\n \n     raise ValueError(\n-        \"Received type {0} that is neither a tensor or a number!\".format(type(a))\n+        f\"Received type {type(a)} that is neither a tensor or a number!\"\n     )\n \n \n def _maybe_convert_to_type(a: NumberType, typ: type) -> NumberType:\n     if not isinstance(a, Number):\n-        msg = \"Found unknown type {0} when trying to convert scalars!\".format(type(a))\n+        msg = f\"Found unknown type {type(a)} when trying to convert scalars!\"\n         raise ValueError(msg)\n     if not utils.is_weakly_lesser_type(type(a), typ):\n-        msg = \"Scalar {0} of type {1} cannot be safely cast to type {2}!\".format(\n+        msg = \"Scalar {} of type {} cannot be safely cast to type {}!\".format(\n             a, type(a), typ\n         )\n         raise ValueError(msg)\n@@ -169,7 +169,7 @@ def _safe_copy_out(\n ):\n     # Checks same device\n     if copy_from.device != copy_to.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             copy_from.device, copy_to.device\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_refs/__init__.py b/torch/_refs/__init__.py\nindex 611c6e3b9a625a..53851d62a5eb44 100644\n--- a/torch/_refs/__init__.py\n+++ b/torch/_refs/__init__.py\n@@ -597,7 +597,7 @@ def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType:\n \n     python_type = utils.dtype_to_type(a.dtype)\n     if not utils.is_weakly_lesser_type(type(value), python_type):\n-        msg = \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+        msg = \"value argument of type {} cannot be safely cast to type {}!\".format(\n             type(value), python_type\n         )\n         raise ValueError(msg)\n@@ -997,10 +997,8 @@ def add(\n         if python_type != bool and not utils.is_weakly_lesser_type(\n             type(alpha), python_type\n         ):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         b = prims.mul(b, alpha)\n@@ -1069,7 +1067,7 @@ def copysign(\n     if isinstance(b, Number) and isinstance(a, Tensor):\n         b = scalar_tensor(b, dtype=a.dtype, device=a.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n-        msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+        msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n             a.device, b.device\n         )\n         raise RuntimeError(msg)\n@@ -1100,7 +1098,7 @@ def div(\n     else:\n         msg = (\n             \"div expected rounding_mode to be one of None, 'trunc', or 'floor' \"\n-            \"but found {0}.\".format(rounding_mode)\n+            \"but found {}.\".format(rounding_mode)\n         )\n         raise ValueError(msg)\n \n@@ -1218,7 +1216,7 @@ def floor_divide(\n         a = scalar_tensor(a, dtype=b.dtype, device=b.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n         if a.device == torch.device(\"cpu\"):\n-            msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+            msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n                 a.device, b.device\n             )\n             raise RuntimeError(msg)\n@@ -1378,19 +1376,19 @@ def _check_close_args(\n ) -> None:\n     torch._check_value(\n         a.dtype == b.dtype,\n-        lambda: \"{0}: Attempting to compare tensors of different dtypes {1} and {2}!\".format(\n+        lambda: \"{}: Attempting to compare tensors of different dtypes {} and {}!\".format(\n             name, a.dtype, b.dtype\n         ),\n     )\n     torch._check(\n         rtol >= 0,\n-        lambda: \"{0}: rtol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: rtol must be greater than or equal to zero, but got {}!\".format(\n             name, rtol\n         ),\n     )\n     torch._check(\n         atol >= 0,\n-        lambda: \"{0}: atol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: atol must be greater than or equal to zero, but got {}!\".format(\n             name, atol\n         ),\n     )\n@@ -1664,10 +1662,8 @@ def sub(\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         if isinstance(b, torch.Tensor):\n@@ -1759,7 +1755,7 @@ def addcdiv(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1788,7 +1784,7 @@ def addcmul(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1892,7 +1888,7 @@ def clone(\n \n def copy_to(a: Tensor, b: Tensor, *, allow_cross_device=True):\n     if not allow_cross_device and a.device != b.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             b.device, a.device\n         )\n         raise RuntimeError(msg)\n@@ -2098,7 +2094,7 @@ def _reduction(\n     assert isinstance(a, TensorLike)\n     if a.ndim > 64:\n         raise RuntimeError(\n-            \"Received a tensor with {0} dimensions, but only tensors with up to 64 dims are supported!\".format(\n+            \"Received a tensor with {} dimensions, but only tensors with up to 64 dims are supported!\".format(\n                 a.ndim\n             )\n         )\n@@ -2864,7 +2860,7 @@ def expand_as(a: Tensor, b: Tensor) -> Tensor:\n \n def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> Tuple[TensorLikeType, ...]:\n     if chunks <= 0:\n-        msg = \"Expected at least one chunk, but got {0}!\".format(chunks)\n+        msg = f\"Expected at least one chunk, but got {chunks}!\"\n         raise ValueError(msg)\n \n     dim = utils.canonicalize_dim(a.ndim, dim)\n@@ -3346,7 +3342,7 @@ def _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorL\n                 if allow_copy:\n                     return prims.reshape(a, shape)\n \n-                msg = \"Cannot view a tensor with shape {0} and strides {1} as a tensor with shape {2}!\".format(\n+                msg = \"Cannot view a tensor with shape {} and strides {} as a tensor with shape {}!\".format(\n                     a.shape, a.stride(), shape\n                 )\n                 raise ValueError(msg)\n@@ -3704,13 +3700,13 @@ def tensor_split(\n     # If indices_or_sections is a tensor, it must be a CPU Long tensor\n     if isinstance(indices_or_sections, TensorLike):\n         if not indices_or_sections.device.type == \"cpu\":\n-            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {0}\".format(\n+            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {}\".format(\n                 indices_or_sections.device\n             )\n             raise ValueError(msg)\n         if indices_or_sections.dtype != torch.long:\n             msg = \"tensor_split: if indices_or_sections is a tensor it must have long dtype, \"\n-            \" but received one with dtype {0}\".format(indices_or_sections.dtype)\n+            f\" but received one with dtype {indices_or_sections.dtype}\"\n             raise ValueError(msg)\n \n     # Case 0 -- indices_or_sections is an integer or a scalar tensor n and a is split along dim into n parts of equal-ish length\n@@ -3724,7 +3720,7 @@ def tensor_split(\n         )\n \n         if sections <= 0:\n-            msg = \"tensor_split: number of sections must be greater than 0, but was {0}\".format(\n+            msg = \"tensor_split: number of sections must be greater than 0, but was {}\".format(\n                 sections\n             )\n             raise ValueError(msg)\n@@ -3751,7 +3747,7 @@ def tensor_split(\n         if isinstance(indices_or_sections, TensorLike):\n             if indices_or_sections.ndim != 1:\n                 msg = \"tensor_split: non-scalar indices_or_sections tensors must have only one dimension, \"\n-                \"but received a tensor with {0} dimensions\".format(\n+                \"but received a tensor with {} dimensions\".format(\n                     indices_or_sections.ndim\n                 )\n                 raise ValueError(msg)\ndiff --git a/torch/_refs/nn/functional/__init__.py b/torch/_refs/nn/functional/__init__.py\nindex eaa6618379f356..ba00179c4b2d6f 100644\n--- a/torch/_refs/nn/functional/__init__.py\n+++ b/torch/_refs/nn/functional/__init__.py\n@@ -167,10 +167,8 @@ def celu(\n     if alpha is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         rhs = alpha * torch.expm1(torch.true_divide(a, alpha))  # type: ignore[arg-type]\n@@ -437,7 +435,7 @@ def softplus(\n     if beta is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(beta), python_type):\n-            msg = \"beta argument of type {0} cannot be safely cast to type {1}!\".format(\n+            msg = \"beta argument of type {} cannot be safely cast to type {}!\".format(\n                 type(beta), python_type\n             )\n             raise ValueError(msg)\n@@ -610,11 +608,9 @@ def margin_ranking_loss(\n     # loss_without_reduction = max(0, \u2212target * (input1 \u2212 input2) + margin)\n     if input1.ndim != input2.ndim or input1.ndim != target.ndim:\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(\n-                    input1.shape, input2.shape, target.shape\n-                )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(\n+                input1.shape, input2.shape, target.shape\n             )\n         )\n     _check_reduction_value(reduction)\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint.py b/torch/fx/experimental/migrate_gradual_types/constraint.py\nindex bab7c62347bbd9..0f0d23d0187490 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from torch.fx.experimental.migrate_gradual_types.operation import op_add, op_sub, op_mul, op_div, \\\n     op_mod, op_gt, op_lt, op_neq, op_eq\n from torch.fx.tensor_type import TensorType, Dyn\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\nindex fc1fae790d8300..153a8407fc4113 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n@@ -75,7 +75,7 @@ def transform_index_select(constraint, counter):\n     # if the index is valid then replace the input dimension with the new dimension\n     # otherwise the dimension will not be replaced and the clause will contain False\n     if is_valid_index == T():\n-        new_dims = copy.deepcopy((dims))\n+        new_dims = copy.deepcopy(dims)\n         new_dims[constraint.index] = constraint.dim_replace\n \n     transformed_constraint = Conj([BinConstraintT(constraint.input_var, TensorType(dims), op_eq),\n@@ -803,7 +803,7 @@ def apply_padding(e1_var: TVar,\n         broadcast_padding = []\n \n         # for every padding size, we also consider broadcasting\n-        for j in range((len(d2) - i)):\n+        for j in range(len(d2) - i):\n             broadcast_padding.append(broadcast_dim(simulate_padding, d2, d11, d12, j, True))\n \n         # we consider the possibilities for broadcasting for every dimension. Since we already\ndiff --git a/torch/fx/experimental/migrate_gradual_types/operation.py b/torch/fx/experimental/migrate_gradual_types/operation.py\nindex 68bba2d59a7608..ec2cb91bbcc179 100644\n--- a/torch/fx/experimental/migrate_gradual_types/operation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/operation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n op_add = '+'\n op_sub = '-'\n op_mul = '*'\ndiff --git a/torch/fx/experimental/symbolic_shapes.py b/torch/fx/experimental/symbolic_shapes.py\nindex 90a3c519b3c44b..70762cf1f81b8c 100644\n--- a/torch/fx/experimental/symbolic_shapes.py\n+++ b/torch/fx/experimental/symbolic_shapes.py\n@@ -1488,7 +1488,7 @@ def _print_Symbol(self, expr) -> str:\n         return self.print_source(self.symbol_to_source[expr][0])\n \n     def _print_Relational(self, expr):\n-        return '%s %s %s' % (\n+        return '{} {} {}'.format(\n             self.parenthesize(expr.lhs, precedence(expr)),\n             expr.rel_op,\n             self.parenthesize(expr.rhs, precedence(expr))\n@@ -1887,7 +1887,7 @@ def print_results(grouped, indent, result_fn):\n class ShapeEnvLoggerAdapter(logging.LoggerAdapter):\n     def process(self, msg, kwargs):\n         # TODO: Maybe suppress the envid if not DEBUG?\n-        return '%s: %s' % (self.extra['envid'], msg), kwargs\n+        return '{}: {}'.format(self.extra['envid'], msg), kwargs\n \n \n ENV_COUNTER = collections.Counter()\ndiff --git a/torch/fx/experimental/unification/multipledispatch/dispatcher.py b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\nindex c76d8c60b097c7..ac8bc7d8dd159c 100644\n--- a/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n+++ b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n@@ -205,10 +205,9 @@ def add(self, signature, func):\n             if not isinstance(typ, (type, list)):\n                 str_sig = ', '.join(c.__name__ if isinstance(c, type)\n                                     else str(c) for c in signature)\n-                raise TypeError(\"Tried to dispatch on non-type: %s\\n\"\n-                                \"In signature: <%s>\\n\"\n-                                \"In function: %s\" %\n-                                (typ, str_sig, self.name))\n+                raise TypeError(\"Tried to dispatch on non-type: {}\\n\"\n+                                \"In signature: <{}>\\n\"\n+                                \"In function: {}\".format(typ, str_sig, self.name))\n \n             # handle variadic signatures\n             if isinstance(typ, list):\n@@ -257,8 +256,7 @@ def __call__(self, *args, **kwargs):\n             func = self.dispatch(*types)\n             if not func:\n                 raise NotImplementedError(\n-                    'Could not find signature for %s: <%s>' %\n-                    (self.name, str_signature(types))) from e\n+                    f'Could not find signature for {self.name}: <{str_signature(types)}>') from e\n             self._cache[types] = func\n         try:\n             return func(*args, **kwargs)\n@@ -274,7 +272,7 @@ def __call__(self, *args, **kwargs):\n \n             raise NotImplementedError(\n                 \"Matching functions for \"\n-                \"%s: <%s> found, but none completed successfully\" % (\n+                \"{}: <{}> found, but none completed successfully\".format(\n                     self.name, str_signature(types),),) from e\n \n     def __str__(self):\n@@ -408,8 +406,7 @@ def __call__(self, *args, **kwargs):\n         types = tuple([type(arg) for arg in args])\n         func = self.dispatch(*types)\n         if not func:\n-            raise NotImplementedError('Could not find signature for %s: <%s>' %\n-                                      (self.name, str_signature(types)))\n+            raise NotImplementedError(f'Could not find signature for {self.name}: <{str_signature(types)}>')\n         return func(self.obj, *args, **kwargs)\n \n \ndiff --git a/torch/fx/interpreter.py b/torch/fx/interpreter.py\nindex 7bc5e55288b03c..6ee5706f92ee34 100644\n--- a/torch/fx/interpreter.py\n+++ b/torch/fx/interpreter.py\n@@ -139,7 +139,7 @@ def run(self, *args, initial_env : Optional[Dict[Node, Any]] = None, enable_io_p\n             except Exception as e:\n                 if self.extra_traceback:\n                     msg = f\"While executing {node.format_node()}\"\n-                    msg = '{}\\n\\n{}'.format(e.args[0], msg) if e.args else str(msg)\n+                    msg = f'{e.args[0]}\\n\\n{msg}' if e.args else str(msg)\n                     msg += f\"\\nOriginal traceback:\\n{node.stack_trace}\"\n                     e.args = (msg,) + e.args[1:]\n                     if isinstance(e, KeyError):\ndiff --git a/torch/fx/passes/utils/matcher_utils.py b/torch/fx/passes/utils/matcher_utils.py\nindex 1037b48d8eb61d..8b66561a6e280d 100644\n--- a/torch/fx/passes/utils/matcher_utils.py\n+++ b/torch/fx/passes/utils/matcher_utils.py\n@@ -30,7 +30,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class InternalMatch():\n+class InternalMatch:\n     # Nodes from which the match was found\n     anchors: List[Node]\n     # Maps nodes in the pattern subgraph to nodes in the larger graph\ndiff --git a/torch/fx/passes/utils/source_matcher_utils.py b/torch/fx/passes/utils/source_matcher_utils.py\nindex e00b9695742e36..da8cf9f0f168c4 100644\n--- a/torch/fx/passes/utils/source_matcher_utils.py\n+++ b/torch/fx/passes/utils/source_matcher_utils.py\n@@ -29,7 +29,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class SourcePartition():\n+class SourcePartition:\n     # Nodes in a particular partition\n     nodes: List[Node]\n \n"
  },
  {
    "number": 105411,
    "title": "[BE] Enable ruff's UP rules and autoformat inductor/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "32e9f9d27ebc4de960ebce69b5a6eae73bf1902a",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105411",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105411/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105411.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105411.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105411/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105411/comments",
    "labels": [
      "open source",
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T01:16:12.053031Z",
    "state": "closed",
    "patch": "From 21f789cdc79404f7ce97ce7770cb99f1d3098c7c Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:16:04 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat inductor/\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py                    |  2 +-\n test/inductor/test_cuda_repro.py                   |  2 +-\n test/inductor/test_cudagraph_trees.py              |  2 +-\n test/inductor/test_fused_attention.py              |  4 ++--\n test/inductor/test_mkldnn_pattern_matcher.py       |  8 ++++----\n test/inductor/test_profiler.py                     |  4 ++--\n test/inductor/test_standalone_compile.py           |  2 +-\n test/inductor/test_torchinductor.py                |  6 +++---\n .../test_torchinductor_codegen_dynamic_shapes.py   | 14 +++++++-------\n torch/_inductor/codecache.py                       |  6 +++---\n torch/_inductor/codegen/cpp.py                     |  2 +-\n torch/_inductor/ir.py                              | 10 +++++-----\n torch/_inductor/lowering.py                        |  6 +++---\n torch/_inductor/triton_heuristics.py               |  6 ++----\n torch/_inductor/utils.py                           |  2 +-\n 15 files changed, 37 insertions(+), 39 deletions(-)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 89ea4315b1de6c..2074d8451be9cc 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -93,7 +93,7 @@ def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n     def test_conv2d_bn_mixed_dtype(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     3,\n                     16,\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..7df54f0d35a146 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -783,7 +783,7 @@ def forward(inductor_seeds, mul_4, view_15):\n     def test_issue100806(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.linear1 = torch.nn.Linear(10, 20)\n                 self.linear2 = torch.nn.Linear(20, 30)\n                 self.relu = torch.nn.ReLU()\ndiff --git a/test/inductor/test_cudagraph_trees.py b/test/inductor/test_cudagraph_trees.py\nindex 85e44c1e272f9d..8181484c419b58 100644\n--- a/test/inductor/test_cudagraph_trees.py\n+++ b/test/inductor/test_cudagraph_trees.py\n@@ -137,7 +137,7 @@ def tearDown(self):\n \n         def get_manager(self, device_index=None):\n             return torch._inductor.cudagraph_trees.get_container(\n-                (self.device_idx if not device_index else device_index)\n+                self.device_idx if not device_index else device_index\n             ).tree_manager\n \n         def get_roots(self):\ndiff --git a/test/inductor/test_fused_attention.py b/test/inductor/test_fused_attention.py\nindex e509c97b0cb04a..7163ac24de5cf3 100644\n--- a/test/inductor/test_fused_attention.py\n+++ b/test/inductor/test_fused_attention.py\n@@ -297,7 +297,7 @@ def test_pattern_fails_with_tensor_factor(self):\n         # https://github.com/pytorch/pytorch/issues/99124\n         class Model(torch.nn.Module):\n             def __init__(self, is_inv_factor):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.is_inv_factor = is_inv_factor\n \n             def forward(self, query, key, value, scale_factor) -> torch.Tensor:\n@@ -328,7 +328,7 @@ class Model(torch.nn.Module):\n             def __init__(\n                 self,\n             ):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, query, key, value, attn_mask) -> torch.Tensor:\n                 attn_weight = torch.softmax(\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex 3e07c0181994cf..9337a70b2f857e 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -374,7 +374,7 @@ def forward(self, x, negative_slope):\n     def test_conv2d_add_scalar(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n                 )\n@@ -476,7 +476,7 @@ def forward(self, x, other, alpha):\n         # we can't do the fusion when add's inputs are same tensor.\n         class Model2(torch.nn.Module):\n             def __init__(self):\n-                super(Model2, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -490,7 +490,7 @@ def forward(self, x):\n         # we can't do the fusion when add's inputs are mixed dtype.\n         class Model3(torch.nn.Module):\n             def __init__(self):\n-                super(Model3, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -526,7 +526,7 @@ def forward(self, x):\n     def test_reproduce_99842_issue(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n \n             def forward(self, input_tensor):\ndiff --git a/test/inductor/test_profiler.py b/test/inductor/test_profiler.py\nindex eebe884da46b33..68f44df8b9f052 100644\n--- a/test/inductor/test_profiler.py\n+++ b/test/inductor/test_profiler.py\n@@ -21,14 +21,14 @@ def test_inductor_profiling_triton_launch(self):\n         def fn(x, y):\n             return (x + y).sin().cos()\n \n-        x, y = [torch.rand((4, 4), device=\"cuda\") for _ in range(2)]\n+        x, y = (torch.rand((4, 4), device=\"cuda\") for _ in range(2))\n \n         with torch.profiler.profile() as prof:\n             fn(x, y)\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with open(fname, \"r\") as f:\n+            with open(fname) as f:\n                 trace_json = json.load(f)\n \n         self.assertTrue(\"traceEvents\" in trace_json)\ndiff --git a/test/inductor/test_standalone_compile.py b/test/inductor/test_standalone_compile.py\nindex c424c76244cc0e..88c528c891a4fc 100644\n--- a/test/inductor/test_standalone_compile.py\n+++ b/test/inductor/test_standalone_compile.py\n@@ -100,7 +100,7 @@ def test_inductor_via_export2(self):\n     def test_inductor_via_op_with_multiple_outputs(self):\n         x1 = torch.randn((2, 512, 128))\n         x2 = [128]\n-        x3 = torch.randn((128))\n+        x3 = torch.randn(128)\n         x4 = torch.randn((128,))\n         x5 = 1e-6\n         mod, inp = gen_gm_and_inputs(\ndiff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py\nindex 520837d3865a8d..45ee70cd8067ee 100644\n--- a/test/inductor/test_torchinductor.py\n+++ b/test/inductor/test_torchinductor.py\n@@ -2354,7 +2354,7 @@ def fn(x):\n     def test_adaptive_avg_pool2d_low_prec(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n \n             def forward(self, x):\n@@ -5995,7 +5995,7 @@ def fn(x, y):\n \n         self.common(\n             fn,\n-            [torch.randn((4, 2)), torch.randn((4))],\n+            [torch.randn((4, 2)), torch.randn(4)],\n         )\n \n     # Shape padding causes the inputs to all get specialized, so the codegen\n@@ -6047,7 +6047,7 @@ def test_sqrt_dynamic_shapes(self):\n \n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 B, N, C = x.shape\ndiff --git a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\nindex 8f4086af099e64..0cdfbd54ffbe3b 100644\n--- a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n+++ b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n@@ -124,14 +124,14 @@ def run(*ex, **kwargs):\n     \"test_expand_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_glu_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_isinf2_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_layer_norm_dynamic_shapes\": TestFailure((\"cuda\")),\n+    \"test_layer_norm_dynamic_shapes\": TestFailure(\"cuda\"),\n     \"test_linspace1_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_stack_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor2_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor3_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_to_device_constant_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_to_device_constant_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_upsample_nearest2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views3_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views4_dynamic_shapes\": TestFailure((\"cpu\",)),\n@@ -161,9 +161,9 @@ def run(*ex, **kwargs):\n     \"test_empty2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_empty_strided_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_index3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_inductor_bucketize_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_inductor_bucketize_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_like_rands_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n@@ -194,7 +194,7 @@ def run(*ex, **kwargs):\n     \"test_views6_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_view_detach_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_view_on_aliased_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_linear_float64_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_linear_float64_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_adaptive_avg_pool_with_output_size_0_dynamic_shapes\": TestFailure(\n         (\"cpu\", \"cuda\")\n     ),\n@@ -288,7 +288,7 @@ def run(*ex, **kwargs):\n \n if TEST_WITH_ROCM:\n     # aten.miopen_batch_norm is not registered for lowering\n-    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure((\"cuda\"))\n+    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure(\"cuda\")\n \n DynamicShapesCodegenCommonTemplate = make_dynamic_cls(\n     CommonTemplate, xfail_prop=\"_expected_failure_codegen_dynamic\"\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex fd1f4228c9d29a..8ac73569fe1c2a 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -158,7 +158,7 @@ def __init__(self):\n     def get_local_cache(self):\n         if not self.local_cache_path.is_file():\n             return {}\n-        with open(self.local_cache_path, \"r\") as local_cache_fp:\n+        with open(self.local_cache_path) as local_cache_fp:\n             local_cache = json.load(local_cache_fp)\n         return local_cache[\"cache\"]\n \n@@ -201,7 +201,7 @@ class PersistentCache(CacheBase):\n     def get_global_cache(self):\n         if self.global_cache_path is None or not self.global_cache_path.is_file():\n             return {}\n-        with open(self.global_cache_path, \"r\") as global_cache_fp:\n+        with open(self.global_cache_path) as global_cache_fp:\n             global_cache = json.load(global_cache_fp)\n         return global_cache[\"cache\"]\n \n@@ -844,7 +844,7 @@ def wrapper_call(*args):\n # - valid_vec_isa_list()\n # - VecISA.__bool__() <-- takes out a lock\n # - compile_file() <-- imports cpp_prefix_path from cpp, which causes us to try to take out the same lock.\n-@functools.lru_cache()\n+@functools.lru_cache\n def cpp_prefix_path():\n     path = Path(__file__).parent / \"codegen/cpp_prefix.h\"\n     with path.open() as f:\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex bb2469ebffd33d..f844160b7255c0 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -278,7 +278,7 @@ def parallel_num_threads():\n     return threads\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def stride_at(var: sympy.Symbol, index: sympy.Expr):\n     replacement = {var: var + 1}\n     new_index = sympy_subs(index, replacement)\ndiff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py\nindex 869a0fb60861f6..c8172ff36b3dcf 100644\n--- a/torch/_inductor/ir.py\n+++ b/torch/_inductor/ir.py\n@@ -3042,7 +3042,7 @@ class InplaceBernoulliFallback(ExternKernel):\n     kernel = \"aten.bernoulli_\"\n \n     def codegen(self, wrapper):\n-        (x,) = [t.codegen_reference() for t in self.inputs]\n+        (x,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(\n             f\"{self.kernel}({x}, {', '.join(map(repr, self.constant_args))})\"\n         )\n@@ -3073,9 +3073,9 @@ class ScatterFallback(ExternKernel):\n \n     def codegen(self, wrapper):\n         if self.src_is_tensor:\n-            (x, index, src) = [t.codegen_reference() for t in self.inputs]\n+            (x, index, src) = (t.codegen_reference() for t in self.inputs)\n         else:\n-            (x, index) = [t.codegen_reference() for t in self.inputs]\n+            (x, index) = (t.codegen_reference() for t in self.inputs)\n             src = self.constant_args[1]\n         wrapper.generate_scatter_fallback(\n             x,\n@@ -3156,7 +3156,7 @@ class IndexPutFallback(ExternKernel):\n     \"\"\"\n \n     def codegen(self, wrapper):\n-        (x, values, *valid_indices) = [t.codegen_reference() for t in self.inputs]\n+        (x, values, *valid_indices) = (t.codegen_reference() for t in self.inputs)\n         indices = []\n         iter_valid_indices = iter(valid_indices)\n         for i, _ in enumerate(self.indices):\n@@ -4694,7 +4694,7 @@ def codegen(self, wrapper):\n         wrapper.add_import_once(\n             \"from torch.distributed._functional_collectives_impl import _wait_tensor\"\n         )\n-        (input_collective,) = [t.codegen_reference() for t in self.inputs]\n+        (input_collective,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(f\"{input_collective} = _wait_tensor({input_collective})\")\n \n         # wait op still needs to produce a 'buffer' that represents the tensor output.\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 3f17db1e5fed3a..14430ace100534 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -2871,11 +2871,11 @@ def load_bounded(fy, fx):\n \n         iy = ops.to_dtype(in_y, get_int_dtype(iH + 1))\n         ix = ops.to_dtype(in_x, get_int_dtype(iW + 1))\n-        iys_ofs = tuple((ops.add(iy, ofs) for ofs in (-1, 0, 1, 2)))\n-        ixs_ofs = tuple((ops.add(ix, ofs) for ofs in (-1, 0, 1, 2)))\n+        iys_ofs = tuple(ops.add(iy, ofs) for ofs in (-1, 0, 1, 2))\n+        ixs_ofs = tuple(ops.add(ix, ofs) for ofs in (-1, 0, 1, 2))\n \n         def get_x_interp(y):\n-            coeffs_x = tuple((load_bounded(y, x) for x in ixs_ofs))\n+            coeffs_x = tuple(load_bounded(y, x) for x in ixs_ofs)\n             return cubic_interp1d(coeffs_x, t_x)\n \n         coeffs_y = tuple(get_x_interp(y) for y in iys_ofs)\ndiff --git a/torch/_inductor/triton_heuristics.py b/torch/_inductor/triton_heuristics.py\nindex 61027661111e5a..88fa275f8b0102 100644\n--- a/torch/_inductor/triton_heuristics.py\n+++ b/torch/_inductor/triton_heuristics.py\n@@ -482,9 +482,7 @@ def hash_configs(configs: List[Config]):\n     hasher = hashlib.sha256()\n     for cfg in configs:\n         hasher.update(\n-            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode(\n-                \"utf-8\"\n-            )\n+            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode()\n         )\n     return hasher.hexdigest()\n \n@@ -498,7 +496,7 @@ def load_cached_autotuning(\n     if not os.path.exists(cache_filename):\n         return None\n \n-    with open(cache_filename, \"r\") as fd:\n+    with open(cache_filename) as fd:\n         best_config = json.loads(fd.read())\n     if best_config.pop(\"configs_hash\", None) != configs_hash:\n         return None\ndiff --git a/torch/_inductor/utils.py b/torch/_inductor/utils.py\nindex 538d0a2040fb93..c604c45d53d32a 100644\n--- a/torch/_inductor/utils.py\n+++ b/torch/_inductor/utils.py\n@@ -688,7 +688,7 @@ def run_and_get_code(fn, *args, **kwargs):\n \n     def patched_compile_to_module(self):\n         mod = compile_to_module(self)\n-        with open(mod.__file__, \"r\") as f:\n+        with open(mod.__file__) as f:\n             source_codes.append(f.read())\n         return mod\n \n"
  },
  {
    "number": 105410,
    "title": "[BE] Enable ruff's UP rules and autoformat ao/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "4e3695f87fd6631902323e09e4371ab6c150f222",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105410",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105410/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105410.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105410.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105410/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105410/comments",
    "labels": [
      "release notes: AO frontend",
      "open source",
      "release notes: quantization"
    ],
    "_event_time": "2023-07-18T01:16:05.866023Z",
    "state": "closed",
    "patch": "From 0cd76ed6a7232f923ca828c73da409a54de3b9bd Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:58 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat ao/\n\n[ghstack-poisoned]\n---\n .../ao/sparsity/test_activation_sparsifier.py |  1 -\n test/ao/sparsity/test_composability.py        |  1 -\n test/ao/sparsity/test_data_scheduler.py       |  1 -\n test/ao/sparsity/test_data_sparsifier.py      |  1 -\n test/ao/sparsity/test_kernels.py              |  1 -\n test/ao/sparsity/test_parametrization.py      |  1 -\n test/ao/sparsity/test_scheduler.py            |  1 -\n test/ao/sparsity/test_sparsifier.py           |  1 -\n test/ao/sparsity/test_sparsity_utils.py       |  1 -\n .../ao/sparsity/test_structured_sparsifier.py |  1 -\n torch/ao/nn/intrinsic/modules/fused.py        |  2 +-\n .../ao/nn/intrinsic/qat/modules/conv_fused.py | 12 +++----\n .../nn/intrinsic/qat/modules/linear_relu.py   |  2 +-\n .../quantized/dynamic/modules/linear_relu.py  |  2 +-\n .../nn/intrinsic/quantized/modules/bn_relu.py |  4 +--\n .../intrinsic/quantized/modules/conv_relu.py  |  6 ++--\n .../quantized/modules/linear_relu.py          |  2 +-\n torch/ao/nn/quantizable/modules/activation.py |  6 ++--\n torch/ao/nn/quantized/dynamic/modules/conv.py |  1 -\n .../ao/nn/quantized/dynamic/modules/linear.py |  2 +-\n torch/ao/nn/quantized/dynamic/modules/rnn.py  | 32 +++++++++----------\n torch/ao/nn/quantized/modules/__init__.py     |  2 +-\n torch/ao/nn/quantized/modules/conv.py         |  5 ++-\n torch/ao/nn/quantized/modules/linear.py       |  2 +-\n .../ao/nn/quantized/reference/modules/rnn.py  |  2 +-\n .../ao/nn/sparse/quantized/dynamic/linear.py  |  2 +-\n torch/ao/ns/fx/ns_types.py                    |  8 ++---\n .../data_scheduler/base_data_scheduler.py     |  4 +--\n torch/ao/pruning/scheduler/base_scheduler.py  |  4 +--\n torch/ao/pruning/scheduler/cubic_scheduler.py |  1 -\n .../quantization/_learnable_fake_quantize.py  |  4 +--\n .../backend_config/backend_config.py          |  3 +-\n .../ao/quantization/backend_config/onednn.py  | 12 +++----\n .../quantization/experimental/APoT_tensor.py  |  2 +-\n .../ao/quantization/experimental/quantizer.py |  2 +-\n torch/ao/quantization/fake_quantize.py        |  2 +-\n torch/ao/quantization/fuse_modules.py         |  2 +-\n .../ao/quantization/fuser_method_mappings.py  | 10 +++---\n torch/ao/quantization/fx/_equalize.py         |  2 +-\n .../fx/_lower_to_native_backend.py            |  2 +-\n .../quantization/fx/_model_report/detector.py |  8 ++---\n .../_model_report/model_report_visualizer.py  |  2 +-\n torch/ao/quantization/fx/convert.py           |  2 +-\n torch/ao/quantization/fx/custom_config.py     |  9 ++----\n torch/ao/quantization/fx/prepare.py           |  2 +-\n torch/ao/quantization/fx/utils.py             | 24 ++++++--------\n torch/ao/quantization/observer.py             | 18 +++++------\n torch/ao/quantization/pt2e/prepare.py         |  4 +--\n .../quantization/pt2e/quantizer/quantizer.py  | 21 ++----------\n torch/ao/quantization/qconfig.py              |  8 ++---\n .../ao/quantization/quantization_mappings.py  | 10 +++---\n torch/ao/quantization/quantize.py             |  2 +-\n torch/ao/quantization/utils.py                |  2 +-\n 53 files changed, 114 insertions(+), 150 deletions(-)\n\ndiff --git a/test/ao/sparsity/test_activation_sparsifier.py b/test/ao/sparsity/test_activation_sparsifier.py\nindex 573a40762c31cc..01bdfa045da9d1 100644\n--- a/test/ao/sparsity/test_activation_sparsifier.py\n+++ b/test/ao/sparsity/test_activation_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import copy\ndiff --git a/test/ao/sparsity/test_composability.py b/test/ao/sparsity/test_composability.py\nindex 85d78c49ea54ae..cb799f714ca17b 100644\n--- a/test/ao/sparsity/test_composability.py\n+++ b/test/ao/sparsity/test_composability.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_data_scheduler.py b/test/ao/sparsity/test_data_scheduler.py\nindex 9c33a160e76836..ab7c051c21077a 100644\n--- a/test/ao/sparsity/test_data_scheduler.py\n+++ b/test/ao/sparsity/test_data_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_data_sparsifier.py b/test/ao/sparsity/test_data_sparsifier.py\nindex 81a899f6932a0d..9248a371826ebd 100644\n--- a/test/ao/sparsity/test_data_sparsifier.py\n+++ b/test/ao/sparsity/test_data_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_kernels.py b/test/ao/sparsity/test_kernels.py\nindex 4786557ceb3be7..111d51465be109 100644\n--- a/test/ao/sparsity/test_kernels.py\n+++ b/test/ao/sparsity/test_kernels.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.testing._internal.common_utils import run_tests\ndiff --git a/test/ao/sparsity/test_parametrization.py b/test/ao/sparsity/test_parametrization.py\nindex 54b6f778d9fa8f..02f7cc6db7fddf 100644\n--- a/test/ao/sparsity/test_parametrization.py\n+++ b/test/ao/sparsity/test_parametrization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_scheduler.py b/test/ao/sparsity/test_scheduler.py\nindex 52eb54cb9ecb96..835c5143f18bc2 100644\n--- a/test/ao/sparsity/test_scheduler.py\n+++ b/test/ao/sparsity/test_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch import nn\ndiff --git a/test/ao/sparsity/test_sparsifier.py b/test/ao/sparsity/test_sparsifier.py\nindex 4c79416a78dd69..c9309d4b81fe5b 100644\n--- a/test/ao/sparsity/test_sparsifier.py\n+++ b/test/ao/sparsity/test_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import itertools\ndiff --git a/test/ao/sparsity/test_sparsity_utils.py b/test/ao/sparsity/test_sparsity_utils.py\nindex 90aad10ab18db6..9a4fc79e6c454e 100644\n--- a/test/ao/sparsity/test_sparsity_utils.py\n+++ b/test/ao/sparsity/test_sparsity_utils.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_structured_sparsifier.py b/test/ao/sparsity/test_structured_sparsifier.py\nindex f50420c89a199d..13ab245a2efc27 100644\n--- a/test/ao/sparsity/test_structured_sparsifier.py\n+++ b/test/ao/sparsity/test_structured_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n import copy\n import logging\ndiff --git a/torch/ao/nn/intrinsic/modules/fused.py b/torch/ao/nn/intrinsic/modules/fused.py\nindex f70a5430e65c21..7c87154b0e6225 100644\n--- a/torch/ao/nn/intrinsic/modules/fused.py\n+++ b/torch/ao/nn/intrinsic/modules/fused.py\n@@ -125,7 +125,7 @@ class LinearBn1d(_FusedModule):\n     During quantization this will be replaced with the corresponding fused module.\"\"\"\n     def __init__(self, linear, bn):\n         assert type_before_parametrizations(linear) == Linear and type_before_parametrizations(bn) == BatchNorm1d, \\\n-            'Incorrect types for input modules{}{}'.format(type_before_parametrizations(linear), type_before_parametrizations(bn))\n+            f'Incorrect types for input modules{type_before_parametrizations(linear)}{type_before_parametrizations(bn)}'\n         super().__init__(linear, bn)\n \n class LinearLeakyReLU(_FusedModule):\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\nindex 3f457ad5917eba..161280ca079d53 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n@@ -453,7 +453,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU1d(nnqat.Conv1d, nni._FusedModule):\n     r\"\"\"A ConvReLU1d module is a fused module of Conv1d and ReLU, attached with\n@@ -490,7 +490,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn2d(_ConvBnNd, nn.Conv2d):\n     r\"\"\"\n@@ -585,7 +585,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU2d(nnqat.Conv2d, nni._FusedModule):\n     r\"\"\"A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\n@@ -622,7 +622,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn3d(_ConvBnNd, nn.Conv3d):\n     r\"\"\"\n@@ -758,7 +758,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU3d(nnqat.Conv3d, nni._FusedModule):\n     r\"\"\"A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with\n@@ -813,7 +813,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n def update_bn_stats(mod):\n     if type(mod) in {ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvBn1d, ConvBn2d, ConvBn3d}:\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\nindex 93b19537083427..11d11047c2c723 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n@@ -37,7 +37,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     def to_float(self):\n         linear = torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)\ndiff --git a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nindex 9a6502d546641b..a0bccdc0e3d3d4 100644\n--- a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n@@ -48,7 +48,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qlinear_relu):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\nindex 5cd2ed8a757cee..856fa43aac9941 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n@@ -39,7 +39,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU2d\n-        return super(BNReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\n@@ -75,7 +75,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU3d\n-        return super(BNReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\nindex 7a88a7b8f92d3b..30d00474e4a5ad 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n@@ -58,7 +58,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -107,7 +107,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -163,7 +163,7 @@ def from_float(cls, mod):\n                 mod.bn.weight,\n                 mod.bn.bias,\n             )\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\nindex 9c3a7bcd3b4a0c..17cb48f80fda91 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n@@ -41,7 +41,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_linear_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/quantizable/modules/activation.py b/torch/ao/nn/quantizable/modules/activation.py\nindex b7ba9dd8dc72c2..6a25d0f591021d 100644\n--- a/torch/ao/nn/quantizable/modules/activation.py\n+++ b/torch/ao/nn/quantizable/modules/activation.py\n@@ -317,7 +317,7 @@ def _forward_impl(self,\n             raise AssertionError(\"causal mask not supported by AO MHA module\")\n \n         if self.batch_first:\n-            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n+            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n \n         tgt_len, bsz, embed_dim_to_check = query.size()\n         assert self.embed_dim == embed_dim_to_check\n@@ -339,7 +339,7 @@ def _forward_impl(self,\n                 warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n                 attn_mask = attn_mask.to(torch.bool)\n             assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n-                'Only float and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n+                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n \n             if attn_mask.dim() == 2:\n                 attn_mask = attn_mask.unsqueeze(0)\n@@ -349,7 +349,7 @@ def _forward_impl(self,\n                 if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n                     raise RuntimeError('The size of the 3D attn_mask is not correct.')\n             else:\n-                raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n+                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n             # attn_mask's dim is 3 now.\n \n         # convert ByteTensor key_padding_mask to bool\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/conv.py b/torch/ao/nn/quantized/dynamic/modules/conv.py\nindex 125b48edaacde5..f1af7796413655 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/conv.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Dynamically quantized convolution modules.\"\"\"\n \n import torch\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/linear.py b/torch/ao/nn/quantized/dynamic/modules/linear.py\nindex 78e459f9bc63c5..22f483f32fd7a8 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/linear.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/linear.py\n@@ -68,7 +68,7 @@ def extra_repr(self):\n             self.in_features, self.out_features, self._packed_params.dtype\n         )\n         if self._packed_params.dtype == torch.qint8:\n-            extra_repr_str += ', qscheme={}'.format(self.weight().qscheme())\n+            extra_repr_str += f', qscheme={self.weight().qscheme()}'\n         return extra_repr_str\n \n     def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/rnn.py b/torch/ao/nn/quantized/dynamic/modules/rnn.py\nindex 3e78948b5447b2..47c8a9ac2fb43c 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/rnn.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/rnn.py\n@@ -231,8 +231,8 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n     def set_weight_bias(self, weight_bias_dict):\n \n         def weight_bias_name(ihhh, layer, suffix):\n-            weight_name = \"weight_{}_l{}{}\".format(ihhh, layer, suffix)\n-            bias_name = \"bias_{}_l{}{}\".format(ihhh, layer, suffix)\n+            weight_name = f\"weight_{ihhh}_l{layer}{suffix}\"\n+            bias_name = f\"bias_{ihhh}_l{layer}{suffix}\"\n             return weight_name, bias_name\n \n         num_directions = 2 if self.bidirectional else 1\n@@ -286,7 +286,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n         # RNNBase can be either LSTM or GRU\n         qRNNBase: Union[LSTM, GRU]\n         if mod.mode == 'LSTM':\n@@ -308,8 +308,8 @@ def from_float(cls, mod):\n                 suffix = '_reverse' if direction == 1 else ''\n \n                 def retrieve_weight_bias(ihhh):\n-                    weight_name = 'weight_{}_l{}{}'.format(ihhh, layer, suffix)\n-                    bias_name = 'bias_{}_l{}{}'.format(ihhh, layer, suffix)\n+                    weight_name = f'weight_{ihhh}_l{layer}{suffix}'\n+                    bias_name = f'bias_{ihhh}_l{layer}{suffix}'\n                     weight = getattr(mod, weight_name)\n                     bias = getattr(mod, bias_name)\n                     return weight, bias\n@@ -358,15 +358,15 @@ def _weight_bias(self):\n         for layer in range(self.num_layers):\n             for direction in range(num_directions):\n                 suffix = '_reverse' if direction == 1 else ''\n-                key_name1 = 'weight_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'weight_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'weight_ih_l{layer}{suffix}'\n+                key_name2 = f'weight_hh_l{layer}{suffix}'\n                 # packed weights are part of torchbind class, CellParamsSerializationType\n                 # Within the packed weight class, the weight and bias are accessible as Tensors\n                 packed_weight_bias = self._all_weight_values[count].param.__getstate__()[0][4]\n                 weight_bias_dict['weight'][key_name1] = packed_weight_bias[0].__getstate__()[0][0]\n                 weight_bias_dict['weight'][key_name2] = packed_weight_bias[1].__getstate__()[0][0]\n-                key_name1 = 'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'bias_ih_l{layer}{suffix}'\n+                key_name2 = f'bias_hh_l{layer}{suffix}'\n                 weight_bias_dict['bias'][key_name1] = packed_weight_bias[0].__getstate__()[0][1]\n                 weight_bias_dict['bias'][key_name2] = packed_weight_bias[1].__getstate__()[0][1]\n                 count = count + 1\n@@ -494,7 +494,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTM, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -746,7 +746,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -860,7 +860,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n \n         qRNNCellBase: Union[LSTMCell, GRUCell, RNNCell]\n \n@@ -1009,12 +1009,12 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n         return ret\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(RNNCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class LSTMCell(RNNCellBase):\n@@ -1057,7 +1057,7 @@ def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) ->\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTMCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class GRUCell(RNNCellBase):\n@@ -1098,4 +1098,4 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRUCell, cls).from_float(mod)\n+        return super().from_float(mod)\ndiff --git a/torch/ao/nn/quantized/modules/__init__.py b/torch/ao/nn/quantized/modules/__init__.py\nindex 05866f6da4066a..668f765fe3ef0a 100644\n--- a/torch/ao/nn/quantized/modules/__init__.py\n+++ b/torch/ao/nn/quantized/modules/__init__.py\n@@ -104,7 +104,7 @@ def from_float(mod):\n         return Quantize(scale.float().item(), zero_point.long().item(), mod.activation_post_process.dtype)\n \n     def extra_repr(self):\n-        return 'scale={}, zero_point={}, dtype={}'.format(self.scale, self.zero_point, self.dtype)\n+        return f'scale={self.scale}, zero_point={self.zero_point}, dtype={self.dtype}'\n \n \n class DeQuantize(torch.nn.Module):\ndiff --git a/torch/ao/nn/quantized/modules/conv.py b/torch/ao/nn/quantized/modules/conv.py\nindex 727447841ca43c..22a11014375948 100644\n--- a/torch/ao/nn/quantized/modules/conv.py\n+++ b/torch/ao/nn/quantized/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Quantized convolution modules.\"\"\"\n \n from typing import Optional, List, TypeVar\n@@ -64,7 +63,7 @@ def _init(self, in_channels, out_channels, kernel_size, stride,\n         self.output_padding = output_padding\n         self.groups = groups\n         if padding_mode not in _SUPPORTED_PADDING:\n-            raise ValueError(\"'padding_mode' {} is not supported by quantized convolution\".format(padding_mode))\n+            raise ValueError(f\"'padding_mode' {padding_mode} is not supported by quantized convolution\")\n         self.padding_mode = padding_mode\n         # Initialize as NCHW. set_weight will internally transpose to NHWC.\n         if self.transposed:\n@@ -593,7 +592,7 @@ def __init__(self, in_channels, out_channels, kernel_size, stride,\n                  padding, dilation, transposed, output_padding,\n                  groups, bias, padding_mode, device=None, dtype=None):\n         if padding_mode != 'zeros':\n-            raise ValueError('Only \"zeros\" padding mode is supported for {}'.format(self.__class__.__name__))\n+            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n         factory_kwargs = {'device': device, 'dtype': dtype}\n         # Subclasses of _ConvNd need to call _init rather than __init__. See\n         # discussion on PR #49702\ndiff --git a/torch/ao/nn/quantized/modules/linear.py b/torch/ao/nn/quantized/modules/linear.py\nindex e592c5f9b4d015..213934e62962a0 100644\n--- a/torch/ao/nn/quantized/modules/linear.py\n+++ b/torch/ao/nn/quantized/modules/linear.py\n@@ -262,7 +262,7 @@ def from_float(cls, mod):\n             if not isinstance(cls._FLOAT_MODULE, Iterable):\n                 cls._FLOAT_MODULE = [cls._FLOAT_MODULE]  # type: ignore[assignment]\n             supported_modules = ', '.join([float_mod.__name__ for float_mod in cls._FLOAT_MODULE])  # type: ignore[attr-defined]\n-            error_msg = 'nnq.{}.from_float only works for {}, but got: {}'.format(cls.__name__, supported_modules, type(mod))\n+            error_msg = f'nnq.{cls.__name__}.from_float only works for {supported_modules}, but got: {type(mod)}'\n             assert type_before_parametrizations(mod) in cls._FLOAT_MODULE, error_msg.format()  # type: ignore[attr-defined]\n             assert hasattr(mod, 'qconfig'), 'Input float module must have qconfig defined'\n             activation_post_process = mod.activation_post_process\ndiff --git a/torch/ao/nn/quantized/reference/modules/rnn.py b/torch/ao/nn/quantized/reference/modules/rnn.py\nindex 566642832a544d..9f44667c270b56 100644\n--- a/torch/ao/nn/quantized/reference/modules/rnn.py\n+++ b/torch/ao/nn/quantized/reference/modules/rnn.py\n@@ -152,7 +152,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n \n         if not is_batched:\n             ret = ret.squeeze(0)\ndiff --git a/torch/ao/nn/sparse/quantized/dynamic/linear.py b/torch/ao/nn/sparse/quantized/dynamic/linear.py\nindex 87d174db8098ac..4190ebe38c2f93 100644\n--- a/torch/ao/nn/sparse/quantized/dynamic/linear.py\n+++ b/torch/ao/nn/sparse/quantized/dynamic/linear.py\n@@ -60,7 +60,7 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                               missing_keys, unexpected_keys, error_msgs):\n         op_type = int(state_dict[prefix + 'op_type'])\n         assert op_type == 'sparse', \\\n-            \"Cannot load from op_type [{}], expecting [{}]\".format(op_type, self._op_type)\n+            f\"Cannot load from op_type [{op_type}], expecting [{self._op_type}]\"\n         state_dict.pop(prefix + 'op_type')\n \n         version = local_metadata.get('version', None)\ndiff --git a/torch/ao/ns/fx/ns_types.py b/torch/ao/ns/fx/ns_types.py\nindex cf0451a155dd44..5c3c422dd4ae9d 100644\n--- a/torch/ao/ns/fx/ns_types.py\n+++ b/torch/ao/ns/fx/ns_types.py\n@@ -10,10 +10,10 @@ class NSSingleResultValuesType(str, enum.Enum):\n     NODE_OUTPUT = 'node_output'\n     NODE_INPUT = 'node_input'\n \n-NSSubgraph = NamedTuple(\n-    'NSSubgraph',\n-    [('start_node', Node), ('end_node', Node), ('base_op_node', Node)]\n-)\n+class NSSubgraph(NamedTuple):\n+    start_node: Node\n+    end_node: Node\n+    base_op_node: Node\n \n # TODO(future PR): see if we can use typing_extensions's TypedDict instead\n # to properly type the various keys\ndiff --git a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\nindex 26da2146952ffd..0e4060f95435b6 100644\n--- a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n+++ b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n@@ -103,8 +103,8 @@ def get_schedule_param(self):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Data Sparsifier {0}\\n'.format(self.data_sparsifier)\n-        format_string += '    {0}: {1}\\n'.format(self.schedule_param, self.base_param)\n+        format_string += f'Data Sparsifier {self.data_sparsifier}\\n'\n+        format_string += f'    {self.schedule_param}: {self.base_param}\\n'\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/base_scheduler.py b/torch/ao/pruning/scheduler/base_scheduler.py\nindex 8986d5bbdf630f..66863b31c5f8d8 100644\n--- a/torch/ao/pruning/scheduler/base_scheduler.py\n+++ b/torch/ao/pruning/scheduler/base_scheduler.py\n@@ -106,8 +106,8 @@ def print_sl(self, is_verbose, group, sl, epoch=None):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Sparsifier {0}\\n'.format(self.sparsifier)\n-        format_string += '    {0}: {1}\\n'.format('base_sl', self.base_sl)\n+        format_string += f'Sparsifier {self.sparsifier}\\n'\n+        format_string += '    {}: {}\\n'.format('base_sl', self.base_sl)\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/cubic_scheduler.py b/torch/ao/pruning/scheduler/cubic_scheduler.py\nindex 49ee9f51b42ae6..76fc61daa288a6 100644\n--- a/torch/ao/pruning/scheduler/cubic_scheduler.py\n+++ b/torch/ao/pruning/scheduler/cubic_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import warnings\n \n from .base_scheduler import BaseScheduler\ndiff --git a/torch/ao/quantization/_learnable_fake_quantize.py b/torch/ao/quantization/_learnable_fake_quantize.py\nindex df86cd50a2a775..f21fedeb3bc28f 100644\n--- a/torch/ao/quantization/_learnable_fake_quantize.py\n+++ b/torch/ao/quantization/_learnable_fake_quantize.py\n@@ -114,8 +114,8 @@ def toggle_fake_quant(self, enabled=True):\n \n     @torch.jit.export\n     def observe_quant_params(self):\n-        print('_LearnableFakeQuantize Scale: {}'.format(self.scale.detach()))\n-        print('_LearnableFakeQuantize Zero Point: {}'.format(self.zero_point.detach()))\n+        print(f'_LearnableFakeQuantize Scale: {self.scale.detach()}')\n+        print(f'_LearnableFakeQuantize Zero Point: {self.zero_point.detach()}')\n \n     @torch.jit.export\n     def calculate_qparams(self):\ndiff --git a/torch/ao/quantization/backend_config/backend_config.py b/torch/ao/quantization/backend_config/backend_config.py\nindex ef31166b5cdab1..32abcc42e402fb 100644\n--- a/torch/ao/quantization/backend_config/backend_config.py\n+++ b/torch/ao/quantization/backend_config/backend_config.py\n@@ -599,8 +599,7 @@ def _get_dtype_config(obj: Any) -> DTypeConfig:\n                 return obj\n             if isinstance(obj, Dict):\n                 return DTypeConfig.from_dict(obj)\n-            raise ValueError(\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (DTYPE_CONFIGS_DICT_KEY, type(obj)))\n+            raise ValueError(f\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"{DTYPE_CONFIGS_DICT_KEY}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         if PATTERN_DICT_KEY in backend_pattern_config_dict:\ndiff --git a/torch/ao/quantization/backend_config/onednn.py b/torch/ao/quantization/backend_config/onednn.py\nindex 6a896608c9b5a8..8c14637ae3d3f7 100644\n--- a/torch/ao/quantization/backend_config/onednn.py\n+++ b/torch/ao/quantization/backend_config/onednn.py\n@@ -89,7 +89,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n         \"Linear, BN and LeakyReLU all must be in the same mode (train or eval).\"\n \n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((linear, bn, leaky_relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(linear, bn, leaky_relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Linear: nni.LinearLeakyReLU,\n@@ -100,7 +100,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n             fm = fused_module(fused_linear, leaky_relu)\n             return fm\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((linear, bn, leaky_relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(linear, bn, leaky_relu)}\")\n \n # ======================\n # |  CONFIGS FOR CONV  |\n@@ -144,7 +144,7 @@ def _conv_add_extra_inputs_getter_left(pattern):\n def _fuse_conv_bn_add_left(is_qat, add, bn_conv, _):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -216,7 +216,7 @@ def _conv_add_extra_inputs_getter_right(pattern):\n def _fuse_conv_bn_add_right(is_qat, add, _, bn_conv):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -305,7 +305,7 @@ def _fuse_conv_bn_add_relu_left(is_qat, relu, add_pattern):\n     add, bn_conv, _ = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\n@@ -387,7 +387,7 @@ def _fuse_conv_bn_add_relu_right(is_qat, relu, add_pattern):\n     add, _, bn_conv = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\ndiff --git a/torch/ao/quantization/experimental/APoT_tensor.py b/torch/ao/quantization/experimental/APoT_tensor.py\nindex f780e204154147..debda7aea8c0d1 100644\n--- a/torch/ao/quantization/experimental/APoT_tensor.py\n+++ b/torch/ao/quantization/experimental/APoT_tensor.py\n@@ -2,7 +2,7 @@\n from torch.ao.quantization.experimental.quantizer import APoTQuantizer\n \n # class to store APoT quantized tensor\n-class TensorAPoT():\n+class TensorAPoT:\n     quantizer: APoTQuantizer\n     data: torch.Tensor\n \ndiff --git a/torch/ao/quantization/experimental/quantizer.py b/torch/ao/quantization/experimental/quantizer.py\nindex e7e6048fb00e08..df9c0f27847e13 100644\n--- a/torch/ao/quantization/experimental/quantizer.py\n+++ b/torch/ao/quantization/experimental/quantizer.py\n@@ -5,7 +5,7 @@\n \n # class to store APoT quantizer and\n # implement quantize and dequantize\n-class APoTQuantizer():\n+class APoTQuantizer:\n     alpha: torch.Tensor\n     gamma: torch.Tensor\n     quantization_levels: torch.Tensor\ndiff --git a/torch/ao/quantization/fake_quantize.py b/torch/ao/quantization/fake_quantize.py\nindex 881d431dcccb18..0da19e9f09b5a8 100644\n--- a/torch/ao/quantization/fake_quantize.py\n+++ b/torch/ao/quantization/fake_quantize.py\n@@ -268,7 +268,7 @@ class FixedQParamsFakeQuantize(FakeQuantize):\n     def __init__(self, observer):\n         super().__init__(observer=observer)\n         assert type(self.activation_post_process) == FixedQParamsObserver,\\\n-            \"%s's observer must be a %s\" % (self.__class__.__name__, FixedQParamsObserver.__name__)\n+            f\"{self.__class__.__name__}'s observer must be a {FixedQParamsObserver.__name__}\"\n         self._observer_ctr = observer\n         self.scale = self.activation_post_process.scale\n         self.zero_point = self.activation_post_process.zero_point\ndiff --git a/torch/ao/quantization/fuse_modules.py b/torch/ao/quantization/fuse_modules.py\nindex 80c3933ddc06a1..7c7ef1a88e83a7 100644\n--- a/torch/ao/quantization/fuse_modules.py\n+++ b/torch/ao/quantization/fuse_modules.py\n@@ -51,7 +51,7 @@ def fuse_known_modules(mod_list, is_qat, additional_fuser_method_mapping=None):\n     types = tuple(type_before_parametrizations(m) for m in mod_list)\n     fuser_method = get_fuser_method(types, additional_fuser_method_mapping)\n     if fuser_method is None:\n-        raise NotImplementedError(\"Cannot fuse modules: {}\".format(types))\n+        raise NotImplementedError(f\"Cannot fuse modules: {types}\")\n     new_mod : List[Optional[nn.Module]] = [None] * len(mod_list)\n     fused = fuser_method(is_qat, *mod_list)\n     # NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\ndiff --git a/torch/ao/quantization/fuser_method_mappings.py b/torch/ao/quantization/fuser_method_mappings.py\nindex 9971326de1d102..3140f13008ac3d 100644\n--- a/torch/ao/quantization/fuser_method_mappings.py\n+++ b/torch/ao/quantization/fuser_method_mappings.py\n@@ -47,7 +47,7 @@ def fuse_conv_bn(is_qat, conv, bn):\n         if fused_module_class is not None:\n             return fused_module_class(conv, bn)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn)}\")\n     else:\n         return nn.utils.fuse_conv_bn_eval(conv, bn)\n \n@@ -84,7 +84,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n         if fused_module is not None:\n             return fused_module(conv, bn, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Conv1d: nni.ConvReLU1d,\n@@ -96,7 +96,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n             fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n             return fused_module(fused_conv, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(conv, bn, relu)}\")\n \n def fuse_linear_bn(is_qat, linear, bn):\n     r\"\"\"Given the linear and bn modules, fuses them and returns the fused module\n@@ -187,7 +187,7 @@ def get_fuser_method(op_list, additional_fuser_method_mapping=None):\n     all_mappings = get_combined_dict(_DEFAULT_OP_LIST_TO_FUSER_METHOD,\n                                      additional_fuser_method_mapping)\n     fuser_method = all_mappings.get(op_list, None)\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_list)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_list} \"\n     return fuser_method\n \n def _reverse2(f):\n@@ -244,5 +244,5 @@ def get_fuser_method_new(\n         fuser_method = fuser_method_mapping.get(op_pattern, None)\n         if fuser_method is not None:\n             break\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_pattern)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_pattern} \"\n     return fuser_method\ndiff --git a/torch/ao/quantization/fx/_equalize.py b/torch/ao/quantization/fx/_equalize.py\nindex 357db6454032e7..883ddf19682f08 100644\n--- a/torch/ao/quantization/fx/_equalize.py\n+++ b/torch/ao/quantization/fx/_equalize.py\n@@ -227,7 +227,7 @@ def __new__(cls, input_activation=torch.nn.Identity, weight=torch.nn.Identity):\n         if isinstance(input_activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"EqualizationQConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        self = super(EqualizationQConfig, cls).__new__(cls, input_activation, weight)\n+        self = super().__new__(cls, input_activation, weight)\n         return self\n \n \ndiff --git a/torch/ao/quantization/fx/_lower_to_native_backend.py b/torch/ao/quantization/fx/_lower_to_native_backend.py\nindex b897a7e80ab7cf..f1e9c81896f6c5 100644\n--- a/torch/ao/quantization/fx/_lower_to_native_backend.py\n+++ b/torch/ao/quantization/fx/_lower_to_native_backend.py\n@@ -514,7 +514,7 @@ def _match_static_pattern(\n     matched_dequantize = False\n     for i in dequantize_node_arg_indices:\n         assert i < len(ref_node.args),\\\n-            \"Dequantize index %s exceeded reference node's arg length %s\" % (i, len(ref_node.args))\n+            f\"Dequantize index {i} exceeded reference node's arg length {len(ref_node.args)}\"\n         arg = ref_node.args[i]\n         if is_dequantize_node(arg):\n             matched_dequantize = True\ndiff --git a/torch/ao/quantization/fx/_model_report/detector.py b/torch/ao/quantization/fx/_model_report/detector.py\nindex 60c9b26ffacecd..2e3cdc0a316611 100644\n--- a/torch/ao/quantization/fx/_model_report/detector.py\n+++ b/torch/ao/quantization/fx/_model_report/detector.py\n@@ -32,7 +32,7 @@\n DETECTOR_OBS_ARGS_KEY = \"observer_args\"\n \n # Mapping related code\n-class DetectorQConfigInfo():\n+class DetectorQConfigInfo:\n     r\"\"\"\n     This class contains the QConfig information for a single module.\n     The list of variables / values this contains can grow depending on the\n@@ -234,7 +234,7 @@ def __init__(self, backend: str = torch.backends.quantized.engine):\n         if self.backend_chosen in self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES:\n             self.supported_modules = self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES[self.backend_chosen]\n         else:\n-            raise ValueError(\"Not configured to work with {}. Try a different default backend\".format(self.backend_chosen))\n+            raise ValueError(f\"Not configured to work with {self.backend_chosen}. Try a different default backend\")\n \n     def get_detector_name(self) -> str:\n         r\"\"\" returns the string name of this detector\"\"\"\n@@ -352,7 +352,7 @@ def generate_detector_report(self, model: nn.Module) -> Tuple[str, Dict[str, Any\n         per_channel_info = self._detect_per_channel_helper(model)\n \n         # String to let the user know of further optimizations\n-        further_optims_str = \"Further Optimizations for backend {}: \\n\".format(self.backend_chosen)\n+        further_optims_str = f\"Further Optimizations for backend {self.backend_chosen}: \\n\"\n \n         optimizations_possible = False\n         for fqn in per_channel_info:\n@@ -1019,7 +1019,7 @@ def _generate_comparison_values(self, input_info: Dict, weight_info: Dict) -> Di\n \n             # raise error if not in weight info\n             if module_fqn not in weight_info:\n-                raise KeyError(\"Unable to find weight range stats for module {}\".format(module_fqn))\n+                raise KeyError(f\"Unable to find weight range stats for module {module_fqn}\")\n \n             # calculate the ratios of the weight info and input info\n             weight_ratio = self._calculate_range_ratio(weight_info[module_fqn], self.WEIGHT_STR, module_fqn)\ndiff --git a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\nindex f1f17e80982e54..8e04338446dab1 100644\n--- a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n+++ b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n@@ -587,7 +587,7 @@ def generate_plot_visualization(self, feature_filter: str, module_fqn_filter: st\n             avg_vals = [sum(y_data[:][index]) / num_channels for index in range(num_modules)]\n \n             # plot the three things we measured\n-            ax.plot(x_data, avg_vals, label=\"Average Value Across {} Channels\".format(num_channels))\n+            ax.plot(x_data, avg_vals, label=f\"Average Value Across {num_channels} Channels\")\n             ax.legend(loc='upper right')\n         else:\n             ax.set_xlabel(\"idx\")\ndiff --git a/torch/ao/quantization/fx/convert.py b/torch/ao/quantization/fx/convert.py\nindex 14e6ec094ede48..687342cc0ed321 100644\n--- a/torch/ao/quantization/fx/convert.py\n+++ b/torch/ao/quantization/fx/convert.py\n@@ -970,7 +970,7 @@ def convert(\n         # all the values either match what was set in prepare node_name_to_qconfig\n         # or are set to None in the convert_node_name_to_qconfig.\n         for k, v in node_name_to_qconfig.items():\n-            assert k in convert_node_name_to_qconfig, 'Expected key {} in convert node_name_to_qconfig'.format(k)\n+            assert k in convert_node_name_to_qconfig, f'Expected key {k} in convert node_name_to_qconfig'\n             if convert_node_name_to_qconfig[k] is not None:\n                 assert qconfig_equals(v, convert_node_name_to_qconfig[k]), \\\n                     \"Expected k {} to have the same value in prepare and convert QConfigMappings, \" \\\ndiff --git a/torch/ao/quantization/fx/custom_config.py b/torch/ao/quantization/fx/custom_config.py\nindex ef29061796d3a3..4fb2c3a28cb0a5 100644\n--- a/torch/ao/quantization/fx/custom_config.py\n+++ b/torch/ao/quantization/fx/custom_config.py\n@@ -197,8 +197,7 @@ def _get_qconfig_mapping(obj: Any, dict_key: str) -> Optional[QConfigMapping]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return QConfigMapping.from_dict(obj)\n-            raise ValueError(\"Expected QConfigMapping in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected QConfigMapping in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCustomConfig]:\n             \"\"\"\n@@ -208,8 +207,7 @@ def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCusto\n                 return obj\n             if isinstance(obj, Dict):\n                 return PrepareCustomConfig.from_dict(obj)\n-            raise ValueError(\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n             \"\"\"\n@@ -219,8 +217,7 @@ def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return BackendConfig.from_dict(obj)\n-            raise ValueError(\"Expected BackendConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected BackendConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         for (module_name, qconfig_dict, example_inputs, _prepare_custom_config_dict, backend_config_dict) in\\\ndiff --git a/torch/ao/quantization/fx/prepare.py b/torch/ao/quantization/fx/prepare.py\nindex d14bf444ccd8c6..aa9f1f7467f932 100644\n--- a/torch/ao/quantization/fx/prepare.py\n+++ b/torch/ao/quantization/fx/prepare.py\n@@ -238,7 +238,7 @@ def _needs_obs_or_fq(\n     # be converted to choose_qparams -> q -> dq in convert step\n     if cur_target_is_dynamic:\n         assert cur_target_dtype in _OBS_DTYPE_LIST, \\\n-            \"Expected cur_target_dtype to be torch.float, but got: {}\".format(cur_target_dtype)\n+            f\"Expected cur_target_dtype to be torch.float, but got: {cur_target_dtype}\"\n         assert prev_output_dtype not in _DO_NOT_OBS_DTYPE_LIST\n         return is_zeroth_arg\n     if reuse_input_obs_or_fq:\ndiff --git a/torch/ao/quantization/fx/utils.py b/torch/ao/quantization/fx/utils.py\nindex 2e0b6bbb130530..0942fd9462b0a1 100644\n--- a/torch/ao/quantization/fx/utils.py\n+++ b/torch/ao/quantization/fx/utils.py\n@@ -149,7 +149,7 @@ def get_qconv_prepack_op(conv_op: Callable) -> Callable:\n         torch.nn.functional.conv_transpose3d: torch.ops.quantized.conv_transpose3d_prepack,\n     }\n     prepack_op = prepack_ops.get(conv_op, None)\n-    assert prepack_op, \"Didn't find prepack op for {}\".format(conv_op)\n+    assert prepack_op, f\"Didn't find prepack op for {conv_op}\"\n     return prepack_op\n \n # Returns a function that can get a new attribute name for module with given\n@@ -811,24 +811,21 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n         # check quantization ranges\n         if backend_quant_min is not None and backend_quant_max is not None:\n             if app_quant_min is None or app_quant_max is None:\n-                warnings.warn(\"QConfig %s must specify 'quant_min' and 'quant_max', ignoring %s\" %\n-                              (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'quant_min' and 'quant_max', ignoring {qconfig}\")\n                 return False\n             elif app_quant_min < backend_quant_min or app_quant_max > backend_quant_max:\n-                warnings.warn((\"QConfig %s quantization range must fall within the backend's:\\n\"\n-                              \"QConfig range = (%s, %s), BackendConfig range = (%s, %s), ignoring %s\") %\n-                              (debug_string, app_quant_min, app_quant_max,\n+                warnings.warn((\"QConfig {} quantization range must fall within the backend's:\\n\"\n+                              \"QConfig range = ({}, {}), BackendConfig range = ({}, {}), ignoring {}\").format(debug_string, app_quant_min, app_quant_max,\n                               backend_quant_min, backend_quant_max, qconfig))\n                 return False\n         # check scale min\n         if backend_scale_min is not None:\n             if app_scale_min is None:\n-                warnings.warn(\"QConfig %s must specify 'eps', ignoring %s\" % (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'eps', ignoring {qconfig}\")\n                 return False\n             elif app_scale_min < backend_scale_min:\n-                warnings.warn((\"QConfig %s eps (%s) must be greater than or equal to \"\n-                              \"the backend's min scale value (%s), ignoring %s\") %\n-                              (debug_string, app_scale_min, backend_scale_min, qconfig))\n+                warnings.warn((\"QConfig {} eps ({}) must be greater than or equal to \"\n+                              \"the backend's min scale value ({}), ignoring {}\").format(debug_string, app_scale_min, backend_scale_min, qconfig))\n                 return False\n         # check fixed scale and zero point\n         if backend_scale_exact_match is not None and backend_zero_point_exact_match is not None:\n@@ -846,12 +843,11 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n             if not isinstance(activation_post_process, FixedQParamsObserver) and \\\n                     not isinstance(activation_post_process, FixedQParamsFakeQuantize):\n                 warnings.warn((\"QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize \"\n-                              \"for fixed qparams ops, ignoring %s.\\n%s\") % (qconfig, suggestion_str))\n+                              \"for fixed qparams ops, ignoring {}.\\n{}\").format(qconfig, suggestion_str))\n                 return False\n             if observer.scale != backend_scale_exact_match or observer.zero_point != backend_zero_point_exact_match:\n-                warnings.warn((\"QConfig fixed scale (%s) and zero point (%s) do not match the backend's \"\n-                              \"(%s and %s), ignoring %s.\\n%s\") %\n-                              (observer.scale, observer.zero_point, backend_scale_exact_match,\n+                warnings.warn((\"QConfig fixed scale ({}) and zero point ({}) do not match the backend's \"\n+                              \"({} and {}), ignoring {}.\\n{}\").format(observer.scale, observer.zero_point, backend_scale_exact_match,\n                               backend_zero_point_exact_match, qconfig, suggestion_str))\n                 return False\n         return True\ndiff --git a/torch/ao/quantization/observer.py b/torch/ao/quantization/observer.py\nindex 3263ae11564129..f5d24c45787265 100644\n--- a/torch/ao/quantization/observer.py\n+++ b/torch/ao/quantization/observer.py\n@@ -118,7 +118,7 @@ def _with_callable_args(cls_or_self, **kwargs):\n     return r.with_callable_args(**kwargs)\n \n \n-ABC: Any = ABCMeta(str(\"ABC\"), (object,), {})  # compatible with Python 2 *and* 3:\n+ABC: Any = ABCMeta(\"ABC\", (object,), {})  # compatible with Python 2 *and* 3:\n \n \n class ObserverBase(ABC, nn.Module):\n@@ -509,7 +509,7 @@ def calculate_qparams(self):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     @torch.jit.export\n     def reset_min_max_vals(self):\n@@ -712,7 +712,7 @@ def calculate_qparams(self):\n         return self._calculate_qparams(self.min_val, self.max_val)\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     def _load_from_state_dict(\n         self,\n@@ -746,7 +746,7 @@ def _load_from_state_dict(\n                 elif name == expected_max_name:\n                     self.max_val.resize_(val.shape)\n                 else:\n-                    warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                    warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n                 # For torchscript module we need to update the attributes here since we do not\n                 # call the `_load_from_state_dict` function defined module.py\n                 if torch.jit.is_scripting():\n@@ -755,7 +755,7 @@ def _load_from_state_dict(\n                     elif name == expected_max_name:\n                         self.max_val.copy_(val)\n                     else:\n-                        warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                        warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n             elif strict:\n                 missing_keys.append(key)\n \n@@ -1265,7 +1265,7 @@ def _load_from_state_dict(\n         )\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n \n class FixedQParamsObserver(ObserverBase):\n@@ -1363,7 +1363,7 @@ def forward(self, x):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"dtype={}, is_dynamic={}\".format(self.dtype, self.is_dynamic)\n+        return f\"dtype={self.dtype}, is_dynamic={self.is_dynamic}\"\n \n     @torch.jit.export\n     def calculate_qparams(self):\n@@ -1518,10 +1518,10 @@ def load_observer_state_dict(mod, obs_dict):\n                 )\n     for k in missing_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Missing keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Missing keys for observer {k} in state_dict\")\n     for k in unexpected_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Unexpected keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Unexpected keys for observer {k} in state_dict\")\n \n \n # Restrict activations to be in the range (0,127)\ndiff --git a/torch/ao/quantization/pt2e/prepare.py b/torch/ao/quantization/pt2e/prepare.py\nindex 2a29a92a986ced..13f73acca354b4 100644\n--- a/torch/ao/quantization/pt2e/prepare.py\n+++ b/torch/ao/quantization/pt2e/prepare.py\n@@ -68,9 +68,9 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n             assert _is_activation_post_process_node(arg, named_modules)\n             assert arg_as_input_act_obs_or_fq is not None\n             observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), \"expect observed argument to be a Node, but got: {}\".format(type(observed_arg))\n+            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n             assert observed_arg in obs_or_fq_map, \\\n-                \"can't refer to a node that does not have observer/fake_quant inserted yet: {}\".format(observed_arg)\n+                f\"can't refer to a node that does not have observer/fake_quant inserted yet: {observed_arg}\"\n             arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n             new_arg = arg\n             obs_or_fq_map[(observed_arg, node)] = arg_as_input_act_obs_or_fq\ndiff --git a/torch/ao/quantization/pt2e/quantizer/quantizer.py b/torch/ao/quantization/pt2e/quantizer/quantizer.py\nindex 3f7531c33a4e8b..46d7286756d17c 100644\n--- a/torch/ao/quantization/pt2e/quantizer/quantizer.py\n+++ b/torch/ao/quantization/pt2e/quantizer/quantizer.py\n@@ -128,24 +128,9 @@ class QuantizationConfig:\n OperatorPatternType = List[Callable]\n OperatorPatternType.__module__ = \"torch.ao.quantization.pt2e.quantizer.quantizer\"\n \n-OperatorConfig = NamedTuple(\n-    \"OperatorConfig\",\n-    # fix List[str] with List[List[Union[nn.Module, FunctionType, BuiltinFunctionType]]]\n-    # Basically we are mapping a quantization config to some list of patterns.\n-    # a pattern is defined as a list of nn module, function or builtin function names\n-    # e.g. [nn.Conv2d, torch.relu, torch.add]\n-    # We have not resolved whether fusion can be considered internal details of the\n-    # quantizer hence it does not need communication to user.\n-    # Note this pattern is not really informative since it does not really\n-    # tell us the graph structure resulting from the list of ops.\n-    [\n-        (\"config\", QuantizationConfig),\n-        (\n-            \"operators\",\n-            List[OperatorPatternType],\n-        ),\n-    ],\n-)\n+class OperatorConfig(NamedTuple):\n+    config: QuantizationConfig\n+    operators: List[OperatorPatternType]\n \n @dataclass\n class QuantizationAnnotation:\ndiff --git a/torch/ao/quantization/qconfig.py b/torch/ao/quantization/qconfig.py\nindex f7489c1ed4d05a..dc8353d6172990 100644\n--- a/torch/ao/quantization/qconfig.py\n+++ b/torch/ao/quantization/qconfig.py\n@@ -103,7 +103,7 @@ def __new__(cls, activation, weight):\n         if isinstance(activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"QConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        return super(QConfig, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n class QConfigDynamic(namedtuple('QConfigDynamic', ['activation', 'weight'])):\n@@ -128,7 +128,7 @@ def __new__(cls, activation=torch.nn.Identity, weight=torch.nn.Identity):\n             raise ValueError(\"QConfigDynamic received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n         warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n-        return super(QConfigDynamic, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n default_qconfig = QConfig(activation=default_observer,\n@@ -236,7 +236,7 @@ def get_default_qconfig(backend='x86', version=0):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     if version == 0:\n@@ -326,7 +326,7 @@ def get_default_qat_qconfig(backend='x86', version=1):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     # Histogram observer is too slow for quantization aware training\ndiff --git a/torch/ao/quantization/quantization_mappings.py b/torch/ao/quantization/quantization_mappings.py\nindex 96db52624acd34..4b3f4d26b2ac09 100644\n--- a/torch/ao/quantization/quantization_mappings.py\n+++ b/torch/ao/quantization/quantization_mappings.py\n@@ -251,7 +251,7 @@ def get_static_quant_module_class(\n         else DEFAULT_STATIC_QUANT_MODULE_MAPPINGS, additional_static_quant_mapping)\n     static_quant_module_class = all_mappings.get(float_module_class, None)\n     assert static_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(static_quant_module_class)\n \n@@ -266,7 +266,7 @@ def get_dynamic_quant_module_class(\n     all_mappings = get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping)\n     dynamic_quant_module_class = all_mappings.get(float_module_class, None)\n     assert dynamic_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(dynamic_quant_module_class)\n \n@@ -300,10 +300,10 @@ def get_default_qconfig_propagation_list() -> Set[Callable]:\n     attribute to in prepare\n     '''\n     QCONFIG_PROPAGATE_MODULE_CLASS_LIST = (\n-        (set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         _INCLUDE_QCONFIG_PROPAGATE_LIST)\n+         _INCLUDE_QCONFIG_PROPAGATE_LIST\n     )\n     return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n \n@@ -332,7 +332,7 @@ def get_quantized_operator(float_op: Union[Callable, str]) -> Callable:\n     '''\n     quantized_op = DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS.get(float_op, None)\n     assert quantized_op is not None, \\\n-        'Operator {} does not have corresponding quantized op'.format(str(float_op))\n+        f'Operator {str(float_op)} does not have corresponding quantized op'\n     return quantized_op\n \n def _get_special_act_post_process(module: torch.nn.Module) -> Optional[Callable]:\ndiff --git a/torch/ao/quantization/quantize.py b/torch/ao/quantization/quantize.py\nindex ca60475fc95a79..23c234c3f35103 100644\n--- a/torch/ao/quantization/quantize.py\n+++ b/torch/ao/quantization/quantize.py\n@@ -442,7 +442,7 @@ def quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8,\n             }\n         else:\n             raise ValueError(\n-                \"Don't know how to quantize with default settings for {}. Provide full qconfig please\".format(dtype))\n+                f\"Don't know how to quantize with default settings for {dtype}. Provide full qconfig please\")\n     elif isinstance(qconfig_spec, set):\n         if dtype is torch.qint8:\n             default_qconfig = default_dynamic_qconfig\ndiff --git a/torch/ao/quantization/utils.py b/torch/ao/quantization/utils.py\nindex 1a82c0fcf432ae..d97b76fb253389 100644\n--- a/torch/ao/quantization/utils.py\n+++ b/torch/ao/quantization/utils.py\n@@ -327,7 +327,7 @@ def check_min_max_valid(min_val: torch.Tensor, max_val: torch.Tensor) -> bool:\n     else:\n         assert torch.all(\n             min_val <= max_val\n-        ), \"min {} should be less than max {}\".format(min_val, max_val)\n+        ), f\"min {min_val} should be less than max {max_val}\"\n \n     return True\n \n"
  },
  {
    "number": 105409,
    "title": "[BE] Enable ruff's UP rules and autoformat benchmarks/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "269f82a7411c8d6400542cb03215230043a64246",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105409",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105409/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105409.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105409.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105409/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105409/comments",
    "labels": [
      "module: dynamo",
      "open source",
      "ciflow/inductor",
      "release notes: distributed (ddp)"
    ],
    "_event_time": "2023-07-18T01:15:59.762380Z",
    "state": "closed",
    "patch": "From 3fc848a901710b0fab41124a99ebbf58b85c9fd0 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:53 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat benchmarks/\n\n[ghstack-poisoned]\n---\n benchmarks/compare-fastrnn-results.py              |  4 ++--\n benchmarks/cpp/tensorexpr/bench_ops.py             |  4 ++--\n benchmarks/distributed/ddp/benchmark.py            | 14 +++++++-------\n benchmarks/distributed/ddp/diff.py                 | 10 +++++-----\n benchmarks/distributed/pipeline/pipe.py            |  4 ++--\n .../distributed/rpc/parameter_server/launcher.py   |  2 +-\n benchmarks/distributed/rpc/rl/coordinator.py       |  2 +-\n benchmarks/dynamo/_onnx/reporter.py                | 12 ++++++------\n benchmarks/dynamo/benchmarks.py                    |  2 +-\n benchmarks/dynamo/combine_csv.py                   |  2 +-\n benchmarks/dynamo/common.py                        |  4 ++--\n .../dynamo/microbenchmarks/operator_inp_utils.py   |  2 +-\n benchmarks/dynamo/parse_logs.py                    |  2 +-\n benchmarks/dynamo/runner.py                        | 10 +++++-----\n benchmarks/dynamo/timm_models.py                   |  4 ++--\n benchmarks/fastrnns/bench.py                       |  2 +-\n benchmarks/fastrnns/profile.py                     |  8 ++++----\n benchmarks/fastrnns/runner.py                      |  6 +++---\n benchmarks/fastrnns/test.py                        |  4 ++--\n .../framework_overhead_benchmark/C2Module.py       |  4 ++--\n .../framework_overhead_benchmark.py                |  6 +++---\n .../pt_wrapper_module.py                           |  4 ++--\n benchmarks/framework_overhead_benchmark/utils.py   |  2 +-\n .../functional_autograd_benchmark/compare.py       |  4 ++--\n .../functional_autograd_benchmark.py               |  6 +++---\n benchmarks/functional_autograd_benchmark/utils.py  |  2 +-\n benchmarks/instruction_counts/applications/ci.py   |  2 +-\n benchmarks/instruction_counts/core/expand.py       |  2 +-\n benchmarks/operator_benchmark/benchmark_caffe2.py  |  6 +++---\n benchmarks/operator_benchmark/benchmark_core.py    |  8 ++++----\n benchmarks/operator_benchmark/benchmark_pytorch.py |  2 +-\n .../operator_benchmark/common/repeat_benchmark.py  |  2 +-\n benchmarks/overrides_benchmark/bench.py            |  4 ++--\n benchmarks/sparse/dlmc/matmul_bench.py             |  4 ++--\n benchmarks/sparse/dlmc/utils.py                    |  8 ++++----\n benchmarks/tensorexpr/__main__.py                  |  9 ++++-----\n benchmarks/tensorexpr/benchmark.py                 |  4 ++--\n benchmarks/tensorexpr/reduction.py                 |  2 +-\n benchmarks/upload_scribe.py                        |  2 +-\n 39 files changed, 90 insertions(+), 91 deletions(-)\n\ndiff --git a/benchmarks/compare-fastrnn-results.py b/benchmarks/compare-fastrnn-results.py\nindex bc4a55688b17ed..45961ca78de53b 100644\n--- a/benchmarks/compare-fastrnn-results.py\n+++ b/benchmarks/compare-fastrnn-results.py\n@@ -23,9 +23,9 @@ def get_times(json_data):\n parser.add_argument('--format', default='md', type=str, help='output format (csv, md, json, table)')\n args = parser.parse_args()\n \n-with open(args.base, \"r\") as base:\n+with open(args.base) as base:\n     base_times = get_times(json.load(base))\n-with open(args.diff, \"r\") as diff:\n+with open(args.diff) as diff:\n     diff_times = get_times(json.load(diff))\n \n all_keys = set(base_times.keys()).union(diff_times.keys())\ndiff --git a/benchmarks/cpp/tensorexpr/bench_ops.py b/benchmarks/cpp/tensorexpr/bench_ops.py\nindex 12d766ae74862c..fef18f912e75c0 100644\n--- a/benchmarks/cpp/tensorexpr/bench_ops.py\n+++ b/benchmarks/cpp/tensorexpr/bench_ops.py\n@@ -83,8 +83,8 @@ def test_batch_norm():\n         [5, 512, 7, 7]]\n     for n, c, h, w in batch_norm_shapes:\n         x = torch.rand((n, c, h, w))\n-        y = torch.rand((c))\n-        z = torch.rand((c))\n+        y = torch.rand(c)\n+        z = torch.rand(c)\n         traced = torch.jit.trace(lambda x, y, z: op(x, y, z), (x, y, z))\n \n         # Warmup.\ndiff --git a/benchmarks/distributed/ddp/benchmark.py b/benchmarks/distributed/ddp/benchmark.py\nindex c72e3e6a27d961..db592cf6bd32d6 100644\n--- a/benchmarks/distributed/ddp/benchmark.py\n+++ b/benchmarks/distributed/ddp/benchmark.py\n@@ -181,7 +181,7 @@ def __init__(self, device, distributed_backend, bucket_size, model):\n         self.model = model\n \n     def __str__(self):\n-        return \"{} with batch size {}\".format(self.model, self.batch_size)\n+        return f\"{self.model} with batch size {self.batch_size}\"\n \n     def create_model(self):\n         return torchvision.models.__dict__[self.model]().to(self.device)\n@@ -212,7 +212,7 @@ def main():\n     # metadata, like measurements. Not for benchmarking itself.\n     dist.init_process_group(\n         backend=\"gloo\",\n-        init_method=\"tcp://{}:{}\".format(args.master_addr, args.master_port),\n+        init_method=f\"tcp://{args.master_addr}:{args.master_port}\",\n         rank=args.rank,\n         world_size=args.world_size,\n     )\n@@ -227,10 +227,10 @@ def main():\n         print(\"PyTorch distributed benchmark suite\")\n         print(\"-----------------------------------\")\n         print(\"\")\n-        print(\"* PyTorch version: {}\".format(torch.__version__))\n-        print(\"* CUDA version: {}\".format(torch.version.cuda))\n-        print(\"* Distributed backend: {}\".format(args.distributed_backend))\n-        print(\"* Maximum bucket size: {}MB\".format(args.bucket_size))\n+        print(f\"* PyTorch version: {torch.__version__}\")\n+        print(f\"* CUDA version: {torch.version.cuda}\")\n+        print(f\"* Distributed backend: {args.distributed_backend}\")\n+        print(f\"* Maximum bucket size: {args.bucket_size}MB\")\n         print(\"\")\n         print(\"--- nvidia-smi topo -m ---\")\n         print(\"\")\n@@ -261,7 +261,7 @@ def main():\n     benchmark_results = []\n     for benchmark in benchmarks:\n         if args.rank == 0:\n-            print(\"\\nBenchmark: {}\".format(str(benchmark)))\n+            print(f\"\\nBenchmark: {str(benchmark)}\")\n         result = sweep(benchmark)\n         benchmark_results.append({\n             \"model\": benchmark.model,\ndiff --git a/benchmarks/distributed/ddp/diff.py b/benchmarks/distributed/ddp/diff.py\nindex d427a5b29d9199..bce7a8db56c13e 100644\n--- a/benchmarks/distributed/ddp/diff.py\n+++ b/benchmarks/distributed/ddp/diff.py\n@@ -10,7 +10,7 @@\n \n \n def load(path):\n-    with open(path, 'r') as f:\n+    with open(path) as f:\n         return json.load(f)\n \n \n@@ -44,8 +44,8 @@ def main():\n \n         model = ra[\"model\"]\n         batch_size = int(ra[\"batch_size\"])\n-        name = \"{} with batch size {}\".format(model, batch_size)\n-        print(\"Benchmark: {}\".format(name))\n+        name = f\"{model} with batch size {batch_size}\"\n+        print(f\"Benchmark: {name}\")\n \n         # Print header\n         print(\"\")\n@@ -66,13 +66,13 @@ def main():\n             ngpus = len(xa[\"ranks\"])\n             ma = sorted(xa[\"measurements\"])\n             mb = sorted(xb[\"measurements\"])\n-            print(\"{:>4d} GPUs:\".format(ngpus), end='')  # noqa: E999\n+            print(f\"{ngpus:>4d} GPUs:\", end='')  # noqa: E999\n             for p in [75, 95]:\n                 va = np.percentile(ma, p)\n                 vb = np.percentile(mb, p)\n                 # We're measuring time, so lower is better (hence the negation)\n                 delta = -100 * ((vb - va) / va)\n-                print(\"  p{:02d}: {:8.3f}s {:7d}/s {:+8.1f}%\".format(p, vb, int(batch_size / vb), delta), end='')  # noqa: E999\n+                print(f\"  p{p:02d}: {vb:8.3f}s {int(batch_size / vb):7d}/s {delta:+8.1f}%\", end='')  # noqa: E999\n             print(\"\")\n         print(\"\")\n \ndiff --git a/benchmarks/distributed/pipeline/pipe.py b/benchmarks/distributed/pipeline/pipe.py\nindex 8a08d25ca4c940..58f2850e34868e 100644\n--- a/benchmarks/distributed/pipeline/pipe.py\n+++ b/benchmarks/distributed/pipeline/pipe.py\n@@ -16,7 +16,7 @@\n def sizeof_fmt(num, suffix='B'):\n     for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti']:\n         if abs(num) < 1024.0:\n-            return \"%3.2f%sB\" % (num, unit)\n+            return f\"{num:3.2f}{unit}B\"\n         num /= 1024.0\n \n \n@@ -146,7 +146,7 @@ def get_last_device(model):\n             return torch.cuda.current_device()\n \n \n-    print('Number of parameters for model: {}'.format(sum(p.numel() for p in model.parameters())))\n+    print(f'Number of parameters for model: {sum(p.numel() for p in model.parameters())}')\n     for i, batch in enumerate(lm_dataloader):\n         bi = batch[\"input\"]\n         if args.max_batch and i > args.max_batch:\ndiff --git a/benchmarks/distributed/rpc/parameter_server/launcher.py b/benchmarks/distributed/rpc/parameter_server/launcher.py\nindex a4c13cdb29b696..ec6559c8508f9c 100644\n--- a/benchmarks/distributed/rpc/parameter_server/launcher.py\n+++ b/benchmarks/distributed/rpc/parameter_server/launcher.py\n@@ -362,7 +362,7 @@ def get_json_config(file_name, id):\n         file_name (str): name of configuration file to load\n         id (str): configuration that will be loaded\n     \"\"\"\n-    with open(os.path.join(Path(__file__).parent, file_name), \"r\") as f:\n+    with open(os.path.join(Path(__file__).parent, file_name)) as f:\n         json_config = json.load(f)[id]\n     return json_config\n \ndiff --git a/benchmarks/distributed/rpc/rl/coordinator.py b/benchmarks/distributed/rpc/rl/coordinator.py\nindex b488378d5aee58..8b6d246f0861f6 100644\n--- a/benchmarks/distributed/rpc/rl/coordinator.py\n+++ b/benchmarks/distributed/rpc/rl/coordinator.py\n@@ -102,7 +102,7 @@ def run_coordinator(self, episodes, episode_steps, queue):\n                              'observer throughput': {}}\n \n \n-        print(\"For batch size {0}\".format(self.batch_size))\n+        print(f\"For batch size {self.batch_size}\")\n         print(\"\\nAgent Latency - \", len(agent_latency_final))\n         agent_latency_final = sorted(agent_latency_final)\n         for p in [50, 75, 90, 95]:\ndiff --git a/benchmarks/dynamo/_onnx/reporter.py b/benchmarks/dynamo/_onnx/reporter.py\nindex f10b50c4e323df..9318b7a6dc2e47 100644\n--- a/benchmarks/dynamo/_onnx/reporter.py\n+++ b/benchmarks/dynamo/_onnx/reporter.py\n@@ -22,7 +22,7 @@\n _COMPACT_ERROR_GROUP = False\n \n \n-class ErrorAggregator(object):\n+class ErrorAggregator:\n     \"\"\"\n     Collect and group error messages for report at the end.\n \n@@ -47,7 +47,7 @@ class ErrorAggregator(object):\n     ]\n \n     def __init__(self, log: Optional[logging.Logger] = None):\n-        super(ErrorAggregator, self).__init__()\n+        super().__init__()\n         self.error_groups = []\n         self.bigram_to_group_ids = collections.defaultdict(list)\n         self.log = log or logging.getLogger(__name__)\n@@ -141,7 +141,7 @@ def __len__(self):\n         return sum(map(len, self.error_groups))\n \n \n-class ErrorAggregatorDict(object):\n+class ErrorAggregatorDict:\n     \"\"\"\n     Collect error types and individually group their error messages for a debug report at the end.\n \n@@ -152,7 +152,7 @@ class ErrorAggregatorDict(object):\n     \"\"\"\n \n     def __init__(self):\n-        super(ErrorAggregatorDict, self).__init__()\n+        super().__init__()\n         self.aggregator: Dict[str, ErrorAggregator] = dict()\n \n     def __getitem__(self, item: str):\n@@ -179,7 +179,7 @@ def record(self, error_type: str, error: str, module: str):\n             log.exception(\"%s error from %s\", error_type, module)\n \n \n-class ExportErrorCsvParser(object):\n+class ExportErrorCsvParser:\n     \"\"\"Parses `*_export_error.csv` produced by onnxbench, aggregates errors and produces report.\n \n     Two types of aggregations are performed.\n@@ -310,7 +310,7 @@ def row(self) -> List[str]:\n         return [getattr(self, field.name) for field in dataclasses.fields(self)]\n \n \n-class ExportErrorParser(object):\n+class ExportErrorParser:\n     def __init__(self, device: str, model_name: str, batch_size: int):\n         self.device = device\n         self.model_name = model_name\ndiff --git a/benchmarks/dynamo/benchmarks.py b/benchmarks/dynamo/benchmarks.py\nindex 36aaf33df96b1f..cb4cc84867ca9b 100755\n--- a/benchmarks/dynamo/benchmarks.py\n+++ b/benchmarks/dynamo/benchmarks.py\n@@ -9,7 +9,7 @@\n # TOOD(voz): Someday, consolidate all the files into one runner instead of a shim like this...\n def model_names(filename: str) -> Set[str]:\n     names = set()\n-    with open(filename, \"r\") as fh:\n+    with open(filename) as fh:\n         lines = fh.readlines()\n         lines = [line.rstrip() for line in lines]\n         for line in lines:\ndiff --git a/benchmarks/dynamo/combine_csv.py b/benchmarks/dynamo/combine_csv.py\nindex b579e0a1bbbd5c..560b8a3cf2405a 100644\n--- a/benchmarks/dynamo/combine_csv.py\n+++ b/benchmarks/dynamo/combine_csv.py\n@@ -11,7 +11,7 @@\n RESULTS = defaultdict(dict)\n \n for side, f in zip([\"static\", \"dynamic\"], sys.argv[1:]):\n-    with open(f, \"r\") as f:\n+    with open(f) as f:\n         reader = csv.DictReader(f)\n         for row in reader:\n             RESULTS[(row[\"bench\"], row[\"name\"])][side] = row\ndiff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py\nindex cabc18f35697a7..0e1ce36461a122 100644\n--- a/benchmarks/dynamo/common.py\n+++ b/benchmarks/dynamo/common.py\n@@ -341,7 +341,7 @@ def load_model_from_path(path_and_class_str):\n \n def output_csv(filename, headers, row):\n     if os.path.exists(filename):\n-        with open(filename, \"r\") as fd:\n+        with open(filename) as fd:\n             lines = list(csv.reader(fd)) or [[]]\n             if headers and len(headers) > len(lines[0]):\n                 # if prior results failed the header might not be filled in yet\n@@ -1417,7 +1417,7 @@ def read_batch_size_from_file(args, filename, model_name):\n     if os.path.exists(\"benchmarks\"):\n         filename = os.path.join(\"benchmarks\", filename)\n     assert os.path.exists(filename), filename\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n         lines = [i.split(\",\") for i in lines if len(i.strip()) > 0]\n         for val in lines:\ndiff --git a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\nindex 997624f583b4c8..f83568c4db6cc6 100644\n--- a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n+++ b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n@@ -240,7 +240,7 @@ class OperatorInputsLoader:\n     def __init__(self, json_file_path):\n         self.operator_db = defaultdict(Counter)\n \n-        with open(json_file_path, \"r\") as f:\n+        with open(json_file_path) as f:\n             lines = f.readlines()\n \n         i = 0\ndiff --git a/benchmarks/dynamo/parse_logs.py b/benchmarks/dynamo/parse_logs.py\nindex aeb72c231c5462..8ae272897903f9 100644\n--- a/benchmarks/dynamo/parse_logs.py\n+++ b/benchmarks/dynamo/parse_logs.py\n@@ -15,7 +15,7 @@\n \n assert len(sys.argv) == 2\n \n-full_log = open(sys.argv[1], \"r\").read()\n+full_log = open(sys.argv[1]).read()\n \n # If the log contains a gist URL, extract it so we can include it in the CSV\n gist_url = \"\"\ndiff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py\nindex c3c3e8bc046c8e..5a544914ca74bd 100755\n--- a/benchmarks/dynamo/runner.py\n+++ b/benchmarks/dynamo/runner.py\n@@ -608,7 +608,7 @@ def __init__(\n \n     def has_header(self, output_filename):\n         header_present = False\n-        with open(output_filename, \"r\") as f:\n+        with open(output_filename) as f:\n             line = f.readline()\n             if \"dev\" in line:\n                 header_present = True\n@@ -1026,7 +1026,7 @@ def __init__(self, args):\n         assert os.path.exists(self.lookup_file)\n \n     def generate_diff(self, last2, filename, caption):\n-        df_cur, df_prev = [pd.read_csv(os.path.join(path, filename)) for path in last2]\n+        df_cur, df_prev = (pd.read_csv(os.path.join(path, filename)) for path in last2)\n         df_merge = df_cur.merge(df_prev, on=\"Compiler\", suffixes=(\"_cur\", \"_prev\"))\n         data = {col: [] for col in (\"compiler\", \"suite\", \"prev_value\", \"cur_value\")}\n         for _, row in df_merge.iterrows():\n@@ -1145,10 +1145,10 @@ def generate_comment(self):\n                     if last2[compiler] is None:\n                         continue\n \n-                    df_cur, df_prev = [\n+                    df_cur, df_prev = (\n                         last2[compiler][i].untouched_parsed_frames[suite][metric]\n                         for i in (0, 1)\n-                    ]\n+                    )\n                     df_merge = df_cur.merge(\n                         df_prev, on=\"name\", suffixes=(\"_cur\", \"_prev\")\n                     )\n@@ -1367,7 +1367,7 @@ def gen_comment(self):\n         all_lines = []\n         for f in files:\n             try:\n-                with open(os.path.join(self.output_dir, f), \"r\") as fh:\n+                with open(os.path.join(self.output_dir, f)) as fh:\n                     all_lines.extend(fh.readlines())\n             except FileNotFoundError:\n                 pass\ndiff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py\nindex 75769f7cb6c50a..587dbb93683f3f 100755\n--- a/benchmarks/dynamo/timm_models.py\n+++ b/benchmarks/dynamo/timm_models.py\n@@ -31,7 +31,7 @@ def pip_install(package):\n TIMM_MODELS = dict()\n filename = os.path.join(os.path.dirname(__file__), \"timm_models_list.txt\")\n \n-with open(filename, \"r\") as fh:\n+with open(filename) as fh:\n     lines = fh.readlines()\n     lines = [line.rstrip() for line in lines]\n     for line in lines:\n@@ -92,7 +92,7 @@ def read_models_from_docs():\n         models = set()\n         # TODO - set the path to pytorch-image-models repo\n         for fn in glob.glob(\"../pytorch-image-models/docs/models/*.md\"):\n-            with open(fn, \"r\") as f:\n+            with open(fn) as f:\n                 while True:\n                     line = f.readline()\n                     if not line:\ndiff --git a/benchmarks/fastrnns/bench.py b/benchmarks/fastrnns/bench.py\nindex d4b70ff78b7a72..f0e9679b80f275 100644\n--- a/benchmarks/fastrnns/bench.py\n+++ b/benchmarks/fastrnns/bench.py\n@@ -187,7 +187,7 @@ def bench(rnn_runners, group_name, print_json=False, sep=' ', **params):\n \n \n def bench_group(model_list, bench_name, bench_group, bench_args):\n-    print_stderr('Benchmarking {}s...'.format(bench_name))\n+    print_stderr(f'Benchmarking {bench_name}s...')\n     nn_results = bench(get_nn_runners(*model_list), bench_group, **bench_args)\n     print_stderr('')\n     return nn_results\ndiff --git a/benchmarks/fastrnns/profile.py b/benchmarks/fastrnns/profile.py\nindex 7f3de61ef9c39f..10707fab986bc7 100644\n--- a/benchmarks/fastrnns/profile.py\n+++ b/benchmarks/fastrnns/profile.py\n@@ -54,7 +54,7 @@ def profile(rnns, sleep_between_seconds=1, nloops=5,\n \n def system(command):\n     \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n-    print('[system] {}'.format(command))\n+    print(f'[system] {command}')\n     p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, shell=True)\n     output, err = p.communicate()\n@@ -87,13 +87,13 @@ def nvprof_output_filename(rnns, **params):\n \n \n def nvprof(cmd, outpath):\n-    return system('nvprof -o {} {}'.format(outpath, cmd))\n+    return system(f'nvprof -o {outpath} {cmd}')\n \n \n def full_profile(rnns, **args):\n     profile_args = []\n     for k, v in args.items():\n-        profile_args.append('--{}={}'.format(k, v))\n+        profile_args.append(f'--{k}={v}')\n     profile_args.append('--rnns {}'.format(' '.join(rnns)))\n     profile_args.append('--internal-run')\n \n@@ -103,7 +103,7 @@ def full_profile(rnns, **args):\n         sys.executable, ' '.join(profile_args))\n     rc, stdout, stderr = nvprof(cmd, outpath)\n     if rc != 0:\n-        raise RuntimeError('stderr: {}\\nstdout: {}'.format(stderr, stdout))\n+        raise RuntimeError(f'stderr: {stderr}\\nstdout: {stdout}')\n \n \n if __name__ == '__main__':\ndiff --git a/benchmarks/fastrnns/runner.py b/benchmarks/fastrnns/runner.py\nindex c6bf3727f38071..c33f3c92ad0f04 100644\n--- a/benchmarks/fastrnns/runner.py\n+++ b/benchmarks/fastrnns/runner.py\n@@ -11,7 +11,7 @@\n                       varlen_lstm_creator, varlen_pytorch_lstm_creator)\n \n \n-class DisableCuDNN():\n+class DisableCuDNN:\n     def __enter__(self):\n         self.saved = torch.backends.cudnn.enabled\n         torch.backends.cudnn.enabled = False\n@@ -20,7 +20,7 @@ def __exit__(self, *args, **kwargs):\n         torch.backends.cudnn.enabled = self.saved\n \n \n-class DummyContext():\n+class DummyContext:\n     def __enter__(self):\n         pass\n \n@@ -28,7 +28,7 @@ def __exit__(self, *args, **kwargs):\n         pass\n \n \n-class AssertNoJIT():\n+class AssertNoJIT:\n     def __enter__(self):\n         import os\n         enabled = os.environ.get('PYTORCH_JIT', 1)\ndiff --git a/benchmarks/fastrnns/test.py b/benchmarks/fastrnns/test.py\nindex a56cf928fd7add..640af10b95c042 100644\n--- a/benchmarks/fastrnns/test.py\n+++ b/benchmarks/fastrnns/test.py\n@@ -71,7 +71,7 @@ def test_vl_py(**test_args):\n     control_creator = varlen_pytorch_lstm_creator\n     name, experim_creator, context = get_nn_runners('vl_py')[0]\n     with context():\n-        print('testing {}...'.format(name))\n+        print(f'testing {name}...')\n         creator_keys = [\n             'seqLength', 'numLayers', 'inputSize',\n             'hiddenSize', 'miniBatch', 'device', 'seed'\n@@ -154,5 +154,5 @@ def test_vl_py(**test_args):\n \n     for name, creator, context in rnn_runners:\n         with context():\n-            print('testing {}...'.format(name))\n+            print(f'testing {name}...')\n             test_rnns(creator, pytorch_lstm_creator, **test_args)\ndiff --git a/benchmarks/framework_overhead_benchmark/C2Module.py b/benchmarks/framework_overhead_benchmark/C2Module.py\nindex dfc5e6e79098a6..b6b80e83db07d0 100644\n--- a/benchmarks/framework_overhead_benchmark/C2Module.py\n+++ b/benchmarks/framework_overhead_benchmark/C2Module.py\n@@ -20,14 +20,14 @@ class C2SimpleNet:\n     def __init__(self, op_name, num_inputs=1, debug=False):\n         self.input_names = []\n         self.net = core.Net(\"framework_benchmark_net\")\n-        self.input_names = [\"in_{}\".format(i) for i in range(num_inputs)]\n+        self.input_names = [f\"in_{i}\" for i in range(num_inputs)]\n         for i in range(num_inputs):\n             add_blob(workspace, self.input_names[i], [1])\n         self.net.AddExternalInputs(self.input_names)\n         op_constructor = getattr(self.net, op_name)\n         op_constructor(self.input_names)\n         self.output_name = self.net._net.op[-1].output\n-        print(\"Benchmarking op {}:\".format(op_name))\n+        print(f\"Benchmarking op {op_name}:\")\n         for _ in range(NUM_LOOP_ITERS):\n             output_name = self.net._net.op[-1].output\n             self.input_names[-1] = output_name[0]\ndiff --git a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\nindex 727b78197b39bc..fd02a00c43655d 100644\n--- a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n+++ b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n@@ -31,7 +31,7 @@ def parse_op_args(op):\n def print_results(result):\n     print(\"===================================\")\n     for key, value in result.items():\n-        print(\"{}, latency per iter (us):{}\".format(key, ms_to_us(value)))\n+        print(f\"{key}, latency per iter (us):{ms_to_us(value)}\")\n     print(\"===================================\")\n \n def benchmark_simple_fn(args, config, module_config, module_type, result):\n@@ -46,7 +46,7 @@ def benchmark_simple_fn(args, config, module_config, module_type, result):\n         result:         dictionary instance to be populated with the benchmark result (latency per iter).\n     \"\"\"\n     benchmark_c2_net = args.benchmark_c2_net\n-    print(\"Benchmarking {}\".format(module_type.__name__))\n+    print(f\"Benchmarking {module_type.__name__}\")\n     if benchmark_c2_net:\n         op_name = module_config.c2_op\n         num_inputs = module_config.num_params\n@@ -86,7 +86,7 @@ def main():\n     args = parser.parse_args()\n \n     if args.op not in SUPPORTED_OPS:\n-        print(\"Op {} is not supported: Supported ops are:{}\".format(args.op, SUPPORTED_OPS))\n+        print(f\"Op {args.op} is not supported: Supported ops are:{SUPPORTED_OPS}\")\n         return\n     assert not (args.benchmark_c2_net and args.use_throughput_benchmark), \\\n         \"Benchmarking of C2 net via throughput benchmarking is not yet supported\"\ndiff --git a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\nindex 154564f1c6d799..19f2471cbbfaaa 100644\n--- a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n+++ b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n@@ -31,8 +31,8 @@ def __init__(self, wrapped_type, module_config, debug, save=False):\n             if save:\n                 file_name = self.module_name + \"_\" + pt_fn.__name__ + \".pt\"\n                 torch.jit.save(self.module, file_name)\n-                print(\"Generated graph is saved in {}\".format(file_name))\n-        print(\"Benchmarking module {} with fn {}: Graph mode:{}\".format(self.module_name, pt_fn.__name__, module_config.graph_mode))\n+                print(f\"Generated graph is saved in {file_name}\")\n+        print(f\"Benchmarking module {self.module_name} with fn {pt_fn.__name__}: Graph mode:{module_config.graph_mode}\")\n         if (debug and isinstance(self.module, torch.jit.ScriptModule)):\n             print(self.module.graph)\n             print(self.module.code)\ndiff --git a/benchmarks/framework_overhead_benchmark/utils.py b/benchmarks/framework_overhead_benchmark/utils.py\nindex 9e760d404339ed..2efb67a51f7887 100644\n--- a/benchmarks/framework_overhead_benchmark/utils.py\n+++ b/benchmarks/framework_overhead_benchmark/utils.py\n@@ -26,7 +26,7 @@ def benchmark_module(config, module, use_throughput_benchmark=False):\n     if use_throughput_benchmark:\n         return benchmark_using_throughput_benchmark(config, module)\n     module.forward(config.num_warmup_iters)\n-    print(\"Running module for {} iterations\".format(config.num_iters))\n+    print(f\"Running module for {config.num_iters} iterations\")\n     start = time.time()\n     module.forward(config.num_iters)\n     end = time.time()\ndiff --git a/benchmarks/functional_autograd_benchmark/compare.py b/benchmarks/functional_autograd_benchmark/compare.py\nindex c2c4ef6c95d5be..65a4a3afcea881 100644\n--- a/benchmarks/functional_autograd_benchmark/compare.py\n+++ b/benchmarks/functional_autograd_benchmark/compare.py\n@@ -10,11 +10,11 @@ def main():\n     parser.add_argument(\"--output\", type=str, default=\"\", help=\"Text file where to write the output\")\n     args = parser.parse_args()\n \n-    with open(args.before, \"r\") as f:\n+    with open(args.before) as f:\n         content = f.read()\n     res_before = from_markdown_table(content)\n \n-    with open(args.after, \"r\") as f:\n+    with open(args.after) as f:\n         content = f.read()\n     res_after = from_markdown_table(content)\n \ndiff --git a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\nindex 1b0ef20902da9b..76c447f04a496f 100644\n--- a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n+++ b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n@@ -198,7 +198,7 @@ def noop():\n             pass\n         do_sync = noop\n     else:\n-        device = torch.device(\"cuda:{}\".format(args.gpu))\n+        device = torch.device(f\"cuda:{args.gpu}\")\n         do_sync = torch.cuda.synchronize\n \n     model, inp = model_getter(device)\n@@ -257,7 +257,7 @@ def main():\n             runtimes = torch.tensor(runtimes)\n             mean, var = runtimes.mean(), runtimes.var()\n             results[name][task] = (mean.item(), var.item())\n-            print(\"Results for model {} on task {}: {}s (var: {})\".format(name, task, mean, var))\n+            print(f\"Results for model {name} on task {task}: {mean}s (var: {var})\")\n \n             if has_functorch:\n                 try:\n@@ -269,7 +269,7 @@ def main():\n                 runtimes = torch.tensor(runtimes)\n                 mean, var = runtimes.mean(), runtimes.var()\n                 results[name][f\"functorch {task}\"] = (mean.item(), var.item())\n-                print(\"Results for model {} on task {} using Functorch: {}s (var: {})\".format(name, task, mean, var))\n+                print(f\"Results for model {name} on task {task} using Functorch: {mean}s (var: {var})\")\n \n     if args.output:\n         with open(args.output, \"w\") as f:\ndiff --git a/benchmarks/functional_autograd_benchmark/utils.py b/benchmarks/functional_autograd_benchmark/utils.py\nindex dcf03e7a28d085..23f3481cbde117 100644\n--- a/benchmarks/functional_autograd_benchmark/utils.py\n+++ b/benchmarks/functional_autograd_benchmark/utils.py\n@@ -97,7 +97,7 @@ def from_markdown_table(data: str) -> TimingResultType:\n     res = defaultdict(defaultdict)\n \n     for line in out:\n-        model, task, mean, var = [f.strip() for f in line.strip().split(\"|\") if f]\n+        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n         res[model][task] = (float(mean), float(var))\n \n     return res\ndiff --git a/benchmarks/instruction_counts/applications/ci.py b/benchmarks/instruction_counts/applications/ci.py\nindex 85ee9881d83b55..c34c60b094a857 100644\n--- a/benchmarks/instruction_counts/applications/ci.py\n+++ b/benchmarks/instruction_counts/applications/ci.py\n@@ -70,7 +70,7 @@ def main(argv: List[str]) -> None:\n     }\n \n     if args.destination:\n-        with open(args.destination, \"wt\") as f:\n+        with open(args.destination, \"w\") as f:\n             json.dump(final_results, f)\n \n     if in_debug_mode:\ndiff --git a/benchmarks/instruction_counts/core/expand.py b/benchmarks/instruction_counts/core/expand.py\nindex f6713ee65cb93c..c60925d9e14e91 100644\n--- a/benchmarks/instruction_counts/core/expand.py\n+++ b/benchmarks/instruction_counts/core/expand.py\n@@ -58,7 +58,7 @@ def _generate_torchscript_file(model_src: str, name: str) -> Optional[str]:\n         # to confirm.\n         raise ValueError(f\"File {module_path} already exists.\")\n \n-    with open(module_path, \"wt\") as f:\n+    with open(module_path, \"w\") as f:\n         f.write(model_src)\n \n     # Import magic to actually load our function.\ndiff --git a/benchmarks/operator_benchmark/benchmark_caffe2.py b/benchmarks/operator_benchmark/benchmark_caffe2.py\nindex d5939030d03c1a..df27a172739bf6 100644\n--- a/benchmarks/operator_benchmark/benchmark_caffe2.py\n+++ b/benchmarks/operator_benchmark/benchmark_caffe2.py\n@@ -122,7 +122,7 @@ def run_forward(self, num_runs, print_per_iter=False, cuda_sync=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.forward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator test case: {self.test_name}\")\n \n     def run_backward(self, num_runs, print_per_iter=False):\n         \"\"\" Run the backward path of an operator in a loop\n@@ -130,7 +130,7 @@ def run_backward(self, num_runs, print_per_iter=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.backward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator gradient test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator gradient test case: {self.test_name}\")\n \n     def _print_per_iter(self):\n         pass\n@@ -140,7 +140,7 @@ def create_caffe2_op_test_case(op_bench, test_config):\n     test_case = Caffe2OperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\n \n \ndiff --git a/benchmarks/operator_benchmark/benchmark_core.py b/benchmarks/operator_benchmark/benchmark_core.py\nindex d6fd00f0522b38..3dfb6e3b00d42c 100644\n--- a/benchmarks/operator_benchmark/benchmark_core.py\n+++ b/benchmarks/operator_benchmark/benchmark_core.py\n@@ -197,7 +197,7 @@ def _print_header(self):\n             print(\"# List of Operators to run:\")\n             self.printed_ops_list = set()\n             if self.args.operators:\n-                print(\"# {}\".format(self.args.operators))\n+                print(f\"# {self.args.operators}\")\n \n     def _print_perf_result(self, reported_run_time_us, test_case):\n         if self.args.report_aibench:\n@@ -206,7 +206,7 @@ def _print_perf_result(self, reported_run_time_us, test_case):\n             return\n             test_name = '_'.join([test_case.framework, test_case.test_config.test_name])\n             for run in range(self.num_runs):\n-                print(\"{}Observer \".format(test_case.framework) + json.dumps(\n+                print(f\"{test_case.framework}Observer \" + json.dumps(\n                     {\n                         \"type\": test_name,\n                         \"metric\": \"latency\",\n@@ -349,14 +349,14 @@ def _keep_test(self, test_case):\n     def _print_test_case_info(self, test_case):\n         # Print out the test name and skip the real execution\n         if self.args.list_tests:\n-            print(\"# {}\".format(test_case.test_config.test_name))\n+            print(f\"# {test_case.test_config.test_name}\")\n             return True\n         elif self.args.list_ops:\n             if self.args.operators is None:\n                 op_name = test_case.op_bench.module_name()\n \n                 if op_name not in self.printed_ops_list:\n-                    print(\"# {}\".format(op_name))\n+                    print(f\"# {op_name}\")\n                     self.printed_ops_list.add(op_name)\n             return True\n \ndiff --git a/benchmarks/operator_benchmark/benchmark_pytorch.py b/benchmarks/operator_benchmark/benchmark_pytorch.py\nindex e9a9b3c5de42ad..c4a82dff2ba5c1 100644\n--- a/benchmarks/operator_benchmark/benchmark_pytorch.py\n+++ b/benchmarks/operator_benchmark/benchmark_pytorch.py\n@@ -192,5 +192,5 @@ def create_pytorch_op_test_case(op_bench, test_config):\n     test_case = PyTorchOperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\ndiff --git a/benchmarks/operator_benchmark/common/repeat_benchmark.py b/benchmarks/operator_benchmark/common/repeat_benchmark.py\nindex b744a95d217363..337bf525b29caf 100644\n--- a/benchmarks/operator_benchmark/common/repeat_benchmark.py\n+++ b/benchmarks/operator_benchmark/common/repeat_benchmark.py\n@@ -54,4 +54,4 @@ def pt_repeat_n_times(niters):\n     total_time_s = (time.time() - s)\n     total_time_per_iter_s = total_time_s / NUM_BENCHMARK_ITERS\n     achieved_bandwidth = (total_bytes * BYTES_TO_MB) / total_time_per_iter_s\n-    print(\"Time:{} Achieved Bandwidth:{} MB/s\".format(total_time_per_iter_s, achieved_bandwidth))\n+    print(f\"Time:{total_time_per_iter_s} Achieved Bandwidth:{achieved_bandwidth} MB/s\")\ndiff --git a/benchmarks/overrides_benchmark/bench.py b/benchmarks/overrides_benchmark/bench.py\nindex b6dbd0c2f8d64c..2c591a6e569793 100644\n--- a/benchmarks/overrides_benchmark/bench.py\n+++ b/benchmarks/overrides_benchmark/bench.py\n@@ -56,8 +56,8 @@ def main():\n \n         bench_min, bench_std = bench(tensor_1, tensor_2)\n         print(\n-            \"Type {0} had a minimum time of {1} us\"\n-            \" and a standard deviation of {2} us.\".format(\n+            \"Type {} had a minimum time of {} us\"\n+            \" and a standard deviation of {} us.\".format(\n                 t.__name__, (10 ** 6 * bench_min), (10 ** 6) * bench_std\n             )\n         )\ndiff --git a/benchmarks/sparse/dlmc/matmul_bench.py b/benchmarks/sparse/dlmc/matmul_bench.py\nindex 6b896ddf34a635..8d37d3242dd1bf 100644\n--- a/benchmarks/sparse/dlmc/matmul_bench.py\n+++ b/benchmarks/sparse/dlmc/matmul_bench.py\n@@ -62,8 +62,8 @@ def filter_ops(operation):\n             test_name = device + \":matmul-forward\"\n             return list(filter(None, [\n                 (test_name, device, \"torch:\" + operation.replace(\"sparse\", \"dense\"),\n-                 \"{}(dx, dy)\".format(OPS_MAP[operation])),\n-                (test_name, device, \"torch:\" + operation, \"{}(x, y)\".format(OPS_MAP[operation])),\n+                 f\"{OPS_MAP[operation]}(dx, dy)\"),\n+                (test_name, device, \"torch:\" + operation, f\"{OPS_MAP[operation]}(x, y)\"),\n                 (test_name, device, \"scipy:\" + operation, \"scipy_matmul(sx, sy)\") if device == \"cpu\" else None\n             ]))\n \ndiff --git a/benchmarks/sparse/dlmc/utils.py b/benchmarks/sparse/dlmc/utils.py\nindex 3079abf6e1dff2..8fad391f63f83d 100644\n--- a/benchmarks/sparse/dlmc/utils.py\n+++ b/benchmarks/sparse/dlmc/utils.py\n@@ -21,7 +21,7 @@ def sparse_grad_output(a, b):\n \n \n def read_matrix_params(path):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         line = file.readline()\n         nrows, ncols, nnz = (int(el) for el in line.split(', '))\n         return (nrows, ncols), nnz\n@@ -38,7 +38,7 @@ def csr_to_coo(indices, indptr, shape):\n \n \n def load_sparse_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -51,7 +51,7 @@ def load_sparse_matrix(path, device):\n \n \n def gen_vector(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -59,7 +59,7 @@ def gen_vector(path, device):\n \n \n def gen_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\ndiff --git a/benchmarks/tensorexpr/__main__.py b/benchmarks/tensorexpr/__main__.py\nindex ed632e966b2cb4..fa81ea6bb1c611 100644\n--- a/benchmarks/tensorexpr/__main__.py\n+++ b/benchmarks/tensorexpr/__main__.py\n@@ -157,7 +157,7 @@ def main():\n         torch._C._jit_set_nvfuser_enabled(True)\n         torch._C._get_graph_executor_optimize(True)\n     else :\n-        raise ValueError(\"Undefined fuser: {}\".format(args.cuda_fuser))\n+        raise ValueError(f\"Undefined fuser: {args.cuda_fuser}\")\n \n     if args.cpu_fusion:\n         import torch\n@@ -207,7 +207,7 @@ def set_global_threads(num_threads):\n     for index, dtype in enumerate(datatypes):\n         datatypes[index] = getattr(torch, dtype)\n         if not datatypes[index] :\n-            raise AttributeError(\"DataType: {} is not valid!\".format(dtype))\n+            raise AttributeError(f\"DataType: {dtype} is not valid!\")\n \n     tensor_engine.set_engine_mode(args.engine)\n \n@@ -282,7 +282,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                         run_with_input_iter(bench_cls, args.input_iter, allow_skip=True)\n                     else :\n                         if args.input_iter is not None :\n-                            print(\"WARNING: Incompatible benchmark class called with input_iter arg: {}\".format(name))\n+                            print(f\"WARNING: Incompatible benchmark class called with input_iter arg: {name}\")\n                         run_default_configs(bench_cls, allow_skip=True)\n \n             if match_class_name:\n@@ -321,8 +321,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                     [bench_cls.module() for bench_cls in benchmark_classes]\n                 )\n                 raise ValueError(\n-                    \"invalid name: %s\\nAvailable benchmark classes:\\n%s\"\n-                    % (name, available_classes)\n+                    f\"invalid name: {name}\\nAvailable benchmark classes:\\n{available_classes}\"\n                 )\n \n \ndiff --git a/benchmarks/tensorexpr/benchmark.py b/benchmarks/tensorexpr/benchmark.py\nindex 7a5b255da904aa..2730a1be24784b 100644\n--- a/benchmarks/tensorexpr/benchmark.py\n+++ b/benchmarks/tensorexpr/benchmark.py\n@@ -66,7 +66,7 @@ def desc(self):\n         if \"NNC_NUM_THREADS\" in os.environ:\n             num_threads_str = os.environ[\"NNC_NUM_THREADS\"]\n             device += num_threads_str\n-        return \"%s: %s_%s_%s_%s\" % (\n+        return \"{}: {}_{}_{}_{}\".format(\n             self.engine.mode,\n             self.module(),\n             self.mode,\n@@ -203,7 +203,7 @@ def dump_result(self, result_dict):\n         if self.output_type == \"json\":\n             print(json.dumps(result_dict))\n         elif self.output_type == \"stdout\":\n-            msg = \"%s: %.2f us, SOL %.2f GB/s, algorithmic %.2f GB/s\" % (\n+            msg = \"{}: {:.2f} us, SOL {:.2f} GB/s, algorithmic {:.2f} GB/s\".format(\n                 result_dict[\"desc\"],\n                 result_dict[\"us\"],\n                 result_dict[\"sol\"],\ndiff --git a/benchmarks/tensorexpr/reduction.py b/benchmarks/tensorexpr/reduction.py\nindex 77d64074eb81d1..3613001667746d 100644\n--- a/benchmarks/tensorexpr/reduction.py\n+++ b/benchmarks/tensorexpr/reduction.py\n@@ -139,7 +139,7 @@ def __init__(self, mode, device, dtype, red_dim, dim0, dim1):\n         )]\n \n         if red_dim != 0 and red_dim != 1 :\n-            raise ValueError(\"invalid reduction dimension: {}\".format(red_dim))\n+            raise ValueError(f\"invalid reduction dimension: {red_dim}\")\n \n     def forward(self, inputs):\n         x = self.add(inputs, 0.001)\ndiff --git a/benchmarks/upload_scribe.py b/benchmarks/upload_scribe.py\nindex d476ade1b8df4d..551544b2d288ae 100644\n--- a/benchmarks/upload_scribe.py\n+++ b/benchmarks/upload_scribe.py\n@@ -95,7 +95,7 @@ def post_pytest_benchmarks(self, pytest_json):\n         for b in pytest_json['benchmarks']:\n             test = b['name'].split('[')[0]\n             net_name = b['params']['net_name']\n-            benchmark_name = '{}[{}]'.format(test, net_name)\n+            benchmark_name = f'{test}[{net_name}]'\n             executor = b['params']['executor']\n             fuser = b['params']['fuser']\n             m = self.format_message({\n"
  },
  {
    "number": 105408,
    "title": "[BE] Enable ruff's UP rules and autoformat tools and scripts",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "dbf0f36412813a35a38afd5695d7e1d94a9e84bf",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105408",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105408/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105408.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105408.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105408/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105408/comments",
    "labels": [
      "open source",
      "release notes: releng"
    ],
    "_event_time": "2023-07-18T01:15:55.440450Z",
    "state": "closed",
    "patch": "From d967d20a887023af0c94865cc96ffa6d73f60c93 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:48 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat tools and scripts\n\n[ghstack-poisoned]\n---\n .ci/pytorch/create_test_cert.py               |  4 +-\n .../perf_test/compare_with_baseline.py        |  4 +-\n .../cimodel/data/binary_build_definitions.py  |  2 +-\n .../cimodel/data/pytorch_build_definitions.py |  4 +-\n .circleci/generate_config_yml.py              |  4 +-\n .github/scripts/ensure_actions_will_cancel.py |  4 +-\n .github/scripts/file_io_utils.py              |  2 +-\n .github/scripts/filter_test_configs.py        |  2 +-\n .github/scripts/generate_pytorch_version.py   |  2 +-\n .github/scripts/label_utils.py                |  2 +-\n .github/scripts/lint_native_functions.py      |  2 +-\n .github/scripts/run_torchbench.py             |  4 +-\n .github/scripts/test_check_labels.py          |  2 +-\n .github/scripts/test_trymerge.py              |  2 +-\n .github/scripts/trymerge.py                   |  6 +--\n .github/scripts/trymerge_explainer.py         |  2 +-\n docs/caffe2/process.py                        |  4 +-\n docs/cpp/source/conf.py                       |  3 +-\n docs/source/conf.py                           |  3 +-\n .../scripts/exportdb/generate_example_rst.py  |  4 +-\n setup.py                                      | 38 +++++++++----------\n tools/amd_build/build_amd.py                  |  8 ++--\n tools/autograd/gen_python_functions.py        |  8 ++--\n tools/autograd/load_derivatives.py            |  2 +-\n .../gen_op_registration_allowlist.py          |  4 +-\n tools/code_analyzer/gen_oplist.py             |  2 +-\n tools/coverage_plugins_package/setup.py       |  2 +-\n tools/download_mnist.py                       | 10 ++---\n tools/gen_vulkan_spv.py                       | 20 +++++-----\n tools/generate_torch_version.py               | 12 +++---\n tools/jit/gen_unboxing.py                     |  2 +-\n tools/linter/adapters/constexpr_linter.py     |  2 +-\n tools/linter/adapters/grep_linter.py          |  2 +-\n tools/nightly.py                              |  8 ++--\n tools/onnx/gen_diagnostics.py                 |  2 +-\n tools/onnx/update_default_opset_version.py    |  2 +-\n tools/pyi/gen_pyi.py                          | 20 +++++-----\n tools/setup_helpers/cmake.py                  | 18 ++++-----\n tools/setup_helpers/cmake_utils.py            |  2 +-\n tools/setup_helpers/generate_code.py          |  2 +-\n tools/stats/import_test_stats.py              |  2 +-\n tools/stats/upload_stats_lib.py               |  4 +-\n tools/substitute.py                           |  2 +-\n tools/test/test_executorch_gen.py             |  4 +-\n tools/test/test_executorch_signatures.py      |  8 ++--\n tools/test/test_vulkan_codegen.py             |  4 +-\n tools/testing/explicit_ci_jobs.py             |  2 +-\n tools/testing/test_selections.py              |  2 +-\n torch/package/package_exporter.py             | 20 ++++------\n 49 files changed, 134 insertions(+), 142 deletions(-)\n\ndiff --git a/.ci/pytorch/create_test_cert.py b/.ci/pytorch/create_test_cert.py\nindex d3ead7ae259434..4e31f97878f41b 100644\n--- a/.ci/pytorch/create_test_cert.py\n+++ b/.ci/pytorch/create_test_cert.py\n@@ -88,9 +88,9 @@ def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n \n \n ca_key = genrsa(temp_dir + \"/ca.key\")\n-ca_cert = create_cert(temp_dir + \"/ca.pem\", u\"US\", u\"New York\", u\"New York\", u\"Gloo Certificate Authority\", ca_key)\n+ca_cert = create_cert(temp_dir + \"/ca.pem\", \"US\", \"New York\", \"New York\", \"Gloo Certificate Authority\", ca_key)\n \n pkey = genrsa(temp_dir + \"/pkey.key\")\n-csr = create_req(temp_dir + \"/csr.csr\", u\"US\", u\"California\", u\"San Francisco\", u\"Gloo Testing Company\", pkey)\n+csr = create_req(temp_dir + \"/csr.csr\", \"US\", \"California\", \"San Francisco\", \"Gloo Testing Company\", pkey)\n \n cert = sign_certificate_request(temp_dir + \"/cert.pem\", csr, ca_cert, ca_key)\ndiff --git a/.ci/pytorch/perf_test/compare_with_baseline.py b/.ci/pytorch/perf_test/compare_with_baseline.py\nindex 6d2839ac1db412..f7b962632cd79f 100644\n--- a/.ci/pytorch/perf_test/compare_with_baseline.py\n+++ b/.ci/pytorch/perf_test/compare_with_baseline.py\n@@ -19,7 +19,7 @@\n elif 'gpu' in test_name:\n     backend = 'gpu'\n \n-data_file_path = '../{}_runtime.json'.format(backend)\n+data_file_path = f'../{backend}_runtime.json'\n \n with open(data_file_path) as data_file:\n     data = json.load(data_file)\n@@ -69,7 +69,7 @@\n     print(\"z-value < 3, no perf regression detected.\")\n     if args.update:\n         print(\"We will use these numbers as new baseline.\")\n-        new_data_file_path = '../new_{}_runtime.json'.format(backend)\n+        new_data_file_path = f'../new_{backend}_runtime.json'\n         with open(new_data_file_path) as new_data_file:\n             new_data = json.load(new_data_file)\n         new_data[test_name] = {}\ndiff --git a/.circleci/cimodel/data/binary_build_definitions.py b/.circleci/cimodel/data/binary_build_definitions.py\nindex 45981e8e9ea77b..7dccbdc7cbf689 100644\n--- a/.circleci/cimodel/data/binary_build_definitions.py\n+++ b/.circleci/cimodel/data/binary_build_definitions.py\n@@ -5,7 +5,7 @@\n import cimodel.lib.conf_tree as conf_tree\n import cimodel.lib.miniutils as miniutils\n \n-class Conf(object):\n+class Conf:\n     def __init__(self, os, gpu_version, pydistro, parms, smoke, libtorch_variant, gcc_config_variant, libtorch_config_variant):\n \n         self.os = os\ndiff --git a/.circleci/cimodel/data/pytorch_build_definitions.py b/.circleci/cimodel/data/pytorch_build_definitions.py\nindex 76e87b07c1889f..e6e44bd2b5aeb0 100644\n--- a/.circleci/cimodel/data/pytorch_build_definitions.py\n+++ b/.circleci/cimodel/data/pytorch_build_definitions.py\n@@ -143,7 +143,7 @@ def gen_workflow_job(self, phase):\n \n \n # TODO This is a hack to special case some configs just for the workflow list\n-class HiddenConf(object):\n+class HiddenConf:\n     def __init__(self, name, parent_build=None, filters=None):\n         self.name = name\n         self.parent_build = parent_build\n@@ -160,7 +160,7 @@ def gen_workflow_job(self, phase):\n     def gen_build_name(self, _):\n         return self.name\n \n-class DocPushConf(object):\n+class DocPushConf:\n     def __init__(self, name, parent_build=None, branch=\"master\"):\n         self.name = name\n         self.parent_build = parent_build\ndiff --git a/.circleci/generate_config_yml.py b/.circleci/generate_config_yml.py\nindex b3e47eed8b4317..d1ef439941d4b2 100755\n--- a/.circleci/generate_config_yml.py\n+++ b/.circleci/generate_config_yml.py\n@@ -18,7 +18,7 @@\n import cimodel.lib.miniyaml as miniyaml\n \n \n-class File(object):\n+class File:\n     \"\"\"\n     Verbatim copy the contents of a file into config.yml\n     \"\"\"\n@@ -57,7 +57,7 @@ def horizontal_rule():\n     return \"\".join(\"#\" * 78)\n \n \n-class Header(object):\n+class Header:\n     def __init__(self, title, summary=None):\n         self.title = title\n         self.summary_lines = summary or []\ndiff --git a/.github/scripts/ensure_actions_will_cancel.py b/.github/scripts/ensure_actions_will_cancel.py\nindex 92eb3441acd3cf..8d53f2bed5e18b 100755\n--- a/.github/scripts/ensure_actions_will_cancel.py\n+++ b/.github/scripts/ensure_actions_will_cancel.py\n@@ -17,7 +17,7 @@\n \n \n def should_check(filename: Path) -> bool:\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         content = f.read()\n \n     data = yaml.safe_load(content)\n@@ -37,7 +37,7 @@ def should_check(filename: Path) -> bool:\n     files = [f for f in files if should_check(f)]\n     names = set()\n     for filename in files:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             data = yaml.safe_load(f)\n \n         name = data.get(\"name\")\ndiff --git a/.github/scripts/file_io_utils.py b/.github/scripts/file_io_utils.py\nindex 097b092bc904af..faba9f06d2ac65 100644\n--- a/.github/scripts/file_io_utils.py\n+++ b/.github/scripts/file_io_utils.py\n@@ -44,7 +44,7 @@ def load_json_file(file_path: Path) -> Any:\n     \"\"\"\n     Returns the deserialized json object\n     \"\"\"\n-    with open(file_path, \"r\") as f:\n+    with open(file_path) as f:\n         return json.load(f)\n \n \ndiff --git a/.github/scripts/filter_test_configs.py b/.github/scripts/filter_test_configs.py\nindex 9d1f39a833c571..92968179702273 100755\n--- a/.github/scripts/filter_test_configs.py\n+++ b/.github/scripts/filter_test_configs.py\n@@ -319,7 +319,7 @@ def process_jobs(\n     try:\n         # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking\n         # it into its two components first\n-        current_platform, _ = [n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n]\n+        current_platform, _ = (n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n)\n     except ValueError as error:\n         warnings.warn(f\"Invalid job name {job_name}, returning\")\n         return test_matrix\ndiff --git a/.github/scripts/generate_pytorch_version.py b/.github/scripts/generate_pytorch_version.py\nindex f9a6d49505b308..e70b97165e61d7 100755\n--- a/.github/scripts/generate_pytorch_version.py\n+++ b/.github/scripts/generate_pytorch_version.py\n@@ -50,7 +50,7 @@ def get_tag() -> str:\n \n def get_base_version() -> str:\n     root = get_pytorch_root()\n-    dirty_version = open(root / \"version.txt\", \"r\").read().strip()\n+    dirty_version = open(root / \"version.txt\").read().strip()\n     # Strips trailing a0 from version.txt, not too sure why it's there in the\n     # first place\n     return re.sub(LEGACY_BASE_VERSION_SUFFIX_PATTERN, \"\", dirty_version)\ndiff --git a/.github/scripts/label_utils.py b/.github/scripts/label_utils.py\nindex 812c33b426f441..e3ce0f52fe853f 100644\n--- a/.github/scripts/label_utils.py\n+++ b/.github/scripts/label_utils.py\n@@ -51,7 +51,7 @@ def get_last_page_num_from_header(header: Any) -> int:\n     )\n \n \n-@lru_cache()\n+@lru_cache\n def gh_get_labels(org: str, repo: str) -> List[str]:\n     prefix = f\"https://api.github.com/repos/{org}/{repo}/labels?per_page=100\"\n     header, info = request_for_labels(prefix + \"&page=1\")\ndiff --git a/.github/scripts/lint_native_functions.py b/.github/scripts/lint_native_functions.py\nindex 9bde9e8d84e5f9..4dfe9fd63e2e4e 100755\n--- a/.github/scripts/lint_native_functions.py\n+++ b/.github/scripts/lint_native_functions.py\n@@ -26,7 +26,7 @@ def fn(base: str) -> str:\n     return str(base / Path(\"aten/src/ATen/native/native_functions.yaml\"))\n \n \n-with open(Path(__file__).parent.parent.parent / fn(\".\"), \"r\") as f:\n+with open(Path(__file__).parent.parent.parent / fn(\".\")) as f:\n     contents = f.read()\n \n yaml = ruamel.yaml.YAML()  # type: ignore[attr-defined]\ndiff --git a/.github/scripts/run_torchbench.py b/.github/scripts/run_torchbench.py\nindex 3a80ebbeb9970a..e5e3c7a03dea07 100644\n--- a/.github/scripts/run_torchbench.py\n+++ b/.github/scripts/run_torchbench.py\n@@ -129,7 +129,7 @@ def extract_models_from_pr(\n     model_list = []\n     userbenchmark_list = []\n     pr_list = []\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(filter(lambda x: x.startswith(MAGIC_PREFIX), lines))\n         if magic_lines:\n@@ -157,7 +157,7 @@ def extract_models_from_pr(\n \n def find_torchbench_branch(prbody_file: str) -> str:\n     branch_name: str = \"\"\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(\n             filter(lambda x: x.startswith(MAGIC_TORCHBENCH_PREFIX), lines)\ndiff --git a/.github/scripts/test_check_labels.py b/.github/scripts/test_check_labels.py\nindex 17d33158f2efb4..2b2cd7b6c5204b 100644\n--- a/.github/scripts/test_check_labels.py\n+++ b/.github/scripts/test_check_labels.py\n@@ -15,7 +15,7 @@\n \n \n def mock_parse_args() -> object:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.pr_num = 76123\n \ndiff --git a/.github/scripts/test_trymerge.py b/.github/scripts/test_trymerge.py\nindex 7cf70882ed3d40..a46ef9032a6459 100755\n--- a/.github/scripts/test_trymerge.py\n+++ b/.github/scripts/test_trymerge.py\n@@ -114,7 +114,7 @@ def mocked_rockset_results(head_sha: str, merge_base: str, num_retries: int = 3)\n \n \n def mock_parse_args(revert: bool = False, force: bool = False) -> Any:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.revert = revert\n             self.force = force\ndiff --git a/.github/scripts/trymerge.py b/.github/scripts/trymerge.py\nindex 3cffcaa14efaf8..cc253f36cbd1b7 100755\n--- a/.github/scripts/trymerge.py\n+++ b/.github/scripts/trymerge.py\n@@ -1628,10 +1628,8 @@ def validate_revert(\n         allowed_reverters.append(\"CONTRIBUTOR\")\n     if author_association not in allowed_reverters:\n         raise PostCommentError(\n-            (\n-                f\"Will not revert as @{author_login} is not one of \"\n-                f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n-            )\n+            f\"Will not revert as @{author_login} is not one of \"\n+            f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n         )\n     skip_internal_checks = can_skip_internal_checks(pr, comment_id)\n \ndiff --git a/.github/scripts/trymerge_explainer.py b/.github/scripts/trymerge_explainer.py\nindex ebc74cf63eb833..8aa6ab59b94cbc 100644\n--- a/.github/scripts/trymerge_explainer.py\n+++ b/.github/scripts/trymerge_explainer.py\n@@ -17,7 +17,7 @@ def has_label(labels: List[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:\n     return len(list(filter(pattern.match, labels))) > 0\n \n \n-class TryMergeExplainer(object):\n+class TryMergeExplainer:\n     force: bool\n     labels: List[str]\n     pr_num: int\ndiff --git a/docs/caffe2/process.py b/docs/caffe2/process.py\nindex 3b94b9d38502a2..4a59eec388d90b 100644\n--- a/docs/caffe2/process.py\n+++ b/docs/caffe2/process.py\n@@ -8,7 +8,7 @@\n \n # Module caffe2...caffe2.python.control_test\n def insert(originalfile, first_line, description):\n-    with open(originalfile, 'r') as f:\n+    with open(originalfile) as f:\n         f1 = f.readline()\n         if(f1.find(first_line) < 0):\n             docs = first_line + description + f1\n@@ -30,7 +30,7 @@ def insert(originalfile, first_line, description):\n     for file in files:\n         if (file.endswith(\".py\") and not file.endswith(\"_test.py\") and not file.endswith(\"__.py\")):\n             filepath = os.path.join(root, file)\n-            print((\"filepath: \" + filepath))\n+            print(\"filepath: \" + filepath)\n             directory = os.path.dirname(filepath)[2:]\n             directory = directory.replace(\"/\", \".\")\n             print(\"directory: \" + directory)\ndiff --git a/docs/cpp/source/conf.py b/docs/cpp/source/conf.py\nindex 88648787fa8c8e..2b94cfdb5058fb 100644\n--- a/docs/cpp/source/conf.py\n+++ b/docs/cpp/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -99,7 +98,7 @@\n     ############################################################################\n     # Main library page layout example configuration.                          #\n     ############################################################################\n-    \"afterTitleDescription\": textwrap.dedent(u'''\n+    \"afterTitleDescription\": textwrap.dedent('''\n         Welcome to the developer reference for the PyTorch C++ API.\n     '''),\n }\ndiff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 8bbb189853eec7..8fec5f16f9852c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -624,7 +623,7 @@ def visit_reference(self, node):\n                 anchor = ref_anchor[1]\n                 txt = node.parent.astext()\n                 if txt == anchor or txt == anchor.split('.')[-1]:\n-                    self.body.append('<p id=\"{}\"/>'.format(ref_anchor[1]))\n+                    self.body.append(f'<p id=\"{ref_anchor[1]}\"/>')\n         return old_call(self, node)\n     Klass.visit_reference = visit_reference\n \ndiff --git a/docs/source/scripts/exportdb/generate_example_rst.py b/docs/source/scripts/exportdb/generate_example_rst.py\nindex 58dca5f31ea73d..38f71b905245c3 100644\n--- a/docs/source/scripts/exportdb/generate_example_rst.py\n+++ b/docs/source/scripts/exportdb/generate_example_rst.py\n@@ -31,7 +31,7 @@ def generate_example_rst(example_case: ExportCase):\n         if isinstance(model, torch.nn.Module)\n         else inspect.getfile(model)\n     )\n-    with open(source_file, \"r\") as file:\n+    with open(source_file) as file:\n         source_code = file.read()\n     source_code = re.sub(r\"from torch\\._export\\.db\\.case import .*\\n\", \"\", source_code)\n     source_code = re.sub(r\"@export_case\\((.|\\n)*?\\)\\n\", \"\", source_code)\n@@ -114,7 +114,7 @@ def generate_index_rst(example_cases, tag_to_modules, support_level_to_modules):\n \n     tag_names = \"\\n    \".join(t for t in tag_to_modules.keys())\n \n-    with open(os.path.join(PWD, \"blurb.txt\"), \"r\") as file:\n+    with open(os.path.join(PWD, \"blurb.txt\")) as file:\n         blurb = file.read()\n \n     # Generate contents of the .rst file\ndiff --git a/setup.py b/setup.py\nindex 5f0180cb0c8ac6..d454b2e62f33b4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -323,7 +323,7 @@ def report(*args):\n package_name = os.getenv('TORCH_PACKAGE_NAME', 'torch')\n package_type = os.getenv('PACKAGE_TYPE', 'wheel')\n version = get_torch_version()\n-report(\"Building wheel {}-{}\".format(package_name, version))\n+report(f\"Building wheel {package_name}-{version}\")\n \n cmake = CMake()\n \n@@ -361,7 +361,7 @@ def not_exists_or_empty(folder):\n             start = time.time()\n             subprocess.check_call([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd)\n             end = time.time()\n-            print(' --- Submodule initialization took {:.2f} sec'.format(end - start))\n+            print(f' --- Submodule initialization took {end - start:.2f} sec')\n         except Exception:\n             print(' --- Submodule initalization failed')\n             print('Please run:\\n\\tgit submodule update --init --recursive')\n@@ -616,16 +616,16 @@ def build_extensions(self):\n                 continue\n             fullname = self.get_ext_fullname(ext.name)\n             filename = self.get_ext_filename(fullname)\n-            report(\"\\nCopying extension {}\".format(ext.name))\n+            report(f\"\\nCopying extension {ext.name}\")\n \n             relative_site_packages = sysconfig.get_path('purelib').replace(sysconfig.get_path('data'), '').lstrip(os.path.sep)\n             src = os.path.join(\"torch\", relative_site_packages, filename)\n             if not os.path.exists(src):\n-                report(\"{} does not exist\".format(src))\n+                report(f\"{src} does not exist\")\n                 del self.extensions[i]\n             else:\n                 dst = os.path.join(os.path.realpath(self.build_lib), filename)\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -642,7 +642,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -658,7 +658,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"nvfuser\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -670,7 +670,7 @@ def build_extensions(self):\n     def get_outputs(self):\n         outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n         outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n-        report(\"setup.py::get_outputs returning {}\".format(outputs))\n+        report(f\"setup.py::get_outputs returning {outputs}\")\n         return outputs\n \n     def create_compile_commands(self):\n@@ -694,13 +694,13 @@ def load(filename):\n         new_contents = json.dumps(all_commands, indent=2)\n         contents = ''\n         if os.path.exists('compile_commands.json'):\n-            with open('compile_commands.json', 'r') as f:\n+            with open('compile_commands.json') as f:\n                 contents = f.read()\n         if contents != new_contents:\n             with open('compile_commands.json', 'w') as f:\n                 f.write(new_contents)\n \n-class concat_license_files():\n+class concat_license_files:\n     \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n \n     LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n@@ -723,7 +723,7 @@ def __enter__(self):\n         finally:\n             sys.path = old_path\n \n-        with open(self.f1, 'r') as f1:\n+        with open(self.f1) as f1:\n             self.bsd_text = f1.read()\n \n         with open(self.f1, 'a') as f1:\n@@ -771,7 +771,7 @@ def finalize_options(self):\n     def run(self):\n         import glob\n         import re\n-        with open('.gitignore', 'r') as f:\n+        with open('.gitignore') as f:\n             ignores = f.read()\n             pat = re.compile(r'^#( BEGIN NOT-CLEAN-FILES )?')\n             for wildcard in filter(None, ignores.split('\\n')):\n@@ -934,31 +934,31 @@ def make_relative_rpath_args(path):\n     if cmake_cache_vars['BUILD_CAFFE2']:\n         extensions.append(\n             Extension(\n-                name=str('caffe2.python.caffe2_pybind11_state'),\n+                name='caffe2.python.caffe2_pybind11_state',\n                 sources=[]),\n         )\n         if cmake_cache_vars['USE_CUDA']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_gpu'),\n+                    name='caffe2.python.caffe2_pybind11_state_gpu',\n                     sources=[]),\n             )\n         if cmake_cache_vars['USE_ROCM']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_hip'),\n+                    name='caffe2.python.caffe2_pybind11_state_hip',\n                     sources=[]),\n             )\n     if cmake_cache_vars['BUILD_FUNCTORCH']:\n         extensions.append(\n             Extension(\n-                name=str('functorch._C'),\n+                name='functorch._C',\n                 sources=[]),\n         )\n     if cmake_cache_vars['BUILD_NVFUSER']:\n         extensions.append(\n             Extension(\n-                name=str('nvfuser._C'),\n+                name='nvfuser._C',\n                 sources=[]),\n         )\n \n@@ -1272,7 +1272,7 @@ def main():\n         download_url='https://github.com/pytorch/pytorch/tags',\n         author='PyTorch Team',\n         author_email='packages@pytorch.org',\n-        python_requires='>={}'.format(python_min_version_str),\n+        python_requires=f'>={python_min_version_str}',\n         # PyPI package information.\n         classifiers=[\n             'Development Status :: 5 - Production/Stable',\n@@ -1288,7 +1288,7 @@ def main():\n             'Topic :: Software Development :: Libraries :: Python Modules',\n             'Programming Language :: C++',\n             'Programming Language :: Python :: 3',\n-        ] + ['Programming Language :: Python :: 3.{}'.format(i) for i in range(python_min_version[1], version_range_max)],\n+        ] + [f'Programming Language :: Python :: 3.{i}' for i in range(python_min_version[1], version_range_max)],\n         license='BSD-3',\n         keywords='pytorch, machine learning',\n     )\ndiff --git a/tools/amd_build/build_amd.py b/tools/amd_build/build_amd.py\nindex 59f806b361102e..5d14e9266f3b4a 100755\n--- a/tools/amd_build/build_amd.py\n+++ b/tools/amd_build/build_amd.py\n@@ -140,7 +140,7 @@ def is_hip_clang() -> bool:\n         hip_path = os.getenv(\"HIP_PATH\", \"/opt/rocm/hip\")\n         with open(hip_path + \"/lib/.hipInfo\") as f:\n             return \"HIP_COMPILER=clang\" in f.read()\n-    except IOError:\n+    except OSError:\n         return False\n \n \n@@ -149,7 +149,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Hip.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\" hip_hcc \", \" amdhip64 \") for line in lines]\n         if lines == newlines:\n@@ -163,7 +163,7 @@ def is_hip_clang() -> bool:\n gloo_cmake_file = \"third_party/gloo/cmake/Modules/Findrccl.cmake\"\n if os.path.exists(gloo_cmake_file):\n     do_write = False\n-    with open(gloo_cmake_file, \"r\") as sources:\n+    with open(gloo_cmake_file) as sources:\n         lines = sources.readlines()\n     newlines = [line.replace(\"RCCL_LIBRARY\", \"RCCL_LIB_PATH\") for line in lines]\n     if lines == newlines:\n@@ -179,7 +179,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Dependencies.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\"HIP_HCC_FLAGS\", \"HIP_CLANG_FLAGS\") for line in lines]\n         if lines == newlines:\ndiff --git a/tools/autograd/gen_python_functions.py b/tools/autograd/gen_python_functions.py\nindex 211aacafaa8728..d1e9d60737defc 100644\n--- a/tools/autograd/gen_python_functions.py\n+++ b/tools/autograd/gen_python_functions.py\n@@ -553,7 +553,7 @@ def load_deprecated_signatures(\n     # find matching original signatures for each deprecated signature\n     results: List[PythonSignatureNativeFunctionPair] = []\n \n-    with open(deprecated_yaml_path, \"r\") as f:\n+    with open(deprecated_yaml_path) as f:\n         deprecated_defs = yaml.load(f, Loader=YamlLoader)\n \n     for deprecated in deprecated_defs:\n@@ -873,7 +873,7 @@ def method_impl(\n         name=name,\n         pycname=pycname,\n         method_header=method_header,\n-        max_args=max((o.signature.arguments_count() for o in overloads)),\n+        max_args=max(o.signature.arguments_count() for o in overloads),\n         signatures=signatures,\n         traceable=traceable,\n         check_has_torch_function=gen_has_torch_function_check(\n@@ -1255,10 +1255,10 @@ def go(f: NativeFunction) -> str:\n         # dispatch lambda signature\n         name = cpp.name(f.func)\n         lambda_formals = \", \".join(\n-            (\n+\n                 f\"{a.type_str} {a.name}\"\n                 for a in dispatch_lambda_args(ps, f, symint=symint)\n-            )\n+\n         )\n         lambda_return = dispatch_lambda_return_str(f)\n \ndiff --git a/tools/autograd/load_derivatives.py b/tools/autograd/load_derivatives.py\nindex b51b625b2ea28f..b846892b0e3ed4 100644\n--- a/tools/autograd/load_derivatives.py\n+++ b/tools/autograd/load_derivatives.py\n@@ -98,7 +98,7 @@ def load_derivatives(\n     global _GLOBAL_LOAD_DERIVATIVE_CACHE\n     key = (derivatives_yaml_path, native_yaml_path)\n     if key not in _GLOBAL_LOAD_DERIVATIVE_CACHE:\n-        with open(derivatives_yaml_path, \"r\") as f:\n+        with open(derivatives_yaml_path) as f:\n             definitions = yaml.load(f, Loader=YamlLoader)\n \n         funcs = parse_native_yaml(native_yaml_path, tags_yaml_path).native_functions\ndiff --git a/tools/code_analyzer/gen_op_registration_allowlist.py b/tools/code_analyzer/gen_op_registration_allowlist.py\nindex b01142c872f1c2..b5d15ca1ae8417 100644\n--- a/tools/code_analyzer/gen_op_registration_allowlist.py\n+++ b/tools/code_analyzer/gen_op_registration_allowlist.py\n@@ -24,7 +24,7 @@ def canonical_name(opname: str) -> str:\n \n \n def load_op_dep_graph(fname: str) -> DepGraph:\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         result = defaultdict(set)\n         for op in yaml.safe_load(stream):\n             op_name = canonical_name(op[\"name\"])\n@@ -36,7 +36,7 @@ def load_op_dep_graph(fname: str) -> DepGraph:\n \n def load_root_ops(fname: str) -> List[str]:\n     result = []\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         for op in yaml.safe_load(stream):\n             result.append(canonical_name(op))\n     return result\ndiff --git a/tools/code_analyzer/gen_oplist.py b/tools/code_analyzer/gen_oplist.py\nindex 7c1764deda5b2f..0a9e2a1539b6a7 100644\n--- a/tools/code_analyzer/gen_oplist.py\n+++ b/tools/code_analyzer/gen_oplist.py\n@@ -79,7 +79,7 @@ def gen_supported_mobile_models(model_dicts: List[Any], output_dir: str) -> None\n \n     supported_hashes = \"\"\n     for md5 in md5_hashes:\n-        supported_hashes += '\"{}\",\\n'.format(md5)\n+        supported_hashes += f'\"{md5}\",\\n'\n     with open(\n         os.path.join(output_dir, \"SupportedMobileModelsRegistration.cpp\"), \"wb\"\n     ) as out_file:\ndiff --git a/tools/coverage_plugins_package/setup.py b/tools/coverage_plugins_package/setup.py\nindex 01250694550423..e3e88067cb08db 100644\n--- a/tools/coverage_plugins_package/setup.py\n+++ b/tools/coverage_plugins_package/setup.py\n@@ -1,6 +1,6 @@\n import setuptools  # type: ignore[import]\n \n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n+with open(\"README.md\", encoding=\"utf-8\") as fh:\n     long_description = fh.read()\n \n setuptools.setup(\ndiff --git a/tools/download_mnist.py b/tools/download_mnist.py\nindex 52fa411eda9f88..ac9c049bdeedb6 100644\n--- a/tools/download_mnist.py\n+++ b/tools/download_mnist.py\n@@ -32,16 +32,16 @@ def report_download_progress(\n def download(destination_path: str, resource: str, quiet: bool) -> None:\n     if os.path.exists(destination_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ...\".format(destination_path))\n+            print(f\"{destination_path} already exists, skipping ...\")\n     else:\n         for mirror in MIRRORS:\n             url = mirror + resource\n-            print(\"Downloading {} ...\".format(url))\n+            print(f\"Downloading {url} ...\")\n             try:\n                 hook = None if quiet else report_download_progress\n                 urlretrieve(url, destination_path, reporthook=hook)\n             except (URLError, ConnectionError) as e:\n-                print(\"Failed to download (trying next):\\n{}\".format(e))\n+                print(f\"Failed to download (trying next):\\n{e}\")\n                 continue\n             finally:\n                 if not quiet:\n@@ -56,13 +56,13 @@ def unzip(zipped_path: str, quiet: bool) -> None:\n     unzipped_path = os.path.splitext(zipped_path)[0]\n     if os.path.exists(unzipped_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ... \".format(unzipped_path))\n+            print(f\"{unzipped_path} already exists, skipping ... \")\n         return\n     with gzip.open(zipped_path, \"rb\") as zipped_file:\n         with open(unzipped_path, \"wb\") as unzipped_file:\n             unzipped_file.write(zipped_file.read())\n             if not quiet:\n-                print(\"Unzipped {} ...\".format(zipped_path))\n+                print(f\"Unzipped {zipped_path} ...\")\n \n \n def main() -> None:\ndiff --git a/tools/gen_vulkan_spv.py b/tools/gen_vulkan_spv.py\nindex 02c9c39270b107..8d38dfa0e82bcb 100644\n--- a/tools/gen_vulkan_spv.py\n+++ b/tools/gen_vulkan_spv.py\n@@ -74,7 +74,7 @@ def __init__(self: \"VulkanShaderGenerator\") -> None:\n \n     def add_params_yaml(self, parameters_yaml_file):  # type: ignore[no-untyped-def]\n         all_template_params = OrderedDict()\n-        with open(parameters_yaml_file, \"r\") as f:\n+        with open(parameters_yaml_file) as f:\n             contents = yaml.load(f, Loader=UniqueKeyLoader)\n             for key in contents:\n                 all_template_params[key] = contents[key]\n@@ -205,7 +205,7 @@ def determineDescriptorType(lineStr: str) -> str:\n \n def getShaderInfo(srcFilePath: str) -> ShaderInfo:\n     shader_info = ShaderInfo([], [], \"\")\n-    with open(srcFilePath, 'r') as srcFile:\n+    with open(srcFilePath) as srcFile:\n         for line in srcFile:\n             if isDescriptorLine(line):\n                 shader_info.layouts.append(determineDescriptorType(line))\n@@ -272,13 +272,13 @@ def genCppH(\n         if len(f) > 1:\n             templateSrcPaths.append(f)\n             templateSrcPaths.sort()\n-    print(\"templateSrcPaths:{}\".format(templateSrcPaths))\n+    print(f\"templateSrcPaths:{templateSrcPaths}\")\n \n     spvPaths = {}\n     for templateSrcPath in templateSrcPaths:\n-        print(\"templateSrcPath {}\".format(templateSrcPath))\n+        print(f\"templateSrcPath {templateSrcPath}\")\n         name = getName(templateSrcPath).replace(\"_glsl\", \"\")\n-        print(\"name {}\".format(name))\n+        print(f\"name {name}\")\n \n         codeTemplate = CodeTemplate.from_file(templateSrcPath)\n         srcPath = tmpDirPath + \"/\" + name + \".glsl\"\n@@ -287,7 +287,7 @@ def genCppH(\n             fw.write(content)\n \n         spvPath = tmpDirPath + \"/\" + name + \".spv\"\n-        print(\"spvPath {}\".format(spvPath))\n+        print(f\"spvPath {spvPath}\")\n \n         cmd = [\n             glslcPath, \"-fshader-stage=compute\",\n@@ -328,7 +328,7 @@ def genCppH(\n     h += nsend\n \n     cpp = \"#include <ATen/native/vulkan/api/Shader.h>\\n\"\n-    cpp += \"#include <ATen/native/vulkan/{}>\\n\".format(H_NAME)\n+    cpp += f\"#include <ATen/native/vulkan/{H_NAME}>\\n\"\n     cpp += \"#include <stdint.h>\\n\"\n     cpp += \"#include <vector>\\n\"\n     cpp += nsbegin\n@@ -340,7 +340,7 @@ def genCppH(\n     for spvPath, srcPath in spvPaths.items():\n         name = getName(spvPath).replace(\"_spv\", \"\")\n \n-        print(\"spvPath:{}\".format(spvPath))\n+        print(f\"spvPath:{spvPath}\")\n         with open(spvPath, 'rb') as fr:\n             next_bin = array.array('I', fr.read())\n             sizeBytes = 4 * len(next_bin)\n@@ -362,8 +362,8 @@ def genCppH(\n         shader_info_layouts = \"{{{}}}\".format(\",\\n \".join(shader_info.layouts))\n \n         shader_info_args = [\n-            \"\\\"vulkan.{}\\\"\".format(name),\n-            \"{}_bin\".format(name),\n+            f\"\\\"vulkan.{name}\\\"\",\n+            f\"{name}_bin\",\n             str(sizeBytes),\n             shader_info_layouts,\n             tile_size,\ndiff --git a/tools/generate_torch_version.py b/tools/generate_torch_version.py\nindex 9e9f73b031f810..d90d3646ab1910 100644\n--- a/tools/generate_torch_version.py\n+++ b/tools/generate_torch_version.py\n@@ -41,7 +41,7 @@ def get_tag(pytorch_root: Union[str, Path]) -> str:\n \n def get_torch_version(sha: Optional[str] = None) -> str:\n     pytorch_root = Path(__file__).parent.parent\n-    version = open(pytorch_root / \"version.txt\", \"r\").read().strip()\n+    version = open(pytorch_root / \"version.txt\").read().strip()\n \n     if os.getenv(\"PYTORCH_BUILD_VERSION\"):\n         assert os.getenv(\"PYTORCH_BUILD_NUMBER\") is not None\n@@ -86,11 +86,11 @@ def get_torch_version(sha: Optional[str] = None) -> str:\n         version = tagged_version\n \n     with open(version_path, \"w\") as f:\n-        f.write(\"__version__ = '{}'\\n\".format(version))\n+        f.write(f\"__version__ = '{version}'\\n\")\n         # NB: This is not 100% accurate, because you could have built the\n         # library code with DEBUG, but csrc without DEBUG (in which case\n         # this would claim to be a release build when it's not.)\n-        f.write(\"debug = {}\\n\".format(repr(bool(args.is_debug))))\n-        f.write(\"cuda = {}\\n\".format(repr(args.cuda_version)))\n-        f.write(\"git_version = {}\\n\".format(repr(sha)))\n-        f.write(\"hip = {}\\n\".format(repr(args.hip_version)))\n+        f.write(f\"debug = {repr(bool(args.is_debug))}\\n\")\n+        f.write(f\"cuda = {repr(args.cuda_version)}\\n\")\n+        f.write(f\"git_version = {repr(sha)}\\n\")\n+        f.write(f\"hip = {repr(args.hip_version)}\\n\")\ndiff --git a/tools/jit/gen_unboxing.py b/tools/jit/gen_unboxing.py\nindex 6179d6afe482ff..ee4e2fc2ddb188 100644\n--- a/tools/jit/gen_unboxing.py\n+++ b/tools/jit/gen_unboxing.py\n@@ -250,7 +250,7 @@ def main(args: List[str]) -> None:\n     if options.op_registration_allowlist:\n         op_registration_allowlist = options.op_registration_allowlist\n     elif options.TEST_ONLY_op_registration_allowlist_yaml_path:\n-        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path, \"r\") as f:\n+        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path) as f:\n             op_registration_allowlist = yaml.safe_load(f)\n     else:\n         op_registration_allowlist = None\ndiff --git a/tools/linter/adapters/constexpr_linter.py b/tools/linter/adapters/constexpr_linter.py\nindex 8992f30ac46b29..24ecc83b238e30 100644\n--- a/tools/linter/adapters/constexpr_linter.py\n+++ b/tools/linter/adapters/constexpr_linter.py\n@@ -35,7 +35,7 @@ class LintMessage(NamedTuple):\n def check_file(filename: str) -> Optional[LintMessage]:\n     logging.debug(\"Checking file %s\", filename)\n \n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n \n     for idx, line in enumerate(lines):\ndiff --git a/tools/linter/adapters/grep_linter.py b/tools/linter/adapters/grep_linter.py\nindex 21c8a210b2b697..64dac4cdc079cd 100644\n--- a/tools/linter/adapters/grep_linter.py\n+++ b/tools/linter/adapters/grep_linter.py\n@@ -108,7 +108,7 @@ def lint_file(\n     original = None\n     replacement = None\n     if replace_pattern:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             original = f.read()\n \n         try:\ndiff --git a/tools/nightly.py b/tools/nightly.py\nindex 1544eb0692b661..28a8c6eb2331e7 100755\n--- a/tools/nightly.py\n+++ b/tools/nightly.py\n@@ -105,7 +105,7 @@ def redact(self, needle: str, replace: str = \"<REDACTED>\") -> None:\n         self.redactions[needle] = replace\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_base_dir() -> str:\n     meta_dir = os.getcwd()\n     base_dir = os.path.join(meta_dir, \"nightly\", \"log\")\n@@ -113,17 +113,17 @@ def logging_base_dir() -> str:\n     return base_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_run_dir() -> str:\n     cur_dir = os.path.join(\n         logging_base_dir(),\n-        \"{}_{}\".format(datetime.datetime.now().strftime(DATETIME_FORMAT), uuid.uuid1()),\n+        f\"{datetime.datetime.now().strftime(DATETIME_FORMAT)}_{uuid.uuid1()}\",\n     )\n     os.makedirs(cur_dir, exist_ok=True)\n     return cur_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_record_argv() -> None:\n     s = subprocess.list2cmdline(sys.argv)\n     with open(os.path.join(logging_run_dir(), \"argv\"), \"w\") as f:\ndiff --git a/tools/onnx/gen_diagnostics.py b/tools/onnx/gen_diagnostics.py\nindex 2aeb61a06318d7..4cf70289296050 100644\n--- a/tools/onnx/gen_diagnostics.py\n+++ b/tools/onnx/gen_diagnostics.py\n@@ -205,7 +205,7 @@ def gen_diagnostics(\n     out_cpp_dir: str,\n     out_docs_dir: str,\n ) -> None:\n-    with open(rules_path, \"r\") as f:\n+    with open(rules_path) as f:\n         rules = yaml.load(f, Loader=YamlLoader)\n \n     template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"templates\")\ndiff --git a/tools/onnx/update_default_opset_version.py b/tools/onnx/update_default_opset_version.py\nindex 6dc6ffbd2890f4..6463c6271b6ee6 100755\n--- a/tools/onnx/update_default_opset_version.py\n+++ b/tools/onnx/update_default_opset_version.py\n@@ -23,7 +23,7 @@\n def read_sub_write(path: str, prefix_pat: str, new_default: int) -> None:\n     with open(path, encoding=\"utf-8\") as f:\n         content_str = f.read()\n-    content_str = re.sub(prefix_pat, r\"\\g<1>{}\".format(new_default), content_str)\n+    content_str = re.sub(prefix_pat, fr\"\\g<1>{new_default}\", content_str)\n     with open(path, \"w\", encoding=\"utf-8\") as f:\n         f.write(content_str)\n     print(\"modified\", path)\ndiff --git a/tools/pyi/gen_pyi.py b/tools/pyi/gen_pyi.py\nindex 24ee8ad1bfe580..c74d737416870a 100644\n--- a/tools/pyi/gen_pyi.py\n+++ b/tools/pyi/gen_pyi.py\n@@ -191,15 +191,15 @@ def sig_for_ops(opname: str) -> List[str]:\n \n     name = opname[2:-2]\n     if name in binary_ops:\n-        return [\"def {}(self, other: Any) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self, other: Any) -> Tensor: ...\"]\n     elif name in comparison_ops:\n-        sig = \"def {}(self, other: Any) -> Tensor: ...\".format(opname)\n+        sig = f\"def {opname}(self, other: Any) -> Tensor: ...\"\n         if name in symmetric_comparison_ops:\n             # unsafe override https://github.com/python/mypy/issues/5704\n             sig += \"  # type: ignore[override]\"\n         return [sig]\n     elif name in unary_ops:\n-        return [\"def {}(self) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self) -> Tensor: ...\"]\n     elif name in to_py_type_ops:\n         if name in {\"bool\", \"float\", \"complex\"}:\n             tname = name\n@@ -209,7 +209,7 @@ def sig_for_ops(opname: str) -> List[str]:\n             tname = \"int\"\n         if tname in {\"float\", \"int\", \"bool\", \"complex\"}:\n             tname = \"builtins.\" + tname\n-        return [\"def {}(self) -> {}: ...\".format(opname, tname)]\n+        return [f\"def {opname}(self) -> {tname}: ...\"]\n     else:\n         raise Exception(\"unknown op\", opname)\n \n@@ -1120,7 +1120,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     for name in simple_conversions:\n         unsorted_tensor_method_hints[name].append(\n-            \"def {}(self) -> Tensor: ...\".format(name)\n+            f\"def {name}(self) -> Tensor: ...\"\n         )\n \n     # pyi tensor methods don't currently include deprecated signatures for some reason\n@@ -1150,7 +1150,7 @@ def replace_special_case(hint: str) -> str:\n                 namedtuples[tuple_name] = tuple_def\n \n     for op in all_ops:\n-        name = \"__{}__\".format(op)\n+        name = f\"__{op}__\"\n         unsorted_tensor_method_hints[name] += sig_for_ops(name)\n \n     tensor_method_hints = []\n@@ -1164,7 +1164,7 @@ def replace_special_case(hint: str) -> str:\n     # Generate namedtuple definitions\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-    namedtuple_defs = [\"{}\\n\".format(defn) for defn in namedtuples.values()]\n+    namedtuple_defs = [f\"{defn}\\n\" for defn in namedtuples.values()]\n \n     # Generate type signatures for legacy classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1183,7 +1183,7 @@ def replace_special_case(hint: str) -> str:\n         \"ByteTensor\",\n         \"BoolTensor\",\n     ):\n-        legacy_class_hints.append(\"class {}(Tensor): ...\".format(c))\n+        legacy_class_hints.append(f\"class {c}(Tensor): ...\")\n \n     # Generate type signatures for dtype classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1191,7 +1191,7 @@ def replace_special_case(hint: str) -> str:\n     # TODO: don't explicitly list dtypes here; get it from canonical\n     # source\n     dtype_class_hints = [\n-        \"{}: dtype = ...\".format(n)\n+        f\"{n}: dtype = ...\"\n         for n in [\n             \"float32\",\n             \"float\",\n@@ -1232,7 +1232,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     all_symbols = sorted(list(namedtuples.keys()) + hinted_function_names)\n     all_directive = pformat(all_symbols, width=100, compact=True).split(\"\\n\")\n-    all_directive[0] = \"__all__ = {}\".format(all_directive[0])\n+    all_directive[0] = f\"__all__ = {all_directive[0]}\"\n \n     # Dispatch key hints\n     # ~~~~~~~~~~~~~~~~~~\ndiff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py\nindex 0bd6e5d4c2adc9..cf80bd3eb1e62c 100644\n--- a/tools/setup_helpers/cmake.py\n+++ b/tools/setup_helpers/cmake.py\n@@ -61,10 +61,10 @@ def _get_cmake_command() -> str:\n \n         _cmake_min_version = LooseVersion(\"3.13.0\")\n         if all(\n-            (\n+\n                 ver is None or ver < _cmake_min_version\n                 for ver in [cmake_version, cmake3_version]\n-            )\n+\n         ):\n             raise RuntimeError(\"no cmake or cmake3 with version >= 3.13.0 found\")\n \n@@ -108,7 +108,7 @@ def defines(args: List[str], **kwargs: CMakeValue) -> None:\n         \"Adds definitions to a cmake argument list.\"\n         for key, value in sorted(kwargs.items()):\n             if value is not None:\n-                args.append(\"-D{}={}\".format(key, value))\n+                args.append(f\"-D{key}={value}\")\n \n     def get_cmake_cache_variables(self) -> Dict[str, CMakeValue]:\n         r\"\"\"Gets values in CMakeCache.txt into a dictionary.\n@@ -173,7 +173,7 @@ def generate(\n                     toolset_dict[\"host\"] = \"x64\"\n             if toolset_dict:\n                 toolset_expr = \",\".join(\n-                    [\"{}={}\".format(k, v) for k, v in toolset_dict.items()]\n+                    [f\"{k}={v}\" for k, v in toolset_dict.items()]\n                 )\n                 args.append(\"-T\" + toolset_expr)\n \n@@ -322,10 +322,10 @@ def generate(\n         expected_wrapper = \"/usr/local/opt/ccache/libexec\"\n         if IS_DARWIN and os.path.exists(expected_wrapper):\n             if \"CMAKE_C_COMPILER\" not in build_options and \"CC\" not in os.environ:\n-                CMake.defines(args, CMAKE_C_COMPILER=\"{}/gcc\".format(expected_wrapper))\n+                CMake.defines(args, CMAKE_C_COMPILER=f\"{expected_wrapper}/gcc\")\n             if \"CMAKE_CXX_COMPILER\" not in build_options and \"CXX\" not in os.environ:\n                 CMake.defines(\n-                    args, CMAKE_CXX_COMPILER=\"{}/g++\".format(expected_wrapper)\n+                    args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\"\n                 )\n \n         for env_var_name in my_env:\n@@ -336,10 +336,10 @@ def generate(\n                     my_env[env_var_name] = str(my_env[env_var_name].encode(\"utf-8\"))\n                 except UnicodeDecodeError as e:\n                     shex = \":\".join(\n-                        \"{:02x}\".format(ord(c)) for c in my_env[env_var_name]\n+                        f\"{ord(c):02x}\" for c in my_env[env_var_name]\n                     )\n                     print(\n-                        \"Invalid ENV[{}] = {}\".format(env_var_name, shex),\n+                        f\"Invalid ENV[{env_var_name}] = {shex}\",\n                         file=sys.stderr,\n                     )\n                     print(e, file=sys.stderr)\n@@ -396,7 +396,7 @@ def build(self, my_env: Dict[str, str]) -> None:\n             build_args += [\"--\"]\n             if IS_WINDOWS and not USE_NINJA:\n                 # We are likely using msbuild here\n-                build_args += [\"/p:CL_MPCount={}\".format(max_jobs)]\n+                build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n             else:\n                 build_args += [\"-j\", max_jobs]\n         self.run(build_args, my_env)\ndiff --git a/tools/setup_helpers/cmake_utils.py b/tools/setup_helpers/cmake_utils.py\nindex dabd66a4e838bb..c15b6f7592c015 100644\n--- a/tools/setup_helpers/cmake_utils.py\n+++ b/tools/setup_helpers/cmake_utils.py\n@@ -72,7 +72,7 @@ def get_cmake_cache_variables_from_file(\n         )\n         if matched is None:  # Illegal line\n             raise ValueError(\n-                \"Unexpected line {} in {}: {}\".format(i, repr(cmake_cache_file), line)\n+                f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\"\n             )\n         _, variable, type_, value = matched.groups()\n         if type_ is None:\ndiff --git a/tools/setup_helpers/generate_code.py b/tools/setup_helpers/generate_code.py\nindex c03fd87f25b6aa..afdd168d179fd6 100644\n--- a/tools/setup_helpers/generate_code.py\n+++ b/tools/setup_helpers/generate_code.py\n@@ -75,7 +75,7 @@ def generate_code(\n def get_selector_from_legacy_operator_selection_list(\n     selected_op_list_path: str,\n ) -> Any:\n-    with open(selected_op_list_path, \"r\") as f:\n+    with open(selected_op_list_path) as f:\n         # strip out the overload part\n         # It's only for legacy config - do NOT copy this code!\n         selected_op_list = {\ndiff --git a/tools/stats/import_test_stats.py b/tools/stats/import_test_stats.py\nindex b0719fc56d97b6..28d8ee0961bd9e 100644\n--- a/tools/stats/import_test_stats.py\n+++ b/tools/stats/import_test_stats.py\n@@ -46,7 +46,7 @@ def is_cached_file_valid() -> bool:\n \n     if os.path.exists(path) and is_cached_file_valid():\n         # Another test process already download the file, so don't re-do it\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             return cast(Dict[str, Any], json.load(f))\n \n     for _ in range(3):\ndiff --git a/tools/stats/upload_stats_lib.py b/tools/stats/upload_stats_lib.py\nindex c9223c528fe163..c7e90286d00a2b 100644\n--- a/tools/stats/upload_stats_lib.py\n+++ b/tools/stats/upload_stats_lib.py\n@@ -249,10 +249,10 @@ def value(self) -> Any:\n         value = os.environ.get(self.env_var)\n         if value is None and self.required:\n             raise ValueError(\n-                (\n+\n                     f\"Missing {self.name}. Please set the {self.env_var}\"\n                     \"environment variable to pass in this value.\"\n-                )\n+\n             )\n         if self.type_conversion_fn:\n             return self.type_conversion_fn(value)\ndiff --git a/tools/substitute.py b/tools/substitute.py\nindex c3b353bf740115..e9c05990c75f9a 100644\n--- a/tools/substitute.py\n+++ b/tools/substitute.py\n@@ -11,7 +11,7 @@\n     parser.add_argument(\"--replace\", action=\"append\", nargs=2)\n     options = parser.parse_args()\n \n-    with open(options.input_file, \"r\") as f:\n+    with open(options.input_file) as f:\n         contents = f.read()\n \n     output_file = os.path.join(options.install_dir, options.output_file)\ndiff --git a/tools/test/test_executorch_gen.py b/tools/test/test_executorch_gen.py\nindex b2a0f6768271bf..c9d6c79b85c1ea 100644\n--- a/tools/test/test_executorch_gen.py\n+++ b/tools/test/test_executorch_gen.py\n@@ -181,7 +181,7 @@ def test_translate_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\n@@ -268,7 +268,7 @@ def test_translate_kernel_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\ndiff --git a/tools/test/test_executorch_signatures.py b/tools/test/test_executorch_signatures.py\nindex c137f6982ec2b7..543926d4c31ef0 100644\n--- a/tools/test/test_executorch_signatures.py\n+++ b/tools/test/test_executorch_signatures.py\n@@ -21,7 +21,7 @@ def test_runtime_signature_contains_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=True)\n-            self.assertEquals(len(args), 3)\n+            self.assertEqual(len(args), 3)\n             self.assertTrue(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n@@ -30,7 +30,7 @@ def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=False)\n-            self.assertEquals(len(args), 2)\n+            self.assertEqual(len(args), 2)\n             self.assertFalse(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_declaration_correct(self) -> None:\n@@ -38,7 +38,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             decl = self.sig.decl(include_context=True)\n-            self.assertEquals(\n+            self.assertEqual(\n                 decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\n@@ -48,7 +48,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n                 ),\n             )\n             no_context_decl = self.sig.decl(include_context=False)\n-            self.assertEquals(\n+            self.assertEqual(\n                 no_context_decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\ndiff --git a/tools/test/test_vulkan_codegen.py b/tools/test/test_vulkan_codegen.py\nindex ae87c27e7aeb8e..196be229b348d2 100644\n--- a/tools/test/test_vulkan_codegen.py\n+++ b/tools/test/test_vulkan_codegen.py\n@@ -92,9 +92,9 @@ def test_missing_key_default_val(self) -> None:\n                     file_name_2 = os.path.join(tmp_dir, \"conv2d_pw_1x2.glsl\")\n                     self.assertTrue(os.path.exists(file_name_1))\n                     self.assertTrue(os.path.exists(file_name_2))\n-                    with open(file_name_1, \"r\") as f:\n+                    with open(file_name_1) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 1\" in contents)\n-                    with open(file_name_2, \"r\") as f:\n+                    with open(file_name_2) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 2\" in contents)\ndiff --git a/tools/testing/explicit_ci_jobs.py b/tools/testing/explicit_ci_jobs.py\nindex daff3cce8956ff..594e00d437f9f7 100755\n--- a/tools/testing/explicit_ci_jobs.py\n+++ b/tools/testing/explicit_ci_jobs.py\n@@ -127,7 +127,7 @@ def commit_ci(files: List[str], message: str) -> None:\n     args = parser.parse_args()\n \n     touched_files = [CONFIG_YML]\n-    with open(CONFIG_YML, \"r\") as f:\n+    with open(CONFIG_YML) as f:\n         config_yml = yaml.safe_load(f.read())\n \n     config_yml[\"workflows\"] = get_filtered_circleci_config(\ndiff --git a/tools/testing/test_selections.py b/tools/testing/test_selections.py\nindex 24fb7278d206f9..76f841b8902bdd 100644\n--- a/tools/testing/test_selections.py\n+++ b/tools/testing/test_selections.py\n@@ -163,7 +163,7 @@ def _get_previously_failing_tests() -> Set[str]:\n         )\n         return set()\n \n-    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH, \"r\") as f:\n+    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH) as f:\n         last_failed_tests = json.load(f)\n \n     prioritized_tests = _parse_prev_failing_test_files(last_failed_tests)\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex 053ce0c0a89552..ebd24383e0b53f 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -156,14 +156,12 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-\n-                            \"      Note: While we usually use modules in the python standard library \"\n-                            f\"from the local environment, `{module_name}` has a lot of system \"\n-                            \"level access and therefore can pose a security risk. We heavily \"\n-                            f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n-                            \"is not possible, add it to the extern list by calling \"\n-                            f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-\n+                        \"      Note: While we usually use modules in the python standard library \"\n+                        f\"from the local environment, `{module_name}` has a lot of system \"\n+                        \"level access and therefore can pose a security risk. We heavily \"\n+                        f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n+                        \"is not possible, add it to the extern list by calling \"\n+                        f'PackageExporter.extern(\"`{module_name}`\")\\n'\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +171,8 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-\n-                    \"Set debug=True when invoking PackageExporter for a visualization of where \"\n-                    \"broken modules are coming from!\\n\"\n-\n+                \"Set debug=True when invoking PackageExporter for a visualization of where \"\n+                \"broken modules are coming from!\\n\"\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\n"
  },
  {
    "number": 105407,
    "title": "[BE] Enable ruff's UP rules and autoformat onnx/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "0909457a89ee90d6386a3dab31c5cebdf4342e66",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105407",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105407/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105407.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105407.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105407/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105407/comments",
    "labels": [
      "release notes: onnx",
      "open source"
    ],
    "_event_time": "2023-07-18T01:15:49.733242Z",
    "state": "closed",
    "patch": "From a250298c80637b4bb5811820c33774969dfe7ce0 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:43 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat onnx/\n\n[ghstack-poisoned]\n---\n torch/onnx/_internal/diagnostics/infra/utils.py    | 2 +-\n torch/onnx/_internal/exporter.py                   | 4 +---\n torch/onnx/_internal/fx/onnxfunction_dispatcher.py | 6 +++---\n torch/onnx/_internal/fx/passes/type_promotion.py   | 2 +-\n torch/onnx/_internal/fx/registration.py            | 4 ++--\n torch/onnx/_internal/io_adapter.py                 | 6 +++---\n torch/onnx/_internal/jit_utils.py                  | 2 +-\n torch/onnx/symbolic_opset11.py                     | 2 +-\n torch/onnx/symbolic_opset17.py                     | 8 ++++----\n torch/onnx/verification.py                         | 4 ++--\n 10 files changed, 19 insertions(+), 21 deletions(-)\n\ndiff --git a/torch/onnx/_internal/diagnostics/infra/utils.py b/torch/onnx/_internal/diagnostics/infra/utils.py\nindex f287268df5727b..4648b477515025 100644\n--- a/torch/onnx/_internal/diagnostics/infra/utils.py\n+++ b/torch/onnx/_internal/diagnostics/infra/utils.py\n@@ -43,7 +43,7 @@ def python_call_stack(frames_to_skip: int = 0, frames_to_log: int = 16) -> _infr\n     return stack\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def _function_source_info(fn: Callable) -> Tuple[Sequence[str], int, Optional[str]]:\n     \"\"\"Returns the source lines, line number, and source file path for the given function.\n \ndiff --git a/torch/onnx/_internal/exporter.py b/torch/onnx/_internal/exporter.py\nindex aae7eb6f8e3804..33bf8c7afe44f5 100644\n--- a/torch/onnx/_internal/exporter.py\n+++ b/torch/onnx/_internal/exporter.py\n@@ -145,9 +145,7 @@ class ResolvedExportOptions(ExportOptions):\n     logging diagnostics, and generating the SARIF log.\"\"\"\n \n     @_beartype.beartype\n-    def __init__(\n-        self, options: Optional[Union[ExportOptions, \"ResolvedExportOptions\"]]\n-    ):\n+    def __init__(self, options: Optional[Union[ExportOptions, ResolvedExportOptions]]):\n         if options is None:\n             options = ExportOptions()\n         if isinstance(options, ResolvedExportOptions):\ndiff --git a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\nindex a312cf1aed3d80..2b489ca076b430 100644\n--- a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n+++ b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n@@ -105,7 +105,7 @@ def dispatch(\n         ],\n         onnx_kwargs: Dict[str, fx_type_utils.Argument],\n         diagnostic_context: diagnostics.DiagnosticContext,\n-    ) -> Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]:\n+    ) -> Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]:\n         \"\"\"Dispatches an ONNX function based on the given FX node, arguments, and keyword arguments.\n         Args:\n             node: The TorchFX node to dispatch the function for.\n@@ -405,7 +405,7 @@ def aten_new_full_dtype(self: TTensor, size: INT64, fill_value: TTensor, dtype:\n \n     \"\"\"\n \n-    def __init__(self, onnxfunction: \"onnxscript.OnnxFunction\"):\n+    def __init__(self, onnxfunction: onnxscript.OnnxFunction):\n         \"\"\"Initialize the OnnxSchemaChecker .\n \n         Args:\n@@ -579,7 +579,7 @@ def _record_matching_score(\n     @_beartype.beartype\n     def _separate_input_attributes_from_arguments(\n         self,\n-        param_schemas: Sequence[\"onnxscript.values.ParamSchema\"],\n+        param_schemas: Sequence[onnxscript.values.ParamSchema],\n         args: Sequence[\n             Optional[Union[fx_type_utils.TensorLike, str, int, float, bool, list]]\n         ],\ndiff --git a/torch/onnx/_internal/fx/passes/type_promotion.py b/torch/onnx/_internal/fx/passes/type_promotion.py\nindex e100afefe7814a..c8a10cc322fe90 100644\n--- a/torch/onnx/_internal/fx/passes/type_promotion.py\n+++ b/torch/onnx/_internal/fx/passes/type_promotion.py\n@@ -1217,7 +1217,7 @@ def add_rule(self, rule: TypePromotionRule) -> None:\n             ValueError: If the rule is invalid.\n         \"\"\"\n         if not rule.is_valid():\n-            raise ValueError(\"Invalid type promotion rule: {}\".format(rule))\n+            raise ValueError(f\"Invalid type promotion rule: {rule}\")\n         self._rule_table[f\"{rule.namespace}.{rule.op_name}\"] = rule\n \n     @_beartype.beartype\ndiff --git a/torch/onnx/_internal/fx/registration.py b/torch/onnx/_internal/fx/registration.py\nindex 135c9afe9cdd57..b7c8c3521e55f5 100644\n--- a/torch/onnx/_internal/fx/registration.py\n+++ b/torch/onnx/_internal/fx/registration.py\n@@ -29,7 +29,7 @@ class SymbolicFunction:\n \n     \"\"\"\n \n-    onnx_function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]\n+    onnx_function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]\n     op_full_name: str\n     is_custom: bool = False\n \n@@ -99,7 +99,7 @@ def _register(\n     @_beartype.beartype\n     def register_custom_op(\n         self,\n-        function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"],\n+        function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction],\n         namespace: str,\n         op_name: str,\n         overload: Optional[str] = None,\ndiff --git a/torch/onnx/_internal/io_adapter.py b/torch/onnx/_internal/io_adapter.py\nindex 2654a1ade32ac4..1a80e179ac0b67 100644\n--- a/torch/onnx/_internal/io_adapter.py\n+++ b/torch/onnx/_internal/io_adapter.py\n@@ -60,7 +60,7 @@ def append_step(self, step: InputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, *model_args, **model_kwargs\n-    ) -> Sequence[Union[int, float, bool, str, \"torch.Tensor\", None]]:\n+    ) -> Sequence[Union[int, float, bool, str, torch.Tensor, None]]:\n         \"\"\"Converts the PyTorch model inputs to exported ONNX model inputs format.\n \n         Args:\n@@ -113,7 +113,7 @@ def append_step(self, step: OutputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, model_outputs: Any\n-    ) -> Sequence[Union[\"torch.Tensor\", int, float, bool, str]]:\n+    ) -> Sequence[Union[torch.Tensor, int, float, bool, str]]:\n         \"\"\"Converts the PyTorch model outputs to exported ONNX model outputs format.\n \n         Args:\n@@ -228,7 +228,7 @@ def apply(\n class LiftParametersAndBuffersIntoArgsStep:\n     \"\"\"Append parameters and buffers to model's positional argument list.\"\"\"\n \n-    def __init__(self, inputs: Tuple[\"torch.Tensor\", ...]) -> None:\n+    def __init__(self, inputs: Tuple[torch.Tensor, ...]) -> None:\n         self.inputs = inputs\n \n     def apply(\ndiff --git a/torch/onnx/_internal/jit_utils.py b/torch/onnx/_internal/jit_utils.py\nindex 9052961fc7a646..c46a82c40dfec8 100644\n--- a/torch/onnx/_internal/jit_utils.py\n+++ b/torch/onnx/_internal/jit_utils.py\n@@ -40,7 +40,7 @@ class GraphContext:\n     block: _C.Block\n     opset: int\n     original_node: _C.Node\n-    params_dict: Dict[str, \"_C.IValue\"]\n+    params_dict: Dict[str, _C.IValue]\n     env: Dict[_C.Value, _C.Value]\n \n     # Relay methods from _C.Graph for compatibility with symbolic functions that expect\ndiff --git a/torch/onnx/symbolic_opset11.py b/torch/onnx/symbolic_opset11.py\nindex b432244c42aaa7..3bb63e0e8fa36b 100644\n--- a/torch/onnx/symbolic_opset11.py\n+++ b/torch/onnx/symbolic_opset11.py\n@@ -888,7 +888,7 @@ def _get_arange_dtype(dtype):\n         dtype = symbolic_helper._maybe_get_const(dtype, \"i\")\n         return dtype\n \n-    if len(args) == 2 and all((isinstance(val, int) for val in args)):\n+    if len(args) == 2 and all(isinstance(val, int) for val in args):\n         # aten::arange(Scalar start, Scalar end)\n         dtype = torch.int64\n         # Start index.\ndiff --git a/torch/onnx/symbolic_opset17.py b/torch/onnx/symbolic_opset17.py\nindex 92151a5e58d977..3fa03ab8e11915 100644\n--- a/torch/onnx/symbolic_opset17.py\n+++ b/torch/onnx/symbolic_opset17.py\n@@ -144,8 +144,8 @@ def stft(\n         # Center window around zeros if needed (required by ONNX's STFT)\n         if n_win < n_fft:\n             left, right = _compute_edge_sizes(n_fft, n_win)\n-            left_win = g.op(\"Constant\", value_t=torch.zeros((left)))\n-            right_win = g.op(\"Constant\", value_t=torch.zeros((right)))\n+            left_win = g.op(\"Constant\", value_t=torch.zeros(left))\n+            right_win = g.op(\"Constant\", value_t=torch.zeros(right))\n             window = g.op(\"Concat\", left_win, window, right_win, axis_i=0)\n \n     # Create window, if needed\n@@ -161,11 +161,11 @@ def stft(\n             # Center window, if needed\n             left, right = _compute_edge_sizes(n_fft, win_length)\n             torch_window = torch.hstack(\n-                (torch.zeros((left)), torch.ones((win_length)), torch.zeros((right)))\n+                (torch.zeros(left), torch.ones(win_length), torch.zeros(right))\n             )\n         else:\n             # Rectangle window\n-            torch_window = torch.ones((n_fft))\n+            torch_window = torch.ones(n_fft)\n         assert torch_window.shape[0] == n_fft\n         window = g.op(\"Constant\", value_t=torch_window)\n     window = g.op(\ndiff --git a/torch/onnx/verification.py b/torch/onnx/verification.py\nindex abfa4677eb21cb..27fe4e28e32cf6 100644\n--- a/torch/onnx/verification.py\n+++ b/torch/onnx/verification.py\n@@ -1310,7 +1310,7 @@ def essential_node_kinds(self) -> Set[str]:\n         }\n \n     @_beartype.beartype\n-    def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n+    def all_mismatch_leaf_graph_info(self) -> List[GraphInfo]:\n         \"\"\"Return a list of all leaf `GraphInfo` objects that have mismatch.\"\"\"\n         if not self.has_mismatch():\n             return []\n@@ -1333,7 +1333,7 @@ def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n         return results\n \n     @_beartype.beartype\n-    def find_partition(self, id: str) -> Optional[\"GraphInfo\"]:\n+    def find_partition(self, id: str) -> Optional[GraphInfo]:\n         \"\"\"Find the `GraphInfo` object with the given id.\"\"\"\n         if id == self.id:\n             return self\n"
  },
  {
    "number": 105406,
    "title": "[BE] Enable ruff's UP rules and autoformat optim/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "74bb3ec0ac8e001fa39ef9f3e63dcbd66548126b",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105406",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105406/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105406.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105406.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105406/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105406/comments",
    "labels": [
      "open source",
      "release notes: optim"
    ],
    "_event_time": "2023-07-18T01:15:45.374226Z",
    "state": "closed",
    "patch": "From a1ba46d8b368d1a8898ac8455f708a37c3f55499 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:39 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat optim/\n\n[ghstack-poisoned]\n---\n test/distributions/test_constraints.py        |  14 +-\n test/distributions/test_distributions.py      | 238 +++++++++---------\n test/distributions/test_transforms.py         |  20 +-\n test/optim/test_optim.py                      |   4 +-\n torch/distributions/constraints.py            |  16 +-\n torch/distributions/independent.py            |   2 +-\n torch/distributions/kl.py                     |   6 +-\n .../lowrank_multivariate_normal.py            |   2 +-\n torch/distributions/mixture_same_family.py    |   8 +-\n .../distributions/transformed_distribution.py |   2 +-\n torch/distributions/transforms.py             |   6 +-\n torch/optim/adadelta.py                       |   8 +-\n torch/optim/adagrad.py                        |   8 +-\n torch/optim/adam.py                           |  10 +-\n torch/optim/adamax.py                         |  10 +-\n torch/optim/adamw.py                          |  10 +-\n torch/optim/asgd.py                           |   4 +-\n torch/optim/lr_scheduler.py                   |  18 +-\n torch/optim/nadam.py                          |  12 +-\n torch/optim/optimizer.py                      |  12 +-\n torch/optim/radam.py                          |  10 +-\n torch/optim/rmsprop.py                        |  10 +-\n torch/optim/rprop.py                          |   4 +-\n torch/optim/sgd.py                            |   6 +-\n torch/optim/sparse_adam.py                    |   8 +-\n torch/package/_importlib.py                   |   6 +-\n .../package/file_structure_representation.py  |   1 -\n torch/package/package_exporter.py             |  10 +-\n torch/package/package_importer.py             |   6 +-\n torch/profiler/_memory_profiler.py            |   4 +-\n torch/profiler/_pattern_matcher.py            |   2 +-\n torch/signal/windows/windows.py               |   1 -\n torch/sparse/semi_structured.py               |  16 +-\n 33 files changed, 246 insertions(+), 248 deletions(-)\n\ndiff --git a/test/distributions/test_constraints.py b/test/distributions/test_constraints.py\nindex b733cbc021e153..0753b246e37948 100644\n--- a/test/distributions/test_constraints.py\n+++ b/test/distributions/test_constraints.py\n@@ -83,7 +83,7 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         t = biject_to(constraint)\n     except NotImplementedError:\n         pytest.skip('`biject_to` not implemented.')\n-    assert t.bijective, \"biject_to({}) is not bijective\".format(constraint)\n+    assert t.bijective, f\"biject_to({constraint}) is not bijective\"\n     if constraint_fn is constraints.corr_cholesky:\n         # (D * (D-1)) / 2 (where D = 4) = 6 (size of last dim)\n         x = torch.randn(6, 6, dtype=torch.double)\n@@ -93,12 +93,12 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         x = x.cuda()\n     y = t(x)\n     assert constraint.check(y).all(), '\\n'.join([\n-        \"Failed to biject_to({})\".format(constraint),\n-        \"x = {}\".format(x),\n-        \"biject_to(...)(x) = {}\".format(y),\n+        f\"Failed to biject_to({constraint})\",\n+        f\"x = {x}\",\n+        f\"biject_to(...)(x) = {y}\",\n     ])\n     x2 = t.inv(y)\n-    assert torch.allclose(x, x2), \"Error in biject_to({}) inverse\".format(constraint)\n+    assert torch.allclose(x, x2), f\"Error in biject_to({constraint}) inverse\"\n \n     j = t.log_abs_det_jacobian(x, y)\n     assert j.shape == x.shape[:x.dim() - t.domain.event_dim]\n@@ -119,10 +119,10 @@ def test_transform_to(constraint_fn, args, is_cuda):\n     if is_cuda:\n         x = x.cuda()\n     y = t(x)\n-    assert constraint.check(y).all(), \"Failed to transform_to({})\".format(constraint)\n+    assert constraint.check(y).all(), f\"Failed to transform_to({constraint})\"\n     x2 = t.inv(y)\n     y2 = t(x2)\n-    assert torch.allclose(y, y2), \"Error in transform_to({}) pseudoinverse\".format(constraint)\n+    assert torch.allclose(y, y2), f\"Error in transform_to({constraint}) pseudoinverse\"\n \n \n if __name__ == \"__main__\":\ndiff --git a/test/distributions/test_distributions.py b/test/distributions/test_distributions.py\nindex 69591d31c5ed20..2f4d256516c849 100644\n--- a/test/distributions/test_distributions.py\n+++ b/test/distributions/test_distributions.py\n@@ -862,7 +862,7 @@ def _check_sampler_sampler(self, torch_dist, ref_dist, message, multivariate=Fal\n         bins = samples.reshape((num_bins, samples_per_bin)).mean(axis=1)\n         stddev = samples_per_bin ** -0.5\n         threshold = stddev * scipy.special.erfinv(1 - 2 * failure_rate / num_bins)\n-        message = '{}.sample() is biased:\\n{}'.format(message, bins)\n+        message = f'{message}.sample() is biased:\\n{bins}'\n         for bias in bins:\n             self.assertLess(-threshold, bias, message)\n             self.assertLess(bias, threshold, message)\n@@ -971,7 +971,7 @@ def test_has_examples(self):\n             if isinstance(Dist, type) and issubclass(Dist, Distribution) \\\n                     and Dist is not Distribution and Dist is not ExponentialFamily:\n                 self.assertIn(Dist, distributions_with_examples,\n-                              \"Please add {} to the EXAMPLES list in test_distributions.py\".format(Dist.__name__))\n+                              f\"Please add {Dist.__name__} to the EXAMPLES list in test_distributions.py\")\n \n     def test_support_attributes(self):\n         for Dist, params in EXAMPLES:\n@@ -1120,7 +1120,7 @@ def test_geometric_sample(self):\n         for prob in [0.01, 0.18, 0.8]:\n             self._check_sampler_discrete(Geometric(prob),\n                                          scipy.stats.geom(p=prob, loc=-1),\n-                                         'Geometric(prob={})'.format(prob))\n+                                         f'Geometric(prob={prob})')\n \n     def test_binomial(self):\n         p = torch.arange(0.05, 1, 0.1).requires_grad_()\n@@ -1136,7 +1136,7 @@ def test_binomial_sample(self):\n             for count in [2, 10, 100, 500]:\n                 self._check_sampler_discrete(Binomial(total_count=count, probs=prob),\n                                              scipy.stats.binom(count, prob),\n-                                             'Binomial(total_count={}, probs={})'.format(count, prob))\n+                                             f'Binomial(total_count={count}, probs={prob})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_binomial_log_prob_and_entropy(self):\n@@ -1431,7 +1431,7 @@ def test_poisson_sample(self):\n         for rate in [0.1, 1.0, 5.0]:\n             self._check_sampler_discrete(Poisson(rate),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={})'.format(rate),\n+                                         f'Poisson(lambda={rate})',\n                                          failure_rate=1e-3)\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n@@ -1441,7 +1441,7 @@ def test_poisson_gpu_sample(self):\n         for rate in [0.12, 0.9, 4.0]:\n             self._check_sampler_discrete(Poisson(torch.tensor([rate]).cuda()),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={}, cuda)'.format(rate),\n+                                         f'Poisson(lambda={rate}, cuda)',\n                                          failure_rate=1e-3)\n \n     def test_relaxed_bernoulli(self):\n@@ -1476,7 +1476,7 @@ def sample(self, *args, **kwargs):\n         for probs, temp in product([0.1, 0.2, 0.8], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(Rounded(RelaxedBernoulli(temp, probs)),\n                                          scipy.stats.bernoulli(probs),\n-                                         'Rounded(RelaxedBernoulli(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedBernoulli(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [0.001, 0.2, 0.999]:\n@@ -1534,7 +1534,7 @@ def pmf(self, samples):\n         for probs, temp in product([torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(ArgMax(RelaxedOneHotCategorical(temp, probs)),\n                                          ScipyCategorical(scipy.stats.multinomial(1, probs)),\n-                                         'Rounded(RelaxedOneHotCategorical(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedOneHotCategorical(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])]:\n@@ -1588,7 +1588,7 @@ def test_vonmises_sample(self):\n             for concentration in [0.03, 0.3, 1.0, 10.0, 100.0]:\n                 self._check_sampler_sampler(VonMises(loc, concentration),\n                                             scipy.stats.vonmises(loc=loc, kappa=concentration),\n-                                            \"VonMises(loc={}, concentration={})\".format(loc, concentration),\n+                                            f\"VonMises(loc={loc}, concentration={concentration})\",\n                                             num_samples=int(1e5), circular=True)\n \n     def test_vonmises_logprob(self):\n@@ -1694,7 +1694,7 @@ def test_halfnormal_sample(self):\n         for std in [0.1, 1.0, 10.0]:\n             self._check_sampler_sampler(HalfNormal(std),\n                                         scipy.stats.halfnorm(scale=std),\n-                                        'HalfNormal(scale={})'.format(std))\n+                                        f'HalfNormal(scale={std})')\n \n     def test_lognormal(self):\n         mean = torch.randn(5, 5, requires_grad=True)\n@@ -1746,7 +1746,7 @@ def test_lognormal_sample(self):\n         for mean, std in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(LogNormal(mean, std),\n                                         scipy.stats.lognorm(scale=math.exp(mean), s=std),\n-                                        'LogNormal(loc={}, scale={})'.format(mean, std))\n+                                        f'LogNormal(loc={mean}, scale={std})')\n \n     def test_logisticnormal(self):\n         set_rng_seed(1)  # see Note [Randomized statistical tests]\n@@ -1814,7 +1814,7 @@ def test_logisticnormal_sample(self):\n             std_th = torch.tensor(np.sqrt(np.diag(cov)))\n             self._check_sampler_sampler(\n                 LogisticNormal(mean_th, std_th), ref_dist,\n-                'LogisticNormal(loc={}, scale={})'.format(mean_th, std_th),\n+                f'LogisticNormal(loc={mean_th}, scale={std_th})',\n                 multivariate=True)\n \n     def test_mixture_same_family_shape(self):\n@@ -1958,7 +1958,7 @@ def test_normal_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Normal(loc, scale),\n                                         scipy.stats.norm(loc=loc, scale=scale),\n-                                        'Normal(mean={}, std={})'.format(loc, scale))\n+                                        f'Normal(mean={loc}, std={scale})')\n \n     def test_lowrank_multivariate_normal_shape(self):\n         mean = torch.randn(5, 3, requires_grad=True)\n@@ -2191,15 +2191,15 @@ def test_multivariate_normal_sample(self):\n \n         self._check_sampler_sampler(MultivariateNormal(mean, cov),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, cov={})'.format(mean, cov),\n+                                    f'MultivariateNormal(loc={mean}, cov={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, precision_matrix=prec),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, atol={})'.format(mean, prec),\n+                                    f'MultivariateNormal(loc={mean}, atol={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, scale_tril=scale_tril),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, scale_tril={})'.format(mean, scale_tril),\n+                                    f'MultivariateNormal(loc={mean}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_multivariate_normal_properties(self):\n@@ -2352,15 +2352,15 @@ def test_wishart_sample(self):\n \n         self._check_sampler_sampler(Wishart(df, cov),\n                                     ref_dist,\n-                                    'Wishart(df={}, covariance_matrix={})'.format(df, cov),\n+                                    f'Wishart(df={df}, covariance_matrix={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, precision_matrix=prec),\n                                     ref_dist,\n-                                    'Wishart(df={}, precision_matrix={})'.format(df, prec),\n+                                    f'Wishart(df={df}, precision_matrix={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, scale_tril=scale_tril),\n                                     ref_dist,\n-                                    'Wishart(df={}, scale_tril={})'.format(df, scale_tril),\n+                                    f'Wishart(df={df}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_wishart_properties(self):\n@@ -2431,7 +2431,7 @@ def test_exponential_sample(self):\n         for rate in [1e-5, 1.0, 10.]:\n             self._check_sampler_sampler(Exponential(rate),\n                                         scipy.stats.expon(scale=1. / rate),\n-                                        'Exponential(rate={})'.format(rate))\n+                                        f'Exponential(rate={rate})')\n \n     def test_laplace(self):\n         loc = torch.randn(5, 5, requires_grad=True)\n@@ -2482,7 +2482,7 @@ def test_laplace_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Laplace(loc, scale),\n                                         scipy.stats.laplace(loc=loc, scale=scale),\n-                                        'Laplace(loc={}, scale={})'.format(loc, scale))\n+                                        f'Laplace(loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gamma_shape(self):\n@@ -2533,7 +2533,7 @@ def test_gamma_sample(self):\n         for alpha, beta in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gamma(alpha, beta),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(concentration={}, rate={})'.format(alpha, beta))\n+                                        f'Gamma(concentration={alpha}, rate={beta})')\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n@@ -2543,7 +2543,7 @@ def test_gamma_gpu_sample(self):\n             a, b = torch.tensor([alpha]).cuda(), torch.tensor([beta]).cuda()\n             self._check_sampler_sampler(Gamma(a, b),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(alpha={}, beta={})'.format(alpha, beta),\n+                                        f'Gamma(alpha={alpha}, beta={beta})',\n                                         failure_rate=1e-4)\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -2575,7 +2575,7 @@ def test_pareto_sample(self):\n         for scale, alpha in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Pareto(scale, alpha),\n                                         scipy.stats.pareto(alpha, scale=scale),\n-                                        'Pareto(scale={}, alpha={})'.format(scale, alpha))\n+                                        f'Pareto(scale={scale}, alpha={alpha})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gumbel(self):\n@@ -2616,7 +2616,7 @@ def test_gumbel_sample(self):\n         for loc, scale in product([-5.0, -1.0, -0.1, 0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gumbel(loc, scale),\n                                         scipy.stats.gumbel_r(loc=loc, scale=scale),\n-                                        'Gumbel(loc={}, scale={})'.format(loc, scale))\n+                                        f'Gumbel(loc={loc}, scale={scale})')\n \n     def test_kumaraswamy_shape(self):\n         concentration1 = torch.randn(2, 3).abs().requires_grad_()\n@@ -2646,13 +2646,13 @@ def test_kumaraswamy_mean_variance(self):\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .mean\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .mean\")\n             expected = samples.var(0)\n             actual = m.variance\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .variance\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .variance\")\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_fishersnedecor(self):\n@@ -2683,7 +2683,7 @@ def test_fishersnedecor_sample(self):\n         for df1, df2 in product([0.1, 0.5, 1.0, 5.0, 10.0], [0.1, 0.5, 1.0, 5.0, 10.0]):\n             self._check_sampler_sampler(FisherSnedecor(df1, df2),\n                                         scipy.stats.f(df1, df2),\n-                                        'FisherSnedecor(loc={}, scale={})'.format(df1, df2))\n+                                        f'FisherSnedecor(loc={df1}, scale={df2})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_chi2_shape(self):\n@@ -2710,7 +2710,7 @@ def test_chi2_sample(self):\n         for df in [0.1, 1.0, 5.0]:\n             self._check_sampler_sampler(Chi2(df),\n                                         scipy.stats.chi2(df),\n-                                        'Chi2(df={})'.format(df))\n+                                        f'Chi2(df={df})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT(self):\n@@ -2740,7 +2740,7 @@ def test_studentT_sample(self):\n         for df, loc, scale in product([0.1, 1.0, 5.0, 10.0], [-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(StudentT(df=df, loc=loc, scale=scale),\n                                         scipy.stats.t(df=df, loc=loc, scale=scale),\n-                                        'StudentT(df={}, loc={}, scale={})'.format(df, loc, scale))\n+                                        f'StudentT(df={df}, loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT_log_prob(self):\n@@ -2793,7 +2793,7 @@ def test_dirichlet_sample(self):\n         alpha = torch.exp(torch.randn(3))\n         self._check_sampler_sampler(Dirichlet(alpha),\n                                     scipy.stats.dirichlet(alpha.numpy()),\n-                                    'Dirichlet(alpha={})'.format(list(alpha)),\n+                                    f'Dirichlet(alpha={list(alpha)})',\n                                     multivariate=True)\n \n     def test_dirichlet_mode(self):\n@@ -2837,11 +2837,11 @@ def test_beta_sample(self):\n         for con1, con0 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Beta(con1, con0),\n                                         scipy.stats.beta(con1, con0),\n-                                        'Beta(alpha={}, beta={})'.format(con1, con0))\n+                                        f'Beta(alpha={con1}, beta={con0})')\n         # Check that small alphas do not cause NANs.\n         for Tensor in [torch.FloatTensor, torch.DoubleTensor]:\n             x = Beta(Tensor([1e-6]), Tensor([1e-6])).sample()[0]\n-            self.assertTrue(np.isfinite(x) and x > 0, 'Invalid Beta.sample(): {}'.format(x))\n+            self.assertTrue(np.isfinite(x) and x > 0, f'Invalid Beta.sample(): {x}')\n \n     def test_beta_underflow(self):\n         # For low values of (alpha, beta), the gamma samples can underflow\n@@ -2997,10 +2997,10 @@ def test_cdf_icdf_inverse(self):\n                     continue\n                 rel_error = torch.abs(actual - samples) / (1e-10 + torch.abs(samples))\n                 self.assertLess(rel_error.max(), 1e-4, msg='\\n'.join([\n-                    '{} example {}/{}, icdf(cdf(x)) != x'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf(x) = {}'.format(cdf),\n-                    'icdf(cdf(x)) = {}'.format(actual),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, icdf(cdf(x)) != x',\n+                    f'x = {samples}',\n+                    f'cdf(x) = {cdf}',\n+                    f'icdf(cdf(x)) = {actual}',\n                 ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3029,11 +3029,11 @@ def test_cdf_log_prob(self):\n                     continue\n                 cdfs_derivative = grad(cdfs.sum(), [samples])[0]  # this should not be wrapped in torch.abs()\n                 self.assertEqual(cdfs_derivative, pdfs, msg='\\n'.join([\n-                    '{} example {}/{}, d(cdf)/dx != pdf(x)'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf = {}'.format(cdfs),\n-                    'pdf = {}'.format(pdfs),\n-                    'grad(cdf) = {}'.format(cdfs_derivative),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, d(cdf)/dx != pdf(x)',\n+                    f'x = {samples}',\n+                    f'cdf = {cdfs}',\n+                    f'pdf = {pdfs}',\n+                    f'grad(cdf) = {cdfs_derivative}',\n                 ]))\n \n     def test_valid_parameter_broadcasting(self):\n@@ -3144,13 +3144,13 @@ def test_valid_parameter_broadcasting(self):\n         for dist, expected_size in valid_examples:\n             actual_size = dist.sample().size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n             sample_shape = torch.Size((2,))\n             expected_size = sample_shape + expected_size\n             actual_size = dist.sample(sample_shape).size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n     def test_invalid_parameter_broadcasting(self):\n         # invalid broadcasting cases; should throw error\n@@ -3303,13 +3303,13 @@ def test_gamma(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.0005, '\\n'.join([\n-                'Bad gradient dx/alpha for x ~ Gamma({}, 1)'.format(alpha),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at alpha={}, x={}'.format(alpha, x[rel_error.argmax()]),\n+                f'Bad gradient dx/alpha for x ~ Gamma({alpha}, 1)',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at alpha={alpha}, x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3331,12 +3331,12 @@ def test_chi2(self):\n             expected_grad = -cdf_df / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx/ddf for x ~ Chi2({})'.format(df),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n+                f'Bad gradient dx/ddf for x ~ Chi2({df})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3361,13 +3361,13 @@ def test_dirichlet_on_diagonal(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx[0]/dalpha[0] for Dirichlet([{}, {}, {}])'.format(a0, a1, a2),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x={}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx[0]/dalpha[0] for Dirichlet([{a0}, {a1}, {a2}])',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3391,13 +3391,13 @@ def test_beta_wrt_alpha(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon1 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon1 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3421,13 +3421,13 @@ def test_beta_wrt_beta(self):\n             expected_grad = -cdf_beta / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon0 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {!r}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon0 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]!r}',\n             ]))\n \n     def test_dirichlet_multivariate(self):\n@@ -3485,8 +3485,8 @@ def compute_v(x, alpha):\n             # expression in terms of log_prob rather than the less numerically stable log_prob.exp().\n             error = dlogp_da + (dlogp_dx * v).sum(-1) + div_v\n             self.assertLess(torch.abs(error).max(), 0.005, '\\n'.join([\n-                'Dirichlet([{}, {}, {}]) gradient violates continuity equation:'.format(a1, a2, a3),\n-                'error = {}'.format(error),\n+                f'Dirichlet([{a1}, {a2}, {a3}]) gradient violates continuity equation:',\n+                f'error = {error}',\n             ]))\n \n \n@@ -4147,9 +4147,9 @@ def test_kl_monte_carlo(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                'Expected ({} Monte Carlo samples): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                f'Expected ({denominator} Monte Carlo samples): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     # Multivariate normal has a separate Monte Carlo based test due to the requirement of random generation of\n@@ -4174,9 +4174,9 @@ def test_kl_multivariate_normal(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected ({} Monte Carlo sample): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected ({denominator} Monte Carlo sample): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     def test_kl_multivariate_normal_batched(self):\n@@ -4223,23 +4223,23 @@ def test_kl_lowrank_multivariate_normal(self):\n \n             error_lowrank_lowrank = torch.abs(actual_lowrank_lowrank - expected).max()\n             self.assertLess(error_lowrank_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_lowrank),\n+                f'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_lowrank}',\n             ]))\n \n             error_lowrank_full = torch.abs(actual_lowrank_full - expected).max()\n             self.assertLess(error_lowrank_full, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_full),\n+                f'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_full}',\n             ]))\n \n             error_full_lowrank = torch.abs(actual_full_lowrank - expected).max()\n             self.assertLess(error_full_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_full_lowrank),\n+                f'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_full_lowrank}',\n             ]))\n \n     def test_kl_lowrank_multivariate_normal_batched(self):\n@@ -4261,16 +4261,16 @@ def test_kl_exponential_family(self):\n                 actual = kl_divergence(p, q)\n                 expected = _kl_expfamily_expfamily(p, q)\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                    'Expected (using Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                    f'Expected (using Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n     def test_kl_infinite(self):\n         for p, q in self.infinite_examples:\n             self.assertTrue((kl_divergence(p, q) == inf).all(),\n-                            'Incorrect KL({}, {})'.format(type(p).__name__, type(q).__name__))\n+                            f'Incorrect KL({type(p).__name__}, {type(q).__name__})')\n \n     def test_kl_edgecases(self):\n         self.assertEqual(kl_divergence(Bernoulli(0), Bernoulli(0)), 0)\n@@ -4287,9 +4287,9 @@ def test_kl_shape(self):\n                     continue\n                 expected_shape = dist.batch_shape if dist.batch_shape else torch.Size()\n                 self.assertEqual(kl.shape, expected_shape, msg='\\n'.join([\n-                    '{} example {}/{}'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected {}'.format(expected_shape),\n-                    'Actual {}'.format(kl.shape),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}',\n+                    f'Expected {expected_shape}',\n+                    f'Actual {kl.shape}',\n                 ]))\n \n     def test_kl_transformed(self):\n@@ -4316,10 +4316,10 @@ def test_entropy_monte_carlo(self):\n                 ignore = (expected == inf) | (expected == -inf)\n                 expected[ignore] = actual[ignore]\n                 self.assertEqual(actual, expected, atol=0.2, rtol=0, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (monte carlo) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max()),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (monte carlo) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}',\n                 ]))\n \n     def test_entropy_exponential_family(self):\n@@ -4337,10 +4337,10 @@ def test_entropy_exponential_family(self):\n                 except NotImplementedError:\n                     continue\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n \n@@ -4632,7 +4632,7 @@ def test_lazy_logits_initialization(self):\n             dist = Dist(**param)\n             # Create new instance to generate a valid sample\n             dist.log_prob(Dist(**param).sample())\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('probs', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -4649,7 +4649,7 @@ def test_lazy_probs_initialization(self):\n                 continue\n             dist = Dist(**param)\n             dist.sample()\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('logits', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -5161,7 +5161,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_enumerate_support(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5185,7 +5185,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_mean(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5207,7 +5207,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_variance(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5231,7 +5231,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_entropy(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5255,7 +5255,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_cdf(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5276,7 +5276,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n \n if __name__ == '__main__' and torch._C.has_lapack:\ndiff --git a/test/distributions/test_transforms.py b/test/distributions/test_transforms.py\nindex a4a025b83fd36e..6fd4cf818d6b83 100644\n--- a/test/distributions/test_transforms.py\n+++ b/test/distributions/test_transforms.py\n@@ -156,7 +156,7 @@ def generate_data(transform):\n         x /= x.norm(dim=-1, keepdim=True)\n         x.diagonal(dim1=-1).copy_(x.diagonal(dim1=-1).abs())\n         return x\n-    raise ValueError('Unsupported domain: {}'.format(domain))\n+    raise ValueError(f'Unsupported domain: {domain}')\n \n \n TRANSFORMS_CACHE_ACTIVE = get_transforms(cache_size=1)\n@@ -215,19 +215,19 @@ def test_forward_inverse(transform, test_cached):\n     if transform.bijective:\n         # verify function inverse\n         assert torch.allclose(x2, x, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t.inv(t(-)) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n+            f'{transform} t.inv(t(-)) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n         ])\n     else:\n         # verify weaker function pseudo-inverse\n         assert torch.allclose(y2, y, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t(t.inv(t(-))) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n-            'y2 = t(x2) = {}'.format(y2),\n+            f'{transform} t(t.inv(t(-))) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n+            f'y2 = t(x2) = {y2}',\n         ])\n \n \ndiff --git a/test/optim/test_optim.py b/test/optim/test_optim.py\nindex 54307b2417eaf6..2f1f5536fc2fe1 100644\n--- a/test/optim/test_optim.py\n+++ b/test/optim/test_optim.py\n@@ -1701,8 +1701,8 @@ def test_fused_optimizer_does_not_step_if_foundinf(self):\n \n         num_tensors = 5\n         for functional_optim, amsgrad, no_grad_scale in itertools.product((adam.adam, adamw.adamw), (False, True), (False, True)):\n-            params, grads, exp_avgs, exp_avg_sqs = [\n-                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4)]\n+            params, grads, exp_avgs, exp_avg_sqs = (\n+                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4))\n             prev_params = [t.clone().detach() for t in params]\n             max_exp_avg_sqs = [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] if amsgrad else []\n             state_steps = [torch.ones((), dtype=torch.float32, device=\"cuda\") for _ in range(num_tensors)]\ndiff --git a/torch/distributions/constraints.py b/torch/distributions/constraints.py\nindex a4e3c08461cde7..5f284959beb372 100644\n--- a/torch/distributions/constraints.py\n+++ b/torch/distributions/constraints.py\n@@ -258,7 +258,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -277,7 +277,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -296,7 +296,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -321,7 +321,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -338,7 +338,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -355,7 +355,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -373,7 +373,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -391,7 +391,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \ndiff --git a/torch/distributions/independent.py b/torch/distributions/independent.py\nindex 48442650ddcb77..44a01fd62f9130 100644\n--- a/torch/distributions/independent.py\n+++ b/torch/distributions/independent.py\n@@ -109,4 +109,4 @@ def enumerate_support(self, expand=True):\n         return self.base_dist.enumerate_support(expand=expand)\n \n     def __repr__(self):\n-        return self.__class__.__name__ + '({}, {})'.format(self.base_dist, self.reinterpreted_batch_ndims)\n+        return self.__class__.__name__ + f'({self.base_dist}, {self.reinterpreted_batch_ndims})'\ndiff --git a/torch/distributions/kl.py b/torch/distributions/kl.py\nindex 26d7b47d2f51a8..4eda85ef75b68a 100644\n--- a/torch/distributions/kl.py\n+++ b/torch/distributions/kl.py\n@@ -65,9 +65,9 @@ def kl_version2(p, q): ...\n         type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n     \"\"\"\n     if not isinstance(type_p, type) and issubclass(type_p, Distribution):\n-        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n+        raise TypeError(f'Expected type_p to be a Distribution subclass but got {type_p}')\n     if not isinstance(type_q, type) and issubclass(type_q, Distribution):\n-        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n+        raise TypeError(f'Expected type_q to be a Distribution subclass but got {type_q}')\n \n     def decorator(fun):\n         _KL_REGISTRY[type_p, type_q] = fun\n@@ -735,7 +735,7 @@ def _kl_uniform_beta(p, q):\n     common_term = p.high - p.low\n     t1 = torch.log(common_term)\n     t2 = (q.concentration1 - 1) * (_x_log_x(p.high) - _x_log_x(p.low) - common_term) / common_term\n-    t3 = (q.concentration0 - 1) * (_x_log_x((1 - p.high)) - _x_log_x((1 - p.low)) + common_term) / common_term\n+    t3 = (q.concentration0 - 1) * (_x_log_x(1 - p.high) - _x_log_x(1 - p.low) + common_term) / common_term\n     t4 = q.concentration1.lgamma() + q.concentration0.lgamma() - (q.concentration1 + q.concentration0).lgamma()\n     result = t3 + t4 - t1 - t2\n     result[(p.high > q.support.upper_bound) | (p.low < q.support.lower_bound)] = inf\ndiff --git a/torch/distributions/lowrank_multivariate_normal.py b/torch/distributions/lowrank_multivariate_normal.py\nindex f74ea47a7e53a4..5ca125a92dd006 100644\n--- a/torch/distributions/lowrank_multivariate_normal.py\n+++ b/torch/distributions/lowrank_multivariate_normal.py\n@@ -93,7 +93,7 @@ def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n             raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\"\n                              .format(event_shape[0]))\n         if cov_diag.shape[-1:] != event_shape:\n-            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n+            raise ValueError(f\"cov_diag must be a batch of vectors with shape {event_shape}\")\n \n         loc_ = loc.unsqueeze(-1)\n         cov_diag_ = cov_diag.unsqueeze(-1)\ndiff --git a/torch/distributions/mixture_same_family.py b/torch/distributions/mixture_same_family.py\nindex f12bef1da2c54d..a4d7bd6ff4610b 100644\n--- a/torch/distributions/mixture_same_family.py\n+++ b/torch/distributions/mixture_same_family.py\n@@ -71,17 +71,17 @@ def __init__(self,\n         cdbs = self._component_distribution.batch_shape[:-1]\n         for size1, size2 in zip(reversed(mdbs), reversed(cdbs)):\n             if size1 != 1 and size2 != 1 and size1 != size2:\n-                raise ValueError(\"`mixture_distribution.batch_shape` ({0}) is not \"\n+                raise ValueError(\"`mixture_distribution.batch_shape` ({}) is not \"\n                                  \"compatible with `component_distribution.\"\n-                                 \"batch_shape`({1})\".format(mdbs, cdbs))\n+                                 \"batch_shape`({})\".format(mdbs, cdbs))\n \n         # Check that the number of mixture component matches\n         km = self._mixture_distribution.logits.shape[-1]\n         kc = self._component_distribution.batch_shape[-1]\n         if km is not None and kc is not None and km != kc:\n-            raise ValueError(\"`mixture_distribution component` ({0}) does not\"\n+            raise ValueError(\"`mixture_distribution component` ({}) does not\"\n                              \" equal `component_distribution.batch_shape[-1]`\"\n-                             \" ({1})\".format(km, kc))\n+                             \" ({})\".format(km, kc))\n         self._num_component = km\n \n         event_shape = self._component_distribution.event_shape\ndiff --git a/torch/distributions/transformed_distribution.py b/torch/distributions/transformed_distribution.py\nindex d31064210d4ba7..cd7b5f088a99fe 100644\n--- a/torch/distributions/transformed_distribution.py\n+++ b/torch/distributions/transformed_distribution.py\n@@ -51,7 +51,7 @@ def __init__(self, base_distribution, transforms, validate_args=None):\n                 raise ValueError(\"transforms must be a Transform or a list of Transforms\")\n             self.transforms = transforms\n         else:\n-            raise ValueError(\"transforms must be a Transform or list, but was {}\".format(transforms))\n+            raise ValueError(f\"transforms must be a Transform or list, but was {transforms}\")\n \n         # Reshape base_distribution according to transforms.\n         base_shape = base_distribution.batch_shape + base_distribution.event_shape\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 06d21548384e3c..6745d1f6fbd51e 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -135,7 +135,7 @@ def with_cache(self, cache_size=1):\n             return self\n         if type(self).__init__ is Transform.__init__:\n             return type(self)(cache_size=cache_size)\n-        raise NotImplementedError(\"{}.with_cache is not implemented\".format(type(self)))\n+        raise NotImplementedError(f\"{type(self)}.with_cache is not implemented\")\n \n     def __eq__(self, other):\n         return self is other\n@@ -506,7 +506,7 @@ def forward_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.in_shape)\n         if shape[cut:] != self.in_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.in_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.in_shape}\")\n         return shape[:cut] + self.out_shape\n \n     def inverse_shape(self, shape):\n@@ -514,7 +514,7 @@ def inverse_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.out_shape)\n         if shape[cut:] != self.out_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.out_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.out_shape}\")\n         return shape[:cut] + self.in_shape\n \n \ndiff --git a/torch/optim/adadelta.py b/torch/optim/adadelta.py\nindex d4cbd41883af65..a38337426313db 100644\n--- a/torch/optim/adadelta.py\n+++ b/torch/optim/adadelta.py\n@@ -22,13 +22,13 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= rho <= 1.0:\n-            raise ValueError(\"Invalid rho value: {}\".format(rho))\n+            raise ValueError(f\"Invalid rho value: {rho}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adagrad.py b/torch/optim/adagrad.py\nindex 5909818e5bfb6e..1a3e5120004f98 100644\n--- a/torch/optim/adagrad.py\n+++ b/torch/optim/adagrad.py\n@@ -23,11 +23,11 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= lr_decay:\n-            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n+            raise ValueError(f\"Invalid lr_decay value: {lr_decay}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= initial_accumulator_value:\n             raise ValueError(\n                 \"Invalid initial_accumulator_value value: {}\".format(\n@@ -35,7 +35,7 @@ def __init__(\n                 )\n             )\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adam.py b/torch/optim/adam.py\nindex 3c0d550f414b45..c7e4ed45a92156 100644\n--- a/torch/optim/adam.py\n+++ b/torch/optim/adam.py\n@@ -16,15 +16,15 @@ def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                  maximize: bool = False, capturable: bool = False,\n                  differentiable: bool = False, fused: Optional[bool] = None):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, amsgrad=amsgrad,\ndiff --git a/torch/optim/adamax.py b/torch/optim/adamax.py\nindex 9a5bf9131993ad..1ee927274558f1 100644\n--- a/torch/optim/adamax.py\n+++ b/torch/optim/adamax.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adamw.py b/torch/optim/adamw.py\nindex da202d95c2a032..ff8dbef1d46e8a 100644\n--- a/torch/optim/adamw.py\n+++ b/torch/optim/adamw.py\n@@ -26,15 +26,15 @@ def __init__(\n         fused: Optional[bool] = None,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/asgd.py b/torch/optim/asgd.py\nindex 5e5bd759c1d540..e483e1c31fbc7c 100644\n--- a/torch/optim/asgd.py\n+++ b/torch/optim/asgd.py\n@@ -28,9 +28,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/lr_scheduler.py b/torch/optim/lr_scheduler.py\nindex b531f5149d1aff..d0f85a5daea0c8 100644\n--- a/torch/optim/lr_scheduler.py\n+++ b/torch/optim/lr_scheduler.py\n@@ -1366,11 +1366,11 @@ class CosineAnnealingWarmRestarts(LRScheduler):\n \n     def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n         if T_0 <= 0 or not isinstance(T_0, int):\n-            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n+            raise ValueError(f\"Expected positive integer T_0, but got {T_0}\")\n         if T_mult < 1 or not isinstance(T_mult, int):\n-            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n+            raise ValueError(f\"Expected integer T_mult >= 1, but got {T_mult}\")\n         if not isinstance(eta_min, (float, int)):\n-            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))\n+            raise ValueError(f\"Expected float or int eta_min, but got {eta_min} of type {type(eta_min)}\")\n         self.T_0 = T_0\n         self.T_i = T_0\n         self.T_mult = T_mult\n@@ -1425,7 +1425,7 @@ def step(self, epoch=None):\n                 self.T_i = self.T_i * self.T_mult\n         else:\n             if epoch < 0:\n-                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n+                raise ValueError(f\"Expected non-negative epoch, but got {epoch}\")\n             if epoch >= self.T_0:\n                 if self.T_mult == 1:\n                     self.T_cur = epoch % self.T_0\n@@ -1590,13 +1590,13 @@ def __init__(self,\n             raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n         elif total_steps is not None:\n             if total_steps <= 0 or not isinstance(total_steps, int):\n-                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n+                raise ValueError(f\"Expected positive integer total_steps, but got {total_steps}\")\n             self.total_steps = total_steps\n         else:\n             if epochs <= 0 or not isinstance(epochs, int):\n-                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n+                raise ValueError(f\"Expected positive integer epochs, but got {epochs}\")\n             if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n-                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n+                raise ValueError(f\"Expected positive integer steps_per_epoch, but got {steps_per_epoch}\")\n             self.total_steps = epochs * steps_per_epoch\n \n         if three_phase:\n@@ -1643,11 +1643,11 @@ def __init__(self,\n \n         # Validate pct_start\n         if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n-            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n+            raise ValueError(f\"Expected float between 0 and 1 pct_start, but got {pct_start}\")\n \n         # Validate anneal_strategy\n         if anneal_strategy not in ['cos', 'linear']:\n-            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n+            raise ValueError(f\"anneal_strategy must by one of 'cos' or 'linear', instead got {anneal_strategy}\")\n         elif anneal_strategy == 'cos':\n             self.anneal_func = self._annealing_cos\n         elif anneal_strategy == 'linear':\ndiff --git a/torch/optim/nadam.py b/torch/optim/nadam.py\nindex 23fa563f044d0d..aeb3fc8b77dd2c 100644\n--- a/torch/optim/nadam.py\n+++ b/torch/optim/nadam.py\n@@ -11,17 +11,17 @@ def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                  weight_decay=0, momentum_decay=4e-3, *, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= momentum_decay:\n-            raise ValueError(\"Invalid momentum_decay value: {}\".format(momentum_decay))\n+            raise ValueError(f\"Invalid momentum_decay value: {momentum_decay}\")\n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, momentum_decay=momentum_decay,\n                         foreach=foreach, differentiable=differentiable)\ndiff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py\nindex 34d27bdaca6058..2356a073f3719d 100644\n--- a/torch/optim/optimizer.py\n+++ b/torch/optim/optimizer.py\n@@ -246,10 +246,10 @@ def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         for i, group in enumerate(self.param_groups):\n             format_string += '\\n'\n-            format_string += 'Parameter Group {0}\\n'.format(i)\n+            format_string += f'Parameter Group {i}\\n'\n             for key in sorted(group.keys()):\n                 if key != 'params':\n-                    format_string += '    {0}: {1}\\n'.format(key, group[key])\n+                    format_string += f'    {key}: {group[key]}\\n'\n         format_string += ')'\n         return format_string\n \n@@ -304,7 +304,7 @@ def profile_hook_step(func):\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             self, *_ = args\n-            profile_name = \"Optimizer.step#{}.step\".format(self.__class__.__name__)\n+            profile_name = f\"Optimizer.step#{self.__class__.__name__}.step\"\n             with torch.autograd.profiler.record_function(profile_name):\n                 # call optimizer step pre hooks\n                 for pre_hook in chain(_global_optimizer_pre_hooks.values(), self._optimizer_step_pre_hooks.values()):\n@@ -337,7 +337,7 @@ def _group_tensors_by_device_and_dtype(tensorlistlist, with_indices=False):\n             return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)\n \n     def _patch_step_function(self):\n-        self._zero_grad_profile_name = \"Optimizer.zero_grad#{}.zero_grad\".format(self.__class__.__name__)\n+        self._zero_grad_profile_name = f\"Optimizer.zero_grad#{self.__class__.__name__}.zero_grad\"\n         hooked = getattr(self.__class__.step, \"hooked\", None)\n         if not hooked:\n             self.__class__.step = self.profile_hook_step(self.__class__.step)  # type: ignore[method-assign]\n@@ -468,8 +468,8 @@ def load_state_dict(self, state_dict):\n                              \"that doesn't match the size of optimizer's group\")\n \n         # Update the state\n-        id_map = dict(zip(chain.from_iterable((g['params'] for g in saved_groups)),\n-                      chain.from_iterable((g['params'] for g in groups))))\n+        id_map = dict(zip(chain.from_iterable(g['params'] for g in saved_groups),\n+                      chain.from_iterable(g['params'] for g in groups)))\n \n         def cast(param, value, param_id=None, param_groups=None, key=None):\n             r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\ndiff --git a/torch/optim/radam.py b/torch/optim/radam.py\nindex 3078db48cfd2fc..120620ab949cc1 100644\n--- a/torch/optim/radam.py\n+++ b/torch/optim/radam.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/rmsprop.py b/torch/optim/rmsprop.py\nindex 88acf98a1bcbda..cec27d95506840 100644\n--- a/torch/optim/rmsprop.py\n+++ b/torch/optim/rmsprop.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= momentum:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= alpha:\n-            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n+            raise ValueError(f\"Invalid alpha value: {alpha}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/rprop.py b/torch/optim/rprop.py\nindex a0812f5fbc903f..93e7241010500a 100644\n--- a/torch/optim/rprop.py\n+++ b/torch/optim/rprop.py\n@@ -20,9 +20,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < etas[0] < 1.0 < etas[1]:\n-            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n+            raise ValueError(f\"Invalid eta values: {etas[0]}, {etas[1]}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/sgd.py b/torch/optim/sgd.py\nindex c34761d5e48555..d22fb2a697fd41 100644\n--- a/torch/optim/sgd.py\n+++ b/torch/optim/sgd.py\n@@ -11,11 +11,11 @@ def __init__(self, params, lr=required, momentum=0, dampening=0,\n                  weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if lr is not required and lr < 0.0:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if momentum < 0.0:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if weight_decay < 0.0:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                         weight_decay=weight_decay, nesterov=nesterov,\ndiff --git a/torch/optim/sparse_adam.py b/torch/optim/sparse_adam.py\nindex 383b6866e822af..c68441cb389c04 100644\n--- a/torch/optim/sparse_adam.py\n+++ b/torch/optim/sparse_adam.py\n@@ -7,13 +7,13 @@\n class SparseAdam(Optimizer):\n     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, maximize: bool = False):\n         if not 0.0 < lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n \n         params = list(params)\n \ndiff --git a/torch/package/_importlib.py b/torch/package/_importlib.py\nindex 327c79c67ef9d7..011567b89f5d6c 100644\n--- a/torch/package/_importlib.py\n+++ b/torch/package/_importlib.py\n@@ -31,13 +31,13 @@ def _resolve_name(name, package, level):\n     if len(bits) < level:\n         raise ValueError(\"attempted relative import beyond top-level package\")\n     base = bits[0]\n-    return \"{}.{}\".format(base, name) if name else base\n+    return f\"{base}.{name}\" if name else base\n \n \n def _sanity_check(name, package, level):\n     \"\"\"Verify arguments are \"sane\".\"\"\"\n     if not isinstance(name, str):\n-        raise TypeError(\"module name must be str, not {}\".format(type(name)))\n+        raise TypeError(f\"module name must be str, not {type(name)}\")\n     if level < 0:\n         raise ValueError(\"level must be >= 0\")\n     if level > 0:\n@@ -90,6 +90,6 @@ def _normalize_path(path):\n     \"\"\"\n     parent, file_name = os.path.split(path)\n     if parent:\n-        raise ValueError(\"{!r} must be only a file name\".format(path))\n+        raise ValueError(f\"{path!r} must be only a file name\")\n     else:\n         return file_name\ndiff --git a/torch/package/file_structure_representation.py b/torch/package/file_structure_representation.py\nindex 6ea69173ed3f69..cc5f055c1a20ef 100644\n--- a/torch/package/file_structure_representation.py\n+++ b/torch/package/file_structure_representation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Dict, List\n \n from .glob_group import GlobGroup, GlobPattern\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex f9478b66327605..053ce0c0a89552 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -79,7 +79,7 @@ class PackagingErrorReason(Enum):\n     \"\"\"\n \n     def __repr__(self):\n-        return \"<%s.%s>\" % (self.__class__.__name__, self.name)\n+        return f\"<{self.__class__.__name__}.{self.name}>\"\n \n     IS_EXTENSION_MODULE = (\n         \"Module is a C extension module. torch.package supports Python modules only.\"\n@@ -156,14 +156,14 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-                        (\n+\n                             \"      Note: While we usually use modules in the python standard library \"\n                             f\"from the local environment, `{module_name}` has a lot of system \"\n                             \"level access and therefore can pose a security risk. We heavily \"\n                             f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n                             \"is not possible, add it to the extern list by calling \"\n                             f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-                        )\n+\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +173,10 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-                (\n+\n                     \"Set debug=True when invoking PackageExporter for a visualization of where \"\n                     \"broken modules are coming from!\\n\"\n-                )\n+\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\ndiff --git a/torch/package/package_importer.py b/torch/package/package_importer.py\nindex 8369e79e783ad7..2d313c8f14eb45 100644\n--- a/torch/package/package_importer.py\n+++ b/torch/package/package_importer.py\n@@ -539,7 +539,7 @@ def _handle_fromlist(self, module, fromlist, *, recursive=False):\n                     if not recursive and hasattr(module, \"__all__\"):\n                         self._handle_fromlist(module, module.__all__, recursive=True)\n                 elif not hasattr(module, x):\n-                    from_name = \"{}.{}\".format(module_name, x)\n+                    from_name = f\"{module_name}.{x}\"\n                     try:\n                         self._gcd_import(from_name)\n                     except ModuleNotFoundError as exc:\n@@ -587,13 +587,13 @@ def _get_package(self, package):\n         \"\"\"\n         if hasattr(package, \"__spec__\"):\n             if package.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package.__spec__.name))\n+                raise TypeError(f\"{package.__spec__.name!r} is not a package\")\n             else:\n                 return package\n         else:\n             module = self.import_module(package)\n             if module.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package))\n+                raise TypeError(f\"{package!r} is not a package\")\n             else:\n                 return module\n \ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex 7ade85a85caa11..fbbcd4d67b7889 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -738,11 +738,11 @@ def _category_snapshot(self) -> Dict[TensorAndID, Optional[Category]]:\n \n         for node in self._data_flow_graph.flow_nodes:\n             all_tensor_versions.update(((k, v) for k, (_, v) in node.inputs.items()))\n-            all_tensor_versions.update(((key, 0) for key in node.intermediates))\n+            all_tensor_versions.update((key, 0) for key in node.intermediates)\n             all_tensor_versions.update(node.outputs.items())\n \n         for i in self._categories._values.values():\n-            all_tensor_versions.update(((key, 0) for key in i._by_id_keyset))\n+            all_tensor_versions.update((key, 0) for key in i._by_id_keyset)\n \n         return {\n             (key, version): self._categories.get(key, version)\ndiff --git a/torch/profiler/_pattern_matcher.py b/torch/profiler/_pattern_matcher.py\nindex ae95faf0d2bae7..1d85d193ecf894 100644\n--- a/torch/profiler/_pattern_matcher.py\n+++ b/torch/profiler/_pattern_matcher.py\n@@ -642,7 +642,7 @@ def report_all_anti_patterns(prof,\n         json_report_path = os.path.join(json_report_dir,\n                                         \"torchtidy_report.json\")\n         if os.path.exists(json_report_path):\n-            with open(json_report_path, \"r\") as f:\n+            with open(json_report_path) as f:\n                 exisiting_report = json.load(f)\n                 exisiting_report.update(report_dict)\n                 report_dict = exisiting_report\ndiff --git a/torch/signal/windows/windows.py b/torch/signal/windows/windows.py\nindex 1ddfff96228927..d1b8e2529bb97e 100644\n--- a/torch/signal/windows/windows.py\n+++ b/torch/signal/windows/windows.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Optional, Iterable\n \n import torch\ndiff --git a/torch/sparse/semi_structured.py b/torch/sparse/semi_structured.py\nindex 0e4c217a50aafb..d1e4321d66f36b 100644\n--- a/torch/sparse/semi_structured.py\n+++ b/torch/sparse/semi_structured.py\n@@ -136,28 +136,28 @@ def __init__(\n             # check device\n             if not original_tensor.is_cuda:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.device= {original_tensor.device} is not supported! \"\n                         \"Only CUDA tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dim\n             if original_tensor.dim() != 2:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dim = {original_tensor.dim()} is not supported! \"\n                         \"Only 2d tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dtype\n             if original_tensor.dtype not in _DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dtype {original_tensor.dtype} is not a supported dtype! \"\n                         \"dtype must be one of: {_DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG}\"\n-                    )\n+\n                 )\n \n             # check shape\n@@ -167,10 +167,10 @@ def __init__(\n             if m < min_rows or m % min_rows or n < min_cols or n % min_cols:\n                 # TODO in the future we can add in padding to support dimensions that aren't perfect multiples\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.shape {original_tensor.shape} is not supported! \"\n                         \"Both dimensions must be larger or equal than and a multiple of ({min_rows}, {min_cols})\"\n-                    )\n+\n                 )\n \n             # This code calculates the size of the compressed tensor.\n"
  },
  {
    "number": 105405,
    "title": "[BE] Enable ruff's UP rules and autoformat testing/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "bda55e3925c137be765a2ecb96c61270c98e9f6c",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105405",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105405/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105405.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105405.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105405/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105405/comments",
    "labels": [
      "open source",
      "release notes: distributed (rpc)"
    ],
    "_event_time": "2023-07-18T01:15:42.649699Z",
    "state": "closed",
    "patch": "From 9d6e06041a361a9ad30b2b69a7d9eb70e26fe7aa Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:34 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat testing/\n\n[ghstack-poisoned]\n---\n torch/testing/_comparison.py                  | 10 +--\n .../_internal/check_kernel_launches.py        |  2 +-\n .../_internal/codegen/random_topo_test.py     | 16 ++---\n torch/testing/_internal/common_cuda.py        |  2 +-\n torch/testing/_internal/common_device_type.py | 50 +++++++--------\n torch/testing/_internal/common_distributed.py |  4 +-\n torch/testing/_internal/common_fsdp.py        |  2 +-\n .../_internal/common_methods_invocations.py   | 18 +++---\n torch/testing/_internal/common_modules.py     | 64 +++++++++----------\n torch/testing/_internal/common_nn.py          | 12 ++--\n torch/testing/_internal/common_pruning.py     |  1 -\n .../testing/_internal/common_quantization.py  |  5 +-\n torch/testing/_internal/common_utils.py       | 34 +++++-----\n torch/testing/_internal/dist_utils.py         |  4 +-\n .../_internal/distributed/distributed_test.py | 14 ++--\n .../distributed/nn/api/remote_module_test.py  | 20 +++---\n .../distributed/rpc/dist_autograd_test.py     | 22 +++----\n .../reinforcement_learning_rpc_test.py        |  2 +-\n .../distributed/rpc/faulty_agent_rpc_test.py  |  8 +--\n .../rpc/faulty_rpc_agent_test_fixture.py      |  2 +-\n .../_internal/distributed/rpc/jit/rpc_test.py |  2 +-\n .../distributed/rpc/jit/rpc_test_faulty.py    |  8 +--\n .../_internal/distributed/rpc/rpc_test.py     |  4 +-\n .../rpc/tensorpipe_rpc_agent_test_fixture.py  |  2 +-\n .../_internal/jit_metaprogramming_utils.py    | 18 +++---\n torch/testing/_internal/jit_utils.py          | 12 ++--\n torch/testing/_internal/opinfo/core.py        |  9 +--\n .../_internal/opinfo/definitions/linalg.py    |  2 +-\n .../_internal/opinfo/definitions/sparse.py    |  2 +-\n 29 files changed, 172 insertions(+), 179 deletions(-)\n\ndiff --git a/torch/testing/_comparison.py b/torch/testing/_comparison.py\nindex 1ccc7447ed7486..4204a6c0e69441 100644\n--- a/torch/testing/_comparison.py\n+++ b/torch/testing/_comparison.py\n@@ -465,9 +465,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[bool, bool]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_bool(bool_like, id=id) for bool_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_bool(self, bool_like: Any, *, id: Tuple[Any, ...]) -> bool:\n@@ -559,9 +559,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[Union[int, float, complex], Union[int, float, complex]]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_number(number_like, id=id) for number_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_number(\n@@ -675,7 +675,7 @@ def _process_inputs(\n         if not allow_subclasses and type(actual) is not type(expected):\n             self._inputs_not_supported()\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\ndiff --git a/torch/testing/_internal/check_kernel_launches.py b/torch/testing/_internal/check_kernel_launches.py\nindex 667e3412ceaf2c..131ea461ce544a 100644\n--- a/torch/testing/_internal/check_kernel_launches.py\n+++ b/torch/testing/_internal/check_kernel_launches.py\n@@ -111,7 +111,7 @@ def check_file(filename):\n         return 0\n     if should_exclude_file(filename):\n         return 0\n-    with open(filename, \"r\") as fo:\n+    with open(filename) as fo:\n         contents = fo.read()\n         unsafeCount = check_code_for_cuda_kernel_launches(contents, filename)\n     return unsafeCount\ndiff --git a/torch/testing/_internal/codegen/random_topo_test.py b/torch/testing/_internal/codegen/random_topo_test.py\nindex fdb13d4ef139c0..b94f8f60a301d3 100644\n--- a/torch/testing/_internal/codegen/random_topo_test.py\n+++ b/torch/testing/_internal/codegen/random_topo_test.py\n@@ -96,7 +96,7 @@ def get_root(x, dependency_map):\n         out_tensor = None\n \n         if DEBUG_PRINT:\n-            print(\"iteration {0}, num_sets{1}, candidates {2}, tensor_list {3}, lh_index {4}, op_index {5}\".format(\n+            print(\"iteration {}, num_sets{}, candidates {}, tensor_list {}, lh_index {}, op_index {}\".format(\n                 num_operations, num_sets, candidate, len(tensor_list), lh_index, op_index))\n         if num_operations >= 0:\n             num_operations -= 1\n@@ -125,7 +125,7 @@ def get_root(x, dependency_map):\n                     #  right = tensor_list[lh_index]\n                     out_tensor = binary_operations[op_index - u_op_size](left, right)\n                 if DEBUG_PRINT:\n-                    print(\"binary, op_2_index {0}, rh_index ?{1}\".format(op_2_index, rh_index))\n+                    print(f\"binary, op_2_index {op_2_index}, rh_index ?{rh_index}\")\n         else:\n             # binary operation, we just randomly pick two candidates.\n             # this is not the most efficient way to close dependency, as we could have\n@@ -136,7 +136,7 @@ def get_root(x, dependency_map):\n             # [if rh_index: create binary operator output tensor]\n             rh_index = candidate[cand_index]\n             if DEBUG_PRINT:\n-                print(\"binary rh_index ?{0}\".format(rh_index))\n+                print(f\"binary rh_index ?{rh_index}\")\n \n         # update candidate should happen before we remove rh_index\n         candidate[index] = len(tensor_list)\n@@ -185,7 +185,7 @@ def get_root(x, dependency_map):\n             ret_list.append(tensor_list[ind])\n \n     if DEBUG_PRINT:\n-        print(\"ended with tensor_list: {0}\".format(len(tensor_list)))\n+        print(f\"ended with tensor_list: {len(tensor_list)}\")\n \n     return tuple(ret_list)\n \n@@ -248,7 +248,7 @@ def prepareInputTensorsToRandomTopoTest(seed,\n \n \n def reproString(current_seed, args):\n-    repro_str = \"python {0}\".format(__file__)\n+    repro_str = f\"python {__file__}\"\n     if args.cuda_fuser:\n         repro_str += \" --cuda-fuser\"\n     if args.legacy_fuser:\n@@ -259,8 +259,8 @@ def reproString(current_seed, args):\n         repro_str += \" --fp16\"\n     if args.cpu:\n         repro_str += \" --cpu\"\n-    repro_str += \" --max-num-tensor {0} --max-tensor-dim {1} --max-tensor-size {2}\"\\\n-        \" --depth-factor {3} --seed {4} --repro-run\".format(\n+    repro_str += \" --max-num-tensor {} --max-tensor-dim {} --max-tensor-size {}\"\\\n+        \" --depth-factor {} --seed {} --repro-run\".format(\n             args.max_num_tensor, args.max_tensor_dim, args.max_tensor_size,\n             args.depth_factor, current_seed)\n     return repro_str\n@@ -390,7 +390,7 @@ def parse_args():\n         if len(failing_repros) == 0:\n             print(\"test passed\")\n         else:\n-            print(\"{0} out of {1} tests failed;\".format(\n+            print(\"{} out of {} tests failed;\".format(\n                   len(failing_repros), args.iterations))\n             print(\"To repro failing tests, run\\n\")\n             for repro in failing_repros:\ndiff --git a/torch/testing/_internal/common_cuda.py b/torch/testing/_internal/common_cuda.py\nindex c380dd5e6d7250..f427f7b62b9f77 100644\n--- a/torch/testing/_internal/common_cuda.py\n+++ b/torch/testing/_internal/common_cuda.py\n@@ -48,7 +48,7 @@ def initialize_cuda_context_rng():\n     if not __cuda_ctx_rng_initialized:\n         # initialize cuda context and rng for memory tests\n         for i in range(torch.cuda.device_count()):\n-            torch.randn(1, device=\"cuda:{}\".format(i))\n+            torch.randn(1, device=f\"cuda:{i}\")\n         __cuda_ctx_rng_initialized = True\n \n \ndiff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py\nindex d9c362c332479d..891c878cc5f059 100644\n--- a/torch/testing/_internal/common_device_type.py\n+++ b/torch/testing/_internal/common_device_type.py\n@@ -276,9 +276,9 @@ def _dtype_test_suffix(dtypes):\n     if isinstance(dtypes, (list, tuple)):\n         if len(dtypes) == 0:\n             return ''\n-        return '_' + '_'.join((dtype_name(d) for d in dtypes))\n+        return '_' + '_'.join(dtype_name(d) for d in dtypes)\n     elif dtypes:\n-        return '_{}'.format(dtype_name(dtypes))\n+        return f'_{dtype_name(dtypes)}'\n     else:\n         return ''\n \n@@ -286,7 +286,7 @@ def _dtype_test_suffix(dtypes):\n def _update_param_kwargs(param_kwargs, name, value):\n     \"\"\" Adds a kwarg with the specified name and value to the param_kwargs dict. \"\"\"\n     # Make name plural (e.g. devices / dtypes) if the value is composite.\n-    plural_name = '{}s'.format(name)\n+    plural_name = f'{name}s'\n \n     # Clear out old entries of the arg if any.\n     if name in param_kwargs:\n@@ -432,7 +432,7 @@ def instantiated_test(self, param_kwargs=param_kwargs):\n \n                 return result\n \n-            assert not hasattr(cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(cls, name), f\"Redefinition of test {name}\"\n             setattr(cls, name, instantiated_test)\n \n         def default_parametrize_fn(test, generic_cls, device_cls):\n@@ -467,7 +467,7 @@ def dtype_parametrize_fn(test, generic_cls, device_cls, dtypes=dtypes):\n             dtype_kwarg = None\n             if 'dtype' in param_kwargs or 'dtypes' in param_kwargs:\n                 dtype_kwarg = param_kwargs['dtypes'] if 'dtypes' in param_kwargs else param_kwargs['dtype']\n-            test_name = '{}{}{}{}'.format(name, test_suffix, device_suffix, _dtype_test_suffix(dtype_kwarg))\n+            test_name = f'{name}{test_suffix}{device_suffix}{_dtype_test_suffix(dtype_kwarg)}'\n \n             instantiate_test_helper(cls=cls, name=test_name, test=test, param_kwargs=param_kwargs,\n                                     decorator_fn=decorator_fn)\n@@ -523,7 +523,7 @@ def setUpClass(cls):\n         cls.cudnn_version = None if cls.no_cudnn else torch.backends.cudnn.version()\n \n         # Acquires the current device as the primary (test) device\n-        cls.primary_device = 'cuda:{0}'.format(torch.cuda.current_device())\n+        cls.primary_device = f'cuda:{torch.cuda.current_device()}'\n \n # See Note [Lazy Tensor tests in device agnostic testing]\n lazy_ts_backend_init = False\n@@ -589,7 +589,7 @@ def setUpClass(cls):\n         cls.device_mod = getattr(torch, cls.device_type, None)\n         assert cls.device_mod is not None, f'''torch has no module of `{cls.device_type}`, you should register\n                                             a module by `torch._register_device_module`.'''\n-        cls.primary_device = '{device_type}:{id}'.format(device_type=cls.device_type, id=cls.device_mod.current_device())\n+        cls.primary_device = f'{cls.device_type}:{cls.device_mod.current_device()}'\n \n # Adds available device-type-specific test base classes\n def get_device_type_test_bases():\n@@ -744,7 +744,7 @@ def split_if_not_empty(x: str):\n                 else:\n                     device_type_test_class.instantiate_test(name, copy.deepcopy(test))\n             else:  # Ports non-test member\n-                assert name not in device_type_test_class.__dict__, \"Redefinition of directly defined member {0}\".format(name)\n+                assert name not in device_type_test_class.__dict__, f\"Redefinition of directly defined member {name}\"\n                 nontest = getattr(generic_test_class, name)\n                 setattr(device_type_test_class, name, nontest)\n \n@@ -913,7 +913,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for op {1}!\".format(test_name, op.name))\n+                    print(f\"Failed to instantiate {test_name} for op {op.name}!\")\n                     raise ex\n         if op is check_exhausted_iterator:\n             raise ValueError('An empty op_list was passed to @ops. '\n@@ -1034,7 +1034,7 @@ def dep_fn(self, *args, **kwargs):\n             size_bytes = size(self, *args, **kwargs) if callable(size) else size\n             _device = device if device is not None else self.get_primary_device()\n             if not _has_sufficient_memory(_device, size_bytes):\n-                raise unittest.SkipTest('Insufficient {} memory'.format(_device))\n+                raise unittest.SkipTest(f'Insufficient {_device} memory')\n \n             return fn(self, *args, **kwargs)\n         return dep_fn\n@@ -1072,7 +1072,7 @@ def __call__(self, fn):\n         @wraps(fn)\n         def only_fn(slf, *args, **kwargs):\n             if self.device_type != slf.device_type:\n-                reason = \"Only runs on {0}\".format(self.device_type)\n+                reason = f\"Only runs on {self.device_type}\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, *args, **kwargs)\n@@ -1090,13 +1090,13 @@ def __init__(self, num_required_devices):\n         self.num_required_devices = num_required_devices\n \n     def __call__(self, fn):\n-        assert not hasattr(fn, 'num_required_devices'), \"deviceCountAtLeast redefinition for {0}\".format(fn.__name__)\n+        assert not hasattr(fn, 'num_required_devices'), f\"deviceCountAtLeast redefinition for {fn.__name__}\"\n         fn.num_required_devices = self.num_required_devices\n \n         @wraps(fn)\n         def multi_fn(slf, devices, *args, **kwargs):\n             if len(devices) < self.num_required_devices:\n-                reason = \"fewer than {0} devices detected\".format(self.num_required_devices)\n+                reason = f\"fewer than {self.num_required_devices} devices detected\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, devices, *args, **kwargs)\n@@ -1108,7 +1108,7 @@ def onlyNativeDeviceTypes(fn):\n     @wraps(fn)\n     def only_fn(self, *args, **kwargs):\n         if self.device_type not in NATIVE_DEVICES:\n-            reason = \"onlyNativeDeviceTypes: doesn't run on {0}\".format(self.device_type)\n+            reason = f\"onlyNativeDeviceTypes: doesn't run on {self.device_type}\"\n             raise unittest.SkipTest(reason)\n \n         return fn(self, *args, **kwargs)\n@@ -1137,7 +1137,7 @@ class precisionOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"precisionOverride not given a dtype : precision dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"precisionOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"precisionOverride given unknown dtype {dtype}\"\n \n         self.d = d\n \n@@ -1168,7 +1168,7 @@ class toleranceOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"toleranceOverride not given a dtype : tol dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"toleranceOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"toleranceOverride given unknown dtype {dtype}\"\n             assert isinstance(prec, tol), \"toleranceOverride not given a dtype : tol dict!\"\n \n         self.d = d\n@@ -1195,17 +1195,17 @@ def __init__(self, *args, device_type=\"all\"):\n                 assert isinstance(arg, (list, tuple)), \\\n                     \"When one dtype variant is a tuple or list, \" \\\n                     \"all dtype variants must be. \" \\\n-                    \"Received non-list non-tuple dtype {0}\".format(str(arg))\n-                assert all(isinstance(dtype, torch.dtype) for dtype in arg), \"Unknown dtype in {0}\".format(str(arg))\n+                    \"Received non-list non-tuple dtype {}\".format(str(arg))\n+                assert all(isinstance(dtype, torch.dtype) for dtype in arg), f\"Unknown dtype in {str(arg)}\"\n         else:\n-            assert all(isinstance(arg, torch.dtype) for arg in args), \"Unknown dtype in {0}\".format(str(args))\n+            assert all(isinstance(arg, torch.dtype) for arg in args), f\"Unknown dtype in {str(args)}\"\n \n         self.args = args\n         self.device_type = device_type\n \n     def __call__(self, fn):\n         d = getattr(fn, 'dtypes', {})\n-        assert self.device_type not in d, \"dtypes redefinition for {0}\".format(self.device_type)\n+        assert self.device_type not in d, f\"dtypes redefinition for {self.device_type}\"\n         d[self.device_type] = self.args\n         fn.dtypes = d\n         return fn\n@@ -1244,7 +1244,7 @@ def onlyPRIVATEUSE1(fn):\n     device_type = torch._C._get_privateuse1_backend_name()\n     device_mod = getattr(torch, device_type, None)\n     if device_mod is None:\n-        reason = \"Skip as torch has no module of {0}\".format(device_type)\n+        reason = f\"Skip as torch has no module of {device_type}\"\n         return unittest.skip(reason)(fn)\n     return onlyOn(device_type)(fn)\n \n@@ -1358,7 +1358,7 @@ def wrap_fn(self, *args, **kwargs):\n                     raise unittest.SkipTest(reason)\n                 rocm_version_tuple = _get_torch_rocm_version()\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\n@@ -1375,7 +1375,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version in (versions or []):\n-                reason = \"test skipped for CUDA version {0}\".format(version)\n+                reason = f\"test skipped for CUDA version {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1391,7 +1391,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version < versions:\n-                reason = \"test skipped for CUDA versions < {0}\".format(version)\n+                reason = f\"test skipped for CUDA versions < {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1409,7 +1409,7 @@ def wrap_fn(self, *args, **kwargs):\n                     reason = \"cuDNN not available\"\n                     raise unittest.SkipTest(reason)\n                 if self.cudnn_version is None or self.cudnn_version < version:\n-                    reason = \"cuDNN version {0} is available but {1} required\".format(self.cudnn_version, version)\n+                    reason = f\"cuDNN version {self.cudnn_version} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\ndiff --git a/torch/testing/_internal/common_distributed.py b/torch/testing/_internal/common_distributed.py\nindex 8981aa78d06a09..d1cf02749b79ce 100644\n--- a/torch/testing/_internal/common_distributed.py\n+++ b/torch/testing/_internal/common_distributed.py\n@@ -770,7 +770,7 @@ def _check_no_test_errors(self, elapsed_time) -> None:\n         for i, p in enumerate(self.processes):\n             if p.exitcode is None:\n                 raise RuntimeError(\n-                    \"Process {} timed out after {} seconds\".format(i, elapsed_time)\n+                    f\"Process {i} timed out after {elapsed_time} seconds\"\n                 )\n             self.assertNotEqual(self.TEST_ERROR_EXIT_CODE, p.exitcode)\n \n@@ -1102,7 +1102,7 @@ def _check_return_codes(cls, failed_ranks, timeout, fn):\n                     \"Caught exception: \\n%s exiting thread %s\", msg, rank\n                 )\n                 error_msg += (\n-                    \"Thread {} exited with exception:\\n{}\\n\".format(rank, msg)\n+                    f\"Thread {rank} exited with exception:\\n{msg}\\n\"\n                 )\n             elif isinstance(exc, SystemExit):\n                 if type(exc.code) == int and skip_code < 0:\ndiff --git a/torch/testing/_internal/common_fsdp.py b/torch/testing/_internal/common_fsdp.py\nindex 589372eaa8e216..20a35930a7f5f4 100644\n--- a/torch/testing/_internal/common_fsdp.py\n+++ b/torch/testing/_internal/common_fsdp.py\n@@ -881,7 +881,7 @@ def process_group(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     def _check_cpu_offload(self, fsdp_model, cpu_offload):\n         self.assertEqual(cpu_offload, fsdp_model.cpu_offload)\ndiff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py\nindex 3b9d3269853a05..a9f58c76d90f2d 100644\n--- a/torch/testing/_internal/common_methods_invocations.py\n+++ b/torch/testing/_internal/common_methods_invocations.py\n@@ -851,7 +851,7 @@ def error_inputs_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"normal expects std >= 0.0, but found std {}\".format(invalid_std),\n+        error_regex=fr\"normal expects std >= 0.0, but found std {invalid_std}\",\n     )\n \n def sample_inputs_cauchy(op, device, dtype, requires_grad, **kwargs):\n@@ -871,7 +871,7 @@ def error_inputs_cauchy(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_scale,)),\n         error_type=RuntimeError,\n-        error_regex=r\"cauchy_ expects sigma > 0.0, but found sigma={}\".format(invalid_scale),\n+        error_regex=fr\"cauchy_ expects sigma > 0.0, but found sigma={invalid_scale}\",\n     )\n \n \n@@ -893,7 +893,7 @@ def error_inputs_exponential(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(invalid_rate,)),\n         error_type=RuntimeError,\n-        error_regex=r\"exponential_ expects lambda > 0.0, but found lambda={}\".format(invalid_rate),\n+        error_regex=fr\"exponential_ expects lambda > 0.0, but found lambda={invalid_rate}\",\n     )\n \n \n@@ -915,7 +915,7 @@ def error_inputs_geometric(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(neg_prob,)),\n         error_type=RuntimeError,\n-        error_regex=r\"geometric_ expects p to be in \\(0, 1\\), but got p={}\".format(neg_prob),\n+        error_regex=fr\"geometric_ expects p to be in \\(0, 1\\), but got p={neg_prob}\",\n     )\n \n \n@@ -937,7 +937,7 @@ def error_inputs_log_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"log_normal_ expects std > 0.0, but found std={}\".format(invalid_std),\n+        error_regex=fr\"log_normal_ expects std > 0.0, but found std={invalid_std}\",\n     )\n \n \n@@ -1889,9 +1889,9 @@ def sample_inputs_logcumsumexp(self, device, dtype, requires_grad, **kwargs):\n             yield SampleInput(t, dim)\n \n def sample_inputs_trace(self, device, dtype, requires_grad, **kwargs):\n-    yield SampleInput((make_tensor((S, S), dtype=dtype, device=device,\n+    yield SampleInput(make_tensor((S, S), dtype=dtype, device=device,\n                                    low=None, high=None,\n-                                   requires_grad=requires_grad)))\n+                                   requires_grad=requires_grad))\n \n \n def error_inputs_trace(op, device):\n@@ -4020,7 +4020,7 @@ def error_inputs_group_norm(opinfo, device, **kwargs):\n \n     # check that input has minimum number of dimensions\n     err_msg1 = \"Expected at least 2 dimensions for input tensor but received\"\n-    s1 = SampleInput(make_arg((1)), args=(1,))\n+    s1 = SampleInput(make_arg(1), args=(1,))\n     yield ErrorInput(s1, error_regex=err_msg1)\n \n     # check that the channels dimension is compatible with number of groups\n@@ -6950,7 +6950,7 @@ def make_bool_mask(shape):\n \n         if mask_t.sum() == 0:\n             def random_index(shape):\n-                return tuple((random.randrange(0, max_idx) for max_idx in shape))\n+                return tuple(random.randrange(0, max_idx) for max_idx in shape)\n \n             mask_t[random_index(mask_t.shape)] = True\n             return mask_t\ndiff --git a/torch/testing/_internal/common_modules.py b/torch/testing/_internal/common_modules.py\nindex 2119678a33f5ef..c3ac11454ab410 100644\n--- a/torch/testing/_internal/common_modules.py\n+++ b/torch/testing/_internal/common_modules.py\n@@ -123,7 +123,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for module {1}!\".format(test_name, module_info.name))\n+                    print(f\"Failed to instantiate {test_name} for module {module_info.name}!\")\n                     raise ex\n \n \n@@ -252,7 +252,7 @@ def bilinear_reference_fn(m, p, x1, x2, bias=True):\n                     desc='no_bias',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n         ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n-                    forward_input=FunctionInput(make_input((2)), make_input((3))),\n+                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                     desc='no_batch_dim',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n     ]\n@@ -312,9 +312,9 @@ def module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_\n     for desc, constructor_kwargs in cases:\n         module_inputs.append(\n             ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n-                        forward_input=FunctionInput(make_input((3)),\n-                                                    make_target((3)),\n-                                                    make_input((1)).abs()),\n+                        forward_input=FunctionInput(make_input(3),\n+                                                    make_target(3),\n+                                                    make_input(1).abs()),\n                         desc=desc,\n                         reference_fn=no_batch_dim_reference_fn)\n         )\n@@ -454,7 +454,7 @@ def generate_regression_criterion_inputs(make_input):\n             constructor_input=FunctionInput(reduction=reduction),\n             forward_input=FunctionInput(make_input((4, )), make_input(4,)),\n             reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True),\n-            desc='no_batch_dim_{}'.format(reduction)\n+            desc=f'no_batch_dim_{reduction}'\n         ) for reduction in ['none', 'mean', 'sum']]\n \n \n@@ -752,7 +752,7 @@ def module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, tra\n     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n     conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n     kernel_size, C_in, C_out = 3, 4, 5\n-    input_no_batch_shape = (C_in,) + tuple((i + 3 for i in range(N)))\n+    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n     input_batch_shape = (2,) + input_no_batch_shape\n     return [\n         ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n@@ -878,7 +878,7 @@ def module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad,\n         ModuleInput(constructor_input=FunctionInput(),\n                     forward_input=FunctionInput(make_input((3, 2, 5)))),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(0.5),\n@@ -897,10 +897,10 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n \n     return [\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(()))),\n+                    forward_input=FunctionInput(make_input(())),\n                     desc='scalar'),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -908,7 +908,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -916,7 +916,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -924,7 +924,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5, 6)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d_multiparam')]\n \n@@ -1216,11 +1216,11 @@ def module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3, 6, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 6, 5)))),\n+            forward_input=FunctionInput(make_input((4, 6, 5))),\n             desc='1d_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput(3, 12, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 12)))),\n+            forward_input=FunctionInput(make_input((4, 12))),\n             desc='1d_affine_GN'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 6, 1e-3),\n@@ -1334,13 +1334,13 @@ def module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_g\n             constructor_input=(\n                 FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape)))),\n+            forward_input=FunctionInput(make_input(input_batch_shape))),\n         ModuleInput(\n             constructor_input=(\n                 FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                 FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape))),\n+            forward_input=FunctionInput(make_input(input_batch_shape)),\n             desc='tracking_stats'),\n         ModuleInput(\n             constructor_input=(\n@@ -1365,15 +1365,15 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((128, 5, 5)))),\n+            forward_input=FunctionInput(make_input((128, 5, 5))),\n             desc='1d_elementwise_affine_large_batch'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3, False),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_no_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([2, 2, 5], 1e-3),\n@@ -1396,11 +1396,11 @@ def module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, require\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7))),\n             desc='1d'),\n         ModuleInput(\n             constructor_input=FunctionInput(2,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n             desc='2d_uneven_pad'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 1., 0.5, 2.),\n@@ -1415,7 +1415,7 @@ def module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, t\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(1.5, 2),\n-            forward_input=FunctionInput(make_input(((1, 3, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 7))),\n             desc='norm'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, 2, 3),\n@@ -1449,7 +1449,7 @@ def module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(4),\n-            forward_input=FunctionInput(make_input(((2, 10, 4)))),\n+            forward_input=FunctionInput(make_input((2, 10, 4))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput(4, 4),\n@@ -1468,7 +1468,7 @@ def module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n-            forward_input=FunctionInput(make_input(((3, 7, 7)))),\n+            forward_input=FunctionInput(make_input((3, 7, 7))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n@@ -1486,7 +1486,7 @@ def module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2)),\n-            forward_input=FunctionInput(make_input(((2, 3, 5, 5, 5))))),\n+            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n         ModuleInput(\n             constructor_input=FunctionInput(2, (2, 2, 2)),\n             forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n@@ -1511,7 +1511,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n@@ -1521,11 +1521,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((3, 5, 7))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\n@@ -1545,7 +1545,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n@@ -1559,11 +1559,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\ndiff --git a/torch/testing/_internal/common_nn.py b/torch/testing/_internal/common_nn.py\nindex 6bc41ab20f3fcd..85f9e35ac0a6bb 100644\n--- a/torch/testing/_internal/common_nn.py\n+++ b/torch/testing/_internal/common_nn.py\n@@ -2540,7 +2540,7 @@ def unsqueeze_inp(inp):\n         output_size = (2, 3) + tuple(p + 1 for p in padding)  # simplified from `(4 + 2 * p - 3) // 2 + 1`\n         new_module_tests.append(\n             dict(\n-                module_name='Conv{}d'.format(d),\n+                module_name=f'Conv{d}d',\n                 constructor_args=(2, 3, 3, 2, padding, 1, 1, True, padding_mode),\n                 cpp_constructor_args='''torch::nn::Conv{}dOptions(2, 3, 3)\n                                         .stride(2)\n@@ -2552,7 +2552,7 @@ def unsqueeze_inp(inp):\n                 input_size=input_size,\n                 output_size=output_size,\n                 cudnn=True,\n-                desc='{}_stride2_pad2'.format(padding_mode),\n+                desc=f'{padding_mode}_stride2_pad2',\n                 with_tf32=True,\n                 tf32_precision=0.05\n             ),\n@@ -3906,7 +3906,7 @@ def flatten(xs):\n reductions = ['none', 'mean', 'sum']\n for name, reduction in product(regression_criterion_no_batch, reductions):\n     regression_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_size=(3, ),\n         target_size=(3, ),\n@@ -3959,7 +3959,7 @@ def flatten(xs):\n for (name, input_fn, target_fn), reduction in product(classification_criterion_no_batch,\n                                                       reductions):\n     classification_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_fn=lambda f=input_fn: f(),\n         target_fn=lambda f=target_fn: f(),\n@@ -4152,7 +4152,7 @@ def _get_arg(self, name, unpack):\n                 self._arg_cache[name] = self._extra_kwargs[fn_name]()\n             else:\n                 assert size_name in self._extra_kwargs, \\\n-                    \"Missing `{}`, `{}` or `{}` for {}\".format(name, size_name, fn_name, self.get_name())\n+                    f\"Missing `{name}`, `{size_name}` or `{fn_name}` for {self.get_name()}\"\n \n                 def map_tensor_sizes(sizes):\n                     if isinstance(sizes, list):\n@@ -4281,7 +4281,7 @@ def test_cuda(self, test_case):\n         type_map = {torch.double: torch.float}\n         cpu_input_tuple = cpu_input if isinstance(cpu_input, tuple) else (cpu_input,)\n \n-        is_any_input_complex = any((isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple))\n+        is_any_input_complex = any(isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple)\n \n         gpu_input_tuple = to_gpu(cpu_input_tuple, type_map=type_map)\n \ndiff --git a/torch/testing/_internal/common_pruning.py b/torch/testing/_internal/common_pruning.py\nindex 32732818a25b02..b6cbd92105f3f4 100644\n--- a/torch/testing/_internal/common_pruning.py\n+++ b/torch/testing/_internal/common_pruning.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.ao.pruning import BaseSparsifier\ndiff --git a/torch/testing/_internal/common_quantization.py b/torch/testing/_internal/common_quantization.py\nindex 95686be4511264..dcc575c942bc84 100644\n--- a/torch/testing/_internal/common_quantization.py\n+++ b/torch/testing/_internal/common_quantization.py\n@@ -791,8 +791,7 @@ def _get_underlying_op_type(\n                     (exp_type_end_b is act_type_end_b)\n                 self.assertTrue(\n                     types_match,\n-                    'Type mismatch at %s: expected %s, got %s' %\n-                    (k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n+                    'Type mismatch at {}: expected {}, got {}'.format(k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n                         (act_type_start_a, act_type_end_a, act_type_start_b, act_type_end_b))\n                 )\n \n@@ -1601,7 +1600,7 @@ def __init__(self):\n         super().__init__()\n         self.quant = torch.ao.quantization.QuantStub()\n         self.fc1 = torch.nn.Linear(5, 8).to(dtype=torch.float)\n-        self.layer_norm = torch.nn.LayerNorm((8))\n+        self.layer_norm = torch.nn.LayerNorm(8)\n         self.group_norm = torch.nn.GroupNorm(2, 8)\n         self.instance_norm1d = torch.nn.InstanceNorm1d(8)\n         self.instance_norm2d = torch.nn.InstanceNorm2d(8)\ndiff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py\nindex 3ba2331e7cd40e..c96d13bd46ac53 100644\n--- a/torch/testing/_internal/common_utils.py\n+++ b/torch/testing/_internal/common_utils.py\n@@ -203,7 +203,7 @@ def repro_env_var_prefix() -> str:\n \n def maybe_load_json(filename):\n     if os.path.isfile(filename):\n-        with open(filename, 'r') as fp:\n+        with open(filename) as fp:\n             return json.load(fp)\n     log.warning(\"Attempted to load json file '%s' but it does not exist.\", filename)\n     return {}\n@@ -355,12 +355,12 @@ def instantiate_test_helper(cls, name, test, param_kwargs):\n             def instantiated_test(self, param_kwargs=param_kwargs):\n                 test(self, **param_kwargs)\n \n-            assert not hasattr(generic_cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(generic_cls, name), f\"Redefinition of test {name}\"\n             setattr(generic_cls, name, instantiated_test)\n \n         for (test, test_suffix, param_kwargs, decorator_fn) in class_attr.parametrize_fn(\n                 class_attr, generic_cls=generic_cls, device_cls=None):\n-            full_name = '{}_{}'.format(test.__name__, test_suffix)\n+            full_name = f'{test.__name__}_{test_suffix}'\n \n             # Apply decorators based on full param kwargs.\n             for decorator in decorator_fn(param_kwargs):\n@@ -830,7 +830,7 @@ def run_tests(argv=UNITTEST_ARGS):\n     # import test files.\n     if SLOW_TESTS_FILE:\n         if os.path.exists(SLOW_TESTS_FILE):\n-            with open(SLOW_TESTS_FILE, 'r') as fp:\n+            with open(SLOW_TESTS_FILE) as fp:\n                 global slow_tests_dict\n                 slow_tests_dict = json.load(fp)\n                 # use env vars so pytest-xdist subprocesses can still access them\n@@ -839,7 +839,7 @@ def run_tests(argv=UNITTEST_ARGS):\n             warnings.warn(f'slow test file provided but not found: {SLOW_TESTS_FILE}')\n     if DISABLED_TESTS_FILE:\n         if os.path.exists(DISABLED_TESTS_FILE):\n-            with open(DISABLED_TESTS_FILE, 'r') as fp:\n+            with open(DISABLED_TESTS_FILE) as fp:\n                 global disabled_tests_dict\n                 disabled_tests_dict = json.load(fp)\n                 os.environ['DISABLED_TESTS_FILE'] = DISABLED_TESTS_FILE\n@@ -905,7 +905,7 @@ def run_tests(argv=UNITTEST_ARGS):\n         test_batches = chunk_list(get_test_names(test_cases), RUN_PARALLEL)\n         processes = []\n         for i in range(RUN_PARALLEL):\n-            command = [sys.executable] + argv + ['--log-suffix=-shard-{}'.format(i + 1)] + test_batches[i]\n+            command = [sys.executable] + argv + [f'--log-suffix=-shard-{i + 1}'] + test_batches[i]\n             processes.append(subprocess.Popen(command, universal_newlines=True))\n         failed = False\n         for p in processes:\n@@ -1294,7 +1294,7 @@ def wrap_fn(self, *args, **kwargs):\n                 rocm_version = rocm_version.split(\"-\")[0]    # ignore git sha\n                 rocm_version_tuple = tuple(int(x) for x in rocm_version.split(\".\"))\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n         return wrap_fn\n@@ -1672,7 +1672,7 @@ def is_iterable_of_tensors(iterable, include_empty=False):\n     return True\n \n \n-class CudaNonDefaultStream():\n+class CudaNonDefaultStream:\n     def __enter__(self):\n         # Before starting CUDA test save currently active streams on all\n         # CUDA devices and set new non default streams to all CUDA devices\n@@ -1698,7 +1698,7 @@ def __exit__(self, exec_type, exec_value, traceback):\n                                      device_type=self.beforeStreams[d].device_type)\n         torch._C._cuda_setDevice(beforeDevice)\n \n-class CudaMemoryLeakCheck():\n+class CudaMemoryLeakCheck:\n     def __init__(self, testcase, name=None):\n         self.name = testcase.id() if name is None else name\n         self.testcase = testcase\n@@ -2104,7 +2104,7 @@ def __init__(self, actual, expected, *, rtol_override=0.0, atol_override=0.0, **\n     def _process_inputs(self, actual, expected, *, id, allow_subclasses):\n         self._check_inputs_isinstance(actual, expected, cls=(torch.Tensor, np.ndarray))\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\n@@ -2201,7 +2201,7 @@ def set_warn_always_context(new_val: bool):\n         torch.set_warn_always(old_val)\n \n \n-class NoTest():\n+class NoTest:\n     # causes pytest to not recognize this class as a test\n     __test__ = False\n \n@@ -3408,12 +3408,12 @@ def remove_prefix(text, prefix):\n         subname_output = \"\"\n         if subname:\n             expected_file += \"-\" + subname\n-            subname_output = \" ({})\".format(subname)\n+            subname_output = f\" ({subname})\"\n         expected_file += \".expect\"\n         expected = None\n \n         def accept_output(update_type):\n-            print(\"Accepting {} for {}{}:\\n\\n{}\".format(update_type, munged_id, subname_output, s))\n+            print(f\"Accepting {update_type} for {munged_id}{subname_output}:\\n\\n{s}\")\n             with open(expected_file, 'w') as f:\n                 # Adjust for producer_version, leave s unmodified\n                 s_tag = re.sub(r'(producer_version): \"[0-9.]*\"',\n@@ -3423,7 +3423,7 @@ def accept_output(update_type):\n         try:\n             with open(expected_file) as f:\n                 expected = f.read()\n-        except IOError as e:\n+        except OSError as e:\n             if e.errno != errno.ENOENT:\n                 raise\n             elif expecttest.ACCEPT:\n@@ -3442,7 +3442,7 @@ def accept_output(update_type):\n         # Adjust for producer_version\n         expected = expected.replace(\n             'producer_version: \"CURRENT_VERSION\"',\n-            'producer_version: \"{}\"'.format(torch.onnx.producer_version)\n+            f'producer_version: \"{torch.onnx.producer_version}\"'\n         )\n         if expecttest.ACCEPT:\n             if expected != s:\n@@ -3600,7 +3600,7 @@ def download_file(url, binary=True):\n             f.write(data)\n         return path\n     except error.URLError as e:\n-        msg = \"could not download test file '{}'\".format(url)\n+        msg = f\"could not download test file '{url}'\"\n         warnings.warn(msg, RuntimeWarning)\n         raise unittest.SkipTest(msg) from e\n \n@@ -4359,7 +4359,7 @@ def wrap_fn(*args, **kwargs):\n     return wrap_fn\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def get_cycles_per_ms() -> float:\n     \"\"\"Measure and return approximate number of cycles per millisecond for torch.cuda._sleep\n     \"\"\"\ndiff --git a/torch/testing/_internal/dist_utils.py b/torch/testing/_internal/dist_utils.py\nindex 94aafe31773261..daee88e4580cd5 100644\n--- a/torch/testing/_internal/dist_utils.py\n+++ b/torch/testing/_internal/dist_utils.py\n@@ -101,7 +101,7 @@ def wait_until_node_failure(rank: int, expected_error_regex: str = \".*\") -> str:\n     \"\"\"\n     while True:\n         try:\n-            rpc.rpc_sync(\"worker{}\".format(rank), noop, args=())\n+            rpc.rpc_sync(f\"worker{rank}\", noop, args=())\n             time.sleep(0.1)\n         except Exception as e:\n             if re.search(pattern=expected_error_regex, string=str(e)):\n@@ -187,7 +187,7 @@ def initialize_pg(init_method, rank: int, world_size: int) -> None:\n \n \n def worker_name(rank: int) -> str:\n-    return \"worker{}\".format(rank)\n+    return f\"worker{rank}\"\n \n \n def get_function_event(function_events, partial_event_name):\ndiff --git a/torch/testing/_internal/distributed/distributed_test.py b/torch/testing/_internal/distributed/distributed_test.py\nindex 19a0c3f50d1ae4..5fdc796310d44d 100644\n--- a/torch/testing/_internal/distributed/distributed_test.py\n+++ b/torch/testing/_internal/distributed/distributed_test.py\n@@ -517,7 +517,7 @@ def sync(cls, wait_for=None, timeout=10):\n             arrived = 0\n             with _lock():\n                 for f_name in os.listdir(barrier_dir):\n-                    with open(os.path.join(barrier_dir, f_name), \"r\") as f:\n+                    with open(os.path.join(barrier_dir, f_name)) as f:\n                         data = f.read()\n                         if int(data) >= cls.barrier_id:\n                             arrived += 1\n@@ -552,7 +552,7 @@ def tearDown(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     @classmethod\n     def _run(cls, rank, test_name, file_name, pipe):\n@@ -654,7 +654,7 @@ def test_dump_DDP_relevant_env_vars(self):\n                 lines = out.getvalue().splitlines()\n \n             def format_line(var):\n-                return \"env:%s=%s\" % (\n+                return \"env:{}={}\".format(\n                     var,\n                     os.environ[var] if var in os.environ else \"N/A\",\n                 )\n@@ -692,7 +692,7 @@ def test_get_rank(self):\n \n             all_ranks = set()\n             for f_name in os.listdir(test_dir):\n-                with open(os.path.join(test_dir, f_name), \"r\") as f:\n+                with open(os.path.join(test_dir, f_name)) as f:\n                     all_ranks.add(int(f.read()))\n             self.assertEqual(len(all_ranks), num_processes)\n \n@@ -9640,7 +9640,7 @@ def backward(ctx, grad_output):\n \n             class MyModel(nn.Module):\n                 def __init__(self, device):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.error = True\n                     self.fc1 = nn.Linear(10, 10).cuda(device)\n \n@@ -9683,12 +9683,12 @@ def forward(self, inp):\n         def test_ddp_has_finalized(self):\n \n             @dataclass\n-            class MyClass():\n+            class MyClass:\n                 obj: torch.Tensor\n \n             class MyModel(nn.Module):\n                 def __init__(self, rank):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.rank = rank\n                     self.fc1 = nn.Linear(1024, 1024).cuda(rank)\n                     self.fc2 = nn.Linear(1024, 2 * 1024).cuda(rank)\ndiff --git a/torch/testing/_internal/distributed/nn/api/remote_module_test.py b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\nindex f955c0fc1ace1a..4d9f1d9b53ddc4 100644\n--- a/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n+++ b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n@@ -131,7 +131,7 @@ def test_bad_module(self):\n         if self.rank != 0:\n             return\n         dst_worker_name = dist_utils.worker_name((self.rank + 1) % self.world_size)\n-        remote_device = \"{}/cpu\".format(dst_worker_name)\n+        remote_device = f\"{dst_worker_name}/cpu\"\n         args = (1,)\n         kwargs = dict(first_kwarg=2)\n \n@@ -575,7 +575,7 @@ def test_valid_device(self):\n         dst_worker_name = dist_utils.worker_name(dst_rank)\n \n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -585,7 +585,7 @@ def test_valid_device(self):\n \n         # Test rank works as well.\n         for remote_module in self._create_remote_module_iter(\n-            \"rank:{}/cuda:0\".format(dst_rank), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"rank:{dst_rank}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -607,7 +607,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/foo\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/foo\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -618,7 +618,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:100\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:100\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -627,7 +627,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cpu2\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cpu2\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -636,7 +636,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -648,7 +648,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:0/cuda:1\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:0/cuda:1\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -692,7 +692,7 @@ def test_input_moved_to_cuda_device(self):\n \n         # Only test Python nn.Module, because script module methods don't support taking kwargs.\n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             ret_fut = remote_module.forward_async(*args, **kwargs)\n             ret = ret_fut.wait()\n@@ -716,7 +716,7 @@ def test_input_moved_to_cuda_device_script(self):\n \n         scripted_remote_module = next(\n             self._create_remote_module_iter(\n-                \"{}/cuda:0\".format(dst_worker_name),\n+                f\"{dst_worker_name}/cuda:0\",\n                 modes=[ModuleCreationMode.MODULE_CTOR_WITH_INTERFACE],\n             )\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\nindex 8e8c353460e6ff..b08b51c31d9f7d 100644\n--- a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n+++ b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n@@ -226,7 +226,7 @@ def _exec_func_with_dst(self, dst, exec_mode, method, *args):\n             fut = rpc.rpc_async(worker_name(dst), method, args=(args))\n             return fut.wait()\n         else:\n-            raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+            raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n     def _exec_func(self, exec_mode, method, *args):\n         return self._exec_func_with_dst(\n@@ -288,7 +288,7 @@ def _test_graph(self, fn, exec_mode, sparse):\n                     worker_name(dst_rank), fn, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -355,7 +355,7 @@ def _test_graph_for_py_nested_call(self, exec_mode, sparse):\n                     args=(t1, t2, dst_rank, self.world_size, 1),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             # Barrier to ensure all RPCs are done.\n             dist.barrier()\n@@ -449,7 +449,7 @@ def _test_graph_for_py_nested_call_itself(self, exec_mode, sparse):\n                     ),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name((self.rank + 1) % self.world_size),\n@@ -505,7 +505,7 @@ def _test_no_graph_with_tensors_not_require_grad(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.add, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -548,7 +548,7 @@ def _test_rpc_complex_args(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.stack, args=(tensors,)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             self.assertEqual(torch.stack(tensors), ret)\n \n@@ -1292,7 +1292,7 @@ def test_autograd_context(self):\n         for context_id in context_ids:\n             with self.assertRaisesRegex(\n                 RuntimeError,\n-                \"Could not find autograd context with id: {}\".format(context_id),\n+                f\"Could not find autograd context with id: {context_id}\",\n             ):\n                 dist_autograd._retrieve_context(context_id)\n \n@@ -1357,7 +1357,7 @@ def _test_grad_only_on_return_value(self, exec_mode):\n                     worker_name(dst_rank), ret_requires_grad\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             dist_autograd.backward(context_id, [ret.sum()])\n \n@@ -1748,7 +1748,7 @@ def test_backward_without_context(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             res = rpc.rpc_sync(\n                 worker_name(self._next_rank()), torch.add, args=(t1, t2)\n@@ -2031,7 +2031,7 @@ def test_clean_context_during_backward(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             dist_autograd.backward(context_id, [t1.sum()])\n \n@@ -2234,7 +2234,7 @@ def test_multiple_backward_with_errors(self):\n         t2 = torch.rand((3, 3), requires_grad=True)\n         with dist_autograd.context() as context_id:\n             loss = rpc.rpc_sync(\n-                'worker{}'.format(self._next_rank()),\n+                f'worker{self._next_rank()}',\n                 DistAutogradTest._python_udf_with_backward_error,\n                 args=(t1, t2)).sum()\n \ndiff --git a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\nindex 9f8b71911a07cc..98db73d7401845 100644\n--- a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n@@ -225,7 +225,7 @@ def run_agent(agent, n_steps):\n         last_reward = agent.finish_episode()\n \n         if agent.running_reward > agent.reward_threshold:\n-            print(\"Solved! Running reward is now {}!\".format(agent.running_reward))\n+            print(f\"Solved! Running reward is now {agent.running_reward}!\")\n             break\n \n \ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\nindex d050a2138b7922..b7683064dcfd13 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n@@ -70,7 +70,7 @@ def _test_remote_message_dropped_pickle(self, dst=None):\n         if self.rank != 0:\n             return\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -100,7 +100,7 @@ def _test_remote_message_dropped_timeout(self, func, args, dst=None):\n \n         # test the case where rpc.remote() message creation is completely dropped.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -143,7 +143,7 @@ def _test_remote_message_delay_timeout(self, func, args, dst=None):\n         # Test the case where remote message is eventually processed on the owner,\n         # but the future on the creator times out before the response comes back.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, func, args=args, timeout=0.001)\n         # Future corresponding to the remote creation should time out.\n@@ -233,7 +233,7 @@ def test_rref_to_here_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\nindex b08e897ec464af..af73fef4794b06 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n@@ -54,7 +54,7 @@ def get_shutdown_error_regex(self):\n             \"Connection reset by peer\",\n             \"Connection closed by peer\"\n         ]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\nindex fce0b5e8802567..0bb45ddeadb186 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n@@ -310,7 +310,7 @@ def future_return_to_python(\n             dst_rank: int, inputs: Tuple[Tensor, Tensor]\n         ) -> Future[Tensor]:\n             return rpc.rpc_async(\n-                \"worker{}\".format(dst_rank), two_args_two_kwargs, inputs\n+                f\"worker{dst_rank}\", two_args_two_kwargs, inputs\n             )\n \n         fut_res = future_return_to_python(dst_rank, inputs)\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\nindex 96ede7231a9722..2e4eea3a365176 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n@@ -157,7 +157,7 @@ def test_remote_timeout_to_here_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -173,7 +173,7 @@ def test_rref_to_here_timeout_in_jit(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -188,7 +188,7 @@ def test_rref_timeout_pickle_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -205,7 +205,7 @@ def test_rref_timeout_pickle_script_func(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/rpc_test.py b/torch/testing/_internal/distributed/rpc/rpc_test.py\nindex 2d350d06cc6794..47b13a837a0355 100644\n--- a/torch/testing/_internal/distributed/rpc/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/rpc_test.py\n@@ -3153,7 +3153,7 @@ def test_rref_str(self):\n         rref1 = RRef(self.rank)\n         id_class = \"GloballyUniqueId\"\n         self.assertEqual(\n-            \"OwnerRRef({}(created_on={}, local_id=0))\".format(id_class, self.rank), rref1.__str__()\n+            f\"OwnerRRef({id_class}(created_on={self.rank}, local_id=0))\", rref1.__str__()\n         )\n \n         dst_rank = (self.rank + 1) % self.world_size\n@@ -4296,7 +4296,7 @@ def test_rref_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, my_sleep_func, args=(2, ), timeout=0.01)\n         # Future corresponding to the remote creation should time out.\ndiff --git a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\nindex 0f5cb0a4987a8f..191017caad139e 100644\n--- a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n@@ -26,7 +26,7 @@ def get_shutdown_error_regex(self):\n         # FIXME Once we consolidate the error messages returned by the\n         # TensorPipe agent put some more specific regex here.\n         error_regexes = [\".*\"]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/jit_metaprogramming_utils.py b/torch/testing/_internal/jit_metaprogramming_utils.py\nindex 72b7e477c76a49..88137fd1029a18 100644\n--- a/torch/testing/_internal/jit_metaprogramming_utils.py\n+++ b/torch/testing/_internal/jit_metaprogramming_utils.py\n@@ -338,11 +338,11 @@ def get_call(method_name, func_type, args, kwargs):\n     argument_str += kwargs_str\n \n     if func_type == 'functional' or func_type == 'function':\n-        call = 'torch.{}({})'.format(method_name, argument_str)\n+        call = f'torch.{method_name}({argument_str})'\n     elif func_type == 'method':\n-        call = '{}.{}({})'.format(self_arg, method_name, argument_str)\n+        call = f'{self_arg}.{method_name}({argument_str})'\n     elif func_type == 'nn_functional':\n-        call = 'torch.nn.functional.{}({})'.format(method_name, argument_str)\n+        call = f'torch.nn.functional.{method_name}({argument_str})'\n     else:\n         raise TypeError('Unsupported function type')\n \n@@ -361,17 +361,17 @@ def get_script_args(args):\n     actuals: List[str] = []\n     for arg in args:\n         if isinstance(arg, torch.Tensor):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name)\n             actuals.append(name)\n             tensors.append(arg)\n         elif is_iterable_of_tensors(arg):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name + ': List[torch.Tensor]')\n             actuals.append(name)\n             tensors.append(list(arg))\n         elif isinstance(arg, str):\n-            actuals.append(\"'{}'\".format(arg))\n+            actuals.append(f\"'{arg}'\")\n         else:\n             actuals.append(str(get_constant(arg)))\n     return (formals, tensors, actuals)\n@@ -399,7 +399,7 @@ def script_fn(*args, **kwargs):\n         return output\n     return script_fn\n \n-class SplitInputs():\n+class SplitInputs:\n     all_tensors: List[Any]\n     tensor_args: List[Any]\n     nontensor_args: List[Any]\n@@ -584,7 +584,7 @@ def script_module(*args, **kwargs):\n \n         method_args = ', '.join(['self'] + actuals)\n         call_args_str = ', '.join(actuals)\n-        call = \"self.submodule({})\".format(call_args_str)\n+        call = f\"self.submodule({call_args_str})\"\n         script = script_method_template.format(method_args, call)\n \n         submodule_constants = []\n@@ -640,7 +640,7 @@ def get_nn_mod_test_name(**kwargs):\n         test_name = get_nn_module_name_from_kwargs(**kwargs)\n         if 'desc' in kwargs:\n             test_name = \"{}_{}\".format(test_name, kwargs['desc'])\n-    return 'test_nn_{}'.format(test_name)\n+    return f'test_nn_{test_name}'\n \n def get_nn_module_class_from_kwargs(**kwargs):\n     name = get_nn_module_name_from_kwargs(**kwargs)\ndiff --git a/torch/testing/_internal/jit_utils.py b/torch/testing/_internal/jit_utils.py\nindex b72dd5dc1285a6..2f6675234d3e7c 100644\n--- a/torch/testing/_internal/jit_utils.py\n+++ b/torch/testing/_internal/jit_utils.py\n@@ -176,12 +176,12 @@ def get_nodes_and_parents_recursively(block, kind, acc):\n \n         fusion_groups : Dict[torch._C.Block, List[torch._C.Node]] = defaultdict(list)\n         get_nodes_and_parents_recursively(graph, FUSION_GROUP, fusion_groups)\n-        self.assertTrue(len(fusion_groups) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_groups) == 1, f'got {graph}')\n         (graph, fusion_nodes) = list(fusion_groups.items())[0]\n         # the block contains one FUSION_GROUP and the rest of nodes are `allowed_nodes`\n-        self.assertTrue(len(fusion_nodes) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_nodes) == 1, f'got {graph}')\n         self.assertTrue(all(node.kind() in allowed_nodes for node in graph.nodes()),\n-                        'got {}'.format(graph))\n+                        f'got {graph}')\n \n     def _isHookExceptionOk(self, e):\n         se = str(e)\n@@ -294,7 +294,7 @@ def assertGraphContains(self, graph, kind, consider_subgraphs=False):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             self.assertTrue(count > 0)\n             return\n \n@@ -321,7 +321,7 @@ def perform_assert(graph, kind, actual, expected, consider_subgraphs):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             perform_assert(graph, kind, count, num_kind_nodes,\n                            consider_subgraphs)\n             return\n@@ -768,7 +768,7 @@ def _get_py3_code(code, fn_name):\n         fn = getattr(module, fn_name)\n         return fn\n \n-class TensorExprTestOptions():\n+class TensorExprTestOptions:\n     def __init__(self):\n         self.old_profiling_executor = torch._C._jit_set_profiling_executor(True)\n         self.old_profiling_mode = torch._C._get_graph_executor_optimize(True)\ndiff --git a/torch/testing/_internal/opinfo/core.py b/torch/testing/_internal/opinfo/core.py\nindex 8f86cbd06af61a..5e7e1dcd5a49ea 100644\n--- a/torch/testing/_internal/opinfo/core.py\n+++ b/torch/testing/_internal/opinfo/core.py\n@@ -859,9 +859,7 @@ class OpInfo:\n     def __post_init__(self):\n         self._original_opinfo_args = asdict(self).copy()\n \n-        assert self.dtypes is not None, \"OpInfo for {0} has no dtypes!\".format(\n-            self.name\n-        )\n+        assert self.dtypes is not None, \"OpInfo for {} has no dtypes!\".format(self.name)\n \n         dtypes_args = (self.dtypes, self.dtypesIfCUDA, self.dtypesIfROCM)\n \n@@ -874,10 +872,7 @@ def __post_init__(self):\n \n         # Attribute to verify dynamic_dtypes are used.\n         self.dynamic_dtypes = any(\n-            (\n-                isinstance(dtypes, utils._dynamic_dispatch_dtypes)\n-                for dtypes in dtypes_args\n-            )\n+            isinstance(dtypes, utils._dynamic_dispatch_dtypes) for dtypes in dtypes_args\n         )\n \n         if self.dynamic_dtypes:\ndiff --git a/torch/testing/_internal/opinfo/definitions/linalg.py b/torch/testing/_internal/opinfo/definitions/linalg.py\nindex ca84eca5d3d027..a8c29dbf09309d 100644\n--- a/torch/testing/_internal/opinfo/definitions/linalg.py\n+++ b/torch/testing/_internal/opinfo/definitions/linalg.py\n@@ -1007,7 +1007,7 @@ def sample_inputs_linalg_solve(\n         nrhs = [(1,), (3,)]\n \n     for n, batch, rhs in product(ns, batches, nrhs):\n-        yield SampleInput(make_a(*batch, n, n), args=(make_b((batch + (n,) + rhs)),))\n+        yield SampleInput(make_a(*batch, n, n), args=(make_b(batch + (n,) + rhs),))\n \n \n def sample_inputs_linalg_solve_triangular(\ndiff --git a/torch/testing/_internal/opinfo/definitions/sparse.py b/torch/testing/_internal/opinfo/definitions/sparse.py\nindex 6baff3b2f86fea..570b2c546f099a 100644\n--- a/torch/testing/_internal/opinfo/definitions/sparse.py\n+++ b/torch/testing/_internal/opinfo/definitions/sparse.py\n@@ -331,7 +331,7 @@ def _validate_sample_input_sparse_reduction_sum(sample, check_validate=False):\n     }:\n         if (isinstance(dim, int) and (t_inp.dim() != 2 or keepdim)) or (\n             isinstance(dim, (list, tuple))\n-            and (((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim))\n+            and ((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim)\n         ):\n             if layout in {torch.sparse_bsr, torch.sparse_bsc}:\n                 return ErrorInput(\n"
  },
  {
    "number": 105404,
    "title": "[BE] Enable ruff's UP rules and autoformat utils/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "f684f7d0ead70d3e794c86f677f72cff4510c0a4",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105404",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105404/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105404.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105404.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105404/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105404/comments",
    "labels": [
      "release notes: dataloader",
      "open source"
    ],
    "_event_time": "2023-07-18T01:15:36.052150Z",
    "state": "closed",
    "patch": "From 52cd667f73440a1f8cf7db8680fc7c7f6f0dde88 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:29 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat utils/\n\n[ghstack-poisoned]\n---\n torch/utils/_freeze.py                        |  2 +-\n torch/utils/benchmark/examples/end_to_end.py  |  5 ++--\n .../utils/benchmark/examples/op_benchmark.py  |  4 +--\n .../benchmark/examples/sparse/op_benchmark.py |  4 +--\n .../examples/spectral_ops_fuzz_test.py        |  2 +-\n torch/utils/benchmark/utils/common.py         |  4 +--\n torch/utils/benchmark/utils/cpp_jit.py        |  6 ++--\n .../utils/valgrind_wrapper/timer_interface.py | 12 ++++----\n torch/utils/bottleneck/__main__.py            |  6 ++--\n torch/utils/bundled_inputs.py                 |  8 +++---\n torch/utils/checkpoint.py                     |  2 +-\n torch/utils/collect_env.py                    | 26 ++++++++---------\n torch/utils/cpp_extension.py                  |  8 +++---\n torch/utils/data/_utils/pin_memory.py         |  2 +-\n torch/utils/data/_utils/worker.py             |  8 +++---\n torch/utils/data/dataloader.py                |  6 ++--\n torch/utils/data/datapipes/_decorator.py      |  2 +-\n torch/utils/data/datapipes/_typing.py         |  8 +++---\n .../data/datapipes/dataframe/dataframes.py    | 18 ++++++------\n torch/utils/data/datapipes/datapipe.py        | 10 +++----\n torch/utils/data/datapipes/gen_pyi.py         |  2 +-\n torch/utils/data/datapipes/iter/callable.py   |  2 +-\n .../data/datapipes/iter/combinatorics.py      |  4 +--\n torch/utils/data/datapipes/iter/combining.py  |  6 ++--\n torch/utils/data/datapipes/iter/filelister.py |  2 +-\n torch/utils/data/datapipes/iter/fileopener.py |  4 +--\n torch/utils/data/datapipes/iter/grouping.py   |  2 +-\n .../data/datapipes/iter/routeddecoder.py      |  2 +-\n torch/utils/data/datapipes/iter/sharding.py   |  2 +-\n torch/utils/data/datapipes/map/combining.py   |  2 +-\n torch/utils/data/datapipes/map/grouping.py    |  2 +-\n torch/utils/data/datapipes/utils/common.py    |  2 +-\n torch/utils/data/datapipes/utils/decoder.py   |  6 ++--\n torch/utils/data/graph.py                     |  2 +-\n torch/utils/dlpack.py                         |  2 +-\n torch/utils/hipify/cuda_to_hip_mappings.py    |  2 +-\n torch/utils/hipify/hipify_python.py           | 28 +++++++++----------\n torch/utils/jit/log_extract.py                |  2 +-\n torch/utils/mobile_optimizer.py               |  4 +--\n torch/utils/tensorboard/_caffe2_graph.py      |  4 +--\n torch/utils/tensorboard/_embedding.py         |  2 +-\n torch/utils/tensorboard/_pytorch_graph.py     |  6 ++--\n torch/utils/tensorboard/writer.py             |  2 +-\n torch/utils/throughput_benchmark.py           | 10 +++----\n torch/utils/viz/_cycles.py                    | 12 ++++----\n torch/utils/weak.py                           |  5 ++--\n 46 files changed, 131 insertions(+), 131 deletions(-)\n\ndiff --git a/torch/utils/_freeze.py b/torch/utils/_freeze.py\nindex 5245ac011e19ac..6590ff4b769e42 100644\n--- a/torch/utils/_freeze.py\n+++ b/torch/utils/_freeze.py\n@@ -237,7 +237,7 @@ def compile_file(self, path: Path, top_package_path: Path):\n         module_mangled_name = \"__\".join(module_qualname)\n         c_name = \"M_\" + module_mangled_name\n \n-        with open(path, \"r\") as src_file:\n+        with open(path) as src_file:\n             co = self.compile_string(src_file.read())\n \n         bytecode = marshal.dumps(co)\ndiff --git a/torch/utils/benchmark/examples/end_to_end.py b/torch/utils/benchmark/examples/end_to_end.py\nindex 5e0f42712d7c7a..a6d05a91c94253 100644\n--- a/torch/utils/benchmark/examples/end_to_end.py\n+++ b/torch/utils/benchmark/examples/end_to_end.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"End-to-end example to test a PR for regressions:\n \n $ python -m examples.end_to_end --pr 39850\n@@ -111,7 +110,7 @@ def parse_args():\n \n def construct_stmt_and_label(pr, params):\n     if pr == \"39850\":\n-        k0, k1, k2, dim = [params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"]]\n+        k0, k1, k2, dim = (params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"])\n         state = np.random.RandomState(params[\"random_value\"])\n         topk_dim = state.randint(low=0, high=dim)\n         dim_size = [k0, k1, k2][topk_dim]\n@@ -291,7 +290,7 @@ def construct_table(results, device_str, test_variance):\n     )\n \n     _, result_log_file = tempfile.mkstemp(suffix=\".log\")\n-    with open(result_log_file, \"wt\") as f:\n+    with open(result_log_file, \"w\") as f:\n         f.write(f\"{device_str}\\n\\n{column_labels}\\n\")\n         print(f\"\\n{column_labels}\\n[First twenty omitted (these tend to be noisy) ]\")\n         for key, (r_ref, r_pr), rel_diff in results:\ndiff --git a/torch/utils/benchmark/examples/op_benchmark.py b/torch/utils/benchmark/examples/op_benchmark.py\nindex 65b69d84b41f44..b7536b9ec26bb8 100644\n--- a/torch/utils/benchmark/examples/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/op_benchmark.py\n@@ -37,13 +37,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/sparse/op_benchmark.py b/torch/utils/benchmark/examples/sparse/op_benchmark.py\nindex f9ee17d5617e08..d7e97d33cc1101 100644\n--- a/torch/utils/benchmark/examples/sparse/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/sparse/op_benchmark.py\n@@ -32,13 +32,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\nindex d8284ee4187c49..c70395573adb2c 100644\n--- a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n+++ b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n@@ -27,7 +27,7 @@ def run_benchmark(name: str, function: object, dtype: torch.dtype, seed: int, de\n     results = []\n     for tensors, tensor_params, params in spectral_fuzzer.take(samples):\n         shape = [params['k0'], params['k1'], params['k2']][:params['ndim']]\n-        str_shape = ' x '.join([\"{:<4}\".format(s) for s in shape])\n+        str_shape = ' x '.join([f\"{s:<4}\" for s in shape])\n         sub_label = f\"{str_shape} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n         for dim in _dim_options(params['ndim']):\n             for nthreads in (1, 4, 16) if not cuda else (1,):\ndiff --git a/torch/utils/benchmark/utils/common.py b/torch/utils/benchmark/utils/common.py\nindex a8bbef3bfbeb4f..c1636ddb78a2bf 100644\n--- a/torch/utils/benchmark/utils/common.py\n+++ b/torch/utils/benchmark/utils/common.py\n@@ -325,7 +325,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n                 if not os.path.exists(owner_file):\n                     continue\n \n-                with open(owner_file, \"rt\") as f:\n+                with open(owner_file) as f:\n                     owner_pid = int(f.read())\n \n                 if owner_pid == os.getpid():\n@@ -349,7 +349,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n     os.makedirs(path, exist_ok=False)\n \n     if use_dev_shm:\n-        with open(os.path.join(path, \"owner.pid\"), \"wt\") as f:\n+        with open(os.path.join(path, \"owner.pid\"), \"w\") as f:\n             f.write(str(os.getpid()))\n \n     return path\ndiff --git a/torch/utils/benchmark/utils/cpp_jit.py b/torch/utils/benchmark/utils/cpp_jit.py\nindex 65b8c70ee43e6c..a09f1a00aace6f 100644\n--- a/torch/utils/benchmark/utils/cpp_jit.py\n+++ b/torch/utils/benchmark/utils/cpp_jit.py\n@@ -137,7 +137,7 @@ def _compile_template(\n         os.makedirs(build_dir, exist_ok=True)\n \n         src_path = os.path.join(build_dir, \"timer_src.cpp\")\n-        with open(src_path, \"wt\") as f:\n+        with open(src_path, \"w\") as f:\n             f.write(src)\n \n     # `cpp_extension` has its own locking scheme, so we don't need our lock.\n@@ -154,7 +154,7 @@ def _compile_template(\n \n def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> TimeitModuleType:\n     template_path: str = os.path.join(SOURCE_ROOT, \"timeit_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     module = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)\n@@ -164,7 +164,7 @@ def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> Time\n \n def compile_callgrind_template(*, stmt: str, setup: str, global_setup: str) -> str:\n     template_path: str = os.path.join(SOURCE_ROOT, \"valgrind_wrapper\", \"timer_callgrind_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     target = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 71753bd59548ae..11ce6d90fc47f3 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -28,7 +28,9 @@\n     CompletedProcessType = subprocess.CompletedProcess\n \n \n-FunctionCount = NamedTuple(\"FunctionCount\", [(\"count\", int), (\"function\", str)])\n+class FunctionCount(NamedTuple):\n+    count: int\n+    function: str\n \n \n @dataclasses.dataclass(repr=False, eq=False, frozen=True)\n@@ -598,7 +600,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     stderr=subprocess.STDOUT,\n                     **kwargs,\n                 )\n-                with open(stdout_stderr_log, \"rt\") as f:\n+                with open(stdout_stderr_log) as f:\n                     return invocation, f.read()\n             finally:\n                 f_stdout_stderr.close()\n@@ -612,7 +614,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     )\n \n                 script_file = os.path.join(working_dir, \"timer_callgrind.py\")\n-                with open(script_file, \"wt\") as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(self._construct_script(\n                         task_spec,\n                         globals=GlobalsBridge(globals, data_dir),\n@@ -652,7 +654,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n             if valgrind_invocation.returncode:\n                 error_report = \"\"\n                 if os.path.exists(error_log):\n-                    with open(error_log, \"rt\") as f:\n+                    with open(error_log) as f:\n                         error_report = f.read()\n                 if not error_report:\n                     error_report = \"Unknown error.\\n\" + valgrind_invocation_output\n@@ -724,7 +726,7 @@ def read_results(i: int) -> Tuple[FunctionCounts, FunctionCounts, Optional[str]]\n                 fpath = f\"{callgrind_out}.{i + 1}\"  # Callgrind one-indexes files.\n                 callgrind_out_contents: Optional[str] = None\n                 if retain_out_file:\n-                    with open(fpath, \"rt\") as f:\n+                    with open(fpath) as f:\n                         callgrind_out_contents = f.read()\n \n                 return (\ndiff --git a/torch/utils/bottleneck/__main__.py b/torch/utils/bottleneck/__main__.py\nindex 86c1af04baa0e6..f7fd209e1438fa 100644\n--- a/torch/utils/bottleneck/__main__.py\n+++ b/torch/utils/bottleneck/__main__.py\n@@ -16,7 +16,7 @@ def redirect_argv(new_argv):\n \n def compiled_with_cuda(sysinfo):\n     if sysinfo.cuda_compiled_version:\n-        return 'compiled w/ CUDA {}'.format(sysinfo.cuda_compiled_version)\n+        return f'compiled w/ CUDA {sysinfo.cuda_compiled_version}'\n     return 'not compiled w/ CUDA'\n \n \n@@ -59,7 +59,7 @@ def run_env_analysis():\n         'debug_str': debug_str,\n         'pytorch_version': info.torch_version,\n         'cuda_compiled': compiled_with_cuda(info),\n-        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'py_version': f'{sys.version_info[0]}.{sys.version_info[1]}',\n         'cuda_runtime': cuda_avail,\n         'pip_version': pip_version,\n         'pip_list_output': pip_list_output,\n@@ -138,7 +138,7 @@ def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\n \n     result = {\n         'mode': mode,\n-        'description': 'top {} events sorted by {}'.format(topk, sortby),\n+        'description': f'top {topk} events sorted by {sortby}',\n         'output': torch.autograd.profiler_util._build_table(topk_events),\n         'cuda_warning': cuda_warning\n     }\ndiff --git a/torch/utils/bundled_inputs.py b/torch/utils/bundled_inputs.py\nindex 4ae39733ff2e4b..ad34e15e6bfa17 100644\n--- a/torch/utils/bundled_inputs.py\n+++ b/torch/utils/bundled_inputs.py\n@@ -261,11 +261,11 @@ def augment_many_model_functions_with_bundled_inputs(\n \n \n         if input_list is not None and not isinstance(input_list, Sequence):\n-            raise TypeError(\"Error inputs for function {0} is not a Sequence\".format(function_name))\n+            raise TypeError(f\"Error inputs for function {function_name} is not a Sequence\")\n \n         function_arg_types = [arg.type for arg in function.schema.arguments[1:]]  # type: ignore[attr-defined]\n         deflated_inputs_type: ListType = ListType(TupleType(function_arg_types))\n-        model._c._register_attribute(\"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs_type, [])\n+        model._c._register_attribute(f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs_type, [])\n \n         if hasattr(model, \"_generate_bundled_inputs_for_\" + function_name):\n             if input_list is not None:\n@@ -290,7 +290,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             for inp_idx, args in enumerate(input_list):\n                 if not isinstance(args, Tuple) and not isinstance(args, List):  # type: ignore[arg-type]\n                     raise TypeError(\n-                        \"Error bundled input for function {0} idx: {1} is not a Tuple or a List\".format(function_name, inp_idx)\n+                        f\"Error bundled input for function {function_name} idx: {inp_idx} is not a Tuple or a List\"\n                     )\n                 deflated_args = []\n                 parts.append(\"(\")\n@@ -314,7 +314,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             # Back-channel return this expr for debugging.\n             if _receive_inflate_expr is not None:\n                 _receive_inflate_expr.append(expr)\n-            setattr(model, \"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs)\n+            setattr(model, f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs)\n             definition = textwrap.dedent(\"\"\"\n                 def _generate_bundled_inputs_for_{name}(self):\n                     deflated = self._bundled_inputs_deflated_{name}\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..4da281d32ac3bd 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -66,7 +66,7 @@ def _get_device_module(device=\"cuda\"):\n     return device_module\n \n \n-class DefaultDeviceType(object):\n+class DefaultDeviceType:\n     r\"\"\"\n     A class that manages the default device type for checkpointing.\n     If no non-CPU tensors are present, the default device type will\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex de03564a2b75d1..2266d64c1944d5 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n+    out = run_and_read_all(run_lambda, f\"{conda} list\")\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n+        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return 'Probably one of the following:\\n{}'.format(result)\n+    return f'Probably one of the following:\\n{result}'\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = '\"{}\"'.format(candidate_smi)\n+                smi = f'\"{candidate_smi}\"'\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n+    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return 'macOS {} ({})'.format(version, machine())\n+        return f'macOS {version} ({machine()})'\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n-        return '{} ({})'.format(platform, machine())\n+        return f'{platform} ({machine()})'\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n+        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return '\\n{}\\n'.format(string)\n+            return f'\\n{string}\\n'\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               '[{}] '.format(envinfo.pip_version))\n+                                               f'[{envinfo.pip_version}] ')\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n+            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..e847f4e30915b9 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -150,10 +150,10 @@ def _join_rocm_home(*paths) -> str:\n     only once we need to get any ROCm-specific path.\n     '''\n     if ROCM_HOME is None:\n-        raise EnvironmentError('ROCM_HOME environment variable is not set. '\n+        raise OSError('ROCM_HOME environment variable is not set. '\n                                'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n-        raise EnvironmentError('Building PyTorch extensions using '\n+        raise OSError('Building PyTorch extensions using '\n                                'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n@@ -264,7 +264,7 @@ def _maybe_write(filename, new_content):\n     if it already had the right content (to avoid triggering recompile).\n     '''\n     if os.path.exists(filename):\n-        with open(filename, 'r') as f:\n+        with open(filename) as f:\n             content = f.read()\n \n         if content == new_content:\n@@ -2247,7 +2247,7 @@ def _join_cuda_home(*paths) -> str:\n     only once we need to get any CUDA-specific path.\n     '''\n     if CUDA_HOME is None:\n-        raise EnvironmentError('CUDA_HOME environment variable is not set. '\n+        raise OSError('CUDA_HOME environment variable is not set. '\n                                'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \ndiff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py\nindex 074b89b624b9d3..cdd53c2d9ea2b1 100644\n--- a/torch/utils/data/_utils/pin_memory.py\n+++ b/torch/utils/data/_utils/pin_memory.py\n@@ -37,7 +37,7 @@ def do_one_step():\n                 data = pin_memory(data, device)\n             except Exception:\n                 data = ExceptionWrapper(\n-                    where=\"in pin memory thread for device {}\".format(device_id))\n+                    where=f\"in pin memory thread for device {device_id}\")\n             r = (idx, data)\n         while not done_event.is_set():\n             try:\ndiff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py\nindex b4fc8e0748f0f1..0d43f63a6a2f20 100644\n--- a/torch/utils/data/_utils/worker.py\n+++ b/torch/utils/data/_utils/worker.py\n@@ -76,13 +76,13 @@ def __init__(self, **kwargs):\n \n     def __setattr__(self, key, val):\n         if self.__initialized:\n-            raise RuntimeError(\"Cannot assign attributes to {} objects\".format(self.__class__.__name__))\n+            raise RuntimeError(f\"Cannot assign attributes to {self.__class__.__name__} objects\")\n         return super().__setattr__(key, val)\n \n     def __repr__(self):\n         items = []\n         for k in self.__keys:\n-            items.append('{}={}'.format(k, getattr(self, k)))\n+            items.append(f'{k}={getattr(self, k)}')\n         return '{}({})'.format(self.__class__.__name__, ', '.join(items))\n \n \n@@ -252,7 +252,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n             fetcher = _DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)\n         except Exception:\n             init_exception = ExceptionWrapper(\n-                where=\"in DataLoader worker process {}\".format(worker_id))\n+                where=f\"in DataLoader worker process {worker_id}\")\n \n         # When using Iterable mode, some worker can exit earlier than others due\n         # to the IterableDataset behaving differently for different workers.\n@@ -318,7 +318,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n                         # `ExceptionWrapper` does the correct thing.\n                         # See NOTE [ Python Traceback Reference Cycle Problem ]\n                         data = ExceptionWrapper(\n-                            where=\"in DataLoader worker process {}\".format(worker_id))\n+                            where=f\"in DataLoader worker process {worker_id}\")\n             data_queue.put((idx, data))\n             del data, idx, index, r  # save memory\n     except KeyboardInterrupt:\ndiff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py\nindex ec86f778023ba6..1c33592f02f146 100644\n--- a/torch/utils/data/dataloader.py\n+++ b/torch/utils/data/dataloader.py\n@@ -604,7 +604,7 @@ def __init__(self, loader: DataLoader) -> None:\n         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n         self._persistent_workers = loader.persistent_workers\n         self._num_yielded = 0\n-        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n+        self._profile_name = f\"enumerate(DataLoader)#{self.__class__.__name__}.__next__\"\n \n     def __iter__(self) -> '_BaseDataLoaderIter':\n         return self\n@@ -1145,7 +1145,7 @@ def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n                     self._mark_worker_as_unavailable(worker_id)\n             if len(failed_workers) > 0:\n                 pids_str = ', '.join(str(w.pid) for w in failed_workers)\n-                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n+                raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n             if isinstance(e, queue.Empty):\n                 return (False, None)\n             import tempfile\n@@ -1281,7 +1281,7 @@ def _get_data(self):\n             if success:\n                 return data\n             else:\n-                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n+                raise RuntimeError(f'DataLoader timed out after {self._timeout} seconds')\n         elif self._pin_memory:\n             while self._pin_memory_thread.is_alive():\n                 success, data = self._try_get_data()\ndiff --git a/torch/utils/data/datapipes/_decorator.py b/torch/utils/data/datapipes/_decorator.py\nindex e4cc9e4e59365d..96b7e00e076f02 100644\n--- a/torch/utils/data/datapipes/_decorator.py\n+++ b/torch/utils/data/datapipes/_decorator.py\n@@ -80,7 +80,7 @@ def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:\n         elif isinstance(arg, Callable):  # type:ignore[arg-type]\n             self.deterministic_fn = arg  # type: ignore[assignment, misc]\n         else:\n-            raise TypeError(\"{} can not be decorated by non_deterministic\".format(arg))\n+            raise TypeError(f\"{arg} can not be decorated by non_deterministic\")\n \n     def __call__(self, *args, **kwargs):\n         global _determinism\ndiff --git a/torch/utils/data/datapipes/_typing.py b/torch/utils/data/datapipes/_typing.py\nindex 6377a2ec940860..68049ba30d9018 100644\n--- a/torch/utils/data/datapipes/_typing.py\n+++ b/torch/utils/data/datapipes/_typing.py\n@@ -234,7 +234,7 @@ def issubtype(self, other):\n             return issubtype(self.param, other.param)\n         if isinstance(other, type):\n             return issubtype(self.param, other)\n-        raise TypeError(\"Expected '_DataPipeType' or 'type', but found {}\".format(type(other)))\n+        raise TypeError(f\"Expected '_DataPipeType' or 'type', but found {type(other)}\")\n \n     def issubtype_of_instance(self, other):\n         return issubinstance(other, self.param)\n@@ -279,13 +279,13 @@ def __init__(self, name, bases, namespace, **kwargs):\n     @_tp_cache\n     def _getitem_(self, params):\n         if params is None:\n-            raise TypeError('{}[t]: t can not be None'.format(self.__name__))\n+            raise TypeError(f'{self.__name__}[t]: t can not be None')\n         if isinstance(params, str):\n             params = ForwardRef(params)\n         if not isinstance(params, tuple):\n             params = (params, )\n \n-        msg = \"{}[t]: t must be a type\".format(self.__name__)\n+        msg = f\"{self.__name__}[t]: t must be a type\"\n         params = tuple(_type_check(p, msg) for p in params)\n \n         if isinstance(self.type.param, _GenericAlias):\n@@ -303,7 +303,7 @@ def _getitem_(self, params):\n                                        '__type_class__': True})\n \n         if len(params) > 1:\n-            raise TypeError('Too many parameters for {} actual {}, expected 1'.format(self, len(params)))\n+            raise TypeError(f'Too many parameters for {self} actual {len(params)}, expected 1')\n \n         t = _DataPipeType(params[0])\n \ndiff --git a/torch/utils/data/datapipes/dataframe/dataframes.py b/torch/utils/data/datapipes/dataframe/dataframes.py\nindex 06029e07851685..72d93cde66c3cb 100644\n--- a/torch/utils/data/datapipes/dataframe/dataframes.py\n+++ b/torch/utils/data/datapipes/dataframe/dataframes.py\n@@ -36,7 +36,7 @@ def disable_capture():\n     CaptureControl.disabled = True\n \n \n-class CaptureControl():\n+class CaptureControl:\n     disabled = False\n \n \n@@ -184,7 +184,7 @@ def execute(self):\n         return value\n \n \n-class CaptureLikeMock():\n+class CaptureLikeMock:\n     def __init__(self, name):\n         import unittest.mock as mock\n         # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n@@ -232,7 +232,7 @@ class CaptureVariableAssign(CaptureF):\n     def __str__(self):\n         variable = self.kwargs['variable']\n         value = self.kwargs['value']\n-        return \"{variable} = {value}\".format(variable=variable, value=value)\n+        return f\"{variable} = {value}\"\n \n     def execute(self):\n         self.kwargs['variable'].calculated_value = self.kwargs['value'].execute()\n@@ -272,7 +272,7 @@ def __init__(self, left, key, ctx):\n         self.key = key\n \n     def __str__(self):\n-        return \"%s[%s]\" % (self.left, get_val(self.key))\n+        return f\"{self.left}[{get_val(self.key)}]\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -287,7 +287,7 @@ def __init__(self, left, key, value, ctx):\n         self.value = value\n \n     def __str__(self):\n-        return \"%s[%s] = %s\" % (self.left, get_val(self.key), self.value)\n+        return f\"{self.left}[{get_val(self.key)}] = {self.value}\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -302,7 +302,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s + %s\" % (self.left, self.right)\n+        return f\"{self.left} + {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) + get_val(self.right)\n@@ -315,7 +315,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s * %s\" % (self.left, self.right)\n+        return f\"{self.left} * {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) * get_val(self.right)\n@@ -328,7 +328,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s - %s\" % (self.left, self.right)\n+        return f\"{self.left} - {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) - get_val(self.right)\n@@ -341,7 +341,7 @@ def __init__(self, src, name, ctx):\n         self.name = name\n \n     def __str__(self):\n-        return \"%s.%s\" % (self.src, self.name)\n+        return f\"{self.src}.{self.name}\"\n \n     def execute(self):\n         val = get_val(self.src)\ndiff --git a/torch/utils/data/datapipes/datapipe.py b/torch/utils/data/datapipes/datapipe.py\nindex 445400ecb59c32..1017b52af0fbce 100644\n--- a/torch/utils/data/datapipes/datapipe.py\n+++ b/torch/utils/data/datapipes/datapipe.py\n@@ -126,7 +126,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -135,7 +135,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register, enable_df_api_tracing=False):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, enable_df_api_tracing, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -265,7 +265,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -274,7 +274,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -363,7 +363,7 @@ def __len__(self):\n             return len(self._datapipe)\n         except Exception as e:\n             raise TypeError(\n-                \"{} instance doesn't have valid length\".format(type(self).__name__)\n+                f\"{type(self).__name__} instance doesn't have valid length\"\n             ) from e\n \n \ndiff --git a/torch/utils/data/datapipes/gen_pyi.py b/torch/utils/data/datapipes/gen_pyi.py\nindex 1b77fbfecf0290..ed3e75bc5da12e 100644\n--- a/torch/utils/data/datapipes/gen_pyi.py\n+++ b/torch/utils/data/datapipes/gen_pyi.py\n@@ -19,7 +19,7 @@ def gen_from_template(dir: str, template_name: str, output_name: str, replacemen\n     template_path = os.path.join(dir, template_name)\n     output_path = os.path.join(dir, output_name)\n \n-    with open(template_path, \"r\") as f:\n+    with open(template_path) as f:\n         content = f.read()\n     for placeholder, lines, indentation in replacements:\n         with open(output_path, \"w\") as f:\ndiff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py\nindex 4e3dce4b82d1dd..9916b094e408d1 100644\n--- a/torch/utils/data/datapipes/iter/callable.py\n+++ b/torch/utils/data/datapipes/iter/callable.py\n@@ -126,7 +126,7 @@ def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n         raise TypeError(\n-            \"{} instance doesn't have valid length\".format(type(self).__name__)\n+            f\"{type(self).__name__} instance doesn't have valid length\"\n         )\n \n \ndiff --git a/torch/utils/data/datapipes/iter/combinatorics.py b/torch/utils/data/datapipes/iter/combinatorics.py\nindex 30b569e329b654..4d2973bbc5a2e9 100644\n--- a/torch/utils/data/datapipes/iter/combinatorics.py\n+++ b/torch/utils/data/datapipes/iter/combinatorics.py\n@@ -48,7 +48,7 @@ def __len__(self) -> int:\n         # Dataset has been tested as `Sized`\n         if isinstance(self.sampler, Sized):\n             return len(self.sampler)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('shuffle')\n@@ -137,7 +137,7 @@ def __iter__(self) -> Iterator[T_co]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self._buffer = []\ndiff --git a/torch/utils/data/datapipes/iter/combining.py b/torch/utils/data/datapipes/iter/combining.py\nindex 7c76e986b230d4..4fe05ea717cf16 100644\n--- a/torch/utils/data/datapipes/iter/combining.py\n+++ b/torch/utils/data/datapipes/iter/combining.py\n@@ -56,7 +56,7 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return sum(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('fork')\n@@ -567,7 +567,7 @@ def __len__(self):\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes) * len(self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self.buffer = []\n@@ -627,4 +627,4 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/filelister.py b/torch/utils/data/datapipes/iter/filelister.py\nindex b2ecd71b5ce9c9..22e2cd432d6a3a 100644\n--- a/torch/utils/data/datapipes/iter/filelister.py\n+++ b/torch/utils/data/datapipes/iter/filelister.py\n@@ -61,5 +61,5 @@ def __iter__(self) -> Iterator[str] :\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/fileopener.py b/torch/utils/data/datapipes/iter/fileopener.py\nindex 03d5761a9f164c..50737d9b587b25 100644\n--- a/torch/utils/data/datapipes/iter/fileopener.py\n+++ b/torch/utils/data/datapipes/iter/fileopener.py\n@@ -51,7 +51,7 @@ def __init__(\n         self.encoding: Optional[str] = encoding\n \n         if self.mode not in ('b', 't', 'rb', 'rt', 'r'):\n-            raise ValueError(\"Invalid mode {}\".format(mode))\n+            raise ValueError(f\"Invalid mode {mode}\")\n         # TODO: enforce typing for each instance based on mode, otherwise\n         #       `argument_validation` with this DataPipe may be potentially broken\n \n@@ -68,5 +68,5 @@ def __iter__(self):\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/grouping.py b/torch/utils/data/datapipes/iter/grouping.py\nindex c83bd2748b78fb..b26847d7319740 100644\n--- a/torch/utils/data/datapipes/iter/grouping.py\n+++ b/torch/utils/data/datapipes/iter/grouping.py\n@@ -83,7 +83,7 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('unbatch')\ndiff --git a/torch/utils/data/datapipes/iter/routeddecoder.py b/torch/utils/data/datapipes/iter/routeddecoder.py\nindex 8bfbe1442180ab..5e68ae133e05ab 100644\n--- a/torch/utils/data/datapipes/iter/routeddecoder.py\n+++ b/torch/utils/data/datapipes/iter/routeddecoder.py\n@@ -62,4 +62,4 @@ def __iter__(self) -> Iterator[Tuple[str, Any]]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/sharding.py b/torch/utils/data/datapipes/iter/sharding.py\nindex 730caeaf7d4da3..1f4a3a291bd11f 100644\n--- a/torch/utils/data/datapipes/iter/sharding.py\n+++ b/torch/utils/data/datapipes/iter/sharding.py\n@@ -80,4 +80,4 @@ def __len__(self):\n         if isinstance(self.source_datapipe, Sized):\n             return len(self.source_datapipe) // self.num_of_instances +\\\n                 (1 if (self.instance_id < len(self.source_datapipe) % self.num_of_instances) else 0)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/map/combining.py b/torch/utils/data/datapipes/map/combining.py\nindex 85146f8345cbdc..4a4a785eff78e5 100644\n--- a/torch/utils/data/datapipes/map/combining.py\n+++ b/torch/utils/data/datapipes/map/combining.py\n@@ -47,7 +47,7 @@ def __getitem__(self, index) -> T_co:  # type: ignore[type-var]\n                 return dp[index - offset]\n             else:\n                 offset += len(dp)\n-        raise IndexError(\"Index {} is out of range.\".format(index))\n+        raise IndexError(f\"Index {index} is out of range.\")\n \n     def __len__(self) -> int:\n         return sum(len(dp) for dp in self.datapipes)\ndiff --git a/torch/utils/data/datapipes/map/grouping.py b/torch/utils/data/datapipes/map/grouping.py\nindex da3cf5688a1bb0..65b30d8eba1f40 100644\n--- a/torch/utils/data/datapipes/map/grouping.py\n+++ b/torch/utils/data/datapipes/map/grouping.py\n@@ -64,4 +64,4 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/utils/common.py b/torch/utils/data/datapipes/utils/common.py\nindex e39d67ee6c81b9..99ae0cb4cbd024 100644\n--- a/torch/utils/data/datapipes/utils/common.py\n+++ b/torch/utils/data/datapipes/utils/common.py\n@@ -305,7 +305,7 @@ def __init__(self, file_obj, parent_stream=None, name=None):\n         self.closed = False\n         if parent_stream is not None:\n             if not isinstance(parent_stream, StreamWrapper):\n-                raise RuntimeError('Parent stream should be StreamWrapper, {} was given'.format(type(parent_stream)))\n+                raise RuntimeError(f'Parent stream should be StreamWrapper, {type(parent_stream)} was given')\n             parent_stream.child_counter += 1\n             self.parent_stream = parent_stream\n         if StreamWrapper.debug_unclosed_streams:\ndiff --git a/torch/utils/data/datapipes/utils/decoder.py b/torch/utils/data/datapipes/utils/decoder.py\nindex 4da810c3276684..8a7cb71b619de4 100644\n--- a/torch/utils/data/datapipes/utils/decoder.py\n+++ b/torch/utils/data/datapipes/utils/decoder.py\n@@ -137,7 +137,7 @@ class ImageHandler:\n     - pilrgba: pil None rgba\n     \"\"\"\n     def __init__(self, imagespec):\n-        assert imagespec in list(imagespecs.keys()), \"unknown image specification: {}\".format(imagespec)\n+        assert imagespec in list(imagespecs.keys()), f\"unknown image specification: {imagespec}\"\n         self.imagespec = imagespec.lower()\n \n     def __call__(self, extension, data):\n@@ -167,14 +167,14 @@ def __call__(self, extension, data):\n                 return img\n             elif atype == \"numpy\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n                 if etype == \"uint8\":\n                     return result\n                 else:\n                     return result.astype(\"f\") / 255.0\n             elif atype == \"torch\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n \n                 if etype == \"uint8\":\n                     result = np.array(result.transpose(2, 0, 1))\ndiff --git a/torch/utils/data/graph.py b/torch/utils/data/graph.py\nindex 2769e326c03e3b..7fc95d58fa2198 100644\n--- a/torch/utils/data/graph.py\n+++ b/torch/utils/data/graph.py\n@@ -130,7 +130,7 @@ def traverse(datapipe: DataPipe, only_datapipe: Optional[bool] = None) -> DataPi\n # Add cache here to prevent infinite recursion on DataPipe\n def _traverse_helper(datapipe: DataPipe, only_datapipe: bool, cache: Set[int]) -> DataPipeGraph:\n     if not isinstance(datapipe, (IterDataPipe, MapDataPipe)):\n-        raise RuntimeError(\"Expected `IterDataPipe` or `MapDataPipe`, but {} is found\".format(type(datapipe)))\n+        raise RuntimeError(f\"Expected `IterDataPipe` or `MapDataPipe`, but {type(datapipe)} is found\")\n \n     dp_id = id(datapipe)\n     if dp_id in cache:\ndiff --git a/torch/utils/dlpack.py b/torch/utils/dlpack.py\nindex f903de94eb67b2..a987bca6dcd51b 100644\n--- a/torch/utils/dlpack.py\n+++ b/torch/utils/dlpack.py\n@@ -102,7 +102,7 @@ def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n         # device is either CUDA or ROCm, we need to pass the current\n         # stream\n         if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n-            stream = torch.cuda.current_stream('cuda:{}'.format(device[1]))\n+            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n             # cuda_stream is the pointer to the stream and it is a public\n             # attribute, but it is not documented\n             # The array API specify that the default legacy stream must be passed\ndiff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py\nindex 3b583dbf790109..163f3649c41279 100644\n--- a/torch/utils/hipify/cuda_to_hip_mappings.py\n+++ b/torch/utils/hipify/cuda_to_hip_mappings.py\n@@ -46,7 +46,7 @@\n     RE_MINOR = re.compile(r\"#define\\s+ROCM_VERSION_MINOR\\s+(\\d+)\")\n     RE_PATCH = re.compile(r\"#define\\s+ROCM_VERSION_PATCH\\s+(\\d+)\")\n     major, minor, patch = 0, 0, 0\n-    for line in open(rocm_version_h, \"r\"):\n+    for line in open(rocm_version_h):\n         match = RE_MAJOR.search(line)\n         if match:\n             major = int(match.group(1))\ndiff --git a/torch/utils/hipify/hipify_python.py b/torch/utils/hipify/hipify_python.py\nindex 34a066750e1cdc..fa800659595bd7 100755\n--- a/torch/utils/hipify/hipify_python.py\n+++ b/torch/utils/hipify/hipify_python.py\n@@ -219,13 +219,13 @@ def compute_stats(stats):\n     unsupported_calls = {cuda_call for (cuda_call, _filepath) in stats[\"unsupported_calls\"]}\n \n     # Print the number of unsupported calls\n-    print(\"Total number of unsupported CUDA function calls: {0:d}\".format(len(unsupported_calls)))\n+    print(f\"Total number of unsupported CUDA function calls: {len(unsupported_calls):d}\")\n \n     # Print the list of unsupported calls\n     print(\", \".join(unsupported_calls))\n \n     # Print the number of kernel launches\n-    print(\"\\nTotal number of replaced kernel launches: {0:d}\".format(len(stats[\"kernel_launches\"])))\n+    print(\"\\nTotal number of replaced kernel launches: {:d}\".format(len(stats[\"kernel_launches\"])))\n \n \n def add_dim3(kernel_string, cuda_kernel):\n@@ -254,8 +254,8 @@ def add_dim3(kernel_string, cuda_kernel):\n     first_arg_clean = kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace(\"\\n\", \"\").strip(\" \")\n     second_arg_clean = kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace(\"\\n\", \"\").strip(\" \")\n \n-    first_arg_dim3 = \"dim3({})\".format(first_arg_clean)\n-    second_arg_dim3 = \"dim3({})\".format(second_arg_clean)\n+    first_arg_dim3 = f\"dim3({first_arg_clean})\"\n+    second_arg_dim3 = f\"dim3({second_arg_clean})\"\n \n     first_arg_raw_dim3 = first_arg_raw.replace(first_arg_clean, first_arg_dim3)\n     second_arg_raw_dim3 = second_arg_raw.replace(second_arg_clean, second_arg_dim3)\n@@ -269,7 +269,7 @@ def add_dim3(kernel_string, cuda_kernel):\n def processKernelLaunches(string, stats):\n     \"\"\" Replace the CUDA style Kernel launches with the HIP style kernel launches.\"\"\"\n     # Concat the namespace with the kernel names. (Find cleaner way of doing this later).\n-    string = RE_KERNEL_LAUNCH.sub(lambda inp: \"{0}{1}::\".format(inp.group(1), inp.group(2)), string)\n+    string = RE_KERNEL_LAUNCH.sub(lambda inp: f\"{inp.group(1)}{inp.group(2)}::\", string)\n \n     def grab_method_and_template(in_kernel):\n         # The positions for relevant kernel components.\n@@ -482,7 +482,7 @@ def replace_math_functions(input_string):\n     \"\"\"\n     output_string = input_string\n     for func in MATH_TRANSPILATIONS:\n-        output_string = output_string.replace(r'{}('.format(func), '{}('.format(MATH_TRANSPILATIONS[func]))\n+        output_string = output_string.replace(fr'{func}(', f'{MATH_TRANSPILATIONS[func]}(')\n \n     return output_string\n \n@@ -531,7 +531,7 @@ def replace_extern_shared(input_string):\n     \"\"\"\n     output_string = input_string\n     output_string = RE_EXTERN_SHARED.sub(\n-        lambda inp: \"HIP_DYNAMIC_SHARED({0} {1}, {2})\".format(\n+        lambda inp: \"HIP_DYNAMIC_SHARED({} {}, {})\".format(\n             inp.group(1) or \"\", inp.group(2), inp.group(3)), output_string)\n \n     return output_string\n@@ -657,7 +657,7 @@ def is_caffe2_gpu_file(rel_filepath):\n \n \n # Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n-class Trie():\n+class Trie:\n     \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n     The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n \n@@ -750,7 +750,7 @@ def pattern(self):\n             CAFFE2_TRIE.add(src)\n             CAFFE2_MAP[src] = dst\n RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.pattern())\n-RE_PYTORCH_PREPROCESSOR = re.compile(r'(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))\n+RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\\W)({PYTORCH_TRIE.pattern()})(?=\\W)')\n \n RE_QUOTE_HEADER = re.compile(r'#include \"([^\"]+)\"')\n RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')\n@@ -789,7 +789,7 @@ def preprocessor(\n \n     rel_filepath = os.path.relpath(filepath, output_directory)\n \n-    with open(fin_path, 'r', encoding='utf-8') as fin:\n+    with open(fin_path, encoding='utf-8') as fin:\n         if fin.readline() == HIPIFY_C_BREADCRUMB:\n             hipify_result.hipified_path = None\n             hipify_result.status = \"[ignored, input is hipified output]\"\n@@ -929,7 +929,7 @@ def repl(m):\n \n     do_write = True\n     if os.path.exists(fout_path):\n-        with open(fout_path, 'r', encoding='utf-8') as fout_old:\n+        with open(fout_path, encoding='utf-8') as fout_old:\n             do_write = fout_old.read() != output_source\n     if do_write:\n         try:\n@@ -956,7 +956,7 @@ def file_specific_replacement(filepath, search_string, replace_string, strict=Fa\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if strict:\n-            contents = re.sub(r'\\b({0})\\b'.format(re.escape(search_string)), lambda x: replace_string, contents)\n+            contents = re.sub(fr'\\b({re.escape(search_string)})\\b', lambda x: replace_string, contents)\n         else:\n             contents = contents.replace(search_string, replace_string)\n         f.seek(0)\n@@ -968,8 +968,8 @@ def file_add_header(filepath, header):\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if header[0] != \"<\" and header[-1] != \">\":\n-            header = '\"{0}\"'.format(header)\n-        contents = ('#include {0} \\n'.format(header)) + contents\n+            header = f'\"{header}\"'\n+        contents = (f'#include {header} \\n') + contents\n         f.seek(0)\n         f.write(contents)\n         f.truncate()\ndiff --git a/torch/utils/jit/log_extract.py b/torch/utils/jit/log_extract.py\nindex d9d0e442c1dbf6..2e89a769eff0c8 100644\n--- a/torch/utils/jit/log_extract.py\n+++ b/torch/utils/jit/log_extract.py\n@@ -11,7 +11,7 @@ def extract_ir(filename: str) -> List[str]:\n     pfx = None\n     current = \"\"\n     graphs = []\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         split_strs = f.read().split(BEGIN)\n         for i, split_str in enumerate(split_strs):\n             if i == 0:\ndiff --git a/torch/utils/mobile_optimizer.py b/torch/utils/mobile_optimizer.py\nindex ec200423e10c5b..66d57a2372baf9 100644\n--- a/torch/utils/mobile_optimizer.py\n+++ b/torch/utils/mobile_optimizer.py\n@@ -31,7 +31,7 @@ def optimize_for_mobile(\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     if optimization_blocklist is None:\n         optimization_blocklist = set()\n@@ -86,7 +86,7 @@ def generate_mobile_module_lints(script_module: torch.jit.ScriptModule):\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     lint_list = []\n \ndiff --git a/torch/utils/tensorboard/_caffe2_graph.py b/torch/utils/tensorboard/_caffe2_graph.py\nindex 8bba2aeffddef2..2aa162af7ad5c3 100644\n--- a/torch/utils/tensorboard/_caffe2_graph.py\n+++ b/torch/utils/tensorboard/_caffe2_graph.py\n@@ -232,7 +232,7 @@ def _add_gradient_scope(shapes, blob_name_tracker, ops):\n \n     def f(name):\n         if \"_grad\" in name:\n-            return \"GRADIENTS/{}\".format(name)\n+            return f\"GRADIENTS/{name}\"\n         else:\n             return name\n \n@@ -317,7 +317,7 @@ def _tf_device(device_option):\n     ):\n         return \"/cpu:*\"\n     if device_option.device_type == caffe2_pb2.CUDA:\n-        return \"/gpu:{}\".format(device_option.device_id)\n+        return f\"/gpu:{device_option.device_id}\"\n     raise Exception(\"Unhandled device\", device_option)\n \n \ndiff --git a/torch/utils/tensorboard/_embedding.py b/torch/utils/tensorboard/_embedding.py\nindex f172e092608337..afbe68191aa98f 100644\n--- a/torch/utils/tensorboard/_embedding.py\n+++ b/torch/utils/tensorboard/_embedding.py\n@@ -62,7 +62,7 @@ def make_sprite(label_img, save_path):\n \n def get_embedding_info(metadata, label_img, subdir, global_step, tag):\n     info = EmbeddingInfo()\n-    info.tensor_name = \"{}:{}\".format(tag, str(global_step).zfill(5))\n+    info.tensor_name = f\"{tag}:{str(global_step).zfill(5)}\"\n     info.tensor_path = _gfile_join(subdir, \"tensors.tsv\")\n     if metadata is not None:\n         info.metadata_path = _gfile_join(subdir, \"metadata.tsv\")\ndiff --git a/torch/utils/tensorboard/_pytorch_graph.py b/torch/utils/tensorboard/_pytorch_graph.py\nindex f03812b603e1c0..280b503c515c0b 100644\n--- a/torch/utils/tensorboard/_pytorch_graph.py\n+++ b/torch/utils/tensorboard/_pytorch_graph.py\n@@ -275,7 +275,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n                     parent_scope, attr_scope, attr_name\n                 )\n             else:\n-                attr_to_scope[attr_key] = \"__module.{}\".format(attr_name)\n+                attr_to_scope[attr_key] = f\"__module.{attr_name}\"\n             # We don't need classtype nodes; scope will provide this information\n             if node.output().type().kind() != CLASSTYPE_KIND:\n                 node_py = NodePyOP(node)\n@@ -286,7 +286,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n \n     for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n         node_pyio = NodePyIO(node, \"output\")\n-        node_pyio.debugName = \"output.{}\".format(i + 1)\n+        node_pyio.debugName = f\"output.{i + 1}\"\n         node_pyio.inputs = [node.debugName()]\n         nodes_py.append(node_pyio)\n \n@@ -302,7 +302,7 @@ def parse_traced_name(module):\n     for name, module in trace.named_modules(prefix=\"__module\"):\n         mod_name = parse_traced_name(module)\n         attr_name = name.split(\".\")[-1]\n-        alias_to_name[name] = \"{}[{}]\".format(mod_name, attr_name)\n+        alias_to_name[name] = f\"{mod_name}[{attr_name}]\"\n \n     for node in nodes_py.nodes_op:\n         module_aliases = node.scopeName.split(\"/\")\ndiff --git a/torch/utils/tensorboard/writer.py b/torch/utils/tensorboard/writer.py\nindex 2fa34d05e727d9..b592707df2c1e1 100644\n--- a/torch/utils/tensorboard/writer.py\n+++ b/torch/utils/tensorboard/writer.py\n@@ -953,7 +953,7 @@ def add_embedding(\n \n         # Maybe we should encode the tag so slashes don't trip us up?\n         # I don't think this will mess us up, but better safe than sorry.\n-        subdir = \"%s/%s\" % (str(global_step).zfill(5), self._encode(tag))\n+        subdir = f\"{str(global_step).zfill(5)}/{self._encode(tag)}\"\n         save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n \n         fs = tf.io.gfile\ndiff --git a/torch/utils/throughput_benchmark.py b/torch/utils/throughput_benchmark.py\nindex 8b2fd1a76ca8a4..2dc3ce8543a9b6 100644\n--- a/torch/utils/throughput_benchmark.py\n+++ b/torch/utils/throughput_benchmark.py\n@@ -18,10 +18,10 @@ def format_time(time_us=None, time_ms=None, time_s=None):\n             raise AssertionError(\"Shouldn't reach here :)\")\n \n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n \n class ExecutionStats:\n@@ -52,8 +52,8 @@ def total_time_seconds(self):\n     def __str__(self):\n         return '\\n'.join([\n             \"Average latency per example: \" + format_time(time_ms=self.latency_avg_ms),\n-            \"Total number of iterations: {}\".format(self.num_iters),\n-            \"Total number of iterations per second (across all threads): {:.2f}\".format(self.iters_per_second),\n+            f\"Total number of iterations: {self.num_iters}\",\n+            f\"Total number of iterations per second (across all threads): {self.iters_per_second:.2f}\",\n             \"Total time: \" + format_time(time_s=self.total_time_seconds)\n         ])\n \ndiff --git a/torch/utils/viz/_cycles.py b/torch/utils/viz/_cycles.py\nindex a64d5e9c35830a..13a425cd1b8285 100644\n--- a/torch/utils/viz/_cycles.py\n+++ b/torch/utils/viz/_cycles.py\n@@ -220,29 +220,29 @@ def format_sequence(obj):\n     if isinstance(obj, BASE_TYPES):\n         return repr(obj)\n     if type(obj).__name__ == 'function':\n-        return \"function\\n{}\".format(obj.__name__)\n+        return f\"function\\n{obj.__name__}\"\n     elif isinstance(obj, types.MethodType):\n         try:\n             func_name = obj.__func__.__qualname__\n         except AttributeError:\n             func_name = \"<anonymous>\"\n-        return \"instancemethod\\n{}\".format(func_name)\n+        return f\"instancemethod\\n{func_name}\"\n     elif isinstance(obj, list):\n         return f\"[{format_sequence(obj)}]\"\n     elif isinstance(obj, tuple):\n         return f\"({format_sequence(obj)})\"\n     elif isinstance(obj, dict):\n-        return \"dict[{}]\".format(len(obj))\n+        return f\"dict[{len(obj)}]\"\n     elif isinstance(obj, types.ModuleType):\n-        return \"module\\n{}\".format(obj.__name__)\n+        return f\"module\\n{obj.__name__}\"\n     elif isinstance(obj, type):\n-        return \"type\\n{}\".format(obj.__name__)\n+        return f\"type\\n{obj.__name__}\"\n     elif isinstance(obj, weakref.ref):\n         referent = obj()\n         if referent is None:\n             return \"weakref (dead referent)\"\n         else:\n-            return \"weakref to id 0x{:x}\".format(id(referent))\n+            return f\"weakref to id 0x{id(referent):x}\"\n     elif isinstance(obj, types.FrameType):\n         filename = obj.f_code.co_filename\n         if len(filename) > FRAME_FILENAME_LIMIT:\ndiff --git a/torch/utils/weak.py b/torch/utils/weak.py\nindex 2a7d597c4f2a06..bcd3025bc68e3a 100644\n--- a/torch/utils/weak.py\n+++ b/torch/utils/weak.py\n@@ -4,7 +4,6 @@\n from weakref import ref\n from _weakrefset import _IterationGuard  # type: ignore[attr-defined]\n from collections.abc import MutableMapping, Mapping\n-from typing import Dict\n from torch import Tensor\n import collections.abc as _collections_abc\n \n@@ -83,7 +82,7 @@ def __eq__(self, other):\n \n # This is directly adapted from cpython/Lib/weakref.py\n class WeakIdKeyDictionary(MutableMapping):\n-    data: Dict[WeakIdRef, object]\n+    data: dict[WeakIdRef, object]\n \n     def __init__(self, dict=None):\n         self.data = {}\n@@ -144,7 +143,7 @@ def __len__(self):\n         return len(self.data) - len(self._pending_removals)\n \n     def __repr__(self):\n-        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n+        return f\"<{self.__class__.__name__} at {id(self):#x}>\"\n \n     def __setitem__(self, key, value):\n         self.data[WeakIdRef(key, self._remove)] = value  # CHANGED\n"
  },
  {
    "number": 105403,
    "title": "[BE] Enable ruff's UP rules and autoformat torchgen/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "f79238b1cf87898761e9f2e20f123cb1768a668e",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105403",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105403/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105403.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105403.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105403/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105403/comments",
    "labels": [
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:15:31.769890Z",
    "state": "closed",
    "patch": "From ce8f76b7023bc193192e81a1231173a6dcbb2948 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:15:25 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat torchgen/\n\n[ghstack-poisoned]\n---\n torchgen/api/python.py               | 16 ++++++++--------\n torchgen/code_template.py            |  2 +-\n torchgen/executorch/parse.py         |  2 +-\n torchgen/gen.py                      |  4 ++--\n torchgen/gen_backend_stubs.py        |  6 +++---\n torchgen/gen_executorch.py           |  6 +++---\n torchgen/gen_lazy_tensor.py          |  6 +++---\n torchgen/model.py                    |  2 +-\n torchgen/selective_build/operator.py |  2 +-\n torchgen/selective_build/selector.py |  4 ++--\n torchgen/utils.py                    | 10 +++++-----\n 11 files changed, 30 insertions(+), 30 deletions(-)\n\ndiff --git a/torchgen/api/python.py b/torchgen/api/python.py\nindex b4da5d1113dce6..96aa43be1060b5 100644\n--- a/torchgen/api/python.py\n+++ b/torchgen/api/python.py\n@@ -315,7 +315,7 @@ def from_outputs(\n                 outputs=outputs,\n             )\n         elif size > 1:\n-            if any((not a.type.is_tensor_like() for a in outputs)):\n+            if any(not a.type.is_tensor_like() for a in outputs):\n                 raise RuntimeError(f\"Unsupported output type: {outputs}\")\n             return PythonOutArgument(\n                 name=\"out\",\n@@ -882,10 +882,10 @@ def topt_default_init(name: str) -> Optional[str]:\n \n \n def namedtuple_fieldnames(returns: Tuple[Return, ...]) -> List[str]:\n-    if len(returns) <= 1 or all((r.name is None for r in returns)):\n+    if len(returns) <= 1 or all(r.name is None for r in returns):\n         return []\n     else:\n-        if any((r.name is None for r in returns)):\n+        if any(r.name is None for r in returns):\n             # When building on Windows, `PyStructSequence_UnnamedField` could not be\n             # resolved by the linker for some reason, which cause error in building:\n             #\n@@ -1163,7 +1163,7 @@ def dispatch_lambda_return_str(f: NativeFunction) -> str:\n     # mutable reference to temporary.  Maybe we could assign it to a\n     # variable itself.)\n     returns_without_annotation = tuple(\n-        (Return(r.name, r.type, None) for r in f.func.returns)\n+        Return(r.name, r.type, None) for r in f.func.returns\n     )\n     return_str = cpp.returns_type(returns_without_annotation, symint=True).cpp_type()\n     if return_str not in SUPPORTED_RETURN_TYPES:\n@@ -1195,7 +1195,7 @@ def cpp_dispatch_exprs(\n     exprs: Tuple[str, ...] = tuple()\n     if not isinstance(python_signature, PythonSignatureDeprecated):\n         # By default the exprs are consistent with the C++ signature.\n-        exprs = tuple((a.name for a in cpp_args))\n+        exprs = tuple(a.name for a in cpp_args)\n     else:\n         # For deprecated python signature we may need fill in some constants.\n         exprs = tuple(\n@@ -1426,7 +1426,7 @@ def dispatch_lambda_exprs(\n                     f\"{f.func}: unrecognized type '{str(a.type)}' for tensor options field '{a.name}'\"\n                 )\n         if not all(\n-            (a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys())\n+            a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys()\n         ):\n             raise RuntimeError(\n                 f\"{f.func}: incomplete tensor options args: {tensor_options_args_names}\"\n@@ -1454,7 +1454,7 @@ def dispatch_lambda_exprs(\n                 raise RuntimeError(\n                     f\"{f.func}: dtype in tensor_options_args without output arg\"\n                 )\n-            if not all((a in tensor_options_args_names for a in (\"layout\", \"device\"))):\n+            if not all(a in tensor_options_args_names for a in (\"layout\", \"device\")):\n                 raise RuntimeError(\n                     f\"{f.func}: incomplete tensor options for output check\"\n                 )\n@@ -1473,6 +1473,6 @@ def dispatch_lambda_exprs(\n             )\n \n     return DispatchLambdaArgumentExprs(\n-        exprs=tuple((lambda_args_exprs[a.name] for a in lambda_args)),\n+        exprs=tuple(lambda_args_exprs[a.name] for a in lambda_args),\n         inits=inits,\n     )\ndiff --git a/torchgen/code_template.py b/torchgen/code_template.py\nindex 9f877771afe9be..b932a94ecc9192 100644\n--- a/torchgen/code_template.py\n+++ b/torchgen/code_template.py\n@@ -20,7 +20,7 @@ class CodeTemplate:\n \n     @staticmethod\n     def from_file(filename: str) -> \"CodeTemplate\":\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             return CodeTemplate(f.read(), filename)\n \n     def __init__(self, pattern: str, filename: str = \"\") -> None:\ndiff --git a/torchgen/executorch/parse.py b/torchgen/executorch/parse.py\nindex f6f30b4554aafb..89b4b93558a6a2 100644\n--- a/torchgen/executorch/parse.py\n+++ b/torchgen/executorch/parse.py\n@@ -124,7 +124,7 @@ def parse_et_yaml(\n     \"\"\"Parse native_functions.yaml into NativeFunctions and an Operator Indexed Dict\n     of fields to persist from native_functions.yaml to functions.yaml\n     \"\"\"\n-    with open(path, \"r\") as f:\n+    with open(path) as f:\n         es = yaml.load(f, Loader=LineLoader)\n \n     et_kernel = extract_kernel_fields(es)\ndiff --git a/torchgen/gen.py b/torchgen/gen.py\nindex dcdd0945dff019..9766c8af5bc0f5 100644\n--- a/torchgen/gen.py\n+++ b/torchgen/gen.py\n@@ -212,7 +212,7 @@ def parse_tags_yaml_struct(es: object, path: str = \"<stdin>\") -> Set[str]:\n def parse_tags_yaml(path: str) -> Set[str]:\n     global _GLOBAL_PARSE_TAGS_YAML_CACHE\n     if path not in _GLOBAL_PARSE_TAGS_YAML_CACHE:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n             _GLOBAL_PARSE_TAGS_YAML_CACHE[path] = parse_tags_yaml_struct(es, path=path)\n \n@@ -233,7 +233,7 @@ def parse_native_yaml(\n \n         # if a loaded yaml is provided, use that instead of reading from path\n         if loaded_yaml is None:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 es = yaml.load(f, Loader=LineLoader)\n         else:\n             es = loaded_yaml\ndiff --git a/torchgen/gen_backend_stubs.py b/torchgen/gen_backend_stubs.py\nindex 7322daa5dc7602..ff23aa9be39713 100644\n--- a/torchgen/gen_backend_stubs.py\n+++ b/torchgen/gen_backend_stubs.py\n@@ -47,7 +47,7 @@ def parse_backend_yaml(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -253,9 +253,9 @@ def error_on_missing_kernels(\n     full_codegen: Optional[List[OperatorName]] = None,\n ) -> None:\n     try:\n-        with open(kernel_defn_file_path, \"r\") as f:\n+        with open(kernel_defn_file_path) as f:\n             backend_defns = f.read()\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified impl_path file: {kernel_defn_file_path}\"\n         ) from e\ndiff --git a/torchgen/gen_executorch.py b/torchgen/gen_executorch.py\nindex bfd42a7985e49d..6f5df46944f0f6 100644\n--- a/torchgen/gen_executorch.py\n+++ b/torchgen/gen_executorch.py\n@@ -575,7 +575,7 @@ def translate_native_yaml(\n         None\n     \"\"\"\n     if use_aten_lib:\n-        with open(aten_yaml_path, \"r\") as aten_yaml:\n+        with open(aten_yaml_path) as aten_yaml:\n             out_file.writelines(aten_yaml.readlines())\n         return\n \n@@ -604,7 +604,7 @@ def translate_native_yaml(\n         or os.stat(native_yaml_path).st_size == 0\n     ):\n         return\n-    with open(native_yaml_path, \"r\") as native_yaml:\n+    with open(native_yaml_path) as native_yaml:\n         native_es = yaml.load(native_yaml, Loader=LineLoader)\n         if not native_es:\n             return\n@@ -641,7 +641,7 @@ def parse_yaml(\n     Union[Dict[DispatchKey, Dict[OperatorName, BackendMetadata]], ETKernelIndex],\n ]:\n     if path and os.path.exists(path) and os.stat(path).st_size > 0:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n \n         # Check for kernel index structure\ndiff --git a/torchgen/gen_lazy_tensor.py b/torchgen/gen_lazy_tensor.py\nindex f995bdb2619838..3e4e4b0414277c 100644\n--- a/torchgen/gen_lazy_tensor.py\n+++ b/torchgen/gen_lazy_tensor.py\n@@ -115,7 +115,7 @@ def parse_native_functions_keys(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -134,10 +134,10 @@ def validate_shape_inference_header(\n     shape_inference_hdr: str, expected_shape_infr_decls: List[str]\n ) -> None:\n     try:\n-        with open(shape_inference_hdr, \"r\") as f:\n+        with open(shape_inference_hdr) as f:\n             shape_infr_decls = f.read()\n             shape_infr_decl_lines = set(shape_infr_decls.split(\"\\n\"))\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified shape_inference_hdr file: {shape_inference_hdr}\"\n         ) from e\ndiff --git a/torchgen/model.py b/torchgen/model.py\nindex 151fb02bb2c908..0b44732455ea2e 100644\n--- a/torchgen/model.py\n+++ b/torchgen/model.py\n@@ -40,7 +40,7 @@ class Location:\n     line: int\n \n     def __str__(self) -> str:\n-        return \"{}:{}\".format(self.file, self.line)\n+        return f\"{self.file}:{self.line}\"\n \n \n # Valid values of the 'variants' field in native_functions.yaml\ndiff --git a/torchgen/selective_build/operator.py b/torchgen/selective_build/operator.py\nindex 52fdcb74fca84b..d7f5c56f63a60d 100644\n--- a/torchgen/selective_build/operator.py\n+++ b/torchgen/selective_build/operator.py\n@@ -83,7 +83,7 @@ def from_yaml_dict(\n         if \"debug_info\" in op_info:\n             di_list = op_info[\"debug_info\"]\n             assert isinstance(di_list, list)\n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         return SelectiveBuildOperator(\n             name=op_name,\ndiff --git a/torchgen/selective_build/selector.py b/torchgen/selective_build/selector.py\nindex 1d4a00e5968950..4fdc513534444d 100644\n--- a/torchgen/selective_build/selector.py\n+++ b/torchgen/selective_build/selector.py\n@@ -93,7 +93,7 @@ def from_yaml_dict(data: Dict[str, object]) -> \"SelectiveBuilder\":\n             di_list = data[\"debug_info\"]\n             assert isinstance(di_list, list)\n \n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         operators = {}\n         operators_dict = data.get(\"operators\", {})\n@@ -141,7 +141,7 @@ def from_yaml_str(config_contents: str) -> \"SelectiveBuilder\":\n \n     @staticmethod\n     def from_yaml_path(config_path: str) -> \"SelectiveBuilder\":\n-        with open(config_path, \"r\") as f:\n+        with open(config_path) as f:\n             contents = yaml.safe_load(f)\n             return SelectiveBuilder.from_yaml_dict(contents)\n \ndiff --git a/torchgen/utils.py b/torchgen/utils.py\nindex dd187c737c93ed..0729645ef10b92 100644\n--- a/torchgen/utils.py\n+++ b/torchgen/utils.py\n@@ -105,7 +105,7 @@ def context(msg_fn: Callable[[], str]) -> Iterator[None]:\n # for getting mypy to do exhaustiveness checking\n # TODO: put this somewhere else, maybe\n def assert_never(x: NoReturn) -> NoReturn:\n-    raise AssertionError(\"Unhandled type: {}\".format(type(x).__name__))\n+    raise AssertionError(f\"Unhandled type: {type(x).__name__}\")\n \n \n @functools.lru_cache(maxsize=None)\n@@ -137,9 +137,9 @@ def __init__(self, install_dir: str, template_dir: str, dry_run: bool) -> None:\n     def _write_if_changed(self, filename: str, contents: str) -> None:\n         old_contents: Optional[str]\n         try:\n-            with open(filename, \"r\") as f:\n+            with open(filename) as f:\n                 old_contents = f.read()\n-        except IOError:\n+        except OSError:\n             old_contents = None\n         if contents != old_contents:\n             # Create output directory if it doesn't exist\n@@ -157,7 +157,7 @@ def substitute_with_template(\n             # TODO: Update the comment reference to the correct location\n             if \"generated_comment\" not in env:\n                 comment = \"@\" + \"generated by torchgen/gen.py\"\n-                comment += \" from {}\".format(os.path.basename(template_path))\n+                comment += f\" from {os.path.basename(template_path)}\"\n                 env[\"generated_comment\"] = comment\n             template = _read_template(template_path)\n             return template.substitute(env)\n@@ -172,7 +172,7 @@ def write_with_template(\n         template_fn: str,\n         env_callable: Callable[[], Union[str, Dict[str, Any]]],\n     ) -> None:\n-        filename = \"{}/{}\".format(self.install_dir, filename)\n+        filename = f\"{self.install_dir}/{filename}\"\n         assert filename not in self.filenames, \"duplicate file write {filename}\"\n         self.filenames.add(filename)\n         if not self.dry_run:\n"
  },
  {
    "number": 105402,
    "title": "[BE] Enable ruff's UP rules and autoformat dynamo / fx / functorch and refs",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "bad1be785a9db5fc316e9aba616b6d9419f794c7",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105402",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105402/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105402.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105402.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105402/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105402/comments",
    "labels": [
      "open source",
      "release notes: fx",
      "ciflow/inductor",
      "module: dynamo",
      "module: export"
    ],
    "_event_time": "2023-07-18T01:13:00.374754Z",
    "state": "closed",
    "patch": "From e5e14b29c77047a70642703399347e49b071dfa0 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:53 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat dynamo / functorch\n and refs\n\n[ghstack-poisoned]\n---\n functorch/benchmarks/operator_authoring.py    |  2 +-\n functorch/einops/rearrange.py                 |  4 +-\n .../examples/compilation/linear_train.py      |  2 +-\n .../maml_omniglot/support/omniglot_loaders.py |  4 +-\n functorch/op_analysis/gen_data.py             | 10 +--\n test/dynamo/test_autograd_function.py         |  4 +-\n test/dynamo/test_compile.py                   |  2 +-\n test/dynamo/test_logging.py                   |  2 +-\n test/dynamo/test_misc.py                      | 34 +++++-----\n test/dynamo/test_modules.py                   | 12 ++--\n test/dynamo/test_profiler.py                  |  4 +-\n test/dynamo/test_repros.py                    |  2 +-\n test/error_messages/storage.py                |  2 +-\n test/export/test_db.py                        |  6 +-\n test/export/test_serialize.py                 |  2 +-\n .../check_forward_backward_compatibility.py   |  2 +-\n test/functorch/test_aotdispatch.py            |  4 +-\n test/functorch/test_vmap.py                   |  2 +-\n test/fx/test_future.py                        |  4 +-\n test/fx/test_gradual_type.py                  |  8 +--\n torch/_decomp/decompositions.py               | 14 ++---\n torch/_dynamo/debug_utils.py                  | 16 ++---\n torch/_dynamo/eval_frame.py                   |  4 +-\n torch/_dynamo/output_graph.py                 |  2 +-\n torch/_dynamo/resume_execution.py             |  6 +-\n torch/_dynamo/symbolic_convert.py             |  4 +-\n torch/_dynamo/test_minifier_common.py         |  6 +-\n torch/_dynamo/variables/builder.py            | 10 ++-\n torch/_dynamo/variables/misc.py               |  2 +-\n torch/_export/verifier.py                     |  8 +--\n torch/_functorch/eager_transforms.py          |  2 +-\n torch/_functorch/pytree_hacks.py              |  2 +-\n torch/_prims/__init__.py                      | 62 +++++++++----------\n torch/_prims/executor.py                      |  2 +-\n torch/_prims/nvfuser_executor.py              |  4 +-\n torch/_prims_common/__init__.py               | 28 ++++-----\n torch/_prims_common/wrappers.py               |  8 +--\n torch/_refs/__init__.py                       | 46 +++++++-------\n torch/_refs/nn/functional/__init__.py         | 16 ++---\n .../migrate_gradual_types/constraint.py       |  1 -\n .../constraint_transformation.py              |  4 +-\n .../migrate_gradual_types/operation.py        |  1 -\n torch/fx/experimental/symbolic_shapes.py      |  4 +-\n .../multipledispatch/dispatcher.py            | 15 ++---\n torch/fx/interpreter.py                       |  2 +-\n torch/fx/passes/utils/matcher_utils.py        |  2 +-\n torch/fx/passes/utils/source_matcher_utils.py |  2 +-\n 47 files changed, 181 insertions(+), 204 deletions(-)\n\ndiff --git a/functorch/benchmarks/operator_authoring.py b/functorch/benchmarks/operator_authoring.py\nindex cbd816e2ad1324..456f5040d759f2 100644\n--- a/functorch/benchmarks/operator_authoring.py\n+++ b/functorch/benchmarks/operator_authoring.py\n@@ -113,7 +113,7 @@ def out_setup(n):\n def test_backwards(make_args, nnc=nnc_add, aten=torch.add):\n     def backwards_setup(n):\n         args = make_args(n)\n-        (grad_var,) = [a for a in args if a.requires_grad]\n+        (grad_var,) = (a for a in args if a.requires_grad)\n         aten(*args).sum().backward()\n         correct = grad_var.grad.clone()\n         grad_var.grad.zero_()\ndiff --git a/functorch/einops/rearrange.py b/functorch/einops/rearrange.py\nindex c45d2063c7114a..f8f60c4917b766 100644\n--- a/functorch/einops/rearrange.py\n+++ b/functorch/einops/rearrange.py\n@@ -108,7 +108,7 @@ class dims.\"\"\"\n \n     custom_rearrange_callable_name = \"do_rearrange\"\n     custom_rearrange_callable_code = (\n-        (\n+\n             f\"def {custom_rearrange_callable_name}(tensor):\\n\"\n             f\"    {comma_separate(first_class_dims)} = dims({n_dims})\\n\"\n             + (\n@@ -120,7 +120,7 @@ class dims.\"\"\"\n                 f\"    return tensor.sum({comma_separate([anon_dims])}, keepdim=False)\\n\"\n                 if anon_dims else \"    return tensor\\n\"\n             )\n-        )\n+\n     )\n \n     exec(custom_rearrange_callable_code)\ndiff --git a/functorch/examples/compilation/linear_train.py b/functorch/examples/compilation/linear_train.py\nindex 2d5f9d7dd37b44..ee84347470835b 100644\n--- a/functorch/examples/compilation/linear_train.py\n+++ b/functorch/examples/compilation/linear_train.py\n@@ -18,7 +18,7 @@ def bench(f, iters=100, warmup=10):\n     begin = time.time()\n     for _ in range(iters):\n         f()\n-    print((time.time() - begin))\n+    print(time.time() - begin)\n \n \n class Foo(nn.Module):\ndiff --git a/functorch/examples/maml_omniglot/support/omniglot_loaders.py b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\nindex ce636ecca0b1b2..6a4369ba4b208f 100644\n--- a/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n+++ b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n@@ -276,10 +276,10 @@ def load_data_cache(self, data_pack):\n             x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n             y_qrys = np.array(y_qrys).astype(int).reshape(self.batchsz, querysz)\n \n-            x_spts, y_spts, x_qrys, y_qrys = [\n+            x_spts, y_spts, x_qrys, y_qrys = (\n                 torch.from_numpy(z).to(self.device) for z in\n                 [x_spts, y_spts, x_qrys, y_qrys]\n-            ]\n+            )\n \n             data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n \ndiff --git a/functorch/op_analysis/gen_data.py b/functorch/op_analysis/gen_data.py\nindex a9cc84e6f9362c..ab1f3a79125c20 100644\n--- a/functorch/op_analysis/gen_data.py\n+++ b/functorch/op_analysis/gen_data.py\n@@ -23,7 +23,7 @@ def gen_data(special_op_lists, analysis_name):\n     composite_ops = get_ops_for_key('CompositeImplicitAutograd')\n     noncomposite_ops = all_ops - composite_ops\n \n-    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)\n+    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml').read(), Loader=yaml.CLoader)\n \n     annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}\n     from collections import defaultdict\n@@ -132,19 +132,19 @@ def remove_prefix(input_string, prefix):\n \n \n if True:\n-    with open('run_ops.txt', 'r') as f:\n+    with open('run_ops.txt') as f:\n         opinfo_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n-    with open('count_ops.txt', 'r') as f:\n+    with open('count_ops.txt') as f:\n         opinfo_counts = [i.strip() for i in f.readlines()]\n         opinfo_counts = defaultdict(int, dict(zip(opinfo_ops, opinfo_counts)))\n \n     def count_fn(x):\n         return opinfo_counts[x['full_name']]\n \n-    with open('run_decompositions.txt', 'r') as f:\n+    with open('run_decompositions.txt') as f:\n         decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n \n-    with open('public_api', 'r') as f:\n+    with open('public_api') as f:\n         ref_api = [i.strip() for i in f.readlines()]\n \n     def has_ref_impl(x):\ndiff --git a/test/dynamo/test_autograd_function.py b/test/dynamo/test_autograd_function.py\nindex 55165edd61a41d..7de264e5051743 100644\n--- a/test/dynamo/test_autograd_function.py\n+++ b/test/dynamo/test_autograd_function.py\n@@ -207,7 +207,7 @@ def backward(ctx, grad_output):\n \n class ModuleWithGradFunc(torch.nn.Module):\n     def __init__(self, func):\n-        super(ModuleWithGradFunc, self).__init__()\n+        super().__init__()\n         self.f = func.apply\n \n     def forward(self, x):\n@@ -336,7 +336,7 @@ def backward(ctx, grad_output):\n \n         class MyMod(torch.nn.Module):\n             def __init__(self):\n-                super(MyMod, self).__init__()\n+                super().__init__()\n                 self.gamma = torch.nn.Parameter(torch.rand([4, 128, 32, 32]))\n \n             def forward(self, x):\ndiff --git a/test/dynamo/test_compile.py b/test/dynamo/test_compile.py\nindex e3847cbb2ae121..5b2de2b7b3867f 100644\n--- a/test/dynamo/test_compile.py\n+++ b/test/dynamo/test_compile.py\n@@ -11,7 +11,7 @@\n \n class ToyModel(torch.nn.Module):\n     def __init__(self):\n-        super(ToyModel, self).__init__()\n+        super().__init__()\n         self.linear = torch.nn.Linear(10, 10)\n         self.relu = torch.nn.ReLU()\n \ndiff --git a/test/dynamo/test_logging.py b/test/dynamo/test_logging.py\nindex 183910f6db3510..eed99681e2c04e 100644\n--- a/test/dynamo/test_logging.py\n+++ b/test/dynamo/test_logging.py\n@@ -157,7 +157,7 @@ def throw(x):\n     def test_ddp_graphs(self, records):\n         class ToyModel(torch.nn.Module):\n             def __init__(self):\n-                super(ToyModel, self).__init__()\n+                super().__init__()\n                 self.layers = torch.nn.Sequential(\n                     torch.nn.Linear(1024, 1024),\n                     torch.nn.Linear(1024, 1024),\ndiff --git a/test/dynamo/test_misc.py b/test/dynamo/test_misc.py\nindex 6b020adc0d8007..6ec0acf8054fd3 100644\n--- a/test/dynamo/test_misc.py\n+++ b/test/dynamo/test_misc.py\n@@ -823,7 +823,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 3)\n@@ -837,7 +837,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 2)\n@@ -2202,7 +2202,7 @@ def fn():\n def fn():\n     foo.bar(1, 2, 3)\n {str(chr(10)).join(' ' * 4 + 'x' + str(i) + ' = 1' for i in range(1 << 9))}\n-    l = [{str(' ').join('x' + str(i) + ',' for i in range(1 << 9))}]\n+    l = [{' '.join('x' + str(i) + ',' for i in range(1 << 9))}]\n         \"\"\"\n         locals = {}\n         exec(fn_str, {}, locals)\n@@ -3087,7 +3087,7 @@ def foo(self, memo=None, prefix=\"\", remove_duplicate=False):\n                     memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n                 ):\n                     for pn, p in self.named_parameters():\n-                        fpn = \"%s.%s\" % (mn, pn) if mn else pn\n+                        fpn = f\"{mn}.{pn}\" if mn else pn\n                         self.names.append(fpn)\n \n         # Test plain recurse\n@@ -5032,11 +5032,11 @@ def test_compute_exception_table_nested(self):\n             (15, 16, 7),\n             (17, 17, 6),\n         ]\n-        self.assertEquals(len(tab), len(expected))\n+        self.assertEqual(len(tab), len(expected))\n         for entry, exp in zip(tab, expected):\n-            self.assertEquals(entry.start, exp[0] * 2)\n-            self.assertEquals(entry.end, exp[1] * 2)\n-            self.assertEquals(entry.target, exp[2] * 2)\n+            self.assertEqual(entry.start, exp[0] * 2)\n+            self.assertEqual(entry.end, exp[1] * 2)\n+            self.assertEqual(entry.target, exp[2] * 2)\n \n     @skipIfNotPy311\n     def test_remove_dead_code_with_exn_table_entries(self):\n@@ -5060,17 +5060,17 @@ def test_remove_dead_code_with_exn_table_entries(self):\n         )\n         bytecode_transformation.propagate_inst_exn_table_entries(insts)\n         insts = bytecode_analysis.remove_dead_code(insts)\n-        self.assertEquals(len(insts), 5)\n+        self.assertEqual(len(insts), 5)\n         self.assertNotIn(exn_start, insts)\n         self.assertNotIn(exn_end, insts)\n         self.assertIn(target2, insts)\n         self.assertIn(target3, insts)\n         bytecode_transformation.update_offsets(insts)\n         tab = bytecode_transformation.compute_exception_table(insts)\n-        self.assertEquals(len(tab), 1)\n-        self.assertEquals(tab[0].start, 2)\n-        self.assertEquals(tab[0].end, 4)\n-        self.assertEquals(tab[0].target, 6)\n+        self.assertEqual(len(tab), 1)\n+        self.assertEqual(tab[0].start, 2)\n+        self.assertEqual(tab[0].end, 4)\n+        self.assertEqual(tab[0].target, 6)\n \n     def test_unhandled_exception_in_dynamo(self):\n         # traceback.format_exc() approximates an unhandled exception\n@@ -5757,7 +5757,7 @@ def guard(L):\n     def test_dynamo_compiling_fake_tensor_to_vararg_int(self):\n         class MyModule(torch.nn.Module):\n             def __init__(self):\n-                super(MyModule, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 # use numpy int so it's wrapped as fake tensor in dynamo\n@@ -5776,7 +5776,7 @@ def forward(self, x):\n     def test_scalar_tensor_is_equivalent_to_symint_argument(self):\n         class GumbelTopKSampler(torch.nn.Module):\n             def __init__(self, T, k):\n-                super(GumbelTopKSampler, self).__init__()\n+                super().__init__()\n                 self.T = torch.nn.Parameter(\n                     torch.tensor(T, dtype=torch.float32), requires_grad=False\n                 )\n@@ -5803,7 +5803,7 @@ def forward(self, logits):\n     def test_scalar_tensor_is_equivalent_to_symint_list_argument(self):\n         class Jitter(torch.nn.Module):\n             def __init__(self, jitter_val):\n-                super(Jitter, self).__init__()\n+                super().__init__()\n                 self.jitter_val = jitter_val\n \n             def roll_tensor(self, input):\n@@ -5986,7 +5986,7 @@ def _prepare_for_translation_validator(self):\n \n         # Z3 symbols.\n         [validator.add_var(s, int) for s in (s0, s1, s2)]\n-        z0, z1, z2 = [validator.z3var(s) for s in (s0, s1, s2)]\n+        z0, z1, z2 = (validator.z3var(s) for s in (s0, s1, s2))\n \n         return (s0, s1, s2), (z0, z1, z2), validator\n \ndiff --git a/test/dynamo/test_modules.py b/test/dynamo/test_modules.py\nindex 03ef4f07305454..5a881d0053ec1d 100644\n--- a/test/dynamo/test_modules.py\n+++ b/test/dynamo/test_modules.py\n@@ -762,7 +762,7 @@ def forward(self, x):\n \n class ConvCallSuperForwardDirectly(torch.nn.Conv1d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -770,13 +770,13 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n         )\n \n     def forward(self, inputs, mask=None):\n-        outputs = super(ConvCallSuperForwardDirectly, self).forward(inputs)\n+        outputs = super().forward(inputs)\n         return outputs\n \n \n class ConvTransposeCallSuperForwardDirectly(torch.nn.ConvTranspose2d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvTransposeCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -785,7 +785,7 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n \n     def forward(self, x):\n         if x.numel() > 0:\n-            return super(ConvTransposeCallSuperForwardDirectly, self).forward(x)\n+            return super().forward(x)\n         output_shape = [\n             ((i - 1) * d - 2 * p + (di * (k - 1) + 1) + op)\n             for i, p, di, k, d, op in zip(\n@@ -923,7 +923,7 @@ def forward(self, x):\n class SequentialWithDuplicatedModule(torch.nn.Module):\n     # Sequential module(self.layer) contains three duplicated ReLU module.\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             torch.nn.Linear(10, 20),\n@@ -940,7 +940,7 @@ def forward(self, x):\n \n class SequentialWithDuplicatedModule2(torch.nn.Module):\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule2, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             collections.OrderedDict(\ndiff --git a/test/dynamo/test_profiler.py b/test/dynamo/test_profiler.py\nindex 7f58d99863d093..bec7adb33eda98 100644\n--- a/test/dynamo/test_profiler.py\n+++ b/test/dynamo/test_profiler.py\n@@ -20,7 +20,7 @@ def inner_fn(x):\n         def outer_fn(x, y):\n             return inner_fn(x) * y\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             outer_fn(x, y)\n@@ -40,7 +40,7 @@ def test_dynamo_timed_profiling_backend_compile(self):\n         def fn(x, y):\n             return x.sin() * y.cos()\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             torch._dynamo.optimize(\"aot_eager\")(fn)(x, y)\ndiff --git a/test/dynamo/test_repros.py b/test/dynamo/test_repros.py\nindex 2e84776ea76580..77d8859541472c 100644\n--- a/test/dynamo/test_repros.py\n+++ b/test/dynamo/test_repros.py\n@@ -2632,7 +2632,7 @@ def test_error_return_without_exception_set(self):\n         # https://github.com/pytorch/pytorch/issues/93781\n         @torch.compile\n         def f():\n-            _generator_type = type((_ for _ in ()))\n+            _generator_type = type(_ for _ in ())\n \n         self.assertNoUnraisable(f)\n \ndiff --git a/test/error_messages/storage.py b/test/error_messages/storage.py\nindex f3053d862a220c..b33b86e0908a95 100644\n--- a/test/error_messages/storage.py\n+++ b/test/error_messages/storage.py\n@@ -14,7 +14,7 @@ def check_error(desc, fn, *required_substrings):\n         for sub in required_substrings:\n             assert sub in error_message\n         return\n-    raise AssertionError(\"given function ({}) didn't raise an error\".format(desc))\n+    raise AssertionError(f\"given function ({desc}) didn't raise an error\")\n \n check_error(\n     'Wrong argument types',\ndiff --git a/test/export/test_db.py b/test/export/test_db.py\nindex 10d149e096be9d..bfa57baf214c8f 100644\n--- a/test/export/test_db.py\n+++ b/test/export/test_db.py\n@@ -23,7 +23,7 @@ class ExampleTests(TestCase):\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.SUPPORTED).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -51,7 +51,7 @@ def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.NOT_SUPPORTED_YET).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -73,7 +73,7 @@ def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n             ).items()\n             for rewrite_case in get_rewrite_cases(case)\n         ],\n-        name_fn=lambda name, case: \"case_{}_{}\".format(name, case.name),\n+        name_fn=lambda name, case: f\"case_{name}_{case.name}\",\n     )\n     def test_exportdb_not_supported_rewrite(\n         self, name: str, rewrite_case: ExportCase\ndiff --git a/test/export/test_serialize.py b/test/export/test_serialize.py\nindex 01bb32ad2c791b..c3942936e2bc55 100644\n--- a/test/export/test_serialize.py\n+++ b/test/export/test_serialize.py\n@@ -361,7 +361,7 @@ def f(x, y):\n     @parametrize(\n         \"name,case\",\n         get_filtered_export_db_tests(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\ndiff --git a/test/forward_backward_compatibility/check_forward_backward_compatibility.py b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\nindex 886ad32a24bafb..cf4ce8def1adb7 100644\n--- a/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n+++ b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n@@ -501,7 +501,7 @@ def check_fc(existing_schemas):\n     args = parser.parse_args()\n     existing_schema_dict = {}\n     slist = []\n-    with open(args.existing_schemas, \"r\") as f:\n+    with open(args.existing_schemas) as f:\n         while True:\n             line = f.readline()\n             if not line:\ndiff --git a/test/functorch/test_aotdispatch.py b/test/functorch/test_aotdispatch.py\nindex e4357ce0bcafd8..ca92201fedc5ea 100644\n--- a/test/functorch/test_aotdispatch.py\n+++ b/test/functorch/test_aotdispatch.py\n@@ -1805,8 +1805,8 @@ def test_batch_norm_amp(self):\n         device = \"cuda\"\n         input_dtype = torch.float16\n         param_dtype = torch.float32\n-        weight, bias = [torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2)]\n-        running_mean, running_var = [torch.ones(64, device=device, dtype=param_dtype) for _ in range(2)]\n+        weight, bias = (torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2))\n+        running_mean, running_var = (torch.ones(64, device=device, dtype=param_dtype) for _ in range(2))\n \n         def bn(x):\n             return torch.ops.aten.cudnn_batch_norm(\ndiff --git a/test/functorch/test_vmap.py b/test/functorch/test_vmap.py\nindex a0f6e077004344..81b980edd6d006 100644\n--- a/test/functorch/test_vmap.py\n+++ b/test/functorch/test_vmap.py\n@@ -3438,7 +3438,7 @@ def test():\n             check_shape_only = op.name in ('empty_like', 'new_empty')\n             for sample_input in sample_inputs_itr:\n                 args = (sample_input.input,) + sample_input.args\n-                if not any((isinstance(arg, torch.Tensor) for arg in args)):\n+                if not any(isinstance(arg, torch.Tensor) for arg in args):\n                     # Atleast one tensor required for vmap.\n                     continue\n                 kwargs = sample_input.kwargs\ndiff --git a/test/fx/test_future.py b/test/fx/test_future.py\nindex 4f093de54b4f84..4525f678eaeb6c 100644\n--- a/test/fx/test_future.py\n+++ b/test/fx/test_future.py\n@@ -16,7 +16,7 @@ def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n \n # Forward references\n class M2(torch.nn.Module):\n-    def forward(self, x: 'torch.Tensor', a: 'A') -> 'torch.Tensor':\n+    def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n         return a(x)\n \n # Non-torch annotation with no internal forward references\n@@ -26,7 +26,7 @@ def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n \n # Non-torch annotation with internal forward references\n class M4(torch.nn.Module):\n-    def forward(self, x: typing.List['torch.Tensor'], a: A) -> 'torch.Tensor':\n+    def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n         return a(x[0])\n \n x = torch.rand(2, 3)\ndiff --git a/test/fx/test_gradual_type.py b/test/fx/test_gradual_type.py\nindex 23c6496b3a294f..e3f83756eb2668 100644\n--- a/test/fx/test_gradual_type.py\n+++ b/test/fx/test_gradual_type.py\n@@ -990,12 +990,12 @@ def forward(self, x : TensorType((4, 3, Dyn, Dyn))):\n \n         for n in traced.graph.nodes:\n             if n.target == 'conv1':\n-                assert n.type == TensorType((4, 6, sympy.floor((sympy.symbols('~0') - 4)),\n-                                             sympy.floor((sympy.symbols('~1') - 4))))\n+                assert n.type == TensorType((4, 6, sympy.floor(sympy.symbols('~0') - 4),\n+                                             sympy.floor(sympy.symbols('~1') - 4)))\n \n             elif n.target == 'conv2':\n-                assert n.type == TensorType((4, 16, sympy.floor((sympy.symbols('~4') - 4)),\n-                                             sympy.floor((sympy.symbols('~5') - 4))))\n+                assert n.type == TensorType((4, 16, sympy.floor(sympy.symbols('~4') - 4),\n+                                             sympy.floor(sympy.symbols('~5') - 4)))\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py\nindex 70c69ff5cef47a..f6b268abc72a3f 100644\n--- a/torch/_decomp/decompositions.py\n+++ b/torch/_decomp/decompositions.py\n@@ -1331,10 +1331,10 @@ def native_layer_norm_backward(\n     input_shape = input.shape\n     input_ndim = input.dim()\n     computation_dtype = utils.get_computation_dtype(input.dtype)\n-    grad_out_cast, input_cast, weight_cast, bias_cast = [\n+    grad_out_cast, input_cast, weight_cast, bias_cast = (\n         x.to(computation_dtype).contiguous() if x is not None else x\n         for x in (grad_out, input, weight, bias)\n-    ]\n+    )\n     assert grad_out_cast is not None\n \n     axis = input_ndim - len(normalized_shape)\n@@ -1745,7 +1745,7 @@ def native_batch_norm_backward(\n         running_var_cast,\n         save_mean_cast,\n         save_invstd_cast,\n-    ) = [\n+    ) = (\n         x.to(computation_dtype) if x is not None else x\n         for x in (\n             grad_out,\n@@ -1756,7 +1756,7 @@ def native_batch_norm_backward(\n             save_mean,\n             save_invstd,\n         )\n-    ]\n+    )\n     input_shape = input.shape\n     input_rank = input.dim()\n     assert input_rank >= 2, \"rank of the input must be at least 2\"\n@@ -3123,7 +3123,7 @@ def get_coeff(ofs: int) -> Tensor:\n             )\n             return _upsample_cubic_interp1d(cs, tx.unsqueeze(1))\n \n-        coeffs = tuple((get_coeff(ofs) for ofs in range(4)))\n+        coeffs = tuple(get_coeff(ofs) for ofs in range(4))\n         return _upsample_cubic_interp1d(coeffs, ty.unsqueeze(1))\n \n \n@@ -3371,10 +3371,10 @@ def load_bounded(ys, xs):\n         return aten._unsafe_index(a, [N_idx, C_idx, y_idx, x_idx])\n \n     def get_x_interp(y):\n-        coeffs_x = tuple((load_bounded(y, x_ofs) for x_ofs in ixs_ofs))\n+        coeffs_x = tuple(load_bounded(y, x_ofs) for x_ofs in ixs_ofs)\n         return _upsample_cubic_interp1d(coeffs_x, t_x)\n \n-    coeffs_y = tuple((get_x_interp(y_ofs) for y_ofs in iys_ofs))\n+    coeffs_y = tuple(get_x_interp(y_ofs) for y_ofs in iys_ofs)\n     result = _upsample_cubic_interp1d(coeffs_y, t_y)\n \n     # convert output to correct memory format, if necessary\ndiff --git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py\nindex 4d7c98aa222536..bf7a61b0793654 100644\n--- a/torch/_dynamo/debug_utils.py\n+++ b/torch/_dynamo/debug_utils.py\n@@ -385,11 +385,9 @@ def same_two_models(\n         # This means that the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return True.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph.\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph.\"\n         )\n         return True\n \n@@ -465,11 +463,9 @@ def backend_accuracy_fails(\n         # This means that the the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return False.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph\"\n         )\n         return False\n \ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..0bb1dbf7342a44 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -750,9 +750,7 @@ def __init__(\n \n         self.new_args = []\n         for i in range(0, len(flat_args)):\n-            arg = super(FlattenInputOutputSignature, self).placeholder(\n-                f\"arg{i}\", (), {}\n-            )\n+            arg = super().placeholder(f\"arg{i}\", (), {})\n             if i in matched_input_elements_to_fake:\n                 arg.node.meta[\"val\"] = matched_input_elements_to_fake[i]\n             else:\ndiff --git a/torch/_dynamo/output_graph.py b/torch/_dynamo/output_graph.py\nindex f6527f9b2de356..667b2363cff694 100644\n--- a/torch/_dynamo/output_graph.py\n+++ b/torch/_dynamo/output_graph.py\n@@ -1073,7 +1073,7 @@ class SubgraphTracer(fx.Tracer):\n     \"\"\"\n \n     def __init__(self, output_graph, parent=None):\n-        super(SubgraphTracer, self).__init__()\n+        super().__init__()\n         self.output_graph = weakref.proxy(output_graph)\n         self.graph = torch.fx.Graph()\n         # Map from graph input name to its placeholder proxy object, where the\ndiff --git a/torch/_dynamo/resume_execution.py b/torch/_dynamo/resume_execution.py\nindex a3344f3d69bca8..f3eafba1979470 100644\n--- a/torch/_dynamo/resume_execution.py\n+++ b/torch/_dynamo/resume_execution.py\n@@ -490,13 +490,13 @@ def find_new_offset(\n             instructions: List[Instruction], code_options: Dict[str, Any]\n         ):\n             nonlocal new_offset\n-            (target,) = [i for i in instructions if i.offset == offset]\n+            (target,) = (i for i in instructions if i.offset == offset)\n             # match the functions starting at the last instruction as we have added a prefix\n-            (new_target,) = [\n+            (new_target,) = (\n                 i2\n                 for i1, i2 in zip(reversed(instructions), reversed(meta.instructions))\n                 if i1 is target\n-            ]\n+            )\n             assert target.opcode == new_target.opcode\n             new_offset = new_target.offset\n \ndiff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py\nindex 75a44965b63a3e..46d7526ebbc57d 100644\n--- a/torch/_dynamo/symbolic_convert.py\n+++ b/torch/_dynamo/symbolic_convert.py\n@@ -888,7 +888,7 @@ def resolve_name(self, name, package, level):\n         if len(bits) < level:\n             raise ImportError(\"attempted relative import beyond top-level package\")\n         base = bits[0]\n-        return \"{}.{}\".format(base, name) if name else base\n+        return f\"{base}.{name}\" if name else base\n \n     def calc_package(self):\n         \"\"\"\n@@ -1840,7 +1840,7 @@ def format_frame_summary(self, additional_stack_frames=None):\n             additional_stack_frames = []\n         return \"\".join(\n             traceback.format_list(\n-                ([self.frame_summary()] + list(reversed(additional_stack_frames)))\n+                [self.frame_summary()] + list(reversed(additional_stack_frames))\n             )\n         )\n \ndiff --git a/torch/_dynamo/test_minifier_common.py b/torch/_dynamo/test_minifier_common.py\nindex 757e92d2f23b51..e1eadd6da8a595 100644\n--- a/torch/_dynamo/test_minifier_common.py\n+++ b/torch/_dynamo/test_minifier_common.py\n@@ -86,7 +86,7 @@ def _maybe_subprocess_run(self, args, *, isolate, cwd=None):\n                 args = [\"-c\"]\n             else:\n                 assert len(args) >= 2, args\n-                with open(args[1], \"r\") as f:\n+                with open(args[1]) as f:\n                     code = f.read()\n                 args = args[1:]\n \n@@ -156,7 +156,7 @@ def _run_test_code(self, code, *, isolate):\n     def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n         self.assertIsNotNone(repro_dir)\n         launch_file = os.path.join(repro_dir, \"minifier_launcher.py\")\n-        with open(launch_file, \"r\") as f:\n+        with open(launch_file) as f:\n             launch_code = f.read()\n         self.assertTrue(os.path.exists(launch_file))\n \n@@ -175,7 +175,7 @@ def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n     def _run_repro(self, repro_dir, *, isolate=True):\n         self.assertIsNotNone(repro_dir)\n         repro_file = os.path.join(repro_dir, \"repro.py\")\n-        with open(repro_file, \"r\") as f:\n+        with open(repro_file) as f:\n             repro_code = f.read()\n         self.assertTrue(os.path.exists(repro_file))\n \ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex d2aab5a65bd4d4..c720a0e637c478 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -368,12 +368,10 @@ def _wrap(self, value):\n         elif istype(\n             value, (dict, collections.defaultdict, collections.OrderedDict)\n         ) and all(\n-            (\n-                ConstantVariable.is_literal(k)\n-                or self.tensor_can_be_dict_key(k)\n-                or isinstance(k, enum.Enum)\n-                for k in value.keys()\n-            )\n+            ConstantVariable.is_literal(k)\n+            or self.tensor_can_be_dict_key(k)\n+            or isinstance(k, enum.Enum)\n+            for k in value.keys()\n         ):\n             if not value and self.get_source().is_nn_module():\n                 # It is faster to guard on 'false' property than to guard\ndiff --git a/torch/_dynamo/variables/misc.py b/torch/_dynamo/variables/misc.py\nindex b260eab0af0fdb..e049ccfb25269d 100644\n--- a/torch/_dynamo/variables/misc.py\n+++ b/torch/_dynamo/variables/misc.py\n@@ -880,7 +880,7 @@ def as_proxy(self):\n # Used to keep track of NULLs pushed on the stack for Python 3.11 function calls\n class NullVariable(VariableTracker):\n     def __init__(self, **kwargs):\n-        super(NullVariable, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n     def __str__(self):\n         return \"NullVariable\"\ndiff --git a/torch/_export/verifier.py b/torch/_export/verifier.py\nindex 41888230e02242..73906bb6a0d7a8 100644\n--- a/torch/_export/verifier.py\n+++ b/torch/_export/verifier.py\n@@ -38,7 +38,7 @@ def _check_is_fake_tensor(val):\n \n     val = node.meta.get(\"val\", None)\n     if val is None or not _check_is_fake_tensor(val):\n-        raise SpecViolationError(\"Node.meta {} is missing val field.\".format(node.name))\n+        raise SpecViolationError(f\"Node.meta {node.name} is missing val field.\")\n \n \n @compatibility(is_backward_compatible=False)\n@@ -71,7 +71,7 @@ def check_valid_op(self, op):\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         # All ops functional\n@@ -87,7 +87,7 @@ def check_valid(self, gm: GraphModule) -> None:  # noqa: C901\n             # TODO(T140410192): should have fake tensor for all dialects\n             if node.op in {\"call_module\", \"call_method\"}:\n                 raise SpecViolationError(\n-                    \"call_module is not valid: got a class '{}' \".format(node.target),\n+                    f\"call_module is not valid: got a class '{node.target}' \",\n                 )\n \n             if node.op == \"call_function\":\n@@ -122,7 +122,7 @@ def check_valid_op(self, op) -> None:\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         if (\ndiff --git a/torch/_functorch/eager_transforms.py b/torch/_functorch/eager_transforms.py\nindex a25a7bc456bec1..4ba72eb2152c6b 100644\n--- a/torch/_functorch/eager_transforms.py\n+++ b/torch/_functorch/eager_transforms.py\n@@ -548,7 +548,7 @@ def compute_jacobian_stacked():\n             # Iterate and concat the jacobians of different\n             # inputs.\n             for idx in range(len(flat_primals)):\n-                r = tuple((r_[idx] for r_ in chunked_results))\n+                r = tuple(r_[idx] for r_ in chunked_results)\n                 flat_results.append(torch.cat(r, 0))\n \n             return flat_results\ndiff --git a/torch/_functorch/pytree_hacks.py b/torch/_functorch/pytree_hacks.py\nindex 3694a53d7debb0..61bcdfbbf38b16 100644\n--- a/torch/_functorch/pytree_hacks.py\n+++ b/torch/_functorch/pytree_hacks.py\n@@ -13,7 +13,7 @@ def tree_map_(fn_, pytree):\n     return pytree\n \n \n-class PlaceHolder():\n+class PlaceHolder:\n     def __repr__(self):\n         return '*'\n \ndiff --git a/torch/_prims/__init__.py b/torch/_prims/__init__.py\nindex 7e8a37da76b06d..ac447dab410a07 100644\n--- a/torch/_prims/__init__.py\n+++ b/torch/_prims/__init__.py\n@@ -1437,7 +1437,7 @@ def expand_dims(\n     else:\n         dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]\n     if len(set(dims)) != len(dims):\n-        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))\n+        msg = f\"Received duplicate dimensions to expand in {str(dimensions)}\"\n         raise ValueError(msg)\n \n     new_shape = list(a.shape)\n@@ -1463,35 +1463,33 @@ def _slice_meta(\n     _strides = strides if strides is not None else [1] * len(start_indices)\n \n     if a.ndim != len(start_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with start_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with start_indices of length {}!\".format(\n             a.ndim, len(start_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(limit_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with limit_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with limit_indices of length {}!\".format(\n             a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(_strides):\n-        msg = (\n-            \"Attempting to slice tensor of rank {0} with strides of length {1}!\".format(\n-                a.ndim, len(limit_indices)\n-            )\n+        msg = \"Attempting to slice tensor of rank {} with strides of length {}!\".format(\n+            a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     for x, y in zip(start_indices, a.shape):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative start index of {0}!\".format(\n+            msg = \"Attempting to slice a tensor with a negative start index of {}!\".format(\n                 x\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than\"\n-                \" the length of its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a start index in {} is greater than\"\n+                \" the length of its corresponding dimension in shape {}\".format(\n                     start_indices, a.shape\n                 )\n             )\n@@ -1499,30 +1497,30 @@ def _slice_meta(\n \n     for x, y, z in zip(limit_indices, a.shape, start_indices):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative stop index of {0}!\".format(\n-                x\n+            msg = (\n+                \"Attempting to slice a tensor with a negative stop index of {}!\".format(\n+                    x\n+                )\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a stop index in {0} is greater than the length of \"\n-                \" its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a stop index in {} is greater than the length of \"\n+                \" its corresponding dimension in shape {}\".format(\n                     limit_indices, a.shape\n                 )\n             )\n             raise ValueError(msg)\n         if x < z:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than \"\n-                \" its corresponding stop index {1}\".format(x, z)\n+                \"Attempting to slice a tensor but a start index in {} is greater than \"\n+                \" its corresponding stop index {}\".format(x, z)\n             )\n \n     for x in _strides:\n         if x <= 0:\n-            msg = (\n-                \"Attempting to slice a tensor with a non-positive step of {0}!\".format(\n-                    x\n-                )\n+            msg = \"Attempting to slice a tensor with a non-positive step of {}!\".format(\n+                x\n             )\n             raise ValueError(msg)\n \n@@ -1581,38 +1579,38 @@ def _slice_in_dim_meta(\n     axis: int = 0,\n ) -> TensorLikeType:\n     if axis < 0:\n-        msg = \"slice_in_dim: received a negative axis {0}\".format(axis)\n+        msg = f\"slice_in_dim: received a negative axis {axis}\"\n         raise ValueError(msg)\n     if axis >= a.ndim:\n-        msg = \"slice_in_dim: axis {0} is greater or equal to the rank {1} of the tensor\".format(\n+        msg = \"slice_in_dim: axis {} is greater or equal to the rank {} of the tensor\".format(\n             axis, a.ndim\n         )\n         raise ValueError(msg)\n \n     if start_index < 0:\n-        msg = \"slice_in_dim: received a negative start_index {0}\".format(start_index)\n+        msg = f\"slice_in_dim: received a negative start_index {start_index}\"\n         raise ValueError(msg)\n \n     if start_index > a.shape[axis]:\n-        msg = \"slice_in_dim: start_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: start_index is greater than the length {} of dimension {}\".format(\n             start_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index > a.shape[axis]:\n-        msg = \"slice_in_dim: limit_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: limit_index is greater than the length {} of dimension {}\".format(\n             limit_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index < start_index:\n-        msg = \"slice_in_dim: received a limit_index {0} less than the start_index {1}\".format(\n+        msg = \"slice_in_dim: received a limit_index {} less than the start_index {}\".format(\n             limit_index, start_index\n         )\n         raise ValueError(msg)\n \n     if stride < 0:\n-        msg = \"slice_in_dim: received a non-positive stride of {0}!\".format(stride)\n+        msg = f\"slice_in_dim: received a non-positive stride of {stride}!\"\n         raise ValueError(msg)\n \n     start_indices = [0] * a.ndim\n@@ -1667,7 +1665,7 @@ def _split_dim_meta(a: TensorLikeType, dim: int, outer_length: int) -> TensorLik\n     inner_length = a.shape[dim] // outer_length\n \n     if (a.shape[dim] % outer_length) != 0:\n-        msg = \"Attempting to split dimension of length {0}, but outer length of {1} divides it with a remainder!\".format(\n+        msg = \"Attempting to split dimension of length {}, but outer length of {} divides it with a remainder!\".format(\n             a.shape[dim], outer_length\n         )\n         raise ValueError(msg)\n@@ -1746,13 +1744,13 @@ def _squeeze_meta(a: TensorLikeType, dimensions: Sequence) -> TensorLikeType:\n \n def _transpose_meta(a: TensorLikeType, permutation: DimsSequenceType) -> TensorLikeType:\n     if a.ndim != len(permutation):\n-        msg = \"Attempting to permute a tensor of rank {0}, but received a permutation of length {1}!\".format(\n+        msg = \"Attempting to permute a tensor of rank {}, but received a permutation of length {}!\".format(\n             a.ndim, len(permutation)\n         )\n         raise ValueError(msg)\n \n     if not utils.is_valid_permutation(a.ndim, permutation):\n-        msg = \"Received an invalid permutation, {0}!\".format(permutation)\n+        msg = f\"Received an invalid permutation, {permutation}!\"\n         raise ValueError(msg)\n \n     new_shape = [0] * a.ndim\n@@ -1938,7 +1936,7 @@ def _reshape_meta(a: TensorLikeType, shape: ShapeType):\n     # same number of elements\n     numel = reduce(operator.mul, shape)\n     if numel != a.numel():\n-        msg = \"Attempting to reshape a tensor with {0} elements to a shape with {1} elements!\".format(\n+        msg = \"Attempting to reshape a tensor with {} elements to a shape with {} elements!\".format(\n             a.numel(), numel\n         )\n         raise ValueError(msg)\n@@ -2190,7 +2188,7 @@ def _copy_to_meta(a: TensorLikeType, b: TensorLikeType):\n \n     # Validates the tensors have the same number of elements\n     if a.numel() != b.numel():\n-        msg = \"Attempting to copy {0} elements to a tensor with {1} elements!\".format(\n+        msg = \"Attempting to copy {} elements to a tensor with {} elements!\".format(\n             b.numel(), a.numel()\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_prims/executor.py b/torch/_prims/executor.py\nindex 2d8d815f063809..325ac67a665cc3 100644\n--- a/torch/_prims/executor.py\n+++ b/torch/_prims/executor.py\n@@ -28,7 +28,7 @@ def execute(\n     elif executor == \"strictly_nvfuser\":\n         return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)\n \n-    msg = \"Received unexpected value for 'executor': {0}. Allowed values are: aten, nvfuser.\".format(\n+    msg = \"Received unexpected value for 'executor': {}. Allowed values are: aten, nvfuser.\".format(\n         executor\n     )\n     raise ValueError(msg)\ndiff --git a/torch/_prims/nvfuser_executor.py b/torch/_prims/nvfuser_executor.py\nindex d0f51e928650c4..c1e61c1bb72f1f 100644\n--- a/torch/_prims/nvfuser_executor.py\n+++ b/torch/_prims/nvfuser_executor.py\n@@ -282,7 +282,7 @@ def nvfuser_execute(gm: GraphModule, *args, executor_parameters=None):\n \n         if get_nvprim_dump_nvtx():\n             torch.cuda.nvtx.range_push(\n-                \"fusion: {0}, graph: {1}\".format(\n+                \"fusion: {}, graph: {}\".format(\n                     fusion.id(),\n                     str(\n                         [\n@@ -475,7 +475,7 @@ def maybe_partition_graph(\n class NVTXInterpreter(torch.fx.Interpreter):\n     def run_node(self, n):\n         torch.cuda.nvtx.range_push(\n-            \"name: {0}, args: {1}, op: {2}, kwargs: {3}\".format(\n+            \"name: {}, args: {}, op: {}, kwargs: {}\".format(\n                 n.name, n.args, n.op, n.kwargs\n             )\n         )\ndiff --git a/torch/_prims_common/__init__.py b/torch/_prims_common/__init__.py\nindex f8033bb5780f74..4800966f3e2ff5 100644\n--- a/torch/_prims_common/__init__.py\n+++ b/torch/_prims_common/__init__.py\n@@ -120,11 +120,11 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n     assert isinstance(b, TensorLike)\n \n     if not same_shape(a.shape, b.shape):\n-        msg = \"Shapes {0} and {1} are not equal!\".format(a.shape, b.shape)\n+        msg = f\"Shapes {a.shape} and {b.shape} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.dtype != b.dtype:\n-        msg = \"Dtypes {0} and {1} are not equal!\".format(a.dtype, b.dtype)\n+        msg = f\"Dtypes {a.dtype} and {b.dtype} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.device != b.device:\n@@ -135,7 +135,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         ):\n             pass\n         else:\n-            msg = \"Devices {0} and {1} are not equal!\".format(a.device, b.device)\n+            msg = f\"Devices {a.device} and {b.device} are not equal!\"\n             raise AssertionError(msg)\n \n     # Stride checking is currently disabled, see https://github.com/pytorch/pytorch/issues/78050\n@@ -143,7 +143,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         same_strides, idx = check_significant_strides(a, b)\n         if not same_strides:\n             msg = (\n-                \"Stride mismatch! Strides are {0} and {1} (mismatched at {2})!\".format(\n+                \"Stride mismatch! Strides are {} and {} (mismatched at {})!\".format(\n                     a.stride(), b.stride(), idx\n                 )\n             )\n@@ -151,7 +151,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n \n         if a.storage_offset() != b.storage_offset():\n             msg = (\n-                \"Storage offset mismatch! Storage offsets are {0} and {1}!\".format(\n+                \"Storage offset mismatch! Storage offsets are {} and {}!\".format(\n                     a.storage_offset(), b.storage_offset()\n                 )\n             )\n@@ -584,7 +584,7 @@ def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int:\n \n     if _idx < 0 or _idx >= rank:\n         # Same error message as in aten/src/ATen/WrapDimUtils.h:49\n-        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(\n+        msg = \"Dimension out of range (expected to be in range of [{}, {}], but got {})\".format(\n             -rank, rank - 1, idx\n         )\n         raise IndexError(msg)\n@@ -710,7 +710,7 @@ def check_same_shape(*args, allow_cpu_scalar_tensors: bool):\n                 shape = arg.shape\n \n             if not is_same_shape(shape, arg.shape):\n-                msg = \"Shape {0} is not the expected shape {1}!\".format(\n+                msg = \"Shape {} is not the expected shape {}!\".format(\n                     arg.shape, shape\n                 )\n                 raise RuntimeError(msg)\n@@ -1102,7 +1102,7 @@ def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool:\n         if fn(cast_from):\n             return False\n \n-    raise ValueError(\"Received unknown dtypes {0}, {1}!\".format(cast_to, cast_from))\n+    raise ValueError(f\"Received unknown dtypes {cast_to}, {cast_from}!\")\n \n \n def check_same_dtype(*args):\n@@ -1340,7 +1340,7 @@ def elementwise_dtypes(\n     for x in args:\n         if not isinstance(x, (Number, TensorLike, sympy.Symbol)):\n             msg = (\n-                \"Unexpected type {0} when computing elementwise type promotion!\".format(\n+                \"Unexpected type {} when computing elementwise type promotion!\".format(\n                     str(type(x))\n                 )\n             )\n@@ -1424,7 +1424,7 @@ def _find_highest_dtype_filtered(\n         return get_computation_dtype(result_dtype), torch.bool\n     else:\n         raise ValueError(\n-            \"Unknown type promotion kind {0}\".format(str(type_promotion_kind))\n+            f\"Unknown type promotion kind {str(type_promotion_kind)}\"\n         )\n \n \n@@ -1648,8 +1648,8 @@ def check_in_bounds_for_storage(\n     required_length = compute_required_storage_length(shape, strides, storage_offset)\n     if a.size() < required_length:\n         msg = (\n-            \"Can't view a storage of size {0} with an offset of {1}, shape of {2}, and strides of {3}, \"\n-            \"which requires a storage of size {4}\".format(\n+            \"Can't view a storage of size {} with an offset of {}, shape of {}, and strides of {}, \"\n+            \"which requires a storage of size {}\".format(\n                 a.size(), storage_offset, str(shape), str(strides), required_length\n             )\n         )\n@@ -1671,9 +1671,9 @@ def check(\n     .. note:: This function is planned for removal in the future. Please use\n         `torch._check*` functions instead.\n     \"\"\"\n-    warnings.warn(DeprecationWarning((\n+    warnings.warn(DeprecationWarning(\n         \"'torch._prims_common.check' will be removed in the future. Please use \"\n-        \"'torch._check*' functions instead\")))\n+        \"'torch._check*' functions instead\"))\n     torch._check_with(exc_type, b, s)\n \n \ndiff --git a/torch/_prims_common/wrappers.py b/torch/_prims_common/wrappers.py\nindex 938465cac36318..c9755de3e0da63 100644\n--- a/torch/_prims_common/wrappers.py\n+++ b/torch/_prims_common/wrappers.py\n@@ -48,16 +48,16 @@ def _maybe_convert_to_dtype(a, dtype):\n         return None\n \n     raise ValueError(\n-        \"Received type {0} that is neither a tensor or a number!\".format(type(a))\n+        f\"Received type {type(a)} that is neither a tensor or a number!\"\n     )\n \n \n def _maybe_convert_to_type(a: NumberType, typ: type) -> NumberType:\n     if not isinstance(a, Number):\n-        msg = \"Found unknown type {0} when trying to convert scalars!\".format(type(a))\n+        msg = f\"Found unknown type {type(a)} when trying to convert scalars!\"\n         raise ValueError(msg)\n     if not utils.is_weakly_lesser_type(type(a), typ):\n-        msg = \"Scalar {0} of type {1} cannot be safely cast to type {2}!\".format(\n+        msg = \"Scalar {} of type {} cannot be safely cast to type {}!\".format(\n             a, type(a), typ\n         )\n         raise ValueError(msg)\n@@ -169,7 +169,7 @@ def _safe_copy_out(\n ):\n     # Checks same device\n     if copy_from.device != copy_to.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             copy_from.device, copy_to.device\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_refs/__init__.py b/torch/_refs/__init__.py\nindex 611c6e3b9a625a..53851d62a5eb44 100644\n--- a/torch/_refs/__init__.py\n+++ b/torch/_refs/__init__.py\n@@ -597,7 +597,7 @@ def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType:\n \n     python_type = utils.dtype_to_type(a.dtype)\n     if not utils.is_weakly_lesser_type(type(value), python_type):\n-        msg = \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+        msg = \"value argument of type {} cannot be safely cast to type {}!\".format(\n             type(value), python_type\n         )\n         raise ValueError(msg)\n@@ -997,10 +997,8 @@ def add(\n         if python_type != bool and not utils.is_weakly_lesser_type(\n             type(alpha), python_type\n         ):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         b = prims.mul(b, alpha)\n@@ -1069,7 +1067,7 @@ def copysign(\n     if isinstance(b, Number) and isinstance(a, Tensor):\n         b = scalar_tensor(b, dtype=a.dtype, device=a.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n-        msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+        msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n             a.device, b.device\n         )\n         raise RuntimeError(msg)\n@@ -1100,7 +1098,7 @@ def div(\n     else:\n         msg = (\n             \"div expected rounding_mode to be one of None, 'trunc', or 'floor' \"\n-            \"but found {0}.\".format(rounding_mode)\n+            \"but found {}.\".format(rounding_mode)\n         )\n         raise ValueError(msg)\n \n@@ -1218,7 +1216,7 @@ def floor_divide(\n         a = scalar_tensor(a, dtype=b.dtype, device=b.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n         if a.device == torch.device(\"cpu\"):\n-            msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+            msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n                 a.device, b.device\n             )\n             raise RuntimeError(msg)\n@@ -1378,19 +1376,19 @@ def _check_close_args(\n ) -> None:\n     torch._check_value(\n         a.dtype == b.dtype,\n-        lambda: \"{0}: Attempting to compare tensors of different dtypes {1} and {2}!\".format(\n+        lambda: \"{}: Attempting to compare tensors of different dtypes {} and {}!\".format(\n             name, a.dtype, b.dtype\n         ),\n     )\n     torch._check(\n         rtol >= 0,\n-        lambda: \"{0}: rtol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: rtol must be greater than or equal to zero, but got {}!\".format(\n             name, rtol\n         ),\n     )\n     torch._check(\n         atol >= 0,\n-        lambda: \"{0}: atol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: atol must be greater than or equal to zero, but got {}!\".format(\n             name, atol\n         ),\n     )\n@@ -1664,10 +1662,8 @@ def sub(\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         if isinstance(b, torch.Tensor):\n@@ -1759,7 +1755,7 @@ def addcdiv(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1788,7 +1784,7 @@ def addcmul(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1892,7 +1888,7 @@ def clone(\n \n def copy_to(a: Tensor, b: Tensor, *, allow_cross_device=True):\n     if not allow_cross_device and a.device != b.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             b.device, a.device\n         )\n         raise RuntimeError(msg)\n@@ -2098,7 +2094,7 @@ def _reduction(\n     assert isinstance(a, TensorLike)\n     if a.ndim > 64:\n         raise RuntimeError(\n-            \"Received a tensor with {0} dimensions, but only tensors with up to 64 dims are supported!\".format(\n+            \"Received a tensor with {} dimensions, but only tensors with up to 64 dims are supported!\".format(\n                 a.ndim\n             )\n         )\n@@ -2864,7 +2860,7 @@ def expand_as(a: Tensor, b: Tensor) -> Tensor:\n \n def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> Tuple[TensorLikeType, ...]:\n     if chunks <= 0:\n-        msg = \"Expected at least one chunk, but got {0}!\".format(chunks)\n+        msg = f\"Expected at least one chunk, but got {chunks}!\"\n         raise ValueError(msg)\n \n     dim = utils.canonicalize_dim(a.ndim, dim)\n@@ -3346,7 +3342,7 @@ def _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorL\n                 if allow_copy:\n                     return prims.reshape(a, shape)\n \n-                msg = \"Cannot view a tensor with shape {0} and strides {1} as a tensor with shape {2}!\".format(\n+                msg = \"Cannot view a tensor with shape {} and strides {} as a tensor with shape {}!\".format(\n                     a.shape, a.stride(), shape\n                 )\n                 raise ValueError(msg)\n@@ -3704,13 +3700,13 @@ def tensor_split(\n     # If indices_or_sections is a tensor, it must be a CPU Long tensor\n     if isinstance(indices_or_sections, TensorLike):\n         if not indices_or_sections.device.type == \"cpu\":\n-            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {0}\".format(\n+            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {}\".format(\n                 indices_or_sections.device\n             )\n             raise ValueError(msg)\n         if indices_or_sections.dtype != torch.long:\n             msg = \"tensor_split: if indices_or_sections is a tensor it must have long dtype, \"\n-            \" but received one with dtype {0}\".format(indices_or_sections.dtype)\n+            f\" but received one with dtype {indices_or_sections.dtype}\"\n             raise ValueError(msg)\n \n     # Case 0 -- indices_or_sections is an integer or a scalar tensor n and a is split along dim into n parts of equal-ish length\n@@ -3724,7 +3720,7 @@ def tensor_split(\n         )\n \n         if sections <= 0:\n-            msg = \"tensor_split: number of sections must be greater than 0, but was {0}\".format(\n+            msg = \"tensor_split: number of sections must be greater than 0, but was {}\".format(\n                 sections\n             )\n             raise ValueError(msg)\n@@ -3751,7 +3747,7 @@ def tensor_split(\n         if isinstance(indices_or_sections, TensorLike):\n             if indices_or_sections.ndim != 1:\n                 msg = \"tensor_split: non-scalar indices_or_sections tensors must have only one dimension, \"\n-                \"but received a tensor with {0} dimensions\".format(\n+                \"but received a tensor with {} dimensions\".format(\n                     indices_or_sections.ndim\n                 )\n                 raise ValueError(msg)\ndiff --git a/torch/_refs/nn/functional/__init__.py b/torch/_refs/nn/functional/__init__.py\nindex eaa6618379f356..ba00179c4b2d6f 100644\n--- a/torch/_refs/nn/functional/__init__.py\n+++ b/torch/_refs/nn/functional/__init__.py\n@@ -167,10 +167,8 @@ def celu(\n     if alpha is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         rhs = alpha * torch.expm1(torch.true_divide(a, alpha))  # type: ignore[arg-type]\n@@ -437,7 +435,7 @@ def softplus(\n     if beta is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(beta), python_type):\n-            msg = \"beta argument of type {0} cannot be safely cast to type {1}!\".format(\n+            msg = \"beta argument of type {} cannot be safely cast to type {}!\".format(\n                 type(beta), python_type\n             )\n             raise ValueError(msg)\n@@ -610,11 +608,9 @@ def margin_ranking_loss(\n     # loss_without_reduction = max(0, \u2212target * (input1 \u2212 input2) + margin)\n     if input1.ndim != input2.ndim or input1.ndim != target.ndim:\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(\n-                    input1.shape, input2.shape, target.shape\n-                )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(\n+                input1.shape, input2.shape, target.shape\n             )\n         )\n     _check_reduction_value(reduction)\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint.py b/torch/fx/experimental/migrate_gradual_types/constraint.py\nindex bab7c62347bbd9..0f0d23d0187490 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from torch.fx.experimental.migrate_gradual_types.operation import op_add, op_sub, op_mul, op_div, \\\n     op_mod, op_gt, op_lt, op_neq, op_eq\n from torch.fx.tensor_type import TensorType, Dyn\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\nindex fc1fae790d8300..153a8407fc4113 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n@@ -75,7 +75,7 @@ def transform_index_select(constraint, counter):\n     # if the index is valid then replace the input dimension with the new dimension\n     # otherwise the dimension will not be replaced and the clause will contain False\n     if is_valid_index == T():\n-        new_dims = copy.deepcopy((dims))\n+        new_dims = copy.deepcopy(dims)\n         new_dims[constraint.index] = constraint.dim_replace\n \n     transformed_constraint = Conj([BinConstraintT(constraint.input_var, TensorType(dims), op_eq),\n@@ -803,7 +803,7 @@ def apply_padding(e1_var: TVar,\n         broadcast_padding = []\n \n         # for every padding size, we also consider broadcasting\n-        for j in range((len(d2) - i)):\n+        for j in range(len(d2) - i):\n             broadcast_padding.append(broadcast_dim(simulate_padding, d2, d11, d12, j, True))\n \n         # we consider the possibilities for broadcasting for every dimension. Since we already\ndiff --git a/torch/fx/experimental/migrate_gradual_types/operation.py b/torch/fx/experimental/migrate_gradual_types/operation.py\nindex 68bba2d59a7608..ec2cb91bbcc179 100644\n--- a/torch/fx/experimental/migrate_gradual_types/operation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/operation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n op_add = '+'\n op_sub = '-'\n op_mul = '*'\ndiff --git a/torch/fx/experimental/symbolic_shapes.py b/torch/fx/experimental/symbolic_shapes.py\nindex 90a3c519b3c44b..70762cf1f81b8c 100644\n--- a/torch/fx/experimental/symbolic_shapes.py\n+++ b/torch/fx/experimental/symbolic_shapes.py\n@@ -1488,7 +1488,7 @@ def _print_Symbol(self, expr) -> str:\n         return self.print_source(self.symbol_to_source[expr][0])\n \n     def _print_Relational(self, expr):\n-        return '%s %s %s' % (\n+        return '{} {} {}'.format(\n             self.parenthesize(expr.lhs, precedence(expr)),\n             expr.rel_op,\n             self.parenthesize(expr.rhs, precedence(expr))\n@@ -1887,7 +1887,7 @@ def print_results(grouped, indent, result_fn):\n class ShapeEnvLoggerAdapter(logging.LoggerAdapter):\n     def process(self, msg, kwargs):\n         # TODO: Maybe suppress the envid if not DEBUG?\n-        return '%s: %s' % (self.extra['envid'], msg), kwargs\n+        return '{}: {}'.format(self.extra['envid'], msg), kwargs\n \n \n ENV_COUNTER = collections.Counter()\ndiff --git a/torch/fx/experimental/unification/multipledispatch/dispatcher.py b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\nindex c76d8c60b097c7..ac8bc7d8dd159c 100644\n--- a/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n+++ b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n@@ -205,10 +205,9 @@ def add(self, signature, func):\n             if not isinstance(typ, (type, list)):\n                 str_sig = ', '.join(c.__name__ if isinstance(c, type)\n                                     else str(c) for c in signature)\n-                raise TypeError(\"Tried to dispatch on non-type: %s\\n\"\n-                                \"In signature: <%s>\\n\"\n-                                \"In function: %s\" %\n-                                (typ, str_sig, self.name))\n+                raise TypeError(\"Tried to dispatch on non-type: {}\\n\"\n+                                \"In signature: <{}>\\n\"\n+                                \"In function: {}\".format(typ, str_sig, self.name))\n \n             # handle variadic signatures\n             if isinstance(typ, list):\n@@ -257,8 +256,7 @@ def __call__(self, *args, **kwargs):\n             func = self.dispatch(*types)\n             if not func:\n                 raise NotImplementedError(\n-                    'Could not find signature for %s: <%s>' %\n-                    (self.name, str_signature(types))) from e\n+                    f'Could not find signature for {self.name}: <{str_signature(types)}>') from e\n             self._cache[types] = func\n         try:\n             return func(*args, **kwargs)\n@@ -274,7 +272,7 @@ def __call__(self, *args, **kwargs):\n \n             raise NotImplementedError(\n                 \"Matching functions for \"\n-                \"%s: <%s> found, but none completed successfully\" % (\n+                \"{}: <{}> found, but none completed successfully\".format(\n                     self.name, str_signature(types),),) from e\n \n     def __str__(self):\n@@ -408,8 +406,7 @@ def __call__(self, *args, **kwargs):\n         types = tuple([type(arg) for arg in args])\n         func = self.dispatch(*types)\n         if not func:\n-            raise NotImplementedError('Could not find signature for %s: <%s>' %\n-                                      (self.name, str_signature(types)))\n+            raise NotImplementedError(f'Could not find signature for {self.name}: <{str_signature(types)}>')\n         return func(self.obj, *args, **kwargs)\n \n \ndiff --git a/torch/fx/interpreter.py b/torch/fx/interpreter.py\nindex 7bc5e55288b03c..6ee5706f92ee34 100644\n--- a/torch/fx/interpreter.py\n+++ b/torch/fx/interpreter.py\n@@ -139,7 +139,7 @@ def run(self, *args, initial_env : Optional[Dict[Node, Any]] = None, enable_io_p\n             except Exception as e:\n                 if self.extra_traceback:\n                     msg = f\"While executing {node.format_node()}\"\n-                    msg = '{}\\n\\n{}'.format(e.args[0], msg) if e.args else str(msg)\n+                    msg = f'{e.args[0]}\\n\\n{msg}' if e.args else str(msg)\n                     msg += f\"\\nOriginal traceback:\\n{node.stack_trace}\"\n                     e.args = (msg,) + e.args[1:]\n                     if isinstance(e, KeyError):\ndiff --git a/torch/fx/passes/utils/matcher_utils.py b/torch/fx/passes/utils/matcher_utils.py\nindex 1037b48d8eb61d..8b66561a6e280d 100644\n--- a/torch/fx/passes/utils/matcher_utils.py\n+++ b/torch/fx/passes/utils/matcher_utils.py\n@@ -30,7 +30,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class InternalMatch():\n+class InternalMatch:\n     # Nodes from which the match was found\n     anchors: List[Node]\n     # Maps nodes in the pattern subgraph to nodes in the larger graph\ndiff --git a/torch/fx/passes/utils/source_matcher_utils.py b/torch/fx/passes/utils/source_matcher_utils.py\nindex e00b9695742e36..da8cf9f0f168c4 100644\n--- a/torch/fx/passes/utils/source_matcher_utils.py\n+++ b/torch/fx/passes/utils/source_matcher_utils.py\n@@ -29,7 +29,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class SourcePartition():\n+class SourcePartition:\n     # Nodes in a particular partition\n     nodes: List[Node]\n \n"
  },
  {
    "number": 105401,
    "title": "[BE] Enable ruff's UP rules and autoformat inductor/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "79c4d187f0d0fae017727cd77b873caf2b452ed1",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105401",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105401/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105401.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105401.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105401/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105401/comments",
    "labels": [
      "open source",
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T01:12:55.272637Z",
    "state": "closed",
    "patch": "From 70d3668b5d78e1b6b6101e62609765c438f56d77 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:48 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat inductor/\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py                    |  2 +-\n test/inductor/test_cuda_repro.py                   |  2 +-\n test/inductor/test_cudagraph_trees.py              |  2 +-\n test/inductor/test_fused_attention.py              |  4 ++--\n test/inductor/test_mkldnn_pattern_matcher.py       |  8 ++++----\n test/inductor/test_profiler.py                     |  4 ++--\n test/inductor/test_standalone_compile.py           |  2 +-\n test/inductor/test_torchinductor.py                |  6 +++---\n .../test_torchinductor_codegen_dynamic_shapes.py   | 14 +++++++-------\n torch/_inductor/codecache.py                       |  6 +++---\n torch/_inductor/codegen/cpp.py                     |  2 +-\n torch/_inductor/ir.py                              | 10 +++++-----\n torch/_inductor/lowering.py                        |  6 +++---\n torch/_inductor/triton_heuristics.py               |  6 ++----\n torch/_inductor/utils.py                           |  2 +-\n 15 files changed, 37 insertions(+), 39 deletions(-)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 89ea4315b1de6c..2074d8451be9cc 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -93,7 +93,7 @@ def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n     def test_conv2d_bn_mixed_dtype(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     3,\n                     16,\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..7df54f0d35a146 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -783,7 +783,7 @@ def forward(inductor_seeds, mul_4, view_15):\n     def test_issue100806(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.linear1 = torch.nn.Linear(10, 20)\n                 self.linear2 = torch.nn.Linear(20, 30)\n                 self.relu = torch.nn.ReLU()\ndiff --git a/test/inductor/test_cudagraph_trees.py b/test/inductor/test_cudagraph_trees.py\nindex 85e44c1e272f9d..8181484c419b58 100644\n--- a/test/inductor/test_cudagraph_trees.py\n+++ b/test/inductor/test_cudagraph_trees.py\n@@ -137,7 +137,7 @@ def tearDown(self):\n \n         def get_manager(self, device_index=None):\n             return torch._inductor.cudagraph_trees.get_container(\n-                (self.device_idx if not device_index else device_index)\n+                self.device_idx if not device_index else device_index\n             ).tree_manager\n \n         def get_roots(self):\ndiff --git a/test/inductor/test_fused_attention.py b/test/inductor/test_fused_attention.py\nindex e509c97b0cb04a..7163ac24de5cf3 100644\n--- a/test/inductor/test_fused_attention.py\n+++ b/test/inductor/test_fused_attention.py\n@@ -297,7 +297,7 @@ def test_pattern_fails_with_tensor_factor(self):\n         # https://github.com/pytorch/pytorch/issues/99124\n         class Model(torch.nn.Module):\n             def __init__(self, is_inv_factor):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.is_inv_factor = is_inv_factor\n \n             def forward(self, query, key, value, scale_factor) -> torch.Tensor:\n@@ -328,7 +328,7 @@ class Model(torch.nn.Module):\n             def __init__(\n                 self,\n             ):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, query, key, value, attn_mask) -> torch.Tensor:\n                 attn_weight = torch.softmax(\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex 3e07c0181994cf..9337a70b2f857e 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -374,7 +374,7 @@ def forward(self, x, negative_slope):\n     def test_conv2d_add_scalar(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n                 )\n@@ -476,7 +476,7 @@ def forward(self, x, other, alpha):\n         # we can't do the fusion when add's inputs are same tensor.\n         class Model2(torch.nn.Module):\n             def __init__(self):\n-                super(Model2, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -490,7 +490,7 @@ def forward(self, x):\n         # we can't do the fusion when add's inputs are mixed dtype.\n         class Model3(torch.nn.Module):\n             def __init__(self):\n-                super(Model3, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -526,7 +526,7 @@ def forward(self, x):\n     def test_reproduce_99842_issue(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n \n             def forward(self, input_tensor):\ndiff --git a/test/inductor/test_profiler.py b/test/inductor/test_profiler.py\nindex eebe884da46b33..68f44df8b9f052 100644\n--- a/test/inductor/test_profiler.py\n+++ b/test/inductor/test_profiler.py\n@@ -21,14 +21,14 @@ def test_inductor_profiling_triton_launch(self):\n         def fn(x, y):\n             return (x + y).sin().cos()\n \n-        x, y = [torch.rand((4, 4), device=\"cuda\") for _ in range(2)]\n+        x, y = (torch.rand((4, 4), device=\"cuda\") for _ in range(2))\n \n         with torch.profiler.profile() as prof:\n             fn(x, y)\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with open(fname, \"r\") as f:\n+            with open(fname) as f:\n                 trace_json = json.load(f)\n \n         self.assertTrue(\"traceEvents\" in trace_json)\ndiff --git a/test/inductor/test_standalone_compile.py b/test/inductor/test_standalone_compile.py\nindex c424c76244cc0e..88c528c891a4fc 100644\n--- a/test/inductor/test_standalone_compile.py\n+++ b/test/inductor/test_standalone_compile.py\n@@ -100,7 +100,7 @@ def test_inductor_via_export2(self):\n     def test_inductor_via_op_with_multiple_outputs(self):\n         x1 = torch.randn((2, 512, 128))\n         x2 = [128]\n-        x3 = torch.randn((128))\n+        x3 = torch.randn(128)\n         x4 = torch.randn((128,))\n         x5 = 1e-6\n         mod, inp = gen_gm_and_inputs(\ndiff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py\nindex 520837d3865a8d..45ee70cd8067ee 100644\n--- a/test/inductor/test_torchinductor.py\n+++ b/test/inductor/test_torchinductor.py\n@@ -2354,7 +2354,7 @@ def fn(x):\n     def test_adaptive_avg_pool2d_low_prec(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n \n             def forward(self, x):\n@@ -5995,7 +5995,7 @@ def fn(x, y):\n \n         self.common(\n             fn,\n-            [torch.randn((4, 2)), torch.randn((4))],\n+            [torch.randn((4, 2)), torch.randn(4)],\n         )\n \n     # Shape padding causes the inputs to all get specialized, so the codegen\n@@ -6047,7 +6047,7 @@ def test_sqrt_dynamic_shapes(self):\n \n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 B, N, C = x.shape\ndiff --git a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\nindex 8f4086af099e64..0cdfbd54ffbe3b 100644\n--- a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n+++ b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n@@ -124,14 +124,14 @@ def run(*ex, **kwargs):\n     \"test_expand_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_glu_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_isinf2_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_layer_norm_dynamic_shapes\": TestFailure((\"cuda\")),\n+    \"test_layer_norm_dynamic_shapes\": TestFailure(\"cuda\"),\n     \"test_linspace1_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_stack_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor2_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor3_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_to_device_constant_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_to_device_constant_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_upsample_nearest2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views3_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views4_dynamic_shapes\": TestFailure((\"cpu\",)),\n@@ -161,9 +161,9 @@ def run(*ex, **kwargs):\n     \"test_empty2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_empty_strided_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_index3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_inductor_bucketize_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_inductor_bucketize_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_like_rands_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n@@ -194,7 +194,7 @@ def run(*ex, **kwargs):\n     \"test_views6_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_view_detach_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_view_on_aliased_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_linear_float64_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_linear_float64_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_adaptive_avg_pool_with_output_size_0_dynamic_shapes\": TestFailure(\n         (\"cpu\", \"cuda\")\n     ),\n@@ -288,7 +288,7 @@ def run(*ex, **kwargs):\n \n if TEST_WITH_ROCM:\n     # aten.miopen_batch_norm is not registered for lowering\n-    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure((\"cuda\"))\n+    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure(\"cuda\")\n \n DynamicShapesCodegenCommonTemplate = make_dynamic_cls(\n     CommonTemplate, xfail_prop=\"_expected_failure_codegen_dynamic\"\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex fd1f4228c9d29a..8ac73569fe1c2a 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -158,7 +158,7 @@ def __init__(self):\n     def get_local_cache(self):\n         if not self.local_cache_path.is_file():\n             return {}\n-        with open(self.local_cache_path, \"r\") as local_cache_fp:\n+        with open(self.local_cache_path) as local_cache_fp:\n             local_cache = json.load(local_cache_fp)\n         return local_cache[\"cache\"]\n \n@@ -201,7 +201,7 @@ class PersistentCache(CacheBase):\n     def get_global_cache(self):\n         if self.global_cache_path is None or not self.global_cache_path.is_file():\n             return {}\n-        with open(self.global_cache_path, \"r\") as global_cache_fp:\n+        with open(self.global_cache_path) as global_cache_fp:\n             global_cache = json.load(global_cache_fp)\n         return global_cache[\"cache\"]\n \n@@ -844,7 +844,7 @@ def wrapper_call(*args):\n # - valid_vec_isa_list()\n # - VecISA.__bool__() <-- takes out a lock\n # - compile_file() <-- imports cpp_prefix_path from cpp, which causes us to try to take out the same lock.\n-@functools.lru_cache()\n+@functools.lru_cache\n def cpp_prefix_path():\n     path = Path(__file__).parent / \"codegen/cpp_prefix.h\"\n     with path.open() as f:\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex bb2469ebffd33d..f844160b7255c0 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -278,7 +278,7 @@ def parallel_num_threads():\n     return threads\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def stride_at(var: sympy.Symbol, index: sympy.Expr):\n     replacement = {var: var + 1}\n     new_index = sympy_subs(index, replacement)\ndiff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py\nindex 869a0fb60861f6..c8172ff36b3dcf 100644\n--- a/torch/_inductor/ir.py\n+++ b/torch/_inductor/ir.py\n@@ -3042,7 +3042,7 @@ class InplaceBernoulliFallback(ExternKernel):\n     kernel = \"aten.bernoulli_\"\n \n     def codegen(self, wrapper):\n-        (x,) = [t.codegen_reference() for t in self.inputs]\n+        (x,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(\n             f\"{self.kernel}({x}, {', '.join(map(repr, self.constant_args))})\"\n         )\n@@ -3073,9 +3073,9 @@ class ScatterFallback(ExternKernel):\n \n     def codegen(self, wrapper):\n         if self.src_is_tensor:\n-            (x, index, src) = [t.codegen_reference() for t in self.inputs]\n+            (x, index, src) = (t.codegen_reference() for t in self.inputs)\n         else:\n-            (x, index) = [t.codegen_reference() for t in self.inputs]\n+            (x, index) = (t.codegen_reference() for t in self.inputs)\n             src = self.constant_args[1]\n         wrapper.generate_scatter_fallback(\n             x,\n@@ -3156,7 +3156,7 @@ class IndexPutFallback(ExternKernel):\n     \"\"\"\n \n     def codegen(self, wrapper):\n-        (x, values, *valid_indices) = [t.codegen_reference() for t in self.inputs]\n+        (x, values, *valid_indices) = (t.codegen_reference() for t in self.inputs)\n         indices = []\n         iter_valid_indices = iter(valid_indices)\n         for i, _ in enumerate(self.indices):\n@@ -4694,7 +4694,7 @@ def codegen(self, wrapper):\n         wrapper.add_import_once(\n             \"from torch.distributed._functional_collectives_impl import _wait_tensor\"\n         )\n-        (input_collective,) = [t.codegen_reference() for t in self.inputs]\n+        (input_collective,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(f\"{input_collective} = _wait_tensor({input_collective})\")\n \n         # wait op still needs to produce a 'buffer' that represents the tensor output.\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 3f17db1e5fed3a..14430ace100534 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -2871,11 +2871,11 @@ def load_bounded(fy, fx):\n \n         iy = ops.to_dtype(in_y, get_int_dtype(iH + 1))\n         ix = ops.to_dtype(in_x, get_int_dtype(iW + 1))\n-        iys_ofs = tuple((ops.add(iy, ofs) for ofs in (-1, 0, 1, 2)))\n-        ixs_ofs = tuple((ops.add(ix, ofs) for ofs in (-1, 0, 1, 2)))\n+        iys_ofs = tuple(ops.add(iy, ofs) for ofs in (-1, 0, 1, 2))\n+        ixs_ofs = tuple(ops.add(ix, ofs) for ofs in (-1, 0, 1, 2))\n \n         def get_x_interp(y):\n-            coeffs_x = tuple((load_bounded(y, x) for x in ixs_ofs))\n+            coeffs_x = tuple(load_bounded(y, x) for x in ixs_ofs)\n             return cubic_interp1d(coeffs_x, t_x)\n \n         coeffs_y = tuple(get_x_interp(y) for y in iys_ofs)\ndiff --git a/torch/_inductor/triton_heuristics.py b/torch/_inductor/triton_heuristics.py\nindex 61027661111e5a..88fa275f8b0102 100644\n--- a/torch/_inductor/triton_heuristics.py\n+++ b/torch/_inductor/triton_heuristics.py\n@@ -482,9 +482,7 @@ def hash_configs(configs: List[Config]):\n     hasher = hashlib.sha256()\n     for cfg in configs:\n         hasher.update(\n-            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode(\n-                \"utf-8\"\n-            )\n+            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode()\n         )\n     return hasher.hexdigest()\n \n@@ -498,7 +496,7 @@ def load_cached_autotuning(\n     if not os.path.exists(cache_filename):\n         return None\n \n-    with open(cache_filename, \"r\") as fd:\n+    with open(cache_filename) as fd:\n         best_config = json.loads(fd.read())\n     if best_config.pop(\"configs_hash\", None) != configs_hash:\n         return None\ndiff --git a/torch/_inductor/utils.py b/torch/_inductor/utils.py\nindex 538d0a2040fb93..c604c45d53d32a 100644\n--- a/torch/_inductor/utils.py\n+++ b/torch/_inductor/utils.py\n@@ -688,7 +688,7 @@ def run_and_get_code(fn, *args, **kwargs):\n \n     def patched_compile_to_module(self):\n         mod = compile_to_module(self)\n-        with open(mod.__file__, \"r\") as f:\n+        with open(mod.__file__) as f:\n             source_codes.append(f.read())\n         return mod\n \n"
  },
  {
    "number": 105400,
    "title": "[BE] Enable ruff's UP rules and autoformat ao/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "eafcfb8625a5c401512b77badfbc011865f3bcc2",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105400",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105400/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105400.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105400.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105400/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105400/comments",
    "labels": [
      "release notes: AO frontend",
      "open source",
      "release notes: quantization"
    ],
    "_event_time": "2023-07-18T01:12:51.669148Z",
    "state": "closed",
    "patch": "From 2a612e3470e792e48e00c1642c6fdcfc77d0d415 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:44 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat ao/\n\n[ghstack-poisoned]\n---\n .../ao/sparsity/test_activation_sparsifier.py |  1 -\n test/ao/sparsity/test_composability.py        |  1 -\n test/ao/sparsity/test_data_scheduler.py       |  1 -\n test/ao/sparsity/test_data_sparsifier.py      |  1 -\n test/ao/sparsity/test_kernels.py              |  1 -\n test/ao/sparsity/test_parametrization.py      |  1 -\n test/ao/sparsity/test_scheduler.py            |  1 -\n test/ao/sparsity/test_sparsifier.py           |  1 -\n test/ao/sparsity/test_sparsity_utils.py       |  1 -\n .../ao/sparsity/test_structured_sparsifier.py |  1 -\n torch/ao/nn/intrinsic/modules/fused.py        |  2 +-\n .../ao/nn/intrinsic/qat/modules/conv_fused.py | 12 +++----\n .../nn/intrinsic/qat/modules/linear_relu.py   |  2 +-\n .../quantized/dynamic/modules/linear_relu.py  |  2 +-\n .../nn/intrinsic/quantized/modules/bn_relu.py |  4 +--\n .../intrinsic/quantized/modules/conv_relu.py  |  6 ++--\n .../quantized/modules/linear_relu.py          |  2 +-\n torch/ao/nn/quantizable/modules/activation.py |  6 ++--\n torch/ao/nn/quantized/dynamic/modules/conv.py |  1 -\n .../ao/nn/quantized/dynamic/modules/linear.py |  2 +-\n torch/ao/nn/quantized/dynamic/modules/rnn.py  | 32 +++++++++----------\n torch/ao/nn/quantized/modules/__init__.py     |  2 +-\n torch/ao/nn/quantized/modules/conv.py         |  5 ++-\n torch/ao/nn/quantized/modules/linear.py       |  2 +-\n .../ao/nn/quantized/reference/modules/rnn.py  |  2 +-\n .../ao/nn/sparse/quantized/dynamic/linear.py  |  2 +-\n torch/ao/ns/fx/ns_types.py                    |  8 ++---\n .../data_scheduler/base_data_scheduler.py     |  4 +--\n torch/ao/pruning/scheduler/base_scheduler.py  |  4 +--\n torch/ao/pruning/scheduler/cubic_scheduler.py |  1 -\n .../quantization/_learnable_fake_quantize.py  |  4 +--\n .../backend_config/backend_config.py          |  3 +-\n .../ao/quantization/backend_config/onednn.py  | 12 +++----\n .../quantization/experimental/APoT_tensor.py  |  2 +-\n .../ao/quantization/experimental/quantizer.py |  2 +-\n torch/ao/quantization/fake_quantize.py        |  2 +-\n torch/ao/quantization/fuse_modules.py         |  2 +-\n .../ao/quantization/fuser_method_mappings.py  | 10 +++---\n torch/ao/quantization/fx/_equalize.py         |  2 +-\n .../fx/_lower_to_native_backend.py            |  2 +-\n .../quantization/fx/_model_report/detector.py |  8 ++---\n .../_model_report/model_report_visualizer.py  |  2 +-\n torch/ao/quantization/fx/convert.py           |  2 +-\n torch/ao/quantization/fx/custom_config.py     |  9 ++----\n torch/ao/quantization/fx/prepare.py           |  2 +-\n torch/ao/quantization/fx/utils.py             | 24 ++++++--------\n torch/ao/quantization/observer.py             | 18 +++++------\n torch/ao/quantization/pt2e/prepare.py         |  4 +--\n .../quantization/pt2e/quantizer/quantizer.py  | 21 ++----------\n torch/ao/quantization/qconfig.py              |  8 ++---\n .../ao/quantization/quantization_mappings.py  | 10 +++---\n torch/ao/quantization/quantize.py             |  2 +-\n torch/ao/quantization/utils.py                |  2 +-\n 53 files changed, 114 insertions(+), 150 deletions(-)\n\ndiff --git a/test/ao/sparsity/test_activation_sparsifier.py b/test/ao/sparsity/test_activation_sparsifier.py\nindex 573a40762c31cc..01bdfa045da9d1 100644\n--- a/test/ao/sparsity/test_activation_sparsifier.py\n+++ b/test/ao/sparsity/test_activation_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import copy\ndiff --git a/test/ao/sparsity/test_composability.py b/test/ao/sparsity/test_composability.py\nindex 85d78c49ea54ae..cb799f714ca17b 100644\n--- a/test/ao/sparsity/test_composability.py\n+++ b/test/ao/sparsity/test_composability.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_data_scheduler.py b/test/ao/sparsity/test_data_scheduler.py\nindex 9c33a160e76836..ab7c051c21077a 100644\n--- a/test/ao/sparsity/test_data_scheduler.py\n+++ b/test/ao/sparsity/test_data_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_data_sparsifier.py b/test/ao/sparsity/test_data_sparsifier.py\nindex 81a899f6932a0d..9248a371826ebd 100644\n--- a/test/ao/sparsity/test_data_sparsifier.py\n+++ b/test/ao/sparsity/test_data_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_kernels.py b/test/ao/sparsity/test_kernels.py\nindex 4786557ceb3be7..111d51465be109 100644\n--- a/test/ao/sparsity/test_kernels.py\n+++ b/test/ao/sparsity/test_kernels.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.testing._internal.common_utils import run_tests\ndiff --git a/test/ao/sparsity/test_parametrization.py b/test/ao/sparsity/test_parametrization.py\nindex 54b6f778d9fa8f..02f7cc6db7fddf 100644\n--- a/test/ao/sparsity/test_parametrization.py\n+++ b/test/ao/sparsity/test_parametrization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_scheduler.py b/test/ao/sparsity/test_scheduler.py\nindex 52eb54cb9ecb96..835c5143f18bc2 100644\n--- a/test/ao/sparsity/test_scheduler.py\n+++ b/test/ao/sparsity/test_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch import nn\ndiff --git a/test/ao/sparsity/test_sparsifier.py b/test/ao/sparsity/test_sparsifier.py\nindex 4c79416a78dd69..c9309d4b81fe5b 100644\n--- a/test/ao/sparsity/test_sparsifier.py\n+++ b/test/ao/sparsity/test_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import itertools\ndiff --git a/test/ao/sparsity/test_sparsity_utils.py b/test/ao/sparsity/test_sparsity_utils.py\nindex 90aad10ab18db6..9a4fc79e6c454e 100644\n--- a/test/ao/sparsity/test_sparsity_utils.py\n+++ b/test/ao/sparsity/test_sparsity_utils.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_structured_sparsifier.py b/test/ao/sparsity/test_structured_sparsifier.py\nindex f50420c89a199d..13ab245a2efc27 100644\n--- a/test/ao/sparsity/test_structured_sparsifier.py\n+++ b/test/ao/sparsity/test_structured_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n import copy\n import logging\ndiff --git a/torch/ao/nn/intrinsic/modules/fused.py b/torch/ao/nn/intrinsic/modules/fused.py\nindex f70a5430e65c21..7c87154b0e6225 100644\n--- a/torch/ao/nn/intrinsic/modules/fused.py\n+++ b/torch/ao/nn/intrinsic/modules/fused.py\n@@ -125,7 +125,7 @@ class LinearBn1d(_FusedModule):\n     During quantization this will be replaced with the corresponding fused module.\"\"\"\n     def __init__(self, linear, bn):\n         assert type_before_parametrizations(linear) == Linear and type_before_parametrizations(bn) == BatchNorm1d, \\\n-            'Incorrect types for input modules{}{}'.format(type_before_parametrizations(linear), type_before_parametrizations(bn))\n+            f'Incorrect types for input modules{type_before_parametrizations(linear)}{type_before_parametrizations(bn)}'\n         super().__init__(linear, bn)\n \n class LinearLeakyReLU(_FusedModule):\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\nindex 3f457ad5917eba..161280ca079d53 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n@@ -453,7 +453,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU1d(nnqat.Conv1d, nni._FusedModule):\n     r\"\"\"A ConvReLU1d module is a fused module of Conv1d and ReLU, attached with\n@@ -490,7 +490,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn2d(_ConvBnNd, nn.Conv2d):\n     r\"\"\"\n@@ -585,7 +585,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU2d(nnqat.Conv2d, nni._FusedModule):\n     r\"\"\"A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\n@@ -622,7 +622,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn3d(_ConvBnNd, nn.Conv3d):\n     r\"\"\"\n@@ -758,7 +758,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU3d(nnqat.Conv3d, nni._FusedModule):\n     r\"\"\"A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with\n@@ -813,7 +813,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n def update_bn_stats(mod):\n     if type(mod) in {ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvBn1d, ConvBn2d, ConvBn3d}:\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\nindex 93b19537083427..11d11047c2c723 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n@@ -37,7 +37,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     def to_float(self):\n         linear = torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)\ndiff --git a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nindex 9a6502d546641b..a0bccdc0e3d3d4 100644\n--- a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n@@ -48,7 +48,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qlinear_relu):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\nindex 5cd2ed8a757cee..856fa43aac9941 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n@@ -39,7 +39,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU2d\n-        return super(BNReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\n@@ -75,7 +75,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU3d\n-        return super(BNReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\nindex 7a88a7b8f92d3b..30d00474e4a5ad 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n@@ -58,7 +58,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -107,7 +107,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -163,7 +163,7 @@ def from_float(cls, mod):\n                 mod.bn.weight,\n                 mod.bn.bias,\n             )\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\nindex 9c3a7bcd3b4a0c..17cb48f80fda91 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n@@ -41,7 +41,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_linear_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/quantizable/modules/activation.py b/torch/ao/nn/quantizable/modules/activation.py\nindex b7ba9dd8dc72c2..6a25d0f591021d 100644\n--- a/torch/ao/nn/quantizable/modules/activation.py\n+++ b/torch/ao/nn/quantizable/modules/activation.py\n@@ -317,7 +317,7 @@ def _forward_impl(self,\n             raise AssertionError(\"causal mask not supported by AO MHA module\")\n \n         if self.batch_first:\n-            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n+            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n \n         tgt_len, bsz, embed_dim_to_check = query.size()\n         assert self.embed_dim == embed_dim_to_check\n@@ -339,7 +339,7 @@ def _forward_impl(self,\n                 warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n                 attn_mask = attn_mask.to(torch.bool)\n             assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n-                'Only float and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n+                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n \n             if attn_mask.dim() == 2:\n                 attn_mask = attn_mask.unsqueeze(0)\n@@ -349,7 +349,7 @@ def _forward_impl(self,\n                 if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n                     raise RuntimeError('The size of the 3D attn_mask is not correct.')\n             else:\n-                raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n+                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n             # attn_mask's dim is 3 now.\n \n         # convert ByteTensor key_padding_mask to bool\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/conv.py b/torch/ao/nn/quantized/dynamic/modules/conv.py\nindex 125b48edaacde5..f1af7796413655 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/conv.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Dynamically quantized convolution modules.\"\"\"\n \n import torch\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/linear.py b/torch/ao/nn/quantized/dynamic/modules/linear.py\nindex 78e459f9bc63c5..22f483f32fd7a8 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/linear.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/linear.py\n@@ -68,7 +68,7 @@ def extra_repr(self):\n             self.in_features, self.out_features, self._packed_params.dtype\n         )\n         if self._packed_params.dtype == torch.qint8:\n-            extra_repr_str += ', qscheme={}'.format(self.weight().qscheme())\n+            extra_repr_str += f', qscheme={self.weight().qscheme()}'\n         return extra_repr_str\n \n     def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/rnn.py b/torch/ao/nn/quantized/dynamic/modules/rnn.py\nindex 3e78948b5447b2..47c8a9ac2fb43c 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/rnn.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/rnn.py\n@@ -231,8 +231,8 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n     def set_weight_bias(self, weight_bias_dict):\n \n         def weight_bias_name(ihhh, layer, suffix):\n-            weight_name = \"weight_{}_l{}{}\".format(ihhh, layer, suffix)\n-            bias_name = \"bias_{}_l{}{}\".format(ihhh, layer, suffix)\n+            weight_name = f\"weight_{ihhh}_l{layer}{suffix}\"\n+            bias_name = f\"bias_{ihhh}_l{layer}{suffix}\"\n             return weight_name, bias_name\n \n         num_directions = 2 if self.bidirectional else 1\n@@ -286,7 +286,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n         # RNNBase can be either LSTM or GRU\n         qRNNBase: Union[LSTM, GRU]\n         if mod.mode == 'LSTM':\n@@ -308,8 +308,8 @@ def from_float(cls, mod):\n                 suffix = '_reverse' if direction == 1 else ''\n \n                 def retrieve_weight_bias(ihhh):\n-                    weight_name = 'weight_{}_l{}{}'.format(ihhh, layer, suffix)\n-                    bias_name = 'bias_{}_l{}{}'.format(ihhh, layer, suffix)\n+                    weight_name = f'weight_{ihhh}_l{layer}{suffix}'\n+                    bias_name = f'bias_{ihhh}_l{layer}{suffix}'\n                     weight = getattr(mod, weight_name)\n                     bias = getattr(mod, bias_name)\n                     return weight, bias\n@@ -358,15 +358,15 @@ def _weight_bias(self):\n         for layer in range(self.num_layers):\n             for direction in range(num_directions):\n                 suffix = '_reverse' if direction == 1 else ''\n-                key_name1 = 'weight_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'weight_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'weight_ih_l{layer}{suffix}'\n+                key_name2 = f'weight_hh_l{layer}{suffix}'\n                 # packed weights are part of torchbind class, CellParamsSerializationType\n                 # Within the packed weight class, the weight and bias are accessible as Tensors\n                 packed_weight_bias = self._all_weight_values[count].param.__getstate__()[0][4]\n                 weight_bias_dict['weight'][key_name1] = packed_weight_bias[0].__getstate__()[0][0]\n                 weight_bias_dict['weight'][key_name2] = packed_weight_bias[1].__getstate__()[0][0]\n-                key_name1 = 'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'bias_ih_l{layer}{suffix}'\n+                key_name2 = f'bias_hh_l{layer}{suffix}'\n                 weight_bias_dict['bias'][key_name1] = packed_weight_bias[0].__getstate__()[0][1]\n                 weight_bias_dict['bias'][key_name2] = packed_weight_bias[1].__getstate__()[0][1]\n                 count = count + 1\n@@ -494,7 +494,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTM, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -746,7 +746,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -860,7 +860,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n \n         qRNNCellBase: Union[LSTMCell, GRUCell, RNNCell]\n \n@@ -1009,12 +1009,12 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n         return ret\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(RNNCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class LSTMCell(RNNCellBase):\n@@ -1057,7 +1057,7 @@ def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) ->\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTMCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class GRUCell(RNNCellBase):\n@@ -1098,4 +1098,4 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRUCell, cls).from_float(mod)\n+        return super().from_float(mod)\ndiff --git a/torch/ao/nn/quantized/modules/__init__.py b/torch/ao/nn/quantized/modules/__init__.py\nindex 05866f6da4066a..668f765fe3ef0a 100644\n--- a/torch/ao/nn/quantized/modules/__init__.py\n+++ b/torch/ao/nn/quantized/modules/__init__.py\n@@ -104,7 +104,7 @@ def from_float(mod):\n         return Quantize(scale.float().item(), zero_point.long().item(), mod.activation_post_process.dtype)\n \n     def extra_repr(self):\n-        return 'scale={}, zero_point={}, dtype={}'.format(self.scale, self.zero_point, self.dtype)\n+        return f'scale={self.scale}, zero_point={self.zero_point}, dtype={self.dtype}'\n \n \n class DeQuantize(torch.nn.Module):\ndiff --git a/torch/ao/nn/quantized/modules/conv.py b/torch/ao/nn/quantized/modules/conv.py\nindex 727447841ca43c..22a11014375948 100644\n--- a/torch/ao/nn/quantized/modules/conv.py\n+++ b/torch/ao/nn/quantized/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Quantized convolution modules.\"\"\"\n \n from typing import Optional, List, TypeVar\n@@ -64,7 +63,7 @@ def _init(self, in_channels, out_channels, kernel_size, stride,\n         self.output_padding = output_padding\n         self.groups = groups\n         if padding_mode not in _SUPPORTED_PADDING:\n-            raise ValueError(\"'padding_mode' {} is not supported by quantized convolution\".format(padding_mode))\n+            raise ValueError(f\"'padding_mode' {padding_mode} is not supported by quantized convolution\")\n         self.padding_mode = padding_mode\n         # Initialize as NCHW. set_weight will internally transpose to NHWC.\n         if self.transposed:\n@@ -593,7 +592,7 @@ def __init__(self, in_channels, out_channels, kernel_size, stride,\n                  padding, dilation, transposed, output_padding,\n                  groups, bias, padding_mode, device=None, dtype=None):\n         if padding_mode != 'zeros':\n-            raise ValueError('Only \"zeros\" padding mode is supported for {}'.format(self.__class__.__name__))\n+            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n         factory_kwargs = {'device': device, 'dtype': dtype}\n         # Subclasses of _ConvNd need to call _init rather than __init__. See\n         # discussion on PR #49702\ndiff --git a/torch/ao/nn/quantized/modules/linear.py b/torch/ao/nn/quantized/modules/linear.py\nindex e592c5f9b4d015..213934e62962a0 100644\n--- a/torch/ao/nn/quantized/modules/linear.py\n+++ b/torch/ao/nn/quantized/modules/linear.py\n@@ -262,7 +262,7 @@ def from_float(cls, mod):\n             if not isinstance(cls._FLOAT_MODULE, Iterable):\n                 cls._FLOAT_MODULE = [cls._FLOAT_MODULE]  # type: ignore[assignment]\n             supported_modules = ', '.join([float_mod.__name__ for float_mod in cls._FLOAT_MODULE])  # type: ignore[attr-defined]\n-            error_msg = 'nnq.{}.from_float only works for {}, but got: {}'.format(cls.__name__, supported_modules, type(mod))\n+            error_msg = f'nnq.{cls.__name__}.from_float only works for {supported_modules}, but got: {type(mod)}'\n             assert type_before_parametrizations(mod) in cls._FLOAT_MODULE, error_msg.format()  # type: ignore[attr-defined]\n             assert hasattr(mod, 'qconfig'), 'Input float module must have qconfig defined'\n             activation_post_process = mod.activation_post_process\ndiff --git a/torch/ao/nn/quantized/reference/modules/rnn.py b/torch/ao/nn/quantized/reference/modules/rnn.py\nindex 566642832a544d..9f44667c270b56 100644\n--- a/torch/ao/nn/quantized/reference/modules/rnn.py\n+++ b/torch/ao/nn/quantized/reference/modules/rnn.py\n@@ -152,7 +152,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n \n         if not is_batched:\n             ret = ret.squeeze(0)\ndiff --git a/torch/ao/nn/sparse/quantized/dynamic/linear.py b/torch/ao/nn/sparse/quantized/dynamic/linear.py\nindex 87d174db8098ac..4190ebe38c2f93 100644\n--- a/torch/ao/nn/sparse/quantized/dynamic/linear.py\n+++ b/torch/ao/nn/sparse/quantized/dynamic/linear.py\n@@ -60,7 +60,7 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                               missing_keys, unexpected_keys, error_msgs):\n         op_type = int(state_dict[prefix + 'op_type'])\n         assert op_type == 'sparse', \\\n-            \"Cannot load from op_type [{}], expecting [{}]\".format(op_type, self._op_type)\n+            f\"Cannot load from op_type [{op_type}], expecting [{self._op_type}]\"\n         state_dict.pop(prefix + 'op_type')\n \n         version = local_metadata.get('version', None)\ndiff --git a/torch/ao/ns/fx/ns_types.py b/torch/ao/ns/fx/ns_types.py\nindex cf0451a155dd44..5c3c422dd4ae9d 100644\n--- a/torch/ao/ns/fx/ns_types.py\n+++ b/torch/ao/ns/fx/ns_types.py\n@@ -10,10 +10,10 @@ class NSSingleResultValuesType(str, enum.Enum):\n     NODE_OUTPUT = 'node_output'\n     NODE_INPUT = 'node_input'\n \n-NSSubgraph = NamedTuple(\n-    'NSSubgraph',\n-    [('start_node', Node), ('end_node', Node), ('base_op_node', Node)]\n-)\n+class NSSubgraph(NamedTuple):\n+    start_node: Node\n+    end_node: Node\n+    base_op_node: Node\n \n # TODO(future PR): see if we can use typing_extensions's TypedDict instead\n # to properly type the various keys\ndiff --git a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\nindex 26da2146952ffd..0e4060f95435b6 100644\n--- a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n+++ b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n@@ -103,8 +103,8 @@ def get_schedule_param(self):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Data Sparsifier {0}\\n'.format(self.data_sparsifier)\n-        format_string += '    {0}: {1}\\n'.format(self.schedule_param, self.base_param)\n+        format_string += f'Data Sparsifier {self.data_sparsifier}\\n'\n+        format_string += f'    {self.schedule_param}: {self.base_param}\\n'\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/base_scheduler.py b/torch/ao/pruning/scheduler/base_scheduler.py\nindex 8986d5bbdf630f..66863b31c5f8d8 100644\n--- a/torch/ao/pruning/scheduler/base_scheduler.py\n+++ b/torch/ao/pruning/scheduler/base_scheduler.py\n@@ -106,8 +106,8 @@ def print_sl(self, is_verbose, group, sl, epoch=None):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Sparsifier {0}\\n'.format(self.sparsifier)\n-        format_string += '    {0}: {1}\\n'.format('base_sl', self.base_sl)\n+        format_string += f'Sparsifier {self.sparsifier}\\n'\n+        format_string += '    {}: {}\\n'.format('base_sl', self.base_sl)\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/cubic_scheduler.py b/torch/ao/pruning/scheduler/cubic_scheduler.py\nindex 49ee9f51b42ae6..76fc61daa288a6 100644\n--- a/torch/ao/pruning/scheduler/cubic_scheduler.py\n+++ b/torch/ao/pruning/scheduler/cubic_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import warnings\n \n from .base_scheduler import BaseScheduler\ndiff --git a/torch/ao/quantization/_learnable_fake_quantize.py b/torch/ao/quantization/_learnable_fake_quantize.py\nindex df86cd50a2a775..f21fedeb3bc28f 100644\n--- a/torch/ao/quantization/_learnable_fake_quantize.py\n+++ b/torch/ao/quantization/_learnable_fake_quantize.py\n@@ -114,8 +114,8 @@ def toggle_fake_quant(self, enabled=True):\n \n     @torch.jit.export\n     def observe_quant_params(self):\n-        print('_LearnableFakeQuantize Scale: {}'.format(self.scale.detach()))\n-        print('_LearnableFakeQuantize Zero Point: {}'.format(self.zero_point.detach()))\n+        print(f'_LearnableFakeQuantize Scale: {self.scale.detach()}')\n+        print(f'_LearnableFakeQuantize Zero Point: {self.zero_point.detach()}')\n \n     @torch.jit.export\n     def calculate_qparams(self):\ndiff --git a/torch/ao/quantization/backend_config/backend_config.py b/torch/ao/quantization/backend_config/backend_config.py\nindex ef31166b5cdab1..32abcc42e402fb 100644\n--- a/torch/ao/quantization/backend_config/backend_config.py\n+++ b/torch/ao/quantization/backend_config/backend_config.py\n@@ -599,8 +599,7 @@ def _get_dtype_config(obj: Any) -> DTypeConfig:\n                 return obj\n             if isinstance(obj, Dict):\n                 return DTypeConfig.from_dict(obj)\n-            raise ValueError(\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (DTYPE_CONFIGS_DICT_KEY, type(obj)))\n+            raise ValueError(f\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"{DTYPE_CONFIGS_DICT_KEY}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         if PATTERN_DICT_KEY in backend_pattern_config_dict:\ndiff --git a/torch/ao/quantization/backend_config/onednn.py b/torch/ao/quantization/backend_config/onednn.py\nindex 6a896608c9b5a8..8c14637ae3d3f7 100644\n--- a/torch/ao/quantization/backend_config/onednn.py\n+++ b/torch/ao/quantization/backend_config/onednn.py\n@@ -89,7 +89,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n         \"Linear, BN and LeakyReLU all must be in the same mode (train or eval).\"\n \n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((linear, bn, leaky_relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(linear, bn, leaky_relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Linear: nni.LinearLeakyReLU,\n@@ -100,7 +100,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n             fm = fused_module(fused_linear, leaky_relu)\n             return fm\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((linear, bn, leaky_relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(linear, bn, leaky_relu)}\")\n \n # ======================\n # |  CONFIGS FOR CONV  |\n@@ -144,7 +144,7 @@ def _conv_add_extra_inputs_getter_left(pattern):\n def _fuse_conv_bn_add_left(is_qat, add, bn_conv, _):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -216,7 +216,7 @@ def _conv_add_extra_inputs_getter_right(pattern):\n def _fuse_conv_bn_add_right(is_qat, add, _, bn_conv):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -305,7 +305,7 @@ def _fuse_conv_bn_add_relu_left(is_qat, relu, add_pattern):\n     add, bn_conv, _ = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\n@@ -387,7 +387,7 @@ def _fuse_conv_bn_add_relu_right(is_qat, relu, add_pattern):\n     add, _, bn_conv = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\ndiff --git a/torch/ao/quantization/experimental/APoT_tensor.py b/torch/ao/quantization/experimental/APoT_tensor.py\nindex f780e204154147..debda7aea8c0d1 100644\n--- a/torch/ao/quantization/experimental/APoT_tensor.py\n+++ b/torch/ao/quantization/experimental/APoT_tensor.py\n@@ -2,7 +2,7 @@\n from torch.ao.quantization.experimental.quantizer import APoTQuantizer\n \n # class to store APoT quantized tensor\n-class TensorAPoT():\n+class TensorAPoT:\n     quantizer: APoTQuantizer\n     data: torch.Tensor\n \ndiff --git a/torch/ao/quantization/experimental/quantizer.py b/torch/ao/quantization/experimental/quantizer.py\nindex e7e6048fb00e08..df9c0f27847e13 100644\n--- a/torch/ao/quantization/experimental/quantizer.py\n+++ b/torch/ao/quantization/experimental/quantizer.py\n@@ -5,7 +5,7 @@\n \n # class to store APoT quantizer and\n # implement quantize and dequantize\n-class APoTQuantizer():\n+class APoTQuantizer:\n     alpha: torch.Tensor\n     gamma: torch.Tensor\n     quantization_levels: torch.Tensor\ndiff --git a/torch/ao/quantization/fake_quantize.py b/torch/ao/quantization/fake_quantize.py\nindex 881d431dcccb18..0da19e9f09b5a8 100644\n--- a/torch/ao/quantization/fake_quantize.py\n+++ b/torch/ao/quantization/fake_quantize.py\n@@ -268,7 +268,7 @@ class FixedQParamsFakeQuantize(FakeQuantize):\n     def __init__(self, observer):\n         super().__init__(observer=observer)\n         assert type(self.activation_post_process) == FixedQParamsObserver,\\\n-            \"%s's observer must be a %s\" % (self.__class__.__name__, FixedQParamsObserver.__name__)\n+            f\"{self.__class__.__name__}'s observer must be a {FixedQParamsObserver.__name__}\"\n         self._observer_ctr = observer\n         self.scale = self.activation_post_process.scale\n         self.zero_point = self.activation_post_process.zero_point\ndiff --git a/torch/ao/quantization/fuse_modules.py b/torch/ao/quantization/fuse_modules.py\nindex 80c3933ddc06a1..7c7ef1a88e83a7 100644\n--- a/torch/ao/quantization/fuse_modules.py\n+++ b/torch/ao/quantization/fuse_modules.py\n@@ -51,7 +51,7 @@ def fuse_known_modules(mod_list, is_qat, additional_fuser_method_mapping=None):\n     types = tuple(type_before_parametrizations(m) for m in mod_list)\n     fuser_method = get_fuser_method(types, additional_fuser_method_mapping)\n     if fuser_method is None:\n-        raise NotImplementedError(\"Cannot fuse modules: {}\".format(types))\n+        raise NotImplementedError(f\"Cannot fuse modules: {types}\")\n     new_mod : List[Optional[nn.Module]] = [None] * len(mod_list)\n     fused = fuser_method(is_qat, *mod_list)\n     # NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\ndiff --git a/torch/ao/quantization/fuser_method_mappings.py b/torch/ao/quantization/fuser_method_mappings.py\nindex 9971326de1d102..3140f13008ac3d 100644\n--- a/torch/ao/quantization/fuser_method_mappings.py\n+++ b/torch/ao/quantization/fuser_method_mappings.py\n@@ -47,7 +47,7 @@ def fuse_conv_bn(is_qat, conv, bn):\n         if fused_module_class is not None:\n             return fused_module_class(conv, bn)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn)}\")\n     else:\n         return nn.utils.fuse_conv_bn_eval(conv, bn)\n \n@@ -84,7 +84,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n         if fused_module is not None:\n             return fused_module(conv, bn, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Conv1d: nni.ConvReLU1d,\n@@ -96,7 +96,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n             fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n             return fused_module(fused_conv, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(conv, bn, relu)}\")\n \n def fuse_linear_bn(is_qat, linear, bn):\n     r\"\"\"Given the linear and bn modules, fuses them and returns the fused module\n@@ -187,7 +187,7 @@ def get_fuser_method(op_list, additional_fuser_method_mapping=None):\n     all_mappings = get_combined_dict(_DEFAULT_OP_LIST_TO_FUSER_METHOD,\n                                      additional_fuser_method_mapping)\n     fuser_method = all_mappings.get(op_list, None)\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_list)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_list} \"\n     return fuser_method\n \n def _reverse2(f):\n@@ -244,5 +244,5 @@ def get_fuser_method_new(\n         fuser_method = fuser_method_mapping.get(op_pattern, None)\n         if fuser_method is not None:\n             break\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_pattern)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_pattern} \"\n     return fuser_method\ndiff --git a/torch/ao/quantization/fx/_equalize.py b/torch/ao/quantization/fx/_equalize.py\nindex 357db6454032e7..883ddf19682f08 100644\n--- a/torch/ao/quantization/fx/_equalize.py\n+++ b/torch/ao/quantization/fx/_equalize.py\n@@ -227,7 +227,7 @@ def __new__(cls, input_activation=torch.nn.Identity, weight=torch.nn.Identity):\n         if isinstance(input_activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"EqualizationQConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        self = super(EqualizationQConfig, cls).__new__(cls, input_activation, weight)\n+        self = super().__new__(cls, input_activation, weight)\n         return self\n \n \ndiff --git a/torch/ao/quantization/fx/_lower_to_native_backend.py b/torch/ao/quantization/fx/_lower_to_native_backend.py\nindex b897a7e80ab7cf..f1e9c81896f6c5 100644\n--- a/torch/ao/quantization/fx/_lower_to_native_backend.py\n+++ b/torch/ao/quantization/fx/_lower_to_native_backend.py\n@@ -514,7 +514,7 @@ def _match_static_pattern(\n     matched_dequantize = False\n     for i in dequantize_node_arg_indices:\n         assert i < len(ref_node.args),\\\n-            \"Dequantize index %s exceeded reference node's arg length %s\" % (i, len(ref_node.args))\n+            f\"Dequantize index {i} exceeded reference node's arg length {len(ref_node.args)}\"\n         arg = ref_node.args[i]\n         if is_dequantize_node(arg):\n             matched_dequantize = True\ndiff --git a/torch/ao/quantization/fx/_model_report/detector.py b/torch/ao/quantization/fx/_model_report/detector.py\nindex 60c9b26ffacecd..2e3cdc0a316611 100644\n--- a/torch/ao/quantization/fx/_model_report/detector.py\n+++ b/torch/ao/quantization/fx/_model_report/detector.py\n@@ -32,7 +32,7 @@\n DETECTOR_OBS_ARGS_KEY = \"observer_args\"\n \n # Mapping related code\n-class DetectorQConfigInfo():\n+class DetectorQConfigInfo:\n     r\"\"\"\n     This class contains the QConfig information for a single module.\n     The list of variables / values this contains can grow depending on the\n@@ -234,7 +234,7 @@ def __init__(self, backend: str = torch.backends.quantized.engine):\n         if self.backend_chosen in self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES:\n             self.supported_modules = self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES[self.backend_chosen]\n         else:\n-            raise ValueError(\"Not configured to work with {}. Try a different default backend\".format(self.backend_chosen))\n+            raise ValueError(f\"Not configured to work with {self.backend_chosen}. Try a different default backend\")\n \n     def get_detector_name(self) -> str:\n         r\"\"\" returns the string name of this detector\"\"\"\n@@ -352,7 +352,7 @@ def generate_detector_report(self, model: nn.Module) -> Tuple[str, Dict[str, Any\n         per_channel_info = self._detect_per_channel_helper(model)\n \n         # String to let the user know of further optimizations\n-        further_optims_str = \"Further Optimizations for backend {}: \\n\".format(self.backend_chosen)\n+        further_optims_str = f\"Further Optimizations for backend {self.backend_chosen}: \\n\"\n \n         optimizations_possible = False\n         for fqn in per_channel_info:\n@@ -1019,7 +1019,7 @@ def _generate_comparison_values(self, input_info: Dict, weight_info: Dict) -> Di\n \n             # raise error if not in weight info\n             if module_fqn not in weight_info:\n-                raise KeyError(\"Unable to find weight range stats for module {}\".format(module_fqn))\n+                raise KeyError(f\"Unable to find weight range stats for module {module_fqn}\")\n \n             # calculate the ratios of the weight info and input info\n             weight_ratio = self._calculate_range_ratio(weight_info[module_fqn], self.WEIGHT_STR, module_fqn)\ndiff --git a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\nindex f1f17e80982e54..8e04338446dab1 100644\n--- a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n+++ b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n@@ -587,7 +587,7 @@ def generate_plot_visualization(self, feature_filter: str, module_fqn_filter: st\n             avg_vals = [sum(y_data[:][index]) / num_channels for index in range(num_modules)]\n \n             # plot the three things we measured\n-            ax.plot(x_data, avg_vals, label=\"Average Value Across {} Channels\".format(num_channels))\n+            ax.plot(x_data, avg_vals, label=f\"Average Value Across {num_channels} Channels\")\n             ax.legend(loc='upper right')\n         else:\n             ax.set_xlabel(\"idx\")\ndiff --git a/torch/ao/quantization/fx/convert.py b/torch/ao/quantization/fx/convert.py\nindex 14e6ec094ede48..687342cc0ed321 100644\n--- a/torch/ao/quantization/fx/convert.py\n+++ b/torch/ao/quantization/fx/convert.py\n@@ -970,7 +970,7 @@ def convert(\n         # all the values either match what was set in prepare node_name_to_qconfig\n         # or are set to None in the convert_node_name_to_qconfig.\n         for k, v in node_name_to_qconfig.items():\n-            assert k in convert_node_name_to_qconfig, 'Expected key {} in convert node_name_to_qconfig'.format(k)\n+            assert k in convert_node_name_to_qconfig, f'Expected key {k} in convert node_name_to_qconfig'\n             if convert_node_name_to_qconfig[k] is not None:\n                 assert qconfig_equals(v, convert_node_name_to_qconfig[k]), \\\n                     \"Expected k {} to have the same value in prepare and convert QConfigMappings, \" \\\ndiff --git a/torch/ao/quantization/fx/custom_config.py b/torch/ao/quantization/fx/custom_config.py\nindex ef29061796d3a3..4fb2c3a28cb0a5 100644\n--- a/torch/ao/quantization/fx/custom_config.py\n+++ b/torch/ao/quantization/fx/custom_config.py\n@@ -197,8 +197,7 @@ def _get_qconfig_mapping(obj: Any, dict_key: str) -> Optional[QConfigMapping]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return QConfigMapping.from_dict(obj)\n-            raise ValueError(\"Expected QConfigMapping in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected QConfigMapping in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCustomConfig]:\n             \"\"\"\n@@ -208,8 +207,7 @@ def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCusto\n                 return obj\n             if isinstance(obj, Dict):\n                 return PrepareCustomConfig.from_dict(obj)\n-            raise ValueError(\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n             \"\"\"\n@@ -219,8 +217,7 @@ def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return BackendConfig.from_dict(obj)\n-            raise ValueError(\"Expected BackendConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected BackendConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         for (module_name, qconfig_dict, example_inputs, _prepare_custom_config_dict, backend_config_dict) in\\\ndiff --git a/torch/ao/quantization/fx/prepare.py b/torch/ao/quantization/fx/prepare.py\nindex d14bf444ccd8c6..aa9f1f7467f932 100644\n--- a/torch/ao/quantization/fx/prepare.py\n+++ b/torch/ao/quantization/fx/prepare.py\n@@ -238,7 +238,7 @@ def _needs_obs_or_fq(\n     # be converted to choose_qparams -> q -> dq in convert step\n     if cur_target_is_dynamic:\n         assert cur_target_dtype in _OBS_DTYPE_LIST, \\\n-            \"Expected cur_target_dtype to be torch.float, but got: {}\".format(cur_target_dtype)\n+            f\"Expected cur_target_dtype to be torch.float, but got: {cur_target_dtype}\"\n         assert prev_output_dtype not in _DO_NOT_OBS_DTYPE_LIST\n         return is_zeroth_arg\n     if reuse_input_obs_or_fq:\ndiff --git a/torch/ao/quantization/fx/utils.py b/torch/ao/quantization/fx/utils.py\nindex 2e0b6bbb130530..0942fd9462b0a1 100644\n--- a/torch/ao/quantization/fx/utils.py\n+++ b/torch/ao/quantization/fx/utils.py\n@@ -149,7 +149,7 @@ def get_qconv_prepack_op(conv_op: Callable) -> Callable:\n         torch.nn.functional.conv_transpose3d: torch.ops.quantized.conv_transpose3d_prepack,\n     }\n     prepack_op = prepack_ops.get(conv_op, None)\n-    assert prepack_op, \"Didn't find prepack op for {}\".format(conv_op)\n+    assert prepack_op, f\"Didn't find prepack op for {conv_op}\"\n     return prepack_op\n \n # Returns a function that can get a new attribute name for module with given\n@@ -811,24 +811,21 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n         # check quantization ranges\n         if backend_quant_min is not None and backend_quant_max is not None:\n             if app_quant_min is None or app_quant_max is None:\n-                warnings.warn(\"QConfig %s must specify 'quant_min' and 'quant_max', ignoring %s\" %\n-                              (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'quant_min' and 'quant_max', ignoring {qconfig}\")\n                 return False\n             elif app_quant_min < backend_quant_min or app_quant_max > backend_quant_max:\n-                warnings.warn((\"QConfig %s quantization range must fall within the backend's:\\n\"\n-                              \"QConfig range = (%s, %s), BackendConfig range = (%s, %s), ignoring %s\") %\n-                              (debug_string, app_quant_min, app_quant_max,\n+                warnings.warn((\"QConfig {} quantization range must fall within the backend's:\\n\"\n+                              \"QConfig range = ({}, {}), BackendConfig range = ({}, {}), ignoring {}\").format(debug_string, app_quant_min, app_quant_max,\n                               backend_quant_min, backend_quant_max, qconfig))\n                 return False\n         # check scale min\n         if backend_scale_min is not None:\n             if app_scale_min is None:\n-                warnings.warn(\"QConfig %s must specify 'eps', ignoring %s\" % (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'eps', ignoring {qconfig}\")\n                 return False\n             elif app_scale_min < backend_scale_min:\n-                warnings.warn((\"QConfig %s eps (%s) must be greater than or equal to \"\n-                              \"the backend's min scale value (%s), ignoring %s\") %\n-                              (debug_string, app_scale_min, backend_scale_min, qconfig))\n+                warnings.warn((\"QConfig {} eps ({}) must be greater than or equal to \"\n+                              \"the backend's min scale value ({}), ignoring {}\").format(debug_string, app_scale_min, backend_scale_min, qconfig))\n                 return False\n         # check fixed scale and zero point\n         if backend_scale_exact_match is not None and backend_zero_point_exact_match is not None:\n@@ -846,12 +843,11 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n             if not isinstance(activation_post_process, FixedQParamsObserver) and \\\n                     not isinstance(activation_post_process, FixedQParamsFakeQuantize):\n                 warnings.warn((\"QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize \"\n-                              \"for fixed qparams ops, ignoring %s.\\n%s\") % (qconfig, suggestion_str))\n+                              \"for fixed qparams ops, ignoring {}.\\n{}\").format(qconfig, suggestion_str))\n                 return False\n             if observer.scale != backend_scale_exact_match or observer.zero_point != backend_zero_point_exact_match:\n-                warnings.warn((\"QConfig fixed scale (%s) and zero point (%s) do not match the backend's \"\n-                              \"(%s and %s), ignoring %s.\\n%s\") %\n-                              (observer.scale, observer.zero_point, backend_scale_exact_match,\n+                warnings.warn((\"QConfig fixed scale ({}) and zero point ({}) do not match the backend's \"\n+                              \"({} and {}), ignoring {}.\\n{}\").format(observer.scale, observer.zero_point, backend_scale_exact_match,\n                               backend_zero_point_exact_match, qconfig, suggestion_str))\n                 return False\n         return True\ndiff --git a/torch/ao/quantization/observer.py b/torch/ao/quantization/observer.py\nindex 3263ae11564129..f5d24c45787265 100644\n--- a/torch/ao/quantization/observer.py\n+++ b/torch/ao/quantization/observer.py\n@@ -118,7 +118,7 @@ def _with_callable_args(cls_or_self, **kwargs):\n     return r.with_callable_args(**kwargs)\n \n \n-ABC: Any = ABCMeta(str(\"ABC\"), (object,), {})  # compatible with Python 2 *and* 3:\n+ABC: Any = ABCMeta(\"ABC\", (object,), {})  # compatible with Python 2 *and* 3:\n \n \n class ObserverBase(ABC, nn.Module):\n@@ -509,7 +509,7 @@ def calculate_qparams(self):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     @torch.jit.export\n     def reset_min_max_vals(self):\n@@ -712,7 +712,7 @@ def calculate_qparams(self):\n         return self._calculate_qparams(self.min_val, self.max_val)\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     def _load_from_state_dict(\n         self,\n@@ -746,7 +746,7 @@ def _load_from_state_dict(\n                 elif name == expected_max_name:\n                     self.max_val.resize_(val.shape)\n                 else:\n-                    warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                    warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n                 # For torchscript module we need to update the attributes here since we do not\n                 # call the `_load_from_state_dict` function defined module.py\n                 if torch.jit.is_scripting():\n@@ -755,7 +755,7 @@ def _load_from_state_dict(\n                     elif name == expected_max_name:\n                         self.max_val.copy_(val)\n                     else:\n-                        warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                        warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n             elif strict:\n                 missing_keys.append(key)\n \n@@ -1265,7 +1265,7 @@ def _load_from_state_dict(\n         )\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n \n class FixedQParamsObserver(ObserverBase):\n@@ -1363,7 +1363,7 @@ def forward(self, x):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"dtype={}, is_dynamic={}\".format(self.dtype, self.is_dynamic)\n+        return f\"dtype={self.dtype}, is_dynamic={self.is_dynamic}\"\n \n     @torch.jit.export\n     def calculate_qparams(self):\n@@ -1518,10 +1518,10 @@ def load_observer_state_dict(mod, obs_dict):\n                 )\n     for k in missing_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Missing keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Missing keys for observer {k} in state_dict\")\n     for k in unexpected_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Unexpected keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Unexpected keys for observer {k} in state_dict\")\n \n \n # Restrict activations to be in the range (0,127)\ndiff --git a/torch/ao/quantization/pt2e/prepare.py b/torch/ao/quantization/pt2e/prepare.py\nindex 2a29a92a986ced..13f73acca354b4 100644\n--- a/torch/ao/quantization/pt2e/prepare.py\n+++ b/torch/ao/quantization/pt2e/prepare.py\n@@ -68,9 +68,9 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n             assert _is_activation_post_process_node(arg, named_modules)\n             assert arg_as_input_act_obs_or_fq is not None\n             observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), \"expect observed argument to be a Node, but got: {}\".format(type(observed_arg))\n+            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n             assert observed_arg in obs_or_fq_map, \\\n-                \"can't refer to a node that does not have observer/fake_quant inserted yet: {}\".format(observed_arg)\n+                f\"can't refer to a node that does not have observer/fake_quant inserted yet: {observed_arg}\"\n             arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n             new_arg = arg\n             obs_or_fq_map[(observed_arg, node)] = arg_as_input_act_obs_or_fq\ndiff --git a/torch/ao/quantization/pt2e/quantizer/quantizer.py b/torch/ao/quantization/pt2e/quantizer/quantizer.py\nindex 3f7531c33a4e8b..46d7286756d17c 100644\n--- a/torch/ao/quantization/pt2e/quantizer/quantizer.py\n+++ b/torch/ao/quantization/pt2e/quantizer/quantizer.py\n@@ -128,24 +128,9 @@ class QuantizationConfig:\n OperatorPatternType = List[Callable]\n OperatorPatternType.__module__ = \"torch.ao.quantization.pt2e.quantizer.quantizer\"\n \n-OperatorConfig = NamedTuple(\n-    \"OperatorConfig\",\n-    # fix List[str] with List[List[Union[nn.Module, FunctionType, BuiltinFunctionType]]]\n-    # Basically we are mapping a quantization config to some list of patterns.\n-    # a pattern is defined as a list of nn module, function or builtin function names\n-    # e.g. [nn.Conv2d, torch.relu, torch.add]\n-    # We have not resolved whether fusion can be considered internal details of the\n-    # quantizer hence it does not need communication to user.\n-    # Note this pattern is not really informative since it does not really\n-    # tell us the graph structure resulting from the list of ops.\n-    [\n-        (\"config\", QuantizationConfig),\n-        (\n-            \"operators\",\n-            List[OperatorPatternType],\n-        ),\n-    ],\n-)\n+class OperatorConfig(NamedTuple):\n+    config: QuantizationConfig\n+    operators: List[OperatorPatternType]\n \n @dataclass\n class QuantizationAnnotation:\ndiff --git a/torch/ao/quantization/qconfig.py b/torch/ao/quantization/qconfig.py\nindex f7489c1ed4d05a..dc8353d6172990 100644\n--- a/torch/ao/quantization/qconfig.py\n+++ b/torch/ao/quantization/qconfig.py\n@@ -103,7 +103,7 @@ def __new__(cls, activation, weight):\n         if isinstance(activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"QConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        return super(QConfig, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n class QConfigDynamic(namedtuple('QConfigDynamic', ['activation', 'weight'])):\n@@ -128,7 +128,7 @@ def __new__(cls, activation=torch.nn.Identity, weight=torch.nn.Identity):\n             raise ValueError(\"QConfigDynamic received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n         warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n-        return super(QConfigDynamic, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n default_qconfig = QConfig(activation=default_observer,\n@@ -236,7 +236,7 @@ def get_default_qconfig(backend='x86', version=0):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     if version == 0:\n@@ -326,7 +326,7 @@ def get_default_qat_qconfig(backend='x86', version=1):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     # Histogram observer is too slow for quantization aware training\ndiff --git a/torch/ao/quantization/quantization_mappings.py b/torch/ao/quantization/quantization_mappings.py\nindex 96db52624acd34..4b3f4d26b2ac09 100644\n--- a/torch/ao/quantization/quantization_mappings.py\n+++ b/torch/ao/quantization/quantization_mappings.py\n@@ -251,7 +251,7 @@ def get_static_quant_module_class(\n         else DEFAULT_STATIC_QUANT_MODULE_MAPPINGS, additional_static_quant_mapping)\n     static_quant_module_class = all_mappings.get(float_module_class, None)\n     assert static_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(static_quant_module_class)\n \n@@ -266,7 +266,7 @@ def get_dynamic_quant_module_class(\n     all_mappings = get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping)\n     dynamic_quant_module_class = all_mappings.get(float_module_class, None)\n     assert dynamic_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(dynamic_quant_module_class)\n \n@@ -300,10 +300,10 @@ def get_default_qconfig_propagation_list() -> Set[Callable]:\n     attribute to in prepare\n     '''\n     QCONFIG_PROPAGATE_MODULE_CLASS_LIST = (\n-        (set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         _INCLUDE_QCONFIG_PROPAGATE_LIST)\n+         _INCLUDE_QCONFIG_PROPAGATE_LIST\n     )\n     return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n \n@@ -332,7 +332,7 @@ def get_quantized_operator(float_op: Union[Callable, str]) -> Callable:\n     '''\n     quantized_op = DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS.get(float_op, None)\n     assert quantized_op is not None, \\\n-        'Operator {} does not have corresponding quantized op'.format(str(float_op))\n+        f'Operator {str(float_op)} does not have corresponding quantized op'\n     return quantized_op\n \n def _get_special_act_post_process(module: torch.nn.Module) -> Optional[Callable]:\ndiff --git a/torch/ao/quantization/quantize.py b/torch/ao/quantization/quantize.py\nindex ca60475fc95a79..23c234c3f35103 100644\n--- a/torch/ao/quantization/quantize.py\n+++ b/torch/ao/quantization/quantize.py\n@@ -442,7 +442,7 @@ def quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8,\n             }\n         else:\n             raise ValueError(\n-                \"Don't know how to quantize with default settings for {}. Provide full qconfig please\".format(dtype))\n+                f\"Don't know how to quantize with default settings for {dtype}. Provide full qconfig please\")\n     elif isinstance(qconfig_spec, set):\n         if dtype is torch.qint8:\n             default_qconfig = default_dynamic_qconfig\ndiff --git a/torch/ao/quantization/utils.py b/torch/ao/quantization/utils.py\nindex 1a82c0fcf432ae..d97b76fb253389 100644\n--- a/torch/ao/quantization/utils.py\n+++ b/torch/ao/quantization/utils.py\n@@ -327,7 +327,7 @@ def check_min_max_valid(min_val: torch.Tensor, max_val: torch.Tensor) -> bool:\n     else:\n         assert torch.all(\n             min_val <= max_val\n-        ), \"min {} should be less than max {}\".format(min_val, max_val)\n+        ), f\"min {min_val} should be less than max {max_val}\"\n \n     return True\n \n"
  },
  {
    "number": 105399,
    "title": "[BE] Enable ruff's UP rules and autoformat benchmarks/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "999f10d46c51ce89fb879a1f0207cf0ac34c8faa",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105399",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105399/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105399.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105399.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105399/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105399/comments",
    "labels": [
      "module: dynamo",
      "open source",
      "ciflow/inductor",
      "release notes: distributed (ddp)"
    ],
    "_event_time": "2023-07-18T01:12:46.733995Z",
    "state": "closed",
    "patch": "From dcf002278da83bbb49799658f19d602991ee0499 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:39 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat benchmarks/\n\n[ghstack-poisoned]\n---\n benchmarks/compare-fastrnn-results.py              |  4 ++--\n benchmarks/cpp/tensorexpr/bench_ops.py             |  4 ++--\n benchmarks/distributed/ddp/benchmark.py            | 14 +++++++-------\n benchmarks/distributed/ddp/diff.py                 | 10 +++++-----\n benchmarks/distributed/pipeline/pipe.py            |  4 ++--\n .../distributed/rpc/parameter_server/launcher.py   |  2 +-\n benchmarks/distributed/rpc/rl/coordinator.py       |  2 +-\n benchmarks/dynamo/_onnx/reporter.py                | 12 ++++++------\n benchmarks/dynamo/benchmarks.py                    |  2 +-\n benchmarks/dynamo/combine_csv.py                   |  2 +-\n benchmarks/dynamo/common.py                        |  4 ++--\n .../dynamo/microbenchmarks/operator_inp_utils.py   |  2 +-\n benchmarks/dynamo/parse_logs.py                    |  2 +-\n benchmarks/dynamo/runner.py                        | 10 +++++-----\n benchmarks/dynamo/timm_models.py                   |  4 ++--\n benchmarks/fastrnns/bench.py                       |  2 +-\n benchmarks/fastrnns/profile.py                     |  8 ++++----\n benchmarks/fastrnns/runner.py                      |  6 +++---\n benchmarks/fastrnns/test.py                        |  4 ++--\n .../framework_overhead_benchmark/C2Module.py       |  4 ++--\n .../framework_overhead_benchmark.py                |  6 +++---\n .../pt_wrapper_module.py                           |  4 ++--\n benchmarks/framework_overhead_benchmark/utils.py   |  2 +-\n .../functional_autograd_benchmark/compare.py       |  4 ++--\n .../functional_autograd_benchmark.py               |  6 +++---\n benchmarks/functional_autograd_benchmark/utils.py  |  2 +-\n benchmarks/instruction_counts/applications/ci.py   |  2 +-\n benchmarks/instruction_counts/core/expand.py       |  2 +-\n benchmarks/operator_benchmark/benchmark_caffe2.py  |  6 +++---\n benchmarks/operator_benchmark/benchmark_core.py    |  8 ++++----\n benchmarks/operator_benchmark/benchmark_pytorch.py |  2 +-\n .../operator_benchmark/common/repeat_benchmark.py  |  2 +-\n benchmarks/overrides_benchmark/bench.py            |  4 ++--\n benchmarks/sparse/dlmc/matmul_bench.py             |  4 ++--\n benchmarks/sparse/dlmc/utils.py                    |  8 ++++----\n benchmarks/tensorexpr/__main__.py                  |  9 ++++-----\n benchmarks/tensorexpr/benchmark.py                 |  4 ++--\n benchmarks/tensorexpr/reduction.py                 |  2 +-\n benchmarks/upload_scribe.py                        |  2 +-\n 39 files changed, 90 insertions(+), 91 deletions(-)\n\ndiff --git a/benchmarks/compare-fastrnn-results.py b/benchmarks/compare-fastrnn-results.py\nindex bc4a55688b17ed..45961ca78de53b 100644\n--- a/benchmarks/compare-fastrnn-results.py\n+++ b/benchmarks/compare-fastrnn-results.py\n@@ -23,9 +23,9 @@ def get_times(json_data):\n parser.add_argument('--format', default='md', type=str, help='output format (csv, md, json, table)')\n args = parser.parse_args()\n \n-with open(args.base, \"r\") as base:\n+with open(args.base) as base:\n     base_times = get_times(json.load(base))\n-with open(args.diff, \"r\") as diff:\n+with open(args.diff) as diff:\n     diff_times = get_times(json.load(diff))\n \n all_keys = set(base_times.keys()).union(diff_times.keys())\ndiff --git a/benchmarks/cpp/tensorexpr/bench_ops.py b/benchmarks/cpp/tensorexpr/bench_ops.py\nindex 12d766ae74862c..fef18f912e75c0 100644\n--- a/benchmarks/cpp/tensorexpr/bench_ops.py\n+++ b/benchmarks/cpp/tensorexpr/bench_ops.py\n@@ -83,8 +83,8 @@ def test_batch_norm():\n         [5, 512, 7, 7]]\n     for n, c, h, w in batch_norm_shapes:\n         x = torch.rand((n, c, h, w))\n-        y = torch.rand((c))\n-        z = torch.rand((c))\n+        y = torch.rand(c)\n+        z = torch.rand(c)\n         traced = torch.jit.trace(lambda x, y, z: op(x, y, z), (x, y, z))\n \n         # Warmup.\ndiff --git a/benchmarks/distributed/ddp/benchmark.py b/benchmarks/distributed/ddp/benchmark.py\nindex c72e3e6a27d961..db592cf6bd32d6 100644\n--- a/benchmarks/distributed/ddp/benchmark.py\n+++ b/benchmarks/distributed/ddp/benchmark.py\n@@ -181,7 +181,7 @@ def __init__(self, device, distributed_backend, bucket_size, model):\n         self.model = model\n \n     def __str__(self):\n-        return \"{} with batch size {}\".format(self.model, self.batch_size)\n+        return f\"{self.model} with batch size {self.batch_size}\"\n \n     def create_model(self):\n         return torchvision.models.__dict__[self.model]().to(self.device)\n@@ -212,7 +212,7 @@ def main():\n     # metadata, like measurements. Not for benchmarking itself.\n     dist.init_process_group(\n         backend=\"gloo\",\n-        init_method=\"tcp://{}:{}\".format(args.master_addr, args.master_port),\n+        init_method=f\"tcp://{args.master_addr}:{args.master_port}\",\n         rank=args.rank,\n         world_size=args.world_size,\n     )\n@@ -227,10 +227,10 @@ def main():\n         print(\"PyTorch distributed benchmark suite\")\n         print(\"-----------------------------------\")\n         print(\"\")\n-        print(\"* PyTorch version: {}\".format(torch.__version__))\n-        print(\"* CUDA version: {}\".format(torch.version.cuda))\n-        print(\"* Distributed backend: {}\".format(args.distributed_backend))\n-        print(\"* Maximum bucket size: {}MB\".format(args.bucket_size))\n+        print(f\"* PyTorch version: {torch.__version__}\")\n+        print(f\"* CUDA version: {torch.version.cuda}\")\n+        print(f\"* Distributed backend: {args.distributed_backend}\")\n+        print(f\"* Maximum bucket size: {args.bucket_size}MB\")\n         print(\"\")\n         print(\"--- nvidia-smi topo -m ---\")\n         print(\"\")\n@@ -261,7 +261,7 @@ def main():\n     benchmark_results = []\n     for benchmark in benchmarks:\n         if args.rank == 0:\n-            print(\"\\nBenchmark: {}\".format(str(benchmark)))\n+            print(f\"\\nBenchmark: {str(benchmark)}\")\n         result = sweep(benchmark)\n         benchmark_results.append({\n             \"model\": benchmark.model,\ndiff --git a/benchmarks/distributed/ddp/diff.py b/benchmarks/distributed/ddp/diff.py\nindex d427a5b29d9199..bce7a8db56c13e 100644\n--- a/benchmarks/distributed/ddp/diff.py\n+++ b/benchmarks/distributed/ddp/diff.py\n@@ -10,7 +10,7 @@\n \n \n def load(path):\n-    with open(path, 'r') as f:\n+    with open(path) as f:\n         return json.load(f)\n \n \n@@ -44,8 +44,8 @@ def main():\n \n         model = ra[\"model\"]\n         batch_size = int(ra[\"batch_size\"])\n-        name = \"{} with batch size {}\".format(model, batch_size)\n-        print(\"Benchmark: {}\".format(name))\n+        name = f\"{model} with batch size {batch_size}\"\n+        print(f\"Benchmark: {name}\")\n \n         # Print header\n         print(\"\")\n@@ -66,13 +66,13 @@ def main():\n             ngpus = len(xa[\"ranks\"])\n             ma = sorted(xa[\"measurements\"])\n             mb = sorted(xb[\"measurements\"])\n-            print(\"{:>4d} GPUs:\".format(ngpus), end='')  # noqa: E999\n+            print(f\"{ngpus:>4d} GPUs:\", end='')  # noqa: E999\n             for p in [75, 95]:\n                 va = np.percentile(ma, p)\n                 vb = np.percentile(mb, p)\n                 # We're measuring time, so lower is better (hence the negation)\n                 delta = -100 * ((vb - va) / va)\n-                print(\"  p{:02d}: {:8.3f}s {:7d}/s {:+8.1f}%\".format(p, vb, int(batch_size / vb), delta), end='')  # noqa: E999\n+                print(f\"  p{p:02d}: {vb:8.3f}s {int(batch_size / vb):7d}/s {delta:+8.1f}%\", end='')  # noqa: E999\n             print(\"\")\n         print(\"\")\n \ndiff --git a/benchmarks/distributed/pipeline/pipe.py b/benchmarks/distributed/pipeline/pipe.py\nindex 8a08d25ca4c940..58f2850e34868e 100644\n--- a/benchmarks/distributed/pipeline/pipe.py\n+++ b/benchmarks/distributed/pipeline/pipe.py\n@@ -16,7 +16,7 @@\n def sizeof_fmt(num, suffix='B'):\n     for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti']:\n         if abs(num) < 1024.0:\n-            return \"%3.2f%sB\" % (num, unit)\n+            return f\"{num:3.2f}{unit}B\"\n         num /= 1024.0\n \n \n@@ -146,7 +146,7 @@ def get_last_device(model):\n             return torch.cuda.current_device()\n \n \n-    print('Number of parameters for model: {}'.format(sum(p.numel() for p in model.parameters())))\n+    print(f'Number of parameters for model: {sum(p.numel() for p in model.parameters())}')\n     for i, batch in enumerate(lm_dataloader):\n         bi = batch[\"input\"]\n         if args.max_batch and i > args.max_batch:\ndiff --git a/benchmarks/distributed/rpc/parameter_server/launcher.py b/benchmarks/distributed/rpc/parameter_server/launcher.py\nindex a4c13cdb29b696..ec6559c8508f9c 100644\n--- a/benchmarks/distributed/rpc/parameter_server/launcher.py\n+++ b/benchmarks/distributed/rpc/parameter_server/launcher.py\n@@ -362,7 +362,7 @@ def get_json_config(file_name, id):\n         file_name (str): name of configuration file to load\n         id (str): configuration that will be loaded\n     \"\"\"\n-    with open(os.path.join(Path(__file__).parent, file_name), \"r\") as f:\n+    with open(os.path.join(Path(__file__).parent, file_name)) as f:\n         json_config = json.load(f)[id]\n     return json_config\n \ndiff --git a/benchmarks/distributed/rpc/rl/coordinator.py b/benchmarks/distributed/rpc/rl/coordinator.py\nindex b488378d5aee58..8b6d246f0861f6 100644\n--- a/benchmarks/distributed/rpc/rl/coordinator.py\n+++ b/benchmarks/distributed/rpc/rl/coordinator.py\n@@ -102,7 +102,7 @@ def run_coordinator(self, episodes, episode_steps, queue):\n                              'observer throughput': {}}\n \n \n-        print(\"For batch size {0}\".format(self.batch_size))\n+        print(f\"For batch size {self.batch_size}\")\n         print(\"\\nAgent Latency - \", len(agent_latency_final))\n         agent_latency_final = sorted(agent_latency_final)\n         for p in [50, 75, 90, 95]:\ndiff --git a/benchmarks/dynamo/_onnx/reporter.py b/benchmarks/dynamo/_onnx/reporter.py\nindex f10b50c4e323df..9318b7a6dc2e47 100644\n--- a/benchmarks/dynamo/_onnx/reporter.py\n+++ b/benchmarks/dynamo/_onnx/reporter.py\n@@ -22,7 +22,7 @@\n _COMPACT_ERROR_GROUP = False\n \n \n-class ErrorAggregator(object):\n+class ErrorAggregator:\n     \"\"\"\n     Collect and group error messages for report at the end.\n \n@@ -47,7 +47,7 @@ class ErrorAggregator(object):\n     ]\n \n     def __init__(self, log: Optional[logging.Logger] = None):\n-        super(ErrorAggregator, self).__init__()\n+        super().__init__()\n         self.error_groups = []\n         self.bigram_to_group_ids = collections.defaultdict(list)\n         self.log = log or logging.getLogger(__name__)\n@@ -141,7 +141,7 @@ def __len__(self):\n         return sum(map(len, self.error_groups))\n \n \n-class ErrorAggregatorDict(object):\n+class ErrorAggregatorDict:\n     \"\"\"\n     Collect error types and individually group their error messages for a debug report at the end.\n \n@@ -152,7 +152,7 @@ class ErrorAggregatorDict(object):\n     \"\"\"\n \n     def __init__(self):\n-        super(ErrorAggregatorDict, self).__init__()\n+        super().__init__()\n         self.aggregator: Dict[str, ErrorAggregator] = dict()\n \n     def __getitem__(self, item: str):\n@@ -179,7 +179,7 @@ def record(self, error_type: str, error: str, module: str):\n             log.exception(\"%s error from %s\", error_type, module)\n \n \n-class ExportErrorCsvParser(object):\n+class ExportErrorCsvParser:\n     \"\"\"Parses `*_export_error.csv` produced by onnxbench, aggregates errors and produces report.\n \n     Two types of aggregations are performed.\n@@ -310,7 +310,7 @@ def row(self) -> List[str]:\n         return [getattr(self, field.name) for field in dataclasses.fields(self)]\n \n \n-class ExportErrorParser(object):\n+class ExportErrorParser:\n     def __init__(self, device: str, model_name: str, batch_size: int):\n         self.device = device\n         self.model_name = model_name\ndiff --git a/benchmarks/dynamo/benchmarks.py b/benchmarks/dynamo/benchmarks.py\nindex 36aaf33df96b1f..cb4cc84867ca9b 100755\n--- a/benchmarks/dynamo/benchmarks.py\n+++ b/benchmarks/dynamo/benchmarks.py\n@@ -9,7 +9,7 @@\n # TOOD(voz): Someday, consolidate all the files into one runner instead of a shim like this...\n def model_names(filename: str) -> Set[str]:\n     names = set()\n-    with open(filename, \"r\") as fh:\n+    with open(filename) as fh:\n         lines = fh.readlines()\n         lines = [line.rstrip() for line in lines]\n         for line in lines:\ndiff --git a/benchmarks/dynamo/combine_csv.py b/benchmarks/dynamo/combine_csv.py\nindex b579e0a1bbbd5c..560b8a3cf2405a 100644\n--- a/benchmarks/dynamo/combine_csv.py\n+++ b/benchmarks/dynamo/combine_csv.py\n@@ -11,7 +11,7 @@\n RESULTS = defaultdict(dict)\n \n for side, f in zip([\"static\", \"dynamic\"], sys.argv[1:]):\n-    with open(f, \"r\") as f:\n+    with open(f) as f:\n         reader = csv.DictReader(f)\n         for row in reader:\n             RESULTS[(row[\"bench\"], row[\"name\"])][side] = row\ndiff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py\nindex cabc18f35697a7..0e1ce36461a122 100644\n--- a/benchmarks/dynamo/common.py\n+++ b/benchmarks/dynamo/common.py\n@@ -341,7 +341,7 @@ def load_model_from_path(path_and_class_str):\n \n def output_csv(filename, headers, row):\n     if os.path.exists(filename):\n-        with open(filename, \"r\") as fd:\n+        with open(filename) as fd:\n             lines = list(csv.reader(fd)) or [[]]\n             if headers and len(headers) > len(lines[0]):\n                 # if prior results failed the header might not be filled in yet\n@@ -1417,7 +1417,7 @@ def read_batch_size_from_file(args, filename, model_name):\n     if os.path.exists(\"benchmarks\"):\n         filename = os.path.join(\"benchmarks\", filename)\n     assert os.path.exists(filename), filename\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n         lines = [i.split(\",\") for i in lines if len(i.strip()) > 0]\n         for val in lines:\ndiff --git a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\nindex 997624f583b4c8..f83568c4db6cc6 100644\n--- a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n+++ b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n@@ -240,7 +240,7 @@ class OperatorInputsLoader:\n     def __init__(self, json_file_path):\n         self.operator_db = defaultdict(Counter)\n \n-        with open(json_file_path, \"r\") as f:\n+        with open(json_file_path) as f:\n             lines = f.readlines()\n \n         i = 0\ndiff --git a/benchmarks/dynamo/parse_logs.py b/benchmarks/dynamo/parse_logs.py\nindex aeb72c231c5462..8ae272897903f9 100644\n--- a/benchmarks/dynamo/parse_logs.py\n+++ b/benchmarks/dynamo/parse_logs.py\n@@ -15,7 +15,7 @@\n \n assert len(sys.argv) == 2\n \n-full_log = open(sys.argv[1], \"r\").read()\n+full_log = open(sys.argv[1]).read()\n \n # If the log contains a gist URL, extract it so we can include it in the CSV\n gist_url = \"\"\ndiff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py\nindex c3c3e8bc046c8e..5a544914ca74bd 100755\n--- a/benchmarks/dynamo/runner.py\n+++ b/benchmarks/dynamo/runner.py\n@@ -608,7 +608,7 @@ def __init__(\n \n     def has_header(self, output_filename):\n         header_present = False\n-        with open(output_filename, \"r\") as f:\n+        with open(output_filename) as f:\n             line = f.readline()\n             if \"dev\" in line:\n                 header_present = True\n@@ -1026,7 +1026,7 @@ def __init__(self, args):\n         assert os.path.exists(self.lookup_file)\n \n     def generate_diff(self, last2, filename, caption):\n-        df_cur, df_prev = [pd.read_csv(os.path.join(path, filename)) for path in last2]\n+        df_cur, df_prev = (pd.read_csv(os.path.join(path, filename)) for path in last2)\n         df_merge = df_cur.merge(df_prev, on=\"Compiler\", suffixes=(\"_cur\", \"_prev\"))\n         data = {col: [] for col in (\"compiler\", \"suite\", \"prev_value\", \"cur_value\")}\n         for _, row in df_merge.iterrows():\n@@ -1145,10 +1145,10 @@ def generate_comment(self):\n                     if last2[compiler] is None:\n                         continue\n \n-                    df_cur, df_prev = [\n+                    df_cur, df_prev = (\n                         last2[compiler][i].untouched_parsed_frames[suite][metric]\n                         for i in (0, 1)\n-                    ]\n+                    )\n                     df_merge = df_cur.merge(\n                         df_prev, on=\"name\", suffixes=(\"_cur\", \"_prev\")\n                     )\n@@ -1367,7 +1367,7 @@ def gen_comment(self):\n         all_lines = []\n         for f in files:\n             try:\n-                with open(os.path.join(self.output_dir, f), \"r\") as fh:\n+                with open(os.path.join(self.output_dir, f)) as fh:\n                     all_lines.extend(fh.readlines())\n             except FileNotFoundError:\n                 pass\ndiff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py\nindex 75769f7cb6c50a..587dbb93683f3f 100755\n--- a/benchmarks/dynamo/timm_models.py\n+++ b/benchmarks/dynamo/timm_models.py\n@@ -31,7 +31,7 @@ def pip_install(package):\n TIMM_MODELS = dict()\n filename = os.path.join(os.path.dirname(__file__), \"timm_models_list.txt\")\n \n-with open(filename, \"r\") as fh:\n+with open(filename) as fh:\n     lines = fh.readlines()\n     lines = [line.rstrip() for line in lines]\n     for line in lines:\n@@ -92,7 +92,7 @@ def read_models_from_docs():\n         models = set()\n         # TODO - set the path to pytorch-image-models repo\n         for fn in glob.glob(\"../pytorch-image-models/docs/models/*.md\"):\n-            with open(fn, \"r\") as f:\n+            with open(fn) as f:\n                 while True:\n                     line = f.readline()\n                     if not line:\ndiff --git a/benchmarks/fastrnns/bench.py b/benchmarks/fastrnns/bench.py\nindex d4b70ff78b7a72..f0e9679b80f275 100644\n--- a/benchmarks/fastrnns/bench.py\n+++ b/benchmarks/fastrnns/bench.py\n@@ -187,7 +187,7 @@ def bench(rnn_runners, group_name, print_json=False, sep=' ', **params):\n \n \n def bench_group(model_list, bench_name, bench_group, bench_args):\n-    print_stderr('Benchmarking {}s...'.format(bench_name))\n+    print_stderr(f'Benchmarking {bench_name}s...')\n     nn_results = bench(get_nn_runners(*model_list), bench_group, **bench_args)\n     print_stderr('')\n     return nn_results\ndiff --git a/benchmarks/fastrnns/profile.py b/benchmarks/fastrnns/profile.py\nindex 7f3de61ef9c39f..10707fab986bc7 100644\n--- a/benchmarks/fastrnns/profile.py\n+++ b/benchmarks/fastrnns/profile.py\n@@ -54,7 +54,7 @@ def profile(rnns, sleep_between_seconds=1, nloops=5,\n \n def system(command):\n     \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n-    print('[system] {}'.format(command))\n+    print(f'[system] {command}')\n     p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, shell=True)\n     output, err = p.communicate()\n@@ -87,13 +87,13 @@ def nvprof_output_filename(rnns, **params):\n \n \n def nvprof(cmd, outpath):\n-    return system('nvprof -o {} {}'.format(outpath, cmd))\n+    return system(f'nvprof -o {outpath} {cmd}')\n \n \n def full_profile(rnns, **args):\n     profile_args = []\n     for k, v in args.items():\n-        profile_args.append('--{}={}'.format(k, v))\n+        profile_args.append(f'--{k}={v}')\n     profile_args.append('--rnns {}'.format(' '.join(rnns)))\n     profile_args.append('--internal-run')\n \n@@ -103,7 +103,7 @@ def full_profile(rnns, **args):\n         sys.executable, ' '.join(profile_args))\n     rc, stdout, stderr = nvprof(cmd, outpath)\n     if rc != 0:\n-        raise RuntimeError('stderr: {}\\nstdout: {}'.format(stderr, stdout))\n+        raise RuntimeError(f'stderr: {stderr}\\nstdout: {stdout}')\n \n \n if __name__ == '__main__':\ndiff --git a/benchmarks/fastrnns/runner.py b/benchmarks/fastrnns/runner.py\nindex c6bf3727f38071..c33f3c92ad0f04 100644\n--- a/benchmarks/fastrnns/runner.py\n+++ b/benchmarks/fastrnns/runner.py\n@@ -11,7 +11,7 @@\n                       varlen_lstm_creator, varlen_pytorch_lstm_creator)\n \n \n-class DisableCuDNN():\n+class DisableCuDNN:\n     def __enter__(self):\n         self.saved = torch.backends.cudnn.enabled\n         torch.backends.cudnn.enabled = False\n@@ -20,7 +20,7 @@ def __exit__(self, *args, **kwargs):\n         torch.backends.cudnn.enabled = self.saved\n \n \n-class DummyContext():\n+class DummyContext:\n     def __enter__(self):\n         pass\n \n@@ -28,7 +28,7 @@ def __exit__(self, *args, **kwargs):\n         pass\n \n \n-class AssertNoJIT():\n+class AssertNoJIT:\n     def __enter__(self):\n         import os\n         enabled = os.environ.get('PYTORCH_JIT', 1)\ndiff --git a/benchmarks/fastrnns/test.py b/benchmarks/fastrnns/test.py\nindex a56cf928fd7add..640af10b95c042 100644\n--- a/benchmarks/fastrnns/test.py\n+++ b/benchmarks/fastrnns/test.py\n@@ -71,7 +71,7 @@ def test_vl_py(**test_args):\n     control_creator = varlen_pytorch_lstm_creator\n     name, experim_creator, context = get_nn_runners('vl_py')[0]\n     with context():\n-        print('testing {}...'.format(name))\n+        print(f'testing {name}...')\n         creator_keys = [\n             'seqLength', 'numLayers', 'inputSize',\n             'hiddenSize', 'miniBatch', 'device', 'seed'\n@@ -154,5 +154,5 @@ def test_vl_py(**test_args):\n \n     for name, creator, context in rnn_runners:\n         with context():\n-            print('testing {}...'.format(name))\n+            print(f'testing {name}...')\n             test_rnns(creator, pytorch_lstm_creator, **test_args)\ndiff --git a/benchmarks/framework_overhead_benchmark/C2Module.py b/benchmarks/framework_overhead_benchmark/C2Module.py\nindex dfc5e6e79098a6..b6b80e83db07d0 100644\n--- a/benchmarks/framework_overhead_benchmark/C2Module.py\n+++ b/benchmarks/framework_overhead_benchmark/C2Module.py\n@@ -20,14 +20,14 @@ class C2SimpleNet:\n     def __init__(self, op_name, num_inputs=1, debug=False):\n         self.input_names = []\n         self.net = core.Net(\"framework_benchmark_net\")\n-        self.input_names = [\"in_{}\".format(i) for i in range(num_inputs)]\n+        self.input_names = [f\"in_{i}\" for i in range(num_inputs)]\n         for i in range(num_inputs):\n             add_blob(workspace, self.input_names[i], [1])\n         self.net.AddExternalInputs(self.input_names)\n         op_constructor = getattr(self.net, op_name)\n         op_constructor(self.input_names)\n         self.output_name = self.net._net.op[-1].output\n-        print(\"Benchmarking op {}:\".format(op_name))\n+        print(f\"Benchmarking op {op_name}:\")\n         for _ in range(NUM_LOOP_ITERS):\n             output_name = self.net._net.op[-1].output\n             self.input_names[-1] = output_name[0]\ndiff --git a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\nindex 727b78197b39bc..fd02a00c43655d 100644\n--- a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n+++ b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n@@ -31,7 +31,7 @@ def parse_op_args(op):\n def print_results(result):\n     print(\"===================================\")\n     for key, value in result.items():\n-        print(\"{}, latency per iter (us):{}\".format(key, ms_to_us(value)))\n+        print(f\"{key}, latency per iter (us):{ms_to_us(value)}\")\n     print(\"===================================\")\n \n def benchmark_simple_fn(args, config, module_config, module_type, result):\n@@ -46,7 +46,7 @@ def benchmark_simple_fn(args, config, module_config, module_type, result):\n         result:         dictionary instance to be populated with the benchmark result (latency per iter).\n     \"\"\"\n     benchmark_c2_net = args.benchmark_c2_net\n-    print(\"Benchmarking {}\".format(module_type.__name__))\n+    print(f\"Benchmarking {module_type.__name__}\")\n     if benchmark_c2_net:\n         op_name = module_config.c2_op\n         num_inputs = module_config.num_params\n@@ -86,7 +86,7 @@ def main():\n     args = parser.parse_args()\n \n     if args.op not in SUPPORTED_OPS:\n-        print(\"Op {} is not supported: Supported ops are:{}\".format(args.op, SUPPORTED_OPS))\n+        print(f\"Op {args.op} is not supported: Supported ops are:{SUPPORTED_OPS}\")\n         return\n     assert not (args.benchmark_c2_net and args.use_throughput_benchmark), \\\n         \"Benchmarking of C2 net via throughput benchmarking is not yet supported\"\ndiff --git a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\nindex 154564f1c6d799..19f2471cbbfaaa 100644\n--- a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n+++ b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n@@ -31,8 +31,8 @@ def __init__(self, wrapped_type, module_config, debug, save=False):\n             if save:\n                 file_name = self.module_name + \"_\" + pt_fn.__name__ + \".pt\"\n                 torch.jit.save(self.module, file_name)\n-                print(\"Generated graph is saved in {}\".format(file_name))\n-        print(\"Benchmarking module {} with fn {}: Graph mode:{}\".format(self.module_name, pt_fn.__name__, module_config.graph_mode))\n+                print(f\"Generated graph is saved in {file_name}\")\n+        print(f\"Benchmarking module {self.module_name} with fn {pt_fn.__name__}: Graph mode:{module_config.graph_mode}\")\n         if (debug and isinstance(self.module, torch.jit.ScriptModule)):\n             print(self.module.graph)\n             print(self.module.code)\ndiff --git a/benchmarks/framework_overhead_benchmark/utils.py b/benchmarks/framework_overhead_benchmark/utils.py\nindex 9e760d404339ed..2efb67a51f7887 100644\n--- a/benchmarks/framework_overhead_benchmark/utils.py\n+++ b/benchmarks/framework_overhead_benchmark/utils.py\n@@ -26,7 +26,7 @@ def benchmark_module(config, module, use_throughput_benchmark=False):\n     if use_throughput_benchmark:\n         return benchmark_using_throughput_benchmark(config, module)\n     module.forward(config.num_warmup_iters)\n-    print(\"Running module for {} iterations\".format(config.num_iters))\n+    print(f\"Running module for {config.num_iters} iterations\")\n     start = time.time()\n     module.forward(config.num_iters)\n     end = time.time()\ndiff --git a/benchmarks/functional_autograd_benchmark/compare.py b/benchmarks/functional_autograd_benchmark/compare.py\nindex c2c4ef6c95d5be..65a4a3afcea881 100644\n--- a/benchmarks/functional_autograd_benchmark/compare.py\n+++ b/benchmarks/functional_autograd_benchmark/compare.py\n@@ -10,11 +10,11 @@ def main():\n     parser.add_argument(\"--output\", type=str, default=\"\", help=\"Text file where to write the output\")\n     args = parser.parse_args()\n \n-    with open(args.before, \"r\") as f:\n+    with open(args.before) as f:\n         content = f.read()\n     res_before = from_markdown_table(content)\n \n-    with open(args.after, \"r\") as f:\n+    with open(args.after) as f:\n         content = f.read()\n     res_after = from_markdown_table(content)\n \ndiff --git a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\nindex 1b0ef20902da9b..76c447f04a496f 100644\n--- a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n+++ b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n@@ -198,7 +198,7 @@ def noop():\n             pass\n         do_sync = noop\n     else:\n-        device = torch.device(\"cuda:{}\".format(args.gpu))\n+        device = torch.device(f\"cuda:{args.gpu}\")\n         do_sync = torch.cuda.synchronize\n \n     model, inp = model_getter(device)\n@@ -257,7 +257,7 @@ def main():\n             runtimes = torch.tensor(runtimes)\n             mean, var = runtimes.mean(), runtimes.var()\n             results[name][task] = (mean.item(), var.item())\n-            print(\"Results for model {} on task {}: {}s (var: {})\".format(name, task, mean, var))\n+            print(f\"Results for model {name} on task {task}: {mean}s (var: {var})\")\n \n             if has_functorch:\n                 try:\n@@ -269,7 +269,7 @@ def main():\n                 runtimes = torch.tensor(runtimes)\n                 mean, var = runtimes.mean(), runtimes.var()\n                 results[name][f\"functorch {task}\"] = (mean.item(), var.item())\n-                print(\"Results for model {} on task {} using Functorch: {}s (var: {})\".format(name, task, mean, var))\n+                print(f\"Results for model {name} on task {task} using Functorch: {mean}s (var: {var})\")\n \n     if args.output:\n         with open(args.output, \"w\") as f:\ndiff --git a/benchmarks/functional_autograd_benchmark/utils.py b/benchmarks/functional_autograd_benchmark/utils.py\nindex dcf03e7a28d085..23f3481cbde117 100644\n--- a/benchmarks/functional_autograd_benchmark/utils.py\n+++ b/benchmarks/functional_autograd_benchmark/utils.py\n@@ -97,7 +97,7 @@ def from_markdown_table(data: str) -> TimingResultType:\n     res = defaultdict(defaultdict)\n \n     for line in out:\n-        model, task, mean, var = [f.strip() for f in line.strip().split(\"|\") if f]\n+        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n         res[model][task] = (float(mean), float(var))\n \n     return res\ndiff --git a/benchmarks/instruction_counts/applications/ci.py b/benchmarks/instruction_counts/applications/ci.py\nindex 85ee9881d83b55..c34c60b094a857 100644\n--- a/benchmarks/instruction_counts/applications/ci.py\n+++ b/benchmarks/instruction_counts/applications/ci.py\n@@ -70,7 +70,7 @@ def main(argv: List[str]) -> None:\n     }\n \n     if args.destination:\n-        with open(args.destination, \"wt\") as f:\n+        with open(args.destination, \"w\") as f:\n             json.dump(final_results, f)\n \n     if in_debug_mode:\ndiff --git a/benchmarks/instruction_counts/core/expand.py b/benchmarks/instruction_counts/core/expand.py\nindex f6713ee65cb93c..c60925d9e14e91 100644\n--- a/benchmarks/instruction_counts/core/expand.py\n+++ b/benchmarks/instruction_counts/core/expand.py\n@@ -58,7 +58,7 @@ def _generate_torchscript_file(model_src: str, name: str) -> Optional[str]:\n         # to confirm.\n         raise ValueError(f\"File {module_path} already exists.\")\n \n-    with open(module_path, \"wt\") as f:\n+    with open(module_path, \"w\") as f:\n         f.write(model_src)\n \n     # Import magic to actually load our function.\ndiff --git a/benchmarks/operator_benchmark/benchmark_caffe2.py b/benchmarks/operator_benchmark/benchmark_caffe2.py\nindex d5939030d03c1a..df27a172739bf6 100644\n--- a/benchmarks/operator_benchmark/benchmark_caffe2.py\n+++ b/benchmarks/operator_benchmark/benchmark_caffe2.py\n@@ -122,7 +122,7 @@ def run_forward(self, num_runs, print_per_iter=False, cuda_sync=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.forward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator test case: {self.test_name}\")\n \n     def run_backward(self, num_runs, print_per_iter=False):\n         \"\"\" Run the backward path of an operator in a loop\n@@ -130,7 +130,7 @@ def run_backward(self, num_runs, print_per_iter=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.backward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator gradient test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator gradient test case: {self.test_name}\")\n \n     def _print_per_iter(self):\n         pass\n@@ -140,7 +140,7 @@ def create_caffe2_op_test_case(op_bench, test_config):\n     test_case = Caffe2OperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\n \n \ndiff --git a/benchmarks/operator_benchmark/benchmark_core.py b/benchmarks/operator_benchmark/benchmark_core.py\nindex d6fd00f0522b38..3dfb6e3b00d42c 100644\n--- a/benchmarks/operator_benchmark/benchmark_core.py\n+++ b/benchmarks/operator_benchmark/benchmark_core.py\n@@ -197,7 +197,7 @@ def _print_header(self):\n             print(\"# List of Operators to run:\")\n             self.printed_ops_list = set()\n             if self.args.operators:\n-                print(\"# {}\".format(self.args.operators))\n+                print(f\"# {self.args.operators}\")\n \n     def _print_perf_result(self, reported_run_time_us, test_case):\n         if self.args.report_aibench:\n@@ -206,7 +206,7 @@ def _print_perf_result(self, reported_run_time_us, test_case):\n             return\n             test_name = '_'.join([test_case.framework, test_case.test_config.test_name])\n             for run in range(self.num_runs):\n-                print(\"{}Observer \".format(test_case.framework) + json.dumps(\n+                print(f\"{test_case.framework}Observer \" + json.dumps(\n                     {\n                         \"type\": test_name,\n                         \"metric\": \"latency\",\n@@ -349,14 +349,14 @@ def _keep_test(self, test_case):\n     def _print_test_case_info(self, test_case):\n         # Print out the test name and skip the real execution\n         if self.args.list_tests:\n-            print(\"# {}\".format(test_case.test_config.test_name))\n+            print(f\"# {test_case.test_config.test_name}\")\n             return True\n         elif self.args.list_ops:\n             if self.args.operators is None:\n                 op_name = test_case.op_bench.module_name()\n \n                 if op_name not in self.printed_ops_list:\n-                    print(\"# {}\".format(op_name))\n+                    print(f\"# {op_name}\")\n                     self.printed_ops_list.add(op_name)\n             return True\n \ndiff --git a/benchmarks/operator_benchmark/benchmark_pytorch.py b/benchmarks/operator_benchmark/benchmark_pytorch.py\nindex e9a9b3c5de42ad..c4a82dff2ba5c1 100644\n--- a/benchmarks/operator_benchmark/benchmark_pytorch.py\n+++ b/benchmarks/operator_benchmark/benchmark_pytorch.py\n@@ -192,5 +192,5 @@ def create_pytorch_op_test_case(op_bench, test_config):\n     test_case = PyTorchOperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\ndiff --git a/benchmarks/operator_benchmark/common/repeat_benchmark.py b/benchmarks/operator_benchmark/common/repeat_benchmark.py\nindex b744a95d217363..337bf525b29caf 100644\n--- a/benchmarks/operator_benchmark/common/repeat_benchmark.py\n+++ b/benchmarks/operator_benchmark/common/repeat_benchmark.py\n@@ -54,4 +54,4 @@ def pt_repeat_n_times(niters):\n     total_time_s = (time.time() - s)\n     total_time_per_iter_s = total_time_s / NUM_BENCHMARK_ITERS\n     achieved_bandwidth = (total_bytes * BYTES_TO_MB) / total_time_per_iter_s\n-    print(\"Time:{} Achieved Bandwidth:{} MB/s\".format(total_time_per_iter_s, achieved_bandwidth))\n+    print(f\"Time:{total_time_per_iter_s} Achieved Bandwidth:{achieved_bandwidth} MB/s\")\ndiff --git a/benchmarks/overrides_benchmark/bench.py b/benchmarks/overrides_benchmark/bench.py\nindex b6dbd0c2f8d64c..2c591a6e569793 100644\n--- a/benchmarks/overrides_benchmark/bench.py\n+++ b/benchmarks/overrides_benchmark/bench.py\n@@ -56,8 +56,8 @@ def main():\n \n         bench_min, bench_std = bench(tensor_1, tensor_2)\n         print(\n-            \"Type {0} had a minimum time of {1} us\"\n-            \" and a standard deviation of {2} us.\".format(\n+            \"Type {} had a minimum time of {} us\"\n+            \" and a standard deviation of {} us.\".format(\n                 t.__name__, (10 ** 6 * bench_min), (10 ** 6) * bench_std\n             )\n         )\ndiff --git a/benchmarks/sparse/dlmc/matmul_bench.py b/benchmarks/sparse/dlmc/matmul_bench.py\nindex 6b896ddf34a635..8d37d3242dd1bf 100644\n--- a/benchmarks/sparse/dlmc/matmul_bench.py\n+++ b/benchmarks/sparse/dlmc/matmul_bench.py\n@@ -62,8 +62,8 @@ def filter_ops(operation):\n             test_name = device + \":matmul-forward\"\n             return list(filter(None, [\n                 (test_name, device, \"torch:\" + operation.replace(\"sparse\", \"dense\"),\n-                 \"{}(dx, dy)\".format(OPS_MAP[operation])),\n-                (test_name, device, \"torch:\" + operation, \"{}(x, y)\".format(OPS_MAP[operation])),\n+                 f\"{OPS_MAP[operation]}(dx, dy)\"),\n+                (test_name, device, \"torch:\" + operation, f\"{OPS_MAP[operation]}(x, y)\"),\n                 (test_name, device, \"scipy:\" + operation, \"scipy_matmul(sx, sy)\") if device == \"cpu\" else None\n             ]))\n \ndiff --git a/benchmarks/sparse/dlmc/utils.py b/benchmarks/sparse/dlmc/utils.py\nindex 3079abf6e1dff2..8fad391f63f83d 100644\n--- a/benchmarks/sparse/dlmc/utils.py\n+++ b/benchmarks/sparse/dlmc/utils.py\n@@ -21,7 +21,7 @@ def sparse_grad_output(a, b):\n \n \n def read_matrix_params(path):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         line = file.readline()\n         nrows, ncols, nnz = (int(el) for el in line.split(', '))\n         return (nrows, ncols), nnz\n@@ -38,7 +38,7 @@ def csr_to_coo(indices, indptr, shape):\n \n \n def load_sparse_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -51,7 +51,7 @@ def load_sparse_matrix(path, device):\n \n \n def gen_vector(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -59,7 +59,7 @@ def gen_vector(path, device):\n \n \n def gen_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\ndiff --git a/benchmarks/tensorexpr/__main__.py b/benchmarks/tensorexpr/__main__.py\nindex ed632e966b2cb4..fa81ea6bb1c611 100644\n--- a/benchmarks/tensorexpr/__main__.py\n+++ b/benchmarks/tensorexpr/__main__.py\n@@ -157,7 +157,7 @@ def main():\n         torch._C._jit_set_nvfuser_enabled(True)\n         torch._C._get_graph_executor_optimize(True)\n     else :\n-        raise ValueError(\"Undefined fuser: {}\".format(args.cuda_fuser))\n+        raise ValueError(f\"Undefined fuser: {args.cuda_fuser}\")\n \n     if args.cpu_fusion:\n         import torch\n@@ -207,7 +207,7 @@ def set_global_threads(num_threads):\n     for index, dtype in enumerate(datatypes):\n         datatypes[index] = getattr(torch, dtype)\n         if not datatypes[index] :\n-            raise AttributeError(\"DataType: {} is not valid!\".format(dtype))\n+            raise AttributeError(f\"DataType: {dtype} is not valid!\")\n \n     tensor_engine.set_engine_mode(args.engine)\n \n@@ -282,7 +282,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                         run_with_input_iter(bench_cls, args.input_iter, allow_skip=True)\n                     else :\n                         if args.input_iter is not None :\n-                            print(\"WARNING: Incompatible benchmark class called with input_iter arg: {}\".format(name))\n+                            print(f\"WARNING: Incompatible benchmark class called with input_iter arg: {name}\")\n                         run_default_configs(bench_cls, allow_skip=True)\n \n             if match_class_name:\n@@ -321,8 +321,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                     [bench_cls.module() for bench_cls in benchmark_classes]\n                 )\n                 raise ValueError(\n-                    \"invalid name: %s\\nAvailable benchmark classes:\\n%s\"\n-                    % (name, available_classes)\n+                    f\"invalid name: {name}\\nAvailable benchmark classes:\\n{available_classes}\"\n                 )\n \n \ndiff --git a/benchmarks/tensorexpr/benchmark.py b/benchmarks/tensorexpr/benchmark.py\nindex 7a5b255da904aa..2730a1be24784b 100644\n--- a/benchmarks/tensorexpr/benchmark.py\n+++ b/benchmarks/tensorexpr/benchmark.py\n@@ -66,7 +66,7 @@ def desc(self):\n         if \"NNC_NUM_THREADS\" in os.environ:\n             num_threads_str = os.environ[\"NNC_NUM_THREADS\"]\n             device += num_threads_str\n-        return \"%s: %s_%s_%s_%s\" % (\n+        return \"{}: {}_{}_{}_{}\".format(\n             self.engine.mode,\n             self.module(),\n             self.mode,\n@@ -203,7 +203,7 @@ def dump_result(self, result_dict):\n         if self.output_type == \"json\":\n             print(json.dumps(result_dict))\n         elif self.output_type == \"stdout\":\n-            msg = \"%s: %.2f us, SOL %.2f GB/s, algorithmic %.2f GB/s\" % (\n+            msg = \"{}: {:.2f} us, SOL {:.2f} GB/s, algorithmic {:.2f} GB/s\".format(\n                 result_dict[\"desc\"],\n                 result_dict[\"us\"],\n                 result_dict[\"sol\"],\ndiff --git a/benchmarks/tensorexpr/reduction.py b/benchmarks/tensorexpr/reduction.py\nindex 77d64074eb81d1..3613001667746d 100644\n--- a/benchmarks/tensorexpr/reduction.py\n+++ b/benchmarks/tensorexpr/reduction.py\n@@ -139,7 +139,7 @@ def __init__(self, mode, device, dtype, red_dim, dim0, dim1):\n         )]\n \n         if red_dim != 0 and red_dim != 1 :\n-            raise ValueError(\"invalid reduction dimension: {}\".format(red_dim))\n+            raise ValueError(f\"invalid reduction dimension: {red_dim}\")\n \n     def forward(self, inputs):\n         x = self.add(inputs, 0.001)\ndiff --git a/benchmarks/upload_scribe.py b/benchmarks/upload_scribe.py\nindex d476ade1b8df4d..551544b2d288ae 100644\n--- a/benchmarks/upload_scribe.py\n+++ b/benchmarks/upload_scribe.py\n@@ -95,7 +95,7 @@ def post_pytest_benchmarks(self, pytest_json):\n         for b in pytest_json['benchmarks']:\n             test = b['name'].split('[')[0]\n             net_name = b['params']['net_name']\n-            benchmark_name = '{}[{}]'.format(test, net_name)\n+            benchmark_name = f'{test}[{net_name}]'\n             executor = b['params']['executor']\n             fuser = b['params']['fuser']\n             m = self.format_message({\n"
  },
  {
    "number": 105398,
    "title": "[BE] Enable ruff's UP rules and autoformat tools and scripts",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "a14e46d5efd81399d4837c9195d44ea81eab8e1d",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105398",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105398/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105398.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105398.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105398/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105398/comments",
    "labels": [
      "open source",
      "release notes: releng"
    ],
    "_event_time": "2023-07-18T01:12:41.870537Z",
    "state": "closed",
    "patch": "From 30306309cb96b7bce6029fb61bb11c97f075b1af Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:35 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat tools and scripts\n\n[ghstack-poisoned]\n---\n .ci/pytorch/create_test_cert.py               |  4 +-\n .../perf_test/compare_with_baseline.py        |  4 +-\n .../cimodel/data/binary_build_definitions.py  |  2 +-\n .../cimodel/data/pytorch_build_definitions.py |  4 +-\n .circleci/generate_config_yml.py              |  4 +-\n .github/scripts/ensure_actions_will_cancel.py |  4 +-\n .github/scripts/file_io_utils.py              |  2 +-\n .github/scripts/filter_test_configs.py        |  2 +-\n .github/scripts/generate_pytorch_version.py   |  2 +-\n .github/scripts/label_utils.py                |  2 +-\n .github/scripts/lint_native_functions.py      |  2 +-\n .github/scripts/run_torchbench.py             |  4 +-\n .github/scripts/test_check_labels.py          |  2 +-\n .github/scripts/test_trymerge.py              |  2 +-\n .github/scripts/trymerge.py                   |  6 +--\n .github/scripts/trymerge_explainer.py         |  2 +-\n docs/caffe2/process.py                        |  4 +-\n docs/cpp/source/conf.py                       |  3 +-\n docs/source/conf.py                           |  3 +-\n .../scripts/exportdb/generate_example_rst.py  |  4 +-\n setup.py                                      | 38 +++++++++----------\n tools/amd_build/build_amd.py                  |  8 ++--\n tools/autograd/gen_python_functions.py        |  8 ++--\n tools/autograd/load_derivatives.py            |  2 +-\n .../gen_op_registration_allowlist.py          |  4 +-\n tools/code_analyzer/gen_oplist.py             |  2 +-\n tools/coverage_plugins_package/setup.py       |  2 +-\n tools/download_mnist.py                       | 10 ++---\n tools/gen_vulkan_spv.py                       | 20 +++++-----\n tools/generate_torch_version.py               | 12 +++---\n tools/jit/gen_unboxing.py                     |  2 +-\n tools/linter/adapters/constexpr_linter.py     |  2 +-\n tools/linter/adapters/grep_linter.py          |  2 +-\n tools/nightly.py                              |  8 ++--\n tools/onnx/gen_diagnostics.py                 |  2 +-\n tools/onnx/update_default_opset_version.py    |  2 +-\n tools/pyi/gen_pyi.py                          | 20 +++++-----\n tools/setup_helpers/cmake.py                  | 18 ++++-----\n tools/setup_helpers/cmake_utils.py            |  2 +-\n tools/setup_helpers/generate_code.py          |  2 +-\n tools/stats/import_test_stats.py              |  2 +-\n tools/stats/upload_stats_lib.py               |  4 +-\n tools/substitute.py                           |  2 +-\n tools/test/test_executorch_gen.py             |  4 +-\n tools/test/test_executorch_signatures.py      |  8 ++--\n tools/test/test_vulkan_codegen.py             |  4 +-\n tools/testing/explicit_ci_jobs.py             |  2 +-\n tools/testing/test_selections.py              |  2 +-\n torch/package/package_exporter.py             | 20 ++++------\n 49 files changed, 134 insertions(+), 142 deletions(-)\n\ndiff --git a/.ci/pytorch/create_test_cert.py b/.ci/pytorch/create_test_cert.py\nindex d3ead7ae259434..4e31f97878f41b 100644\n--- a/.ci/pytorch/create_test_cert.py\n+++ b/.ci/pytorch/create_test_cert.py\n@@ -88,9 +88,9 @@ def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n \n \n ca_key = genrsa(temp_dir + \"/ca.key\")\n-ca_cert = create_cert(temp_dir + \"/ca.pem\", u\"US\", u\"New York\", u\"New York\", u\"Gloo Certificate Authority\", ca_key)\n+ca_cert = create_cert(temp_dir + \"/ca.pem\", \"US\", \"New York\", \"New York\", \"Gloo Certificate Authority\", ca_key)\n \n pkey = genrsa(temp_dir + \"/pkey.key\")\n-csr = create_req(temp_dir + \"/csr.csr\", u\"US\", u\"California\", u\"San Francisco\", u\"Gloo Testing Company\", pkey)\n+csr = create_req(temp_dir + \"/csr.csr\", \"US\", \"California\", \"San Francisco\", \"Gloo Testing Company\", pkey)\n \n cert = sign_certificate_request(temp_dir + \"/cert.pem\", csr, ca_cert, ca_key)\ndiff --git a/.ci/pytorch/perf_test/compare_with_baseline.py b/.ci/pytorch/perf_test/compare_with_baseline.py\nindex 6d2839ac1db412..f7b962632cd79f 100644\n--- a/.ci/pytorch/perf_test/compare_with_baseline.py\n+++ b/.ci/pytorch/perf_test/compare_with_baseline.py\n@@ -19,7 +19,7 @@\n elif 'gpu' in test_name:\n     backend = 'gpu'\n \n-data_file_path = '../{}_runtime.json'.format(backend)\n+data_file_path = f'../{backend}_runtime.json'\n \n with open(data_file_path) as data_file:\n     data = json.load(data_file)\n@@ -69,7 +69,7 @@\n     print(\"z-value < 3, no perf regression detected.\")\n     if args.update:\n         print(\"We will use these numbers as new baseline.\")\n-        new_data_file_path = '../new_{}_runtime.json'.format(backend)\n+        new_data_file_path = f'../new_{backend}_runtime.json'\n         with open(new_data_file_path) as new_data_file:\n             new_data = json.load(new_data_file)\n         new_data[test_name] = {}\ndiff --git a/.circleci/cimodel/data/binary_build_definitions.py b/.circleci/cimodel/data/binary_build_definitions.py\nindex 45981e8e9ea77b..7dccbdc7cbf689 100644\n--- a/.circleci/cimodel/data/binary_build_definitions.py\n+++ b/.circleci/cimodel/data/binary_build_definitions.py\n@@ -5,7 +5,7 @@\n import cimodel.lib.conf_tree as conf_tree\n import cimodel.lib.miniutils as miniutils\n \n-class Conf(object):\n+class Conf:\n     def __init__(self, os, gpu_version, pydistro, parms, smoke, libtorch_variant, gcc_config_variant, libtorch_config_variant):\n \n         self.os = os\ndiff --git a/.circleci/cimodel/data/pytorch_build_definitions.py b/.circleci/cimodel/data/pytorch_build_definitions.py\nindex 76e87b07c1889f..e6e44bd2b5aeb0 100644\n--- a/.circleci/cimodel/data/pytorch_build_definitions.py\n+++ b/.circleci/cimodel/data/pytorch_build_definitions.py\n@@ -143,7 +143,7 @@ def gen_workflow_job(self, phase):\n \n \n # TODO This is a hack to special case some configs just for the workflow list\n-class HiddenConf(object):\n+class HiddenConf:\n     def __init__(self, name, parent_build=None, filters=None):\n         self.name = name\n         self.parent_build = parent_build\n@@ -160,7 +160,7 @@ def gen_workflow_job(self, phase):\n     def gen_build_name(self, _):\n         return self.name\n \n-class DocPushConf(object):\n+class DocPushConf:\n     def __init__(self, name, parent_build=None, branch=\"master\"):\n         self.name = name\n         self.parent_build = parent_build\ndiff --git a/.circleci/generate_config_yml.py b/.circleci/generate_config_yml.py\nindex b3e47eed8b4317..d1ef439941d4b2 100755\n--- a/.circleci/generate_config_yml.py\n+++ b/.circleci/generate_config_yml.py\n@@ -18,7 +18,7 @@\n import cimodel.lib.miniyaml as miniyaml\n \n \n-class File(object):\n+class File:\n     \"\"\"\n     Verbatim copy the contents of a file into config.yml\n     \"\"\"\n@@ -57,7 +57,7 @@ def horizontal_rule():\n     return \"\".join(\"#\" * 78)\n \n \n-class Header(object):\n+class Header:\n     def __init__(self, title, summary=None):\n         self.title = title\n         self.summary_lines = summary or []\ndiff --git a/.github/scripts/ensure_actions_will_cancel.py b/.github/scripts/ensure_actions_will_cancel.py\nindex 92eb3441acd3cf..8d53f2bed5e18b 100755\n--- a/.github/scripts/ensure_actions_will_cancel.py\n+++ b/.github/scripts/ensure_actions_will_cancel.py\n@@ -17,7 +17,7 @@\n \n \n def should_check(filename: Path) -> bool:\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         content = f.read()\n \n     data = yaml.safe_load(content)\n@@ -37,7 +37,7 @@ def should_check(filename: Path) -> bool:\n     files = [f for f in files if should_check(f)]\n     names = set()\n     for filename in files:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             data = yaml.safe_load(f)\n \n         name = data.get(\"name\")\ndiff --git a/.github/scripts/file_io_utils.py b/.github/scripts/file_io_utils.py\nindex 097b092bc904af..faba9f06d2ac65 100644\n--- a/.github/scripts/file_io_utils.py\n+++ b/.github/scripts/file_io_utils.py\n@@ -44,7 +44,7 @@ def load_json_file(file_path: Path) -> Any:\n     \"\"\"\n     Returns the deserialized json object\n     \"\"\"\n-    with open(file_path, \"r\") as f:\n+    with open(file_path) as f:\n         return json.load(f)\n \n \ndiff --git a/.github/scripts/filter_test_configs.py b/.github/scripts/filter_test_configs.py\nindex 9d1f39a833c571..92968179702273 100755\n--- a/.github/scripts/filter_test_configs.py\n+++ b/.github/scripts/filter_test_configs.py\n@@ -319,7 +319,7 @@ def process_jobs(\n     try:\n         # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking\n         # it into its two components first\n-        current_platform, _ = [n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n]\n+        current_platform, _ = (n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n)\n     except ValueError as error:\n         warnings.warn(f\"Invalid job name {job_name}, returning\")\n         return test_matrix\ndiff --git a/.github/scripts/generate_pytorch_version.py b/.github/scripts/generate_pytorch_version.py\nindex f9a6d49505b308..e70b97165e61d7 100755\n--- a/.github/scripts/generate_pytorch_version.py\n+++ b/.github/scripts/generate_pytorch_version.py\n@@ -50,7 +50,7 @@ def get_tag() -> str:\n \n def get_base_version() -> str:\n     root = get_pytorch_root()\n-    dirty_version = open(root / \"version.txt\", \"r\").read().strip()\n+    dirty_version = open(root / \"version.txt\").read().strip()\n     # Strips trailing a0 from version.txt, not too sure why it's there in the\n     # first place\n     return re.sub(LEGACY_BASE_VERSION_SUFFIX_PATTERN, \"\", dirty_version)\ndiff --git a/.github/scripts/label_utils.py b/.github/scripts/label_utils.py\nindex 812c33b426f441..e3ce0f52fe853f 100644\n--- a/.github/scripts/label_utils.py\n+++ b/.github/scripts/label_utils.py\n@@ -51,7 +51,7 @@ def get_last_page_num_from_header(header: Any) -> int:\n     )\n \n \n-@lru_cache()\n+@lru_cache\n def gh_get_labels(org: str, repo: str) -> List[str]:\n     prefix = f\"https://api.github.com/repos/{org}/{repo}/labels?per_page=100\"\n     header, info = request_for_labels(prefix + \"&page=1\")\ndiff --git a/.github/scripts/lint_native_functions.py b/.github/scripts/lint_native_functions.py\nindex 9bde9e8d84e5f9..4dfe9fd63e2e4e 100755\n--- a/.github/scripts/lint_native_functions.py\n+++ b/.github/scripts/lint_native_functions.py\n@@ -26,7 +26,7 @@ def fn(base: str) -> str:\n     return str(base / Path(\"aten/src/ATen/native/native_functions.yaml\"))\n \n \n-with open(Path(__file__).parent.parent.parent / fn(\".\"), \"r\") as f:\n+with open(Path(__file__).parent.parent.parent / fn(\".\")) as f:\n     contents = f.read()\n \n yaml = ruamel.yaml.YAML()  # type: ignore[attr-defined]\ndiff --git a/.github/scripts/run_torchbench.py b/.github/scripts/run_torchbench.py\nindex 3a80ebbeb9970a..e5e3c7a03dea07 100644\n--- a/.github/scripts/run_torchbench.py\n+++ b/.github/scripts/run_torchbench.py\n@@ -129,7 +129,7 @@ def extract_models_from_pr(\n     model_list = []\n     userbenchmark_list = []\n     pr_list = []\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(filter(lambda x: x.startswith(MAGIC_PREFIX), lines))\n         if magic_lines:\n@@ -157,7 +157,7 @@ def extract_models_from_pr(\n \n def find_torchbench_branch(prbody_file: str) -> str:\n     branch_name: str = \"\"\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(\n             filter(lambda x: x.startswith(MAGIC_TORCHBENCH_PREFIX), lines)\ndiff --git a/.github/scripts/test_check_labels.py b/.github/scripts/test_check_labels.py\nindex 17d33158f2efb4..2b2cd7b6c5204b 100644\n--- a/.github/scripts/test_check_labels.py\n+++ b/.github/scripts/test_check_labels.py\n@@ -15,7 +15,7 @@\n \n \n def mock_parse_args() -> object:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.pr_num = 76123\n \ndiff --git a/.github/scripts/test_trymerge.py b/.github/scripts/test_trymerge.py\nindex 7cf70882ed3d40..a46ef9032a6459 100755\n--- a/.github/scripts/test_trymerge.py\n+++ b/.github/scripts/test_trymerge.py\n@@ -114,7 +114,7 @@ def mocked_rockset_results(head_sha: str, merge_base: str, num_retries: int = 3)\n \n \n def mock_parse_args(revert: bool = False, force: bool = False) -> Any:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.revert = revert\n             self.force = force\ndiff --git a/.github/scripts/trymerge.py b/.github/scripts/trymerge.py\nindex 3cffcaa14efaf8..cc253f36cbd1b7 100755\n--- a/.github/scripts/trymerge.py\n+++ b/.github/scripts/trymerge.py\n@@ -1628,10 +1628,8 @@ def validate_revert(\n         allowed_reverters.append(\"CONTRIBUTOR\")\n     if author_association not in allowed_reverters:\n         raise PostCommentError(\n-            (\n-                f\"Will not revert as @{author_login} is not one of \"\n-                f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n-            )\n+            f\"Will not revert as @{author_login} is not one of \"\n+            f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n         )\n     skip_internal_checks = can_skip_internal_checks(pr, comment_id)\n \ndiff --git a/.github/scripts/trymerge_explainer.py b/.github/scripts/trymerge_explainer.py\nindex ebc74cf63eb833..8aa6ab59b94cbc 100644\n--- a/.github/scripts/trymerge_explainer.py\n+++ b/.github/scripts/trymerge_explainer.py\n@@ -17,7 +17,7 @@ def has_label(labels: List[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:\n     return len(list(filter(pattern.match, labels))) > 0\n \n \n-class TryMergeExplainer(object):\n+class TryMergeExplainer:\n     force: bool\n     labels: List[str]\n     pr_num: int\ndiff --git a/docs/caffe2/process.py b/docs/caffe2/process.py\nindex 3b94b9d38502a2..4a59eec388d90b 100644\n--- a/docs/caffe2/process.py\n+++ b/docs/caffe2/process.py\n@@ -8,7 +8,7 @@\n \n # Module caffe2...caffe2.python.control_test\n def insert(originalfile, first_line, description):\n-    with open(originalfile, 'r') as f:\n+    with open(originalfile) as f:\n         f1 = f.readline()\n         if(f1.find(first_line) < 0):\n             docs = first_line + description + f1\n@@ -30,7 +30,7 @@ def insert(originalfile, first_line, description):\n     for file in files:\n         if (file.endswith(\".py\") and not file.endswith(\"_test.py\") and not file.endswith(\"__.py\")):\n             filepath = os.path.join(root, file)\n-            print((\"filepath: \" + filepath))\n+            print(\"filepath: \" + filepath)\n             directory = os.path.dirname(filepath)[2:]\n             directory = directory.replace(\"/\", \".\")\n             print(\"directory: \" + directory)\ndiff --git a/docs/cpp/source/conf.py b/docs/cpp/source/conf.py\nindex 88648787fa8c8e..2b94cfdb5058fb 100644\n--- a/docs/cpp/source/conf.py\n+++ b/docs/cpp/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -99,7 +98,7 @@\n     ############################################################################\n     # Main library page layout example configuration.                          #\n     ############################################################################\n-    \"afterTitleDescription\": textwrap.dedent(u'''\n+    \"afterTitleDescription\": textwrap.dedent('''\n         Welcome to the developer reference for the PyTorch C++ API.\n     '''),\n }\ndiff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 8bbb189853eec7..8fec5f16f9852c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -624,7 +623,7 @@ def visit_reference(self, node):\n                 anchor = ref_anchor[1]\n                 txt = node.parent.astext()\n                 if txt == anchor or txt == anchor.split('.')[-1]:\n-                    self.body.append('<p id=\"{}\"/>'.format(ref_anchor[1]))\n+                    self.body.append(f'<p id=\"{ref_anchor[1]}\"/>')\n         return old_call(self, node)\n     Klass.visit_reference = visit_reference\n \ndiff --git a/docs/source/scripts/exportdb/generate_example_rst.py b/docs/source/scripts/exportdb/generate_example_rst.py\nindex 58dca5f31ea73d..38f71b905245c3 100644\n--- a/docs/source/scripts/exportdb/generate_example_rst.py\n+++ b/docs/source/scripts/exportdb/generate_example_rst.py\n@@ -31,7 +31,7 @@ def generate_example_rst(example_case: ExportCase):\n         if isinstance(model, torch.nn.Module)\n         else inspect.getfile(model)\n     )\n-    with open(source_file, \"r\") as file:\n+    with open(source_file) as file:\n         source_code = file.read()\n     source_code = re.sub(r\"from torch\\._export\\.db\\.case import .*\\n\", \"\", source_code)\n     source_code = re.sub(r\"@export_case\\((.|\\n)*?\\)\\n\", \"\", source_code)\n@@ -114,7 +114,7 @@ def generate_index_rst(example_cases, tag_to_modules, support_level_to_modules):\n \n     tag_names = \"\\n    \".join(t for t in tag_to_modules.keys())\n \n-    with open(os.path.join(PWD, \"blurb.txt\"), \"r\") as file:\n+    with open(os.path.join(PWD, \"blurb.txt\")) as file:\n         blurb = file.read()\n \n     # Generate contents of the .rst file\ndiff --git a/setup.py b/setup.py\nindex 5f0180cb0c8ac6..d454b2e62f33b4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -323,7 +323,7 @@ def report(*args):\n package_name = os.getenv('TORCH_PACKAGE_NAME', 'torch')\n package_type = os.getenv('PACKAGE_TYPE', 'wheel')\n version = get_torch_version()\n-report(\"Building wheel {}-{}\".format(package_name, version))\n+report(f\"Building wheel {package_name}-{version}\")\n \n cmake = CMake()\n \n@@ -361,7 +361,7 @@ def not_exists_or_empty(folder):\n             start = time.time()\n             subprocess.check_call([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd)\n             end = time.time()\n-            print(' --- Submodule initialization took {:.2f} sec'.format(end - start))\n+            print(f' --- Submodule initialization took {end - start:.2f} sec')\n         except Exception:\n             print(' --- Submodule initalization failed')\n             print('Please run:\\n\\tgit submodule update --init --recursive')\n@@ -616,16 +616,16 @@ def build_extensions(self):\n                 continue\n             fullname = self.get_ext_fullname(ext.name)\n             filename = self.get_ext_filename(fullname)\n-            report(\"\\nCopying extension {}\".format(ext.name))\n+            report(f\"\\nCopying extension {ext.name}\")\n \n             relative_site_packages = sysconfig.get_path('purelib').replace(sysconfig.get_path('data'), '').lstrip(os.path.sep)\n             src = os.path.join(\"torch\", relative_site_packages, filename)\n             if not os.path.exists(src):\n-                report(\"{} does not exist\".format(src))\n+                report(f\"{src} does not exist\")\n                 del self.extensions[i]\n             else:\n                 dst = os.path.join(os.path.realpath(self.build_lib), filename)\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -642,7 +642,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -658,7 +658,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"nvfuser\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -670,7 +670,7 @@ def build_extensions(self):\n     def get_outputs(self):\n         outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n         outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n-        report(\"setup.py::get_outputs returning {}\".format(outputs))\n+        report(f\"setup.py::get_outputs returning {outputs}\")\n         return outputs\n \n     def create_compile_commands(self):\n@@ -694,13 +694,13 @@ def load(filename):\n         new_contents = json.dumps(all_commands, indent=2)\n         contents = ''\n         if os.path.exists('compile_commands.json'):\n-            with open('compile_commands.json', 'r') as f:\n+            with open('compile_commands.json') as f:\n                 contents = f.read()\n         if contents != new_contents:\n             with open('compile_commands.json', 'w') as f:\n                 f.write(new_contents)\n \n-class concat_license_files():\n+class concat_license_files:\n     \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n \n     LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n@@ -723,7 +723,7 @@ def __enter__(self):\n         finally:\n             sys.path = old_path\n \n-        with open(self.f1, 'r') as f1:\n+        with open(self.f1) as f1:\n             self.bsd_text = f1.read()\n \n         with open(self.f1, 'a') as f1:\n@@ -771,7 +771,7 @@ def finalize_options(self):\n     def run(self):\n         import glob\n         import re\n-        with open('.gitignore', 'r') as f:\n+        with open('.gitignore') as f:\n             ignores = f.read()\n             pat = re.compile(r'^#( BEGIN NOT-CLEAN-FILES )?')\n             for wildcard in filter(None, ignores.split('\\n')):\n@@ -934,31 +934,31 @@ def make_relative_rpath_args(path):\n     if cmake_cache_vars['BUILD_CAFFE2']:\n         extensions.append(\n             Extension(\n-                name=str('caffe2.python.caffe2_pybind11_state'),\n+                name='caffe2.python.caffe2_pybind11_state',\n                 sources=[]),\n         )\n         if cmake_cache_vars['USE_CUDA']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_gpu'),\n+                    name='caffe2.python.caffe2_pybind11_state_gpu',\n                     sources=[]),\n             )\n         if cmake_cache_vars['USE_ROCM']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_hip'),\n+                    name='caffe2.python.caffe2_pybind11_state_hip',\n                     sources=[]),\n             )\n     if cmake_cache_vars['BUILD_FUNCTORCH']:\n         extensions.append(\n             Extension(\n-                name=str('functorch._C'),\n+                name='functorch._C',\n                 sources=[]),\n         )\n     if cmake_cache_vars['BUILD_NVFUSER']:\n         extensions.append(\n             Extension(\n-                name=str('nvfuser._C'),\n+                name='nvfuser._C',\n                 sources=[]),\n         )\n \n@@ -1272,7 +1272,7 @@ def main():\n         download_url='https://github.com/pytorch/pytorch/tags',\n         author='PyTorch Team',\n         author_email='packages@pytorch.org',\n-        python_requires='>={}'.format(python_min_version_str),\n+        python_requires=f'>={python_min_version_str}',\n         # PyPI package information.\n         classifiers=[\n             'Development Status :: 5 - Production/Stable',\n@@ -1288,7 +1288,7 @@ def main():\n             'Topic :: Software Development :: Libraries :: Python Modules',\n             'Programming Language :: C++',\n             'Programming Language :: Python :: 3',\n-        ] + ['Programming Language :: Python :: 3.{}'.format(i) for i in range(python_min_version[1], version_range_max)],\n+        ] + [f'Programming Language :: Python :: 3.{i}' for i in range(python_min_version[1], version_range_max)],\n         license='BSD-3',\n         keywords='pytorch, machine learning',\n     )\ndiff --git a/tools/amd_build/build_amd.py b/tools/amd_build/build_amd.py\nindex 59f806b361102e..5d14e9266f3b4a 100755\n--- a/tools/amd_build/build_amd.py\n+++ b/tools/amd_build/build_amd.py\n@@ -140,7 +140,7 @@ def is_hip_clang() -> bool:\n         hip_path = os.getenv(\"HIP_PATH\", \"/opt/rocm/hip\")\n         with open(hip_path + \"/lib/.hipInfo\") as f:\n             return \"HIP_COMPILER=clang\" in f.read()\n-    except IOError:\n+    except OSError:\n         return False\n \n \n@@ -149,7 +149,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Hip.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\" hip_hcc \", \" amdhip64 \") for line in lines]\n         if lines == newlines:\n@@ -163,7 +163,7 @@ def is_hip_clang() -> bool:\n gloo_cmake_file = \"third_party/gloo/cmake/Modules/Findrccl.cmake\"\n if os.path.exists(gloo_cmake_file):\n     do_write = False\n-    with open(gloo_cmake_file, \"r\") as sources:\n+    with open(gloo_cmake_file) as sources:\n         lines = sources.readlines()\n     newlines = [line.replace(\"RCCL_LIBRARY\", \"RCCL_LIB_PATH\") for line in lines]\n     if lines == newlines:\n@@ -179,7 +179,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Dependencies.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\"HIP_HCC_FLAGS\", \"HIP_CLANG_FLAGS\") for line in lines]\n         if lines == newlines:\ndiff --git a/tools/autograd/gen_python_functions.py b/tools/autograd/gen_python_functions.py\nindex 211aacafaa8728..d1e9d60737defc 100644\n--- a/tools/autograd/gen_python_functions.py\n+++ b/tools/autograd/gen_python_functions.py\n@@ -553,7 +553,7 @@ def load_deprecated_signatures(\n     # find matching original signatures for each deprecated signature\n     results: List[PythonSignatureNativeFunctionPair] = []\n \n-    with open(deprecated_yaml_path, \"r\") as f:\n+    with open(deprecated_yaml_path) as f:\n         deprecated_defs = yaml.load(f, Loader=YamlLoader)\n \n     for deprecated in deprecated_defs:\n@@ -873,7 +873,7 @@ def method_impl(\n         name=name,\n         pycname=pycname,\n         method_header=method_header,\n-        max_args=max((o.signature.arguments_count() for o in overloads)),\n+        max_args=max(o.signature.arguments_count() for o in overloads),\n         signatures=signatures,\n         traceable=traceable,\n         check_has_torch_function=gen_has_torch_function_check(\n@@ -1255,10 +1255,10 @@ def go(f: NativeFunction) -> str:\n         # dispatch lambda signature\n         name = cpp.name(f.func)\n         lambda_formals = \", \".join(\n-            (\n+\n                 f\"{a.type_str} {a.name}\"\n                 for a in dispatch_lambda_args(ps, f, symint=symint)\n-            )\n+\n         )\n         lambda_return = dispatch_lambda_return_str(f)\n \ndiff --git a/tools/autograd/load_derivatives.py b/tools/autograd/load_derivatives.py\nindex b51b625b2ea28f..b846892b0e3ed4 100644\n--- a/tools/autograd/load_derivatives.py\n+++ b/tools/autograd/load_derivatives.py\n@@ -98,7 +98,7 @@ def load_derivatives(\n     global _GLOBAL_LOAD_DERIVATIVE_CACHE\n     key = (derivatives_yaml_path, native_yaml_path)\n     if key not in _GLOBAL_LOAD_DERIVATIVE_CACHE:\n-        with open(derivatives_yaml_path, \"r\") as f:\n+        with open(derivatives_yaml_path) as f:\n             definitions = yaml.load(f, Loader=YamlLoader)\n \n         funcs = parse_native_yaml(native_yaml_path, tags_yaml_path).native_functions\ndiff --git a/tools/code_analyzer/gen_op_registration_allowlist.py b/tools/code_analyzer/gen_op_registration_allowlist.py\nindex b01142c872f1c2..b5d15ca1ae8417 100644\n--- a/tools/code_analyzer/gen_op_registration_allowlist.py\n+++ b/tools/code_analyzer/gen_op_registration_allowlist.py\n@@ -24,7 +24,7 @@ def canonical_name(opname: str) -> str:\n \n \n def load_op_dep_graph(fname: str) -> DepGraph:\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         result = defaultdict(set)\n         for op in yaml.safe_load(stream):\n             op_name = canonical_name(op[\"name\"])\n@@ -36,7 +36,7 @@ def load_op_dep_graph(fname: str) -> DepGraph:\n \n def load_root_ops(fname: str) -> List[str]:\n     result = []\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         for op in yaml.safe_load(stream):\n             result.append(canonical_name(op))\n     return result\ndiff --git a/tools/code_analyzer/gen_oplist.py b/tools/code_analyzer/gen_oplist.py\nindex 7c1764deda5b2f..0a9e2a1539b6a7 100644\n--- a/tools/code_analyzer/gen_oplist.py\n+++ b/tools/code_analyzer/gen_oplist.py\n@@ -79,7 +79,7 @@ def gen_supported_mobile_models(model_dicts: List[Any], output_dir: str) -> None\n \n     supported_hashes = \"\"\n     for md5 in md5_hashes:\n-        supported_hashes += '\"{}\",\\n'.format(md5)\n+        supported_hashes += f'\"{md5}\",\\n'\n     with open(\n         os.path.join(output_dir, \"SupportedMobileModelsRegistration.cpp\"), \"wb\"\n     ) as out_file:\ndiff --git a/tools/coverage_plugins_package/setup.py b/tools/coverage_plugins_package/setup.py\nindex 01250694550423..e3e88067cb08db 100644\n--- a/tools/coverage_plugins_package/setup.py\n+++ b/tools/coverage_plugins_package/setup.py\n@@ -1,6 +1,6 @@\n import setuptools  # type: ignore[import]\n \n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n+with open(\"README.md\", encoding=\"utf-8\") as fh:\n     long_description = fh.read()\n \n setuptools.setup(\ndiff --git a/tools/download_mnist.py b/tools/download_mnist.py\nindex 52fa411eda9f88..ac9c049bdeedb6 100644\n--- a/tools/download_mnist.py\n+++ b/tools/download_mnist.py\n@@ -32,16 +32,16 @@ def report_download_progress(\n def download(destination_path: str, resource: str, quiet: bool) -> None:\n     if os.path.exists(destination_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ...\".format(destination_path))\n+            print(f\"{destination_path} already exists, skipping ...\")\n     else:\n         for mirror in MIRRORS:\n             url = mirror + resource\n-            print(\"Downloading {} ...\".format(url))\n+            print(f\"Downloading {url} ...\")\n             try:\n                 hook = None if quiet else report_download_progress\n                 urlretrieve(url, destination_path, reporthook=hook)\n             except (URLError, ConnectionError) as e:\n-                print(\"Failed to download (trying next):\\n{}\".format(e))\n+                print(f\"Failed to download (trying next):\\n{e}\")\n                 continue\n             finally:\n                 if not quiet:\n@@ -56,13 +56,13 @@ def unzip(zipped_path: str, quiet: bool) -> None:\n     unzipped_path = os.path.splitext(zipped_path)[0]\n     if os.path.exists(unzipped_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ... \".format(unzipped_path))\n+            print(f\"{unzipped_path} already exists, skipping ... \")\n         return\n     with gzip.open(zipped_path, \"rb\") as zipped_file:\n         with open(unzipped_path, \"wb\") as unzipped_file:\n             unzipped_file.write(zipped_file.read())\n             if not quiet:\n-                print(\"Unzipped {} ...\".format(zipped_path))\n+                print(f\"Unzipped {zipped_path} ...\")\n \n \n def main() -> None:\ndiff --git a/tools/gen_vulkan_spv.py b/tools/gen_vulkan_spv.py\nindex 02c9c39270b107..8d38dfa0e82bcb 100644\n--- a/tools/gen_vulkan_spv.py\n+++ b/tools/gen_vulkan_spv.py\n@@ -74,7 +74,7 @@ def __init__(self: \"VulkanShaderGenerator\") -> None:\n \n     def add_params_yaml(self, parameters_yaml_file):  # type: ignore[no-untyped-def]\n         all_template_params = OrderedDict()\n-        with open(parameters_yaml_file, \"r\") as f:\n+        with open(parameters_yaml_file) as f:\n             contents = yaml.load(f, Loader=UniqueKeyLoader)\n             for key in contents:\n                 all_template_params[key] = contents[key]\n@@ -205,7 +205,7 @@ def determineDescriptorType(lineStr: str) -> str:\n \n def getShaderInfo(srcFilePath: str) -> ShaderInfo:\n     shader_info = ShaderInfo([], [], \"\")\n-    with open(srcFilePath, 'r') as srcFile:\n+    with open(srcFilePath) as srcFile:\n         for line in srcFile:\n             if isDescriptorLine(line):\n                 shader_info.layouts.append(determineDescriptorType(line))\n@@ -272,13 +272,13 @@ def genCppH(\n         if len(f) > 1:\n             templateSrcPaths.append(f)\n             templateSrcPaths.sort()\n-    print(\"templateSrcPaths:{}\".format(templateSrcPaths))\n+    print(f\"templateSrcPaths:{templateSrcPaths}\")\n \n     spvPaths = {}\n     for templateSrcPath in templateSrcPaths:\n-        print(\"templateSrcPath {}\".format(templateSrcPath))\n+        print(f\"templateSrcPath {templateSrcPath}\")\n         name = getName(templateSrcPath).replace(\"_glsl\", \"\")\n-        print(\"name {}\".format(name))\n+        print(f\"name {name}\")\n \n         codeTemplate = CodeTemplate.from_file(templateSrcPath)\n         srcPath = tmpDirPath + \"/\" + name + \".glsl\"\n@@ -287,7 +287,7 @@ def genCppH(\n             fw.write(content)\n \n         spvPath = tmpDirPath + \"/\" + name + \".spv\"\n-        print(\"spvPath {}\".format(spvPath))\n+        print(f\"spvPath {spvPath}\")\n \n         cmd = [\n             glslcPath, \"-fshader-stage=compute\",\n@@ -328,7 +328,7 @@ def genCppH(\n     h += nsend\n \n     cpp = \"#include <ATen/native/vulkan/api/Shader.h>\\n\"\n-    cpp += \"#include <ATen/native/vulkan/{}>\\n\".format(H_NAME)\n+    cpp += f\"#include <ATen/native/vulkan/{H_NAME}>\\n\"\n     cpp += \"#include <stdint.h>\\n\"\n     cpp += \"#include <vector>\\n\"\n     cpp += nsbegin\n@@ -340,7 +340,7 @@ def genCppH(\n     for spvPath, srcPath in spvPaths.items():\n         name = getName(spvPath).replace(\"_spv\", \"\")\n \n-        print(\"spvPath:{}\".format(spvPath))\n+        print(f\"spvPath:{spvPath}\")\n         with open(spvPath, 'rb') as fr:\n             next_bin = array.array('I', fr.read())\n             sizeBytes = 4 * len(next_bin)\n@@ -362,8 +362,8 @@ def genCppH(\n         shader_info_layouts = \"{{{}}}\".format(\",\\n \".join(shader_info.layouts))\n \n         shader_info_args = [\n-            \"\\\"vulkan.{}\\\"\".format(name),\n-            \"{}_bin\".format(name),\n+            f\"\\\"vulkan.{name}\\\"\",\n+            f\"{name}_bin\",\n             str(sizeBytes),\n             shader_info_layouts,\n             tile_size,\ndiff --git a/tools/generate_torch_version.py b/tools/generate_torch_version.py\nindex 9e9f73b031f810..d90d3646ab1910 100644\n--- a/tools/generate_torch_version.py\n+++ b/tools/generate_torch_version.py\n@@ -41,7 +41,7 @@ def get_tag(pytorch_root: Union[str, Path]) -> str:\n \n def get_torch_version(sha: Optional[str] = None) -> str:\n     pytorch_root = Path(__file__).parent.parent\n-    version = open(pytorch_root / \"version.txt\", \"r\").read().strip()\n+    version = open(pytorch_root / \"version.txt\").read().strip()\n \n     if os.getenv(\"PYTORCH_BUILD_VERSION\"):\n         assert os.getenv(\"PYTORCH_BUILD_NUMBER\") is not None\n@@ -86,11 +86,11 @@ def get_torch_version(sha: Optional[str] = None) -> str:\n         version = tagged_version\n \n     with open(version_path, \"w\") as f:\n-        f.write(\"__version__ = '{}'\\n\".format(version))\n+        f.write(f\"__version__ = '{version}'\\n\")\n         # NB: This is not 100% accurate, because you could have built the\n         # library code with DEBUG, but csrc without DEBUG (in which case\n         # this would claim to be a release build when it's not.)\n-        f.write(\"debug = {}\\n\".format(repr(bool(args.is_debug))))\n-        f.write(\"cuda = {}\\n\".format(repr(args.cuda_version)))\n-        f.write(\"git_version = {}\\n\".format(repr(sha)))\n-        f.write(\"hip = {}\\n\".format(repr(args.hip_version)))\n+        f.write(f\"debug = {repr(bool(args.is_debug))}\\n\")\n+        f.write(f\"cuda = {repr(args.cuda_version)}\\n\")\n+        f.write(f\"git_version = {repr(sha)}\\n\")\n+        f.write(f\"hip = {repr(args.hip_version)}\\n\")\ndiff --git a/tools/jit/gen_unboxing.py b/tools/jit/gen_unboxing.py\nindex 6179d6afe482ff..ee4e2fc2ddb188 100644\n--- a/tools/jit/gen_unboxing.py\n+++ b/tools/jit/gen_unboxing.py\n@@ -250,7 +250,7 @@ def main(args: List[str]) -> None:\n     if options.op_registration_allowlist:\n         op_registration_allowlist = options.op_registration_allowlist\n     elif options.TEST_ONLY_op_registration_allowlist_yaml_path:\n-        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path, \"r\") as f:\n+        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path) as f:\n             op_registration_allowlist = yaml.safe_load(f)\n     else:\n         op_registration_allowlist = None\ndiff --git a/tools/linter/adapters/constexpr_linter.py b/tools/linter/adapters/constexpr_linter.py\nindex 8992f30ac46b29..24ecc83b238e30 100644\n--- a/tools/linter/adapters/constexpr_linter.py\n+++ b/tools/linter/adapters/constexpr_linter.py\n@@ -35,7 +35,7 @@ class LintMessage(NamedTuple):\n def check_file(filename: str) -> Optional[LintMessage]:\n     logging.debug(\"Checking file %s\", filename)\n \n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n \n     for idx, line in enumerate(lines):\ndiff --git a/tools/linter/adapters/grep_linter.py b/tools/linter/adapters/grep_linter.py\nindex 21c8a210b2b697..64dac4cdc079cd 100644\n--- a/tools/linter/adapters/grep_linter.py\n+++ b/tools/linter/adapters/grep_linter.py\n@@ -108,7 +108,7 @@ def lint_file(\n     original = None\n     replacement = None\n     if replace_pattern:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             original = f.read()\n \n         try:\ndiff --git a/tools/nightly.py b/tools/nightly.py\nindex 1544eb0692b661..28a8c6eb2331e7 100755\n--- a/tools/nightly.py\n+++ b/tools/nightly.py\n@@ -105,7 +105,7 @@ def redact(self, needle: str, replace: str = \"<REDACTED>\") -> None:\n         self.redactions[needle] = replace\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_base_dir() -> str:\n     meta_dir = os.getcwd()\n     base_dir = os.path.join(meta_dir, \"nightly\", \"log\")\n@@ -113,17 +113,17 @@ def logging_base_dir() -> str:\n     return base_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_run_dir() -> str:\n     cur_dir = os.path.join(\n         logging_base_dir(),\n-        \"{}_{}\".format(datetime.datetime.now().strftime(DATETIME_FORMAT), uuid.uuid1()),\n+        f\"{datetime.datetime.now().strftime(DATETIME_FORMAT)}_{uuid.uuid1()}\",\n     )\n     os.makedirs(cur_dir, exist_ok=True)\n     return cur_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_record_argv() -> None:\n     s = subprocess.list2cmdline(sys.argv)\n     with open(os.path.join(logging_run_dir(), \"argv\"), \"w\") as f:\ndiff --git a/tools/onnx/gen_diagnostics.py b/tools/onnx/gen_diagnostics.py\nindex 2aeb61a06318d7..4cf70289296050 100644\n--- a/tools/onnx/gen_diagnostics.py\n+++ b/tools/onnx/gen_diagnostics.py\n@@ -205,7 +205,7 @@ def gen_diagnostics(\n     out_cpp_dir: str,\n     out_docs_dir: str,\n ) -> None:\n-    with open(rules_path, \"r\") as f:\n+    with open(rules_path) as f:\n         rules = yaml.load(f, Loader=YamlLoader)\n \n     template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"templates\")\ndiff --git a/tools/onnx/update_default_opset_version.py b/tools/onnx/update_default_opset_version.py\nindex 6dc6ffbd2890f4..6463c6271b6ee6 100755\n--- a/tools/onnx/update_default_opset_version.py\n+++ b/tools/onnx/update_default_opset_version.py\n@@ -23,7 +23,7 @@\n def read_sub_write(path: str, prefix_pat: str, new_default: int) -> None:\n     with open(path, encoding=\"utf-8\") as f:\n         content_str = f.read()\n-    content_str = re.sub(prefix_pat, r\"\\g<1>{}\".format(new_default), content_str)\n+    content_str = re.sub(prefix_pat, fr\"\\g<1>{new_default}\", content_str)\n     with open(path, \"w\", encoding=\"utf-8\") as f:\n         f.write(content_str)\n     print(\"modified\", path)\ndiff --git a/tools/pyi/gen_pyi.py b/tools/pyi/gen_pyi.py\nindex 24ee8ad1bfe580..c74d737416870a 100644\n--- a/tools/pyi/gen_pyi.py\n+++ b/tools/pyi/gen_pyi.py\n@@ -191,15 +191,15 @@ def sig_for_ops(opname: str) -> List[str]:\n \n     name = opname[2:-2]\n     if name in binary_ops:\n-        return [\"def {}(self, other: Any) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self, other: Any) -> Tensor: ...\"]\n     elif name in comparison_ops:\n-        sig = \"def {}(self, other: Any) -> Tensor: ...\".format(opname)\n+        sig = f\"def {opname}(self, other: Any) -> Tensor: ...\"\n         if name in symmetric_comparison_ops:\n             # unsafe override https://github.com/python/mypy/issues/5704\n             sig += \"  # type: ignore[override]\"\n         return [sig]\n     elif name in unary_ops:\n-        return [\"def {}(self) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self) -> Tensor: ...\"]\n     elif name in to_py_type_ops:\n         if name in {\"bool\", \"float\", \"complex\"}:\n             tname = name\n@@ -209,7 +209,7 @@ def sig_for_ops(opname: str) -> List[str]:\n             tname = \"int\"\n         if tname in {\"float\", \"int\", \"bool\", \"complex\"}:\n             tname = \"builtins.\" + tname\n-        return [\"def {}(self) -> {}: ...\".format(opname, tname)]\n+        return [f\"def {opname}(self) -> {tname}: ...\"]\n     else:\n         raise Exception(\"unknown op\", opname)\n \n@@ -1120,7 +1120,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     for name in simple_conversions:\n         unsorted_tensor_method_hints[name].append(\n-            \"def {}(self) -> Tensor: ...\".format(name)\n+            f\"def {name}(self) -> Tensor: ...\"\n         )\n \n     # pyi tensor methods don't currently include deprecated signatures for some reason\n@@ -1150,7 +1150,7 @@ def replace_special_case(hint: str) -> str:\n                 namedtuples[tuple_name] = tuple_def\n \n     for op in all_ops:\n-        name = \"__{}__\".format(op)\n+        name = f\"__{op}__\"\n         unsorted_tensor_method_hints[name] += sig_for_ops(name)\n \n     tensor_method_hints = []\n@@ -1164,7 +1164,7 @@ def replace_special_case(hint: str) -> str:\n     # Generate namedtuple definitions\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-    namedtuple_defs = [\"{}\\n\".format(defn) for defn in namedtuples.values()]\n+    namedtuple_defs = [f\"{defn}\\n\" for defn in namedtuples.values()]\n \n     # Generate type signatures for legacy classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1183,7 +1183,7 @@ def replace_special_case(hint: str) -> str:\n         \"ByteTensor\",\n         \"BoolTensor\",\n     ):\n-        legacy_class_hints.append(\"class {}(Tensor): ...\".format(c))\n+        legacy_class_hints.append(f\"class {c}(Tensor): ...\")\n \n     # Generate type signatures for dtype classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1191,7 +1191,7 @@ def replace_special_case(hint: str) -> str:\n     # TODO: don't explicitly list dtypes here; get it from canonical\n     # source\n     dtype_class_hints = [\n-        \"{}: dtype = ...\".format(n)\n+        f\"{n}: dtype = ...\"\n         for n in [\n             \"float32\",\n             \"float\",\n@@ -1232,7 +1232,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     all_symbols = sorted(list(namedtuples.keys()) + hinted_function_names)\n     all_directive = pformat(all_symbols, width=100, compact=True).split(\"\\n\")\n-    all_directive[0] = \"__all__ = {}\".format(all_directive[0])\n+    all_directive[0] = f\"__all__ = {all_directive[0]}\"\n \n     # Dispatch key hints\n     # ~~~~~~~~~~~~~~~~~~\ndiff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py\nindex 0bd6e5d4c2adc9..cf80bd3eb1e62c 100644\n--- a/tools/setup_helpers/cmake.py\n+++ b/tools/setup_helpers/cmake.py\n@@ -61,10 +61,10 @@ def _get_cmake_command() -> str:\n \n         _cmake_min_version = LooseVersion(\"3.13.0\")\n         if all(\n-            (\n+\n                 ver is None or ver < _cmake_min_version\n                 for ver in [cmake_version, cmake3_version]\n-            )\n+\n         ):\n             raise RuntimeError(\"no cmake or cmake3 with version >= 3.13.0 found\")\n \n@@ -108,7 +108,7 @@ def defines(args: List[str], **kwargs: CMakeValue) -> None:\n         \"Adds definitions to a cmake argument list.\"\n         for key, value in sorted(kwargs.items()):\n             if value is not None:\n-                args.append(\"-D{}={}\".format(key, value))\n+                args.append(f\"-D{key}={value}\")\n \n     def get_cmake_cache_variables(self) -> Dict[str, CMakeValue]:\n         r\"\"\"Gets values in CMakeCache.txt into a dictionary.\n@@ -173,7 +173,7 @@ def generate(\n                     toolset_dict[\"host\"] = \"x64\"\n             if toolset_dict:\n                 toolset_expr = \",\".join(\n-                    [\"{}={}\".format(k, v) for k, v in toolset_dict.items()]\n+                    [f\"{k}={v}\" for k, v in toolset_dict.items()]\n                 )\n                 args.append(\"-T\" + toolset_expr)\n \n@@ -322,10 +322,10 @@ def generate(\n         expected_wrapper = \"/usr/local/opt/ccache/libexec\"\n         if IS_DARWIN and os.path.exists(expected_wrapper):\n             if \"CMAKE_C_COMPILER\" not in build_options and \"CC\" not in os.environ:\n-                CMake.defines(args, CMAKE_C_COMPILER=\"{}/gcc\".format(expected_wrapper))\n+                CMake.defines(args, CMAKE_C_COMPILER=f\"{expected_wrapper}/gcc\")\n             if \"CMAKE_CXX_COMPILER\" not in build_options and \"CXX\" not in os.environ:\n                 CMake.defines(\n-                    args, CMAKE_CXX_COMPILER=\"{}/g++\".format(expected_wrapper)\n+                    args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\"\n                 )\n \n         for env_var_name in my_env:\n@@ -336,10 +336,10 @@ def generate(\n                     my_env[env_var_name] = str(my_env[env_var_name].encode(\"utf-8\"))\n                 except UnicodeDecodeError as e:\n                     shex = \":\".join(\n-                        \"{:02x}\".format(ord(c)) for c in my_env[env_var_name]\n+                        f\"{ord(c):02x}\" for c in my_env[env_var_name]\n                     )\n                     print(\n-                        \"Invalid ENV[{}] = {}\".format(env_var_name, shex),\n+                        f\"Invalid ENV[{env_var_name}] = {shex}\",\n                         file=sys.stderr,\n                     )\n                     print(e, file=sys.stderr)\n@@ -396,7 +396,7 @@ def build(self, my_env: Dict[str, str]) -> None:\n             build_args += [\"--\"]\n             if IS_WINDOWS and not USE_NINJA:\n                 # We are likely using msbuild here\n-                build_args += [\"/p:CL_MPCount={}\".format(max_jobs)]\n+                build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n             else:\n                 build_args += [\"-j\", max_jobs]\n         self.run(build_args, my_env)\ndiff --git a/tools/setup_helpers/cmake_utils.py b/tools/setup_helpers/cmake_utils.py\nindex dabd66a4e838bb..c15b6f7592c015 100644\n--- a/tools/setup_helpers/cmake_utils.py\n+++ b/tools/setup_helpers/cmake_utils.py\n@@ -72,7 +72,7 @@ def get_cmake_cache_variables_from_file(\n         )\n         if matched is None:  # Illegal line\n             raise ValueError(\n-                \"Unexpected line {} in {}: {}\".format(i, repr(cmake_cache_file), line)\n+                f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\"\n             )\n         _, variable, type_, value = matched.groups()\n         if type_ is None:\ndiff --git a/tools/setup_helpers/generate_code.py b/tools/setup_helpers/generate_code.py\nindex c03fd87f25b6aa..afdd168d179fd6 100644\n--- a/tools/setup_helpers/generate_code.py\n+++ b/tools/setup_helpers/generate_code.py\n@@ -75,7 +75,7 @@ def generate_code(\n def get_selector_from_legacy_operator_selection_list(\n     selected_op_list_path: str,\n ) -> Any:\n-    with open(selected_op_list_path, \"r\") as f:\n+    with open(selected_op_list_path) as f:\n         # strip out the overload part\n         # It's only for legacy config - do NOT copy this code!\n         selected_op_list = {\ndiff --git a/tools/stats/import_test_stats.py b/tools/stats/import_test_stats.py\nindex b0719fc56d97b6..28d8ee0961bd9e 100644\n--- a/tools/stats/import_test_stats.py\n+++ b/tools/stats/import_test_stats.py\n@@ -46,7 +46,7 @@ def is_cached_file_valid() -> bool:\n \n     if os.path.exists(path) and is_cached_file_valid():\n         # Another test process already download the file, so don't re-do it\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             return cast(Dict[str, Any], json.load(f))\n \n     for _ in range(3):\ndiff --git a/tools/stats/upload_stats_lib.py b/tools/stats/upload_stats_lib.py\nindex c9223c528fe163..c7e90286d00a2b 100644\n--- a/tools/stats/upload_stats_lib.py\n+++ b/tools/stats/upload_stats_lib.py\n@@ -249,10 +249,10 @@ def value(self) -> Any:\n         value = os.environ.get(self.env_var)\n         if value is None and self.required:\n             raise ValueError(\n-                (\n+\n                     f\"Missing {self.name}. Please set the {self.env_var}\"\n                     \"environment variable to pass in this value.\"\n-                )\n+\n             )\n         if self.type_conversion_fn:\n             return self.type_conversion_fn(value)\ndiff --git a/tools/substitute.py b/tools/substitute.py\nindex c3b353bf740115..e9c05990c75f9a 100644\n--- a/tools/substitute.py\n+++ b/tools/substitute.py\n@@ -11,7 +11,7 @@\n     parser.add_argument(\"--replace\", action=\"append\", nargs=2)\n     options = parser.parse_args()\n \n-    with open(options.input_file, \"r\") as f:\n+    with open(options.input_file) as f:\n         contents = f.read()\n \n     output_file = os.path.join(options.install_dir, options.output_file)\ndiff --git a/tools/test/test_executorch_gen.py b/tools/test/test_executorch_gen.py\nindex b2a0f6768271bf..c9d6c79b85c1ea 100644\n--- a/tools/test/test_executorch_gen.py\n+++ b/tools/test/test_executorch_gen.py\n@@ -181,7 +181,7 @@ def test_translate_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\n@@ -268,7 +268,7 @@ def test_translate_kernel_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\ndiff --git a/tools/test/test_executorch_signatures.py b/tools/test/test_executorch_signatures.py\nindex c137f6982ec2b7..543926d4c31ef0 100644\n--- a/tools/test/test_executorch_signatures.py\n+++ b/tools/test/test_executorch_signatures.py\n@@ -21,7 +21,7 @@ def test_runtime_signature_contains_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=True)\n-            self.assertEquals(len(args), 3)\n+            self.assertEqual(len(args), 3)\n             self.assertTrue(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n@@ -30,7 +30,7 @@ def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=False)\n-            self.assertEquals(len(args), 2)\n+            self.assertEqual(len(args), 2)\n             self.assertFalse(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_declaration_correct(self) -> None:\n@@ -38,7 +38,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             decl = self.sig.decl(include_context=True)\n-            self.assertEquals(\n+            self.assertEqual(\n                 decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\n@@ -48,7 +48,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n                 ),\n             )\n             no_context_decl = self.sig.decl(include_context=False)\n-            self.assertEquals(\n+            self.assertEqual(\n                 no_context_decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\ndiff --git a/tools/test/test_vulkan_codegen.py b/tools/test/test_vulkan_codegen.py\nindex ae87c27e7aeb8e..196be229b348d2 100644\n--- a/tools/test/test_vulkan_codegen.py\n+++ b/tools/test/test_vulkan_codegen.py\n@@ -92,9 +92,9 @@ def test_missing_key_default_val(self) -> None:\n                     file_name_2 = os.path.join(tmp_dir, \"conv2d_pw_1x2.glsl\")\n                     self.assertTrue(os.path.exists(file_name_1))\n                     self.assertTrue(os.path.exists(file_name_2))\n-                    with open(file_name_1, \"r\") as f:\n+                    with open(file_name_1) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 1\" in contents)\n-                    with open(file_name_2, \"r\") as f:\n+                    with open(file_name_2) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 2\" in contents)\ndiff --git a/tools/testing/explicit_ci_jobs.py b/tools/testing/explicit_ci_jobs.py\nindex daff3cce8956ff..594e00d437f9f7 100755\n--- a/tools/testing/explicit_ci_jobs.py\n+++ b/tools/testing/explicit_ci_jobs.py\n@@ -127,7 +127,7 @@ def commit_ci(files: List[str], message: str) -> None:\n     args = parser.parse_args()\n \n     touched_files = [CONFIG_YML]\n-    with open(CONFIG_YML, \"r\") as f:\n+    with open(CONFIG_YML) as f:\n         config_yml = yaml.safe_load(f.read())\n \n     config_yml[\"workflows\"] = get_filtered_circleci_config(\ndiff --git a/tools/testing/test_selections.py b/tools/testing/test_selections.py\nindex 24fb7278d206f9..76f841b8902bdd 100644\n--- a/tools/testing/test_selections.py\n+++ b/tools/testing/test_selections.py\n@@ -163,7 +163,7 @@ def _get_previously_failing_tests() -> Set[str]:\n         )\n         return set()\n \n-    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH, \"r\") as f:\n+    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH) as f:\n         last_failed_tests = json.load(f)\n \n     prioritized_tests = _parse_prev_failing_test_files(last_failed_tests)\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex 053ce0c0a89552..ebd24383e0b53f 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -156,14 +156,12 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-\n-                            \"      Note: While we usually use modules in the python standard library \"\n-                            f\"from the local environment, `{module_name}` has a lot of system \"\n-                            \"level access and therefore can pose a security risk. We heavily \"\n-                            f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n-                            \"is not possible, add it to the extern list by calling \"\n-                            f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-\n+                        \"      Note: While we usually use modules in the python standard library \"\n+                        f\"from the local environment, `{module_name}` has a lot of system \"\n+                        \"level access and therefore can pose a security risk. We heavily \"\n+                        f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n+                        \"is not possible, add it to the extern list by calling \"\n+                        f'PackageExporter.extern(\"`{module_name}`\")\\n'\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +171,8 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-\n-                    \"Set debug=True when invoking PackageExporter for a visualization of where \"\n-                    \"broken modules are coming from!\\n\"\n-\n+                \"Set debug=True when invoking PackageExporter for a visualization of where \"\n+                \"broken modules are coming from!\\n\"\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\n"
  },
  {
    "number": 105397,
    "title": "[BE] Enable ruff's UP rules and autoformat onnx/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "b704bec0833aa54d6505d68b008ed78a8a23016a",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105397",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105397/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105397.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105397.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105397/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105397/comments",
    "labels": [
      "release notes: onnx",
      "open source"
    ],
    "_event_time": "2023-07-18T01:12:36.970787Z",
    "state": "closed",
    "patch": "From d09c8356c66532d4a20485e6042f99a44027a93b Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:30 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat onnx/\n\n[ghstack-poisoned]\n---\n torch/onnx/_internal/diagnostics/infra/utils.py    | 2 +-\n torch/onnx/_internal/exporter.py                   | 4 +---\n torch/onnx/_internal/fx/onnxfunction_dispatcher.py | 6 +++---\n torch/onnx/_internal/fx/passes/type_promotion.py   | 2 +-\n torch/onnx/_internal/fx/registration.py            | 4 ++--\n torch/onnx/_internal/io_adapter.py                 | 6 +++---\n torch/onnx/_internal/jit_utils.py                  | 2 +-\n torch/onnx/symbolic_opset11.py                     | 2 +-\n torch/onnx/symbolic_opset17.py                     | 8 ++++----\n torch/onnx/verification.py                         | 4 ++--\n 10 files changed, 19 insertions(+), 21 deletions(-)\n\ndiff --git a/torch/onnx/_internal/diagnostics/infra/utils.py b/torch/onnx/_internal/diagnostics/infra/utils.py\nindex f287268df5727b..4648b477515025 100644\n--- a/torch/onnx/_internal/diagnostics/infra/utils.py\n+++ b/torch/onnx/_internal/diagnostics/infra/utils.py\n@@ -43,7 +43,7 @@ def python_call_stack(frames_to_skip: int = 0, frames_to_log: int = 16) -> _infr\n     return stack\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def _function_source_info(fn: Callable) -> Tuple[Sequence[str], int, Optional[str]]:\n     \"\"\"Returns the source lines, line number, and source file path for the given function.\n \ndiff --git a/torch/onnx/_internal/exporter.py b/torch/onnx/_internal/exporter.py\nindex aae7eb6f8e3804..33bf8c7afe44f5 100644\n--- a/torch/onnx/_internal/exporter.py\n+++ b/torch/onnx/_internal/exporter.py\n@@ -145,9 +145,7 @@ class ResolvedExportOptions(ExportOptions):\n     logging diagnostics, and generating the SARIF log.\"\"\"\n \n     @_beartype.beartype\n-    def __init__(\n-        self, options: Optional[Union[ExportOptions, \"ResolvedExportOptions\"]]\n-    ):\n+    def __init__(self, options: Optional[Union[ExportOptions, ResolvedExportOptions]]):\n         if options is None:\n             options = ExportOptions()\n         if isinstance(options, ResolvedExportOptions):\ndiff --git a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\nindex a312cf1aed3d80..2b489ca076b430 100644\n--- a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n+++ b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n@@ -105,7 +105,7 @@ def dispatch(\n         ],\n         onnx_kwargs: Dict[str, fx_type_utils.Argument],\n         diagnostic_context: diagnostics.DiagnosticContext,\n-    ) -> Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]:\n+    ) -> Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]:\n         \"\"\"Dispatches an ONNX function based on the given FX node, arguments, and keyword arguments.\n         Args:\n             node: The TorchFX node to dispatch the function for.\n@@ -405,7 +405,7 @@ def aten_new_full_dtype(self: TTensor, size: INT64, fill_value: TTensor, dtype:\n \n     \"\"\"\n \n-    def __init__(self, onnxfunction: \"onnxscript.OnnxFunction\"):\n+    def __init__(self, onnxfunction: onnxscript.OnnxFunction):\n         \"\"\"Initialize the OnnxSchemaChecker .\n \n         Args:\n@@ -579,7 +579,7 @@ def _record_matching_score(\n     @_beartype.beartype\n     def _separate_input_attributes_from_arguments(\n         self,\n-        param_schemas: Sequence[\"onnxscript.values.ParamSchema\"],\n+        param_schemas: Sequence[onnxscript.values.ParamSchema],\n         args: Sequence[\n             Optional[Union[fx_type_utils.TensorLike, str, int, float, bool, list]]\n         ],\ndiff --git a/torch/onnx/_internal/fx/passes/type_promotion.py b/torch/onnx/_internal/fx/passes/type_promotion.py\nindex e100afefe7814a..c8a10cc322fe90 100644\n--- a/torch/onnx/_internal/fx/passes/type_promotion.py\n+++ b/torch/onnx/_internal/fx/passes/type_promotion.py\n@@ -1217,7 +1217,7 @@ def add_rule(self, rule: TypePromotionRule) -> None:\n             ValueError: If the rule is invalid.\n         \"\"\"\n         if not rule.is_valid():\n-            raise ValueError(\"Invalid type promotion rule: {}\".format(rule))\n+            raise ValueError(f\"Invalid type promotion rule: {rule}\")\n         self._rule_table[f\"{rule.namespace}.{rule.op_name}\"] = rule\n \n     @_beartype.beartype\ndiff --git a/torch/onnx/_internal/fx/registration.py b/torch/onnx/_internal/fx/registration.py\nindex 135c9afe9cdd57..b7c8c3521e55f5 100644\n--- a/torch/onnx/_internal/fx/registration.py\n+++ b/torch/onnx/_internal/fx/registration.py\n@@ -29,7 +29,7 @@ class SymbolicFunction:\n \n     \"\"\"\n \n-    onnx_function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]\n+    onnx_function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]\n     op_full_name: str\n     is_custom: bool = False\n \n@@ -99,7 +99,7 @@ def _register(\n     @_beartype.beartype\n     def register_custom_op(\n         self,\n-        function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"],\n+        function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction],\n         namespace: str,\n         op_name: str,\n         overload: Optional[str] = None,\ndiff --git a/torch/onnx/_internal/io_adapter.py b/torch/onnx/_internal/io_adapter.py\nindex 2654a1ade32ac4..1a80e179ac0b67 100644\n--- a/torch/onnx/_internal/io_adapter.py\n+++ b/torch/onnx/_internal/io_adapter.py\n@@ -60,7 +60,7 @@ def append_step(self, step: InputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, *model_args, **model_kwargs\n-    ) -> Sequence[Union[int, float, bool, str, \"torch.Tensor\", None]]:\n+    ) -> Sequence[Union[int, float, bool, str, torch.Tensor, None]]:\n         \"\"\"Converts the PyTorch model inputs to exported ONNX model inputs format.\n \n         Args:\n@@ -113,7 +113,7 @@ def append_step(self, step: OutputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, model_outputs: Any\n-    ) -> Sequence[Union[\"torch.Tensor\", int, float, bool, str]]:\n+    ) -> Sequence[Union[torch.Tensor, int, float, bool, str]]:\n         \"\"\"Converts the PyTorch model outputs to exported ONNX model outputs format.\n \n         Args:\n@@ -228,7 +228,7 @@ def apply(\n class LiftParametersAndBuffersIntoArgsStep:\n     \"\"\"Append parameters and buffers to model's positional argument list.\"\"\"\n \n-    def __init__(self, inputs: Tuple[\"torch.Tensor\", ...]) -> None:\n+    def __init__(self, inputs: Tuple[torch.Tensor, ...]) -> None:\n         self.inputs = inputs\n \n     def apply(\ndiff --git a/torch/onnx/_internal/jit_utils.py b/torch/onnx/_internal/jit_utils.py\nindex 9052961fc7a646..c46a82c40dfec8 100644\n--- a/torch/onnx/_internal/jit_utils.py\n+++ b/torch/onnx/_internal/jit_utils.py\n@@ -40,7 +40,7 @@ class GraphContext:\n     block: _C.Block\n     opset: int\n     original_node: _C.Node\n-    params_dict: Dict[str, \"_C.IValue\"]\n+    params_dict: Dict[str, _C.IValue]\n     env: Dict[_C.Value, _C.Value]\n \n     # Relay methods from _C.Graph for compatibility with symbolic functions that expect\ndiff --git a/torch/onnx/symbolic_opset11.py b/torch/onnx/symbolic_opset11.py\nindex b432244c42aaa7..3bb63e0e8fa36b 100644\n--- a/torch/onnx/symbolic_opset11.py\n+++ b/torch/onnx/symbolic_opset11.py\n@@ -888,7 +888,7 @@ def _get_arange_dtype(dtype):\n         dtype = symbolic_helper._maybe_get_const(dtype, \"i\")\n         return dtype\n \n-    if len(args) == 2 and all((isinstance(val, int) for val in args)):\n+    if len(args) == 2 and all(isinstance(val, int) for val in args):\n         # aten::arange(Scalar start, Scalar end)\n         dtype = torch.int64\n         # Start index.\ndiff --git a/torch/onnx/symbolic_opset17.py b/torch/onnx/symbolic_opset17.py\nindex 92151a5e58d977..3fa03ab8e11915 100644\n--- a/torch/onnx/symbolic_opset17.py\n+++ b/torch/onnx/symbolic_opset17.py\n@@ -144,8 +144,8 @@ def stft(\n         # Center window around zeros if needed (required by ONNX's STFT)\n         if n_win < n_fft:\n             left, right = _compute_edge_sizes(n_fft, n_win)\n-            left_win = g.op(\"Constant\", value_t=torch.zeros((left)))\n-            right_win = g.op(\"Constant\", value_t=torch.zeros((right)))\n+            left_win = g.op(\"Constant\", value_t=torch.zeros(left))\n+            right_win = g.op(\"Constant\", value_t=torch.zeros(right))\n             window = g.op(\"Concat\", left_win, window, right_win, axis_i=0)\n \n     # Create window, if needed\n@@ -161,11 +161,11 @@ def stft(\n             # Center window, if needed\n             left, right = _compute_edge_sizes(n_fft, win_length)\n             torch_window = torch.hstack(\n-                (torch.zeros((left)), torch.ones((win_length)), torch.zeros((right)))\n+                (torch.zeros(left), torch.ones(win_length), torch.zeros(right))\n             )\n         else:\n             # Rectangle window\n-            torch_window = torch.ones((n_fft))\n+            torch_window = torch.ones(n_fft)\n         assert torch_window.shape[0] == n_fft\n         window = g.op(\"Constant\", value_t=torch_window)\n     window = g.op(\ndiff --git a/torch/onnx/verification.py b/torch/onnx/verification.py\nindex abfa4677eb21cb..27fe4e28e32cf6 100644\n--- a/torch/onnx/verification.py\n+++ b/torch/onnx/verification.py\n@@ -1310,7 +1310,7 @@ def essential_node_kinds(self) -> Set[str]:\n         }\n \n     @_beartype.beartype\n-    def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n+    def all_mismatch_leaf_graph_info(self) -> List[GraphInfo]:\n         \"\"\"Return a list of all leaf `GraphInfo` objects that have mismatch.\"\"\"\n         if not self.has_mismatch():\n             return []\n@@ -1333,7 +1333,7 @@ def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n         return results\n \n     @_beartype.beartype\n-    def find_partition(self, id: str) -> Optional[\"GraphInfo\"]:\n+    def find_partition(self, id: str) -> Optional[GraphInfo]:\n         \"\"\"Find the `GraphInfo` object with the given id.\"\"\"\n         if id == self.id:\n             return self\n"
  },
  {
    "number": 105396,
    "title": "[BE] Enable ruff's UP rules and autoformat optim/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "e85a9d89f6a044c306beab46d93a595c34045f57",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105396",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105396/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105396.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105396.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105396/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105396/comments",
    "labels": [
      "open source",
      "release notes: optim"
    ],
    "_event_time": "2023-07-18T01:12:32.053295Z",
    "state": "closed",
    "patch": "From 4dc56ac745c7f7d3024ebc184af45b3ac614d28c Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:25 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat optim/\n\n[ghstack-poisoned]\n---\n test/distributions/test_constraints.py        |  14 +-\n test/distributions/test_distributions.py      | 238 +++++++++---------\n test/distributions/test_transforms.py         |  20 +-\n test/optim/test_optim.py                      |   4 +-\n torch/distributions/constraints.py            |  16 +-\n torch/distributions/independent.py            |   2 +-\n torch/distributions/kl.py                     |   6 +-\n .../lowrank_multivariate_normal.py            |   2 +-\n torch/distributions/mixture_same_family.py    |   8 +-\n .../distributions/transformed_distribution.py |   2 +-\n torch/distributions/transforms.py             |   6 +-\n torch/optim/adadelta.py                       |   8 +-\n torch/optim/adagrad.py                        |   8 +-\n torch/optim/adam.py                           |  10 +-\n torch/optim/adamax.py                         |  10 +-\n torch/optim/adamw.py                          |  10 +-\n torch/optim/asgd.py                           |   4 +-\n torch/optim/lr_scheduler.py                   |  18 +-\n torch/optim/nadam.py                          |  12 +-\n torch/optim/optimizer.py                      |  12 +-\n torch/optim/radam.py                          |  10 +-\n torch/optim/rmsprop.py                        |  10 +-\n torch/optim/rprop.py                          |   4 +-\n torch/optim/sgd.py                            |   6 +-\n torch/optim/sparse_adam.py                    |   8 +-\n torch/package/_importlib.py                   |   6 +-\n .../package/file_structure_representation.py  |   1 -\n torch/package/package_exporter.py             |  10 +-\n torch/package/package_importer.py             |   6 +-\n torch/profiler/_memory_profiler.py            |   4 +-\n torch/profiler/_pattern_matcher.py            |   2 +-\n torch/signal/windows/windows.py               |   1 -\n torch/sparse/semi_structured.py               |  16 +-\n 33 files changed, 246 insertions(+), 248 deletions(-)\n\ndiff --git a/test/distributions/test_constraints.py b/test/distributions/test_constraints.py\nindex b733cbc021e153..0753b246e37948 100644\n--- a/test/distributions/test_constraints.py\n+++ b/test/distributions/test_constraints.py\n@@ -83,7 +83,7 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         t = biject_to(constraint)\n     except NotImplementedError:\n         pytest.skip('`biject_to` not implemented.')\n-    assert t.bijective, \"biject_to({}) is not bijective\".format(constraint)\n+    assert t.bijective, f\"biject_to({constraint}) is not bijective\"\n     if constraint_fn is constraints.corr_cholesky:\n         # (D * (D-1)) / 2 (where D = 4) = 6 (size of last dim)\n         x = torch.randn(6, 6, dtype=torch.double)\n@@ -93,12 +93,12 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         x = x.cuda()\n     y = t(x)\n     assert constraint.check(y).all(), '\\n'.join([\n-        \"Failed to biject_to({})\".format(constraint),\n-        \"x = {}\".format(x),\n-        \"biject_to(...)(x) = {}\".format(y),\n+        f\"Failed to biject_to({constraint})\",\n+        f\"x = {x}\",\n+        f\"biject_to(...)(x) = {y}\",\n     ])\n     x2 = t.inv(y)\n-    assert torch.allclose(x, x2), \"Error in biject_to({}) inverse\".format(constraint)\n+    assert torch.allclose(x, x2), f\"Error in biject_to({constraint}) inverse\"\n \n     j = t.log_abs_det_jacobian(x, y)\n     assert j.shape == x.shape[:x.dim() - t.domain.event_dim]\n@@ -119,10 +119,10 @@ def test_transform_to(constraint_fn, args, is_cuda):\n     if is_cuda:\n         x = x.cuda()\n     y = t(x)\n-    assert constraint.check(y).all(), \"Failed to transform_to({})\".format(constraint)\n+    assert constraint.check(y).all(), f\"Failed to transform_to({constraint})\"\n     x2 = t.inv(y)\n     y2 = t(x2)\n-    assert torch.allclose(y, y2), \"Error in transform_to({}) pseudoinverse\".format(constraint)\n+    assert torch.allclose(y, y2), f\"Error in transform_to({constraint}) pseudoinverse\"\n \n \n if __name__ == \"__main__\":\ndiff --git a/test/distributions/test_distributions.py b/test/distributions/test_distributions.py\nindex 69591d31c5ed20..2f4d256516c849 100644\n--- a/test/distributions/test_distributions.py\n+++ b/test/distributions/test_distributions.py\n@@ -862,7 +862,7 @@ def _check_sampler_sampler(self, torch_dist, ref_dist, message, multivariate=Fal\n         bins = samples.reshape((num_bins, samples_per_bin)).mean(axis=1)\n         stddev = samples_per_bin ** -0.5\n         threshold = stddev * scipy.special.erfinv(1 - 2 * failure_rate / num_bins)\n-        message = '{}.sample() is biased:\\n{}'.format(message, bins)\n+        message = f'{message}.sample() is biased:\\n{bins}'\n         for bias in bins:\n             self.assertLess(-threshold, bias, message)\n             self.assertLess(bias, threshold, message)\n@@ -971,7 +971,7 @@ def test_has_examples(self):\n             if isinstance(Dist, type) and issubclass(Dist, Distribution) \\\n                     and Dist is not Distribution and Dist is not ExponentialFamily:\n                 self.assertIn(Dist, distributions_with_examples,\n-                              \"Please add {} to the EXAMPLES list in test_distributions.py\".format(Dist.__name__))\n+                              f\"Please add {Dist.__name__} to the EXAMPLES list in test_distributions.py\")\n \n     def test_support_attributes(self):\n         for Dist, params in EXAMPLES:\n@@ -1120,7 +1120,7 @@ def test_geometric_sample(self):\n         for prob in [0.01, 0.18, 0.8]:\n             self._check_sampler_discrete(Geometric(prob),\n                                          scipy.stats.geom(p=prob, loc=-1),\n-                                         'Geometric(prob={})'.format(prob))\n+                                         f'Geometric(prob={prob})')\n \n     def test_binomial(self):\n         p = torch.arange(0.05, 1, 0.1).requires_grad_()\n@@ -1136,7 +1136,7 @@ def test_binomial_sample(self):\n             for count in [2, 10, 100, 500]:\n                 self._check_sampler_discrete(Binomial(total_count=count, probs=prob),\n                                              scipy.stats.binom(count, prob),\n-                                             'Binomial(total_count={}, probs={})'.format(count, prob))\n+                                             f'Binomial(total_count={count}, probs={prob})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_binomial_log_prob_and_entropy(self):\n@@ -1431,7 +1431,7 @@ def test_poisson_sample(self):\n         for rate in [0.1, 1.0, 5.0]:\n             self._check_sampler_discrete(Poisson(rate),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={})'.format(rate),\n+                                         f'Poisson(lambda={rate})',\n                                          failure_rate=1e-3)\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n@@ -1441,7 +1441,7 @@ def test_poisson_gpu_sample(self):\n         for rate in [0.12, 0.9, 4.0]:\n             self._check_sampler_discrete(Poisson(torch.tensor([rate]).cuda()),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={}, cuda)'.format(rate),\n+                                         f'Poisson(lambda={rate}, cuda)',\n                                          failure_rate=1e-3)\n \n     def test_relaxed_bernoulli(self):\n@@ -1476,7 +1476,7 @@ def sample(self, *args, **kwargs):\n         for probs, temp in product([0.1, 0.2, 0.8], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(Rounded(RelaxedBernoulli(temp, probs)),\n                                          scipy.stats.bernoulli(probs),\n-                                         'Rounded(RelaxedBernoulli(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedBernoulli(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [0.001, 0.2, 0.999]:\n@@ -1534,7 +1534,7 @@ def pmf(self, samples):\n         for probs, temp in product([torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(ArgMax(RelaxedOneHotCategorical(temp, probs)),\n                                          ScipyCategorical(scipy.stats.multinomial(1, probs)),\n-                                         'Rounded(RelaxedOneHotCategorical(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedOneHotCategorical(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])]:\n@@ -1588,7 +1588,7 @@ def test_vonmises_sample(self):\n             for concentration in [0.03, 0.3, 1.0, 10.0, 100.0]:\n                 self._check_sampler_sampler(VonMises(loc, concentration),\n                                             scipy.stats.vonmises(loc=loc, kappa=concentration),\n-                                            \"VonMises(loc={}, concentration={})\".format(loc, concentration),\n+                                            f\"VonMises(loc={loc}, concentration={concentration})\",\n                                             num_samples=int(1e5), circular=True)\n \n     def test_vonmises_logprob(self):\n@@ -1694,7 +1694,7 @@ def test_halfnormal_sample(self):\n         for std in [0.1, 1.0, 10.0]:\n             self._check_sampler_sampler(HalfNormal(std),\n                                         scipy.stats.halfnorm(scale=std),\n-                                        'HalfNormal(scale={})'.format(std))\n+                                        f'HalfNormal(scale={std})')\n \n     def test_lognormal(self):\n         mean = torch.randn(5, 5, requires_grad=True)\n@@ -1746,7 +1746,7 @@ def test_lognormal_sample(self):\n         for mean, std in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(LogNormal(mean, std),\n                                         scipy.stats.lognorm(scale=math.exp(mean), s=std),\n-                                        'LogNormal(loc={}, scale={})'.format(mean, std))\n+                                        f'LogNormal(loc={mean}, scale={std})')\n \n     def test_logisticnormal(self):\n         set_rng_seed(1)  # see Note [Randomized statistical tests]\n@@ -1814,7 +1814,7 @@ def test_logisticnormal_sample(self):\n             std_th = torch.tensor(np.sqrt(np.diag(cov)))\n             self._check_sampler_sampler(\n                 LogisticNormal(mean_th, std_th), ref_dist,\n-                'LogisticNormal(loc={}, scale={})'.format(mean_th, std_th),\n+                f'LogisticNormal(loc={mean_th}, scale={std_th})',\n                 multivariate=True)\n \n     def test_mixture_same_family_shape(self):\n@@ -1958,7 +1958,7 @@ def test_normal_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Normal(loc, scale),\n                                         scipy.stats.norm(loc=loc, scale=scale),\n-                                        'Normal(mean={}, std={})'.format(loc, scale))\n+                                        f'Normal(mean={loc}, std={scale})')\n \n     def test_lowrank_multivariate_normal_shape(self):\n         mean = torch.randn(5, 3, requires_grad=True)\n@@ -2191,15 +2191,15 @@ def test_multivariate_normal_sample(self):\n \n         self._check_sampler_sampler(MultivariateNormal(mean, cov),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, cov={})'.format(mean, cov),\n+                                    f'MultivariateNormal(loc={mean}, cov={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, precision_matrix=prec),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, atol={})'.format(mean, prec),\n+                                    f'MultivariateNormal(loc={mean}, atol={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, scale_tril=scale_tril),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, scale_tril={})'.format(mean, scale_tril),\n+                                    f'MultivariateNormal(loc={mean}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_multivariate_normal_properties(self):\n@@ -2352,15 +2352,15 @@ def test_wishart_sample(self):\n \n         self._check_sampler_sampler(Wishart(df, cov),\n                                     ref_dist,\n-                                    'Wishart(df={}, covariance_matrix={})'.format(df, cov),\n+                                    f'Wishart(df={df}, covariance_matrix={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, precision_matrix=prec),\n                                     ref_dist,\n-                                    'Wishart(df={}, precision_matrix={})'.format(df, prec),\n+                                    f'Wishart(df={df}, precision_matrix={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, scale_tril=scale_tril),\n                                     ref_dist,\n-                                    'Wishart(df={}, scale_tril={})'.format(df, scale_tril),\n+                                    f'Wishart(df={df}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_wishart_properties(self):\n@@ -2431,7 +2431,7 @@ def test_exponential_sample(self):\n         for rate in [1e-5, 1.0, 10.]:\n             self._check_sampler_sampler(Exponential(rate),\n                                         scipy.stats.expon(scale=1. / rate),\n-                                        'Exponential(rate={})'.format(rate))\n+                                        f'Exponential(rate={rate})')\n \n     def test_laplace(self):\n         loc = torch.randn(5, 5, requires_grad=True)\n@@ -2482,7 +2482,7 @@ def test_laplace_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Laplace(loc, scale),\n                                         scipy.stats.laplace(loc=loc, scale=scale),\n-                                        'Laplace(loc={}, scale={})'.format(loc, scale))\n+                                        f'Laplace(loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gamma_shape(self):\n@@ -2533,7 +2533,7 @@ def test_gamma_sample(self):\n         for alpha, beta in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gamma(alpha, beta),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(concentration={}, rate={})'.format(alpha, beta))\n+                                        f'Gamma(concentration={alpha}, rate={beta})')\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n@@ -2543,7 +2543,7 @@ def test_gamma_gpu_sample(self):\n             a, b = torch.tensor([alpha]).cuda(), torch.tensor([beta]).cuda()\n             self._check_sampler_sampler(Gamma(a, b),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(alpha={}, beta={})'.format(alpha, beta),\n+                                        f'Gamma(alpha={alpha}, beta={beta})',\n                                         failure_rate=1e-4)\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -2575,7 +2575,7 @@ def test_pareto_sample(self):\n         for scale, alpha in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Pareto(scale, alpha),\n                                         scipy.stats.pareto(alpha, scale=scale),\n-                                        'Pareto(scale={}, alpha={})'.format(scale, alpha))\n+                                        f'Pareto(scale={scale}, alpha={alpha})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gumbel(self):\n@@ -2616,7 +2616,7 @@ def test_gumbel_sample(self):\n         for loc, scale in product([-5.0, -1.0, -0.1, 0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gumbel(loc, scale),\n                                         scipy.stats.gumbel_r(loc=loc, scale=scale),\n-                                        'Gumbel(loc={}, scale={})'.format(loc, scale))\n+                                        f'Gumbel(loc={loc}, scale={scale})')\n \n     def test_kumaraswamy_shape(self):\n         concentration1 = torch.randn(2, 3).abs().requires_grad_()\n@@ -2646,13 +2646,13 @@ def test_kumaraswamy_mean_variance(self):\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .mean\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .mean\")\n             expected = samples.var(0)\n             actual = m.variance\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .variance\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .variance\")\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_fishersnedecor(self):\n@@ -2683,7 +2683,7 @@ def test_fishersnedecor_sample(self):\n         for df1, df2 in product([0.1, 0.5, 1.0, 5.0, 10.0], [0.1, 0.5, 1.0, 5.0, 10.0]):\n             self._check_sampler_sampler(FisherSnedecor(df1, df2),\n                                         scipy.stats.f(df1, df2),\n-                                        'FisherSnedecor(loc={}, scale={})'.format(df1, df2))\n+                                        f'FisherSnedecor(loc={df1}, scale={df2})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_chi2_shape(self):\n@@ -2710,7 +2710,7 @@ def test_chi2_sample(self):\n         for df in [0.1, 1.0, 5.0]:\n             self._check_sampler_sampler(Chi2(df),\n                                         scipy.stats.chi2(df),\n-                                        'Chi2(df={})'.format(df))\n+                                        f'Chi2(df={df})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT(self):\n@@ -2740,7 +2740,7 @@ def test_studentT_sample(self):\n         for df, loc, scale in product([0.1, 1.0, 5.0, 10.0], [-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(StudentT(df=df, loc=loc, scale=scale),\n                                         scipy.stats.t(df=df, loc=loc, scale=scale),\n-                                        'StudentT(df={}, loc={}, scale={})'.format(df, loc, scale))\n+                                        f'StudentT(df={df}, loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT_log_prob(self):\n@@ -2793,7 +2793,7 @@ def test_dirichlet_sample(self):\n         alpha = torch.exp(torch.randn(3))\n         self._check_sampler_sampler(Dirichlet(alpha),\n                                     scipy.stats.dirichlet(alpha.numpy()),\n-                                    'Dirichlet(alpha={})'.format(list(alpha)),\n+                                    f'Dirichlet(alpha={list(alpha)})',\n                                     multivariate=True)\n \n     def test_dirichlet_mode(self):\n@@ -2837,11 +2837,11 @@ def test_beta_sample(self):\n         for con1, con0 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Beta(con1, con0),\n                                         scipy.stats.beta(con1, con0),\n-                                        'Beta(alpha={}, beta={})'.format(con1, con0))\n+                                        f'Beta(alpha={con1}, beta={con0})')\n         # Check that small alphas do not cause NANs.\n         for Tensor in [torch.FloatTensor, torch.DoubleTensor]:\n             x = Beta(Tensor([1e-6]), Tensor([1e-6])).sample()[0]\n-            self.assertTrue(np.isfinite(x) and x > 0, 'Invalid Beta.sample(): {}'.format(x))\n+            self.assertTrue(np.isfinite(x) and x > 0, f'Invalid Beta.sample(): {x}')\n \n     def test_beta_underflow(self):\n         # For low values of (alpha, beta), the gamma samples can underflow\n@@ -2997,10 +2997,10 @@ def test_cdf_icdf_inverse(self):\n                     continue\n                 rel_error = torch.abs(actual - samples) / (1e-10 + torch.abs(samples))\n                 self.assertLess(rel_error.max(), 1e-4, msg='\\n'.join([\n-                    '{} example {}/{}, icdf(cdf(x)) != x'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf(x) = {}'.format(cdf),\n-                    'icdf(cdf(x)) = {}'.format(actual),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, icdf(cdf(x)) != x',\n+                    f'x = {samples}',\n+                    f'cdf(x) = {cdf}',\n+                    f'icdf(cdf(x)) = {actual}',\n                 ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3029,11 +3029,11 @@ def test_cdf_log_prob(self):\n                     continue\n                 cdfs_derivative = grad(cdfs.sum(), [samples])[0]  # this should not be wrapped in torch.abs()\n                 self.assertEqual(cdfs_derivative, pdfs, msg='\\n'.join([\n-                    '{} example {}/{}, d(cdf)/dx != pdf(x)'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf = {}'.format(cdfs),\n-                    'pdf = {}'.format(pdfs),\n-                    'grad(cdf) = {}'.format(cdfs_derivative),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, d(cdf)/dx != pdf(x)',\n+                    f'x = {samples}',\n+                    f'cdf = {cdfs}',\n+                    f'pdf = {pdfs}',\n+                    f'grad(cdf) = {cdfs_derivative}',\n                 ]))\n \n     def test_valid_parameter_broadcasting(self):\n@@ -3144,13 +3144,13 @@ def test_valid_parameter_broadcasting(self):\n         for dist, expected_size in valid_examples:\n             actual_size = dist.sample().size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n             sample_shape = torch.Size((2,))\n             expected_size = sample_shape + expected_size\n             actual_size = dist.sample(sample_shape).size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n     def test_invalid_parameter_broadcasting(self):\n         # invalid broadcasting cases; should throw error\n@@ -3303,13 +3303,13 @@ def test_gamma(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.0005, '\\n'.join([\n-                'Bad gradient dx/alpha for x ~ Gamma({}, 1)'.format(alpha),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at alpha={}, x={}'.format(alpha, x[rel_error.argmax()]),\n+                f'Bad gradient dx/alpha for x ~ Gamma({alpha}, 1)',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at alpha={alpha}, x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3331,12 +3331,12 @@ def test_chi2(self):\n             expected_grad = -cdf_df / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx/ddf for x ~ Chi2({})'.format(df),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n+                f'Bad gradient dx/ddf for x ~ Chi2({df})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3361,13 +3361,13 @@ def test_dirichlet_on_diagonal(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx[0]/dalpha[0] for Dirichlet([{}, {}, {}])'.format(a0, a1, a2),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x={}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx[0]/dalpha[0] for Dirichlet([{a0}, {a1}, {a2}])',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3391,13 +3391,13 @@ def test_beta_wrt_alpha(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon1 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon1 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3421,13 +3421,13 @@ def test_beta_wrt_beta(self):\n             expected_grad = -cdf_beta / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon0 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {!r}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon0 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]!r}',\n             ]))\n \n     def test_dirichlet_multivariate(self):\n@@ -3485,8 +3485,8 @@ def compute_v(x, alpha):\n             # expression in terms of log_prob rather than the less numerically stable log_prob.exp().\n             error = dlogp_da + (dlogp_dx * v).sum(-1) + div_v\n             self.assertLess(torch.abs(error).max(), 0.005, '\\n'.join([\n-                'Dirichlet([{}, {}, {}]) gradient violates continuity equation:'.format(a1, a2, a3),\n-                'error = {}'.format(error),\n+                f'Dirichlet([{a1}, {a2}, {a3}]) gradient violates continuity equation:',\n+                f'error = {error}',\n             ]))\n \n \n@@ -4147,9 +4147,9 @@ def test_kl_monte_carlo(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                'Expected ({} Monte Carlo samples): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                f'Expected ({denominator} Monte Carlo samples): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     # Multivariate normal has a separate Monte Carlo based test due to the requirement of random generation of\n@@ -4174,9 +4174,9 @@ def test_kl_multivariate_normal(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected ({} Monte Carlo sample): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected ({denominator} Monte Carlo sample): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     def test_kl_multivariate_normal_batched(self):\n@@ -4223,23 +4223,23 @@ def test_kl_lowrank_multivariate_normal(self):\n \n             error_lowrank_lowrank = torch.abs(actual_lowrank_lowrank - expected).max()\n             self.assertLess(error_lowrank_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_lowrank),\n+                f'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_lowrank}',\n             ]))\n \n             error_lowrank_full = torch.abs(actual_lowrank_full - expected).max()\n             self.assertLess(error_lowrank_full, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_full),\n+                f'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_full}',\n             ]))\n \n             error_full_lowrank = torch.abs(actual_full_lowrank - expected).max()\n             self.assertLess(error_full_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_full_lowrank),\n+                f'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_full_lowrank}',\n             ]))\n \n     def test_kl_lowrank_multivariate_normal_batched(self):\n@@ -4261,16 +4261,16 @@ def test_kl_exponential_family(self):\n                 actual = kl_divergence(p, q)\n                 expected = _kl_expfamily_expfamily(p, q)\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                    'Expected (using Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                    f'Expected (using Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n     def test_kl_infinite(self):\n         for p, q in self.infinite_examples:\n             self.assertTrue((kl_divergence(p, q) == inf).all(),\n-                            'Incorrect KL({}, {})'.format(type(p).__name__, type(q).__name__))\n+                            f'Incorrect KL({type(p).__name__}, {type(q).__name__})')\n \n     def test_kl_edgecases(self):\n         self.assertEqual(kl_divergence(Bernoulli(0), Bernoulli(0)), 0)\n@@ -4287,9 +4287,9 @@ def test_kl_shape(self):\n                     continue\n                 expected_shape = dist.batch_shape if dist.batch_shape else torch.Size()\n                 self.assertEqual(kl.shape, expected_shape, msg='\\n'.join([\n-                    '{} example {}/{}'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected {}'.format(expected_shape),\n-                    'Actual {}'.format(kl.shape),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}',\n+                    f'Expected {expected_shape}',\n+                    f'Actual {kl.shape}',\n                 ]))\n \n     def test_kl_transformed(self):\n@@ -4316,10 +4316,10 @@ def test_entropy_monte_carlo(self):\n                 ignore = (expected == inf) | (expected == -inf)\n                 expected[ignore] = actual[ignore]\n                 self.assertEqual(actual, expected, atol=0.2, rtol=0, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (monte carlo) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max()),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (monte carlo) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}',\n                 ]))\n \n     def test_entropy_exponential_family(self):\n@@ -4337,10 +4337,10 @@ def test_entropy_exponential_family(self):\n                 except NotImplementedError:\n                     continue\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n \n@@ -4632,7 +4632,7 @@ def test_lazy_logits_initialization(self):\n             dist = Dist(**param)\n             # Create new instance to generate a valid sample\n             dist.log_prob(Dist(**param).sample())\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('probs', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -4649,7 +4649,7 @@ def test_lazy_probs_initialization(self):\n                 continue\n             dist = Dist(**param)\n             dist.sample()\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('logits', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -5161,7 +5161,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_enumerate_support(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5185,7 +5185,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_mean(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5207,7 +5207,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_variance(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5231,7 +5231,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_entropy(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5255,7 +5255,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_cdf(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5276,7 +5276,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n \n if __name__ == '__main__' and torch._C.has_lapack:\ndiff --git a/test/distributions/test_transforms.py b/test/distributions/test_transforms.py\nindex a4a025b83fd36e..6fd4cf818d6b83 100644\n--- a/test/distributions/test_transforms.py\n+++ b/test/distributions/test_transforms.py\n@@ -156,7 +156,7 @@ def generate_data(transform):\n         x /= x.norm(dim=-1, keepdim=True)\n         x.diagonal(dim1=-1).copy_(x.diagonal(dim1=-1).abs())\n         return x\n-    raise ValueError('Unsupported domain: {}'.format(domain))\n+    raise ValueError(f'Unsupported domain: {domain}')\n \n \n TRANSFORMS_CACHE_ACTIVE = get_transforms(cache_size=1)\n@@ -215,19 +215,19 @@ def test_forward_inverse(transform, test_cached):\n     if transform.bijective:\n         # verify function inverse\n         assert torch.allclose(x2, x, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t.inv(t(-)) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n+            f'{transform} t.inv(t(-)) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n         ])\n     else:\n         # verify weaker function pseudo-inverse\n         assert torch.allclose(y2, y, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t(t.inv(t(-))) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n-            'y2 = t(x2) = {}'.format(y2),\n+            f'{transform} t(t.inv(t(-))) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n+            f'y2 = t(x2) = {y2}',\n         ])\n \n \ndiff --git a/test/optim/test_optim.py b/test/optim/test_optim.py\nindex 54307b2417eaf6..2f1f5536fc2fe1 100644\n--- a/test/optim/test_optim.py\n+++ b/test/optim/test_optim.py\n@@ -1701,8 +1701,8 @@ def test_fused_optimizer_does_not_step_if_foundinf(self):\n \n         num_tensors = 5\n         for functional_optim, amsgrad, no_grad_scale in itertools.product((adam.adam, adamw.adamw), (False, True), (False, True)):\n-            params, grads, exp_avgs, exp_avg_sqs = [\n-                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4)]\n+            params, grads, exp_avgs, exp_avg_sqs = (\n+                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4))\n             prev_params = [t.clone().detach() for t in params]\n             max_exp_avg_sqs = [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] if amsgrad else []\n             state_steps = [torch.ones((), dtype=torch.float32, device=\"cuda\") for _ in range(num_tensors)]\ndiff --git a/torch/distributions/constraints.py b/torch/distributions/constraints.py\nindex a4e3c08461cde7..5f284959beb372 100644\n--- a/torch/distributions/constraints.py\n+++ b/torch/distributions/constraints.py\n@@ -258,7 +258,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -277,7 +277,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -296,7 +296,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -321,7 +321,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -338,7 +338,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -355,7 +355,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -373,7 +373,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -391,7 +391,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \ndiff --git a/torch/distributions/independent.py b/torch/distributions/independent.py\nindex 48442650ddcb77..44a01fd62f9130 100644\n--- a/torch/distributions/independent.py\n+++ b/torch/distributions/independent.py\n@@ -109,4 +109,4 @@ def enumerate_support(self, expand=True):\n         return self.base_dist.enumerate_support(expand=expand)\n \n     def __repr__(self):\n-        return self.__class__.__name__ + '({}, {})'.format(self.base_dist, self.reinterpreted_batch_ndims)\n+        return self.__class__.__name__ + f'({self.base_dist}, {self.reinterpreted_batch_ndims})'\ndiff --git a/torch/distributions/kl.py b/torch/distributions/kl.py\nindex 26d7b47d2f51a8..4eda85ef75b68a 100644\n--- a/torch/distributions/kl.py\n+++ b/torch/distributions/kl.py\n@@ -65,9 +65,9 @@ def kl_version2(p, q): ...\n         type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n     \"\"\"\n     if not isinstance(type_p, type) and issubclass(type_p, Distribution):\n-        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n+        raise TypeError(f'Expected type_p to be a Distribution subclass but got {type_p}')\n     if not isinstance(type_q, type) and issubclass(type_q, Distribution):\n-        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n+        raise TypeError(f'Expected type_q to be a Distribution subclass but got {type_q}')\n \n     def decorator(fun):\n         _KL_REGISTRY[type_p, type_q] = fun\n@@ -735,7 +735,7 @@ def _kl_uniform_beta(p, q):\n     common_term = p.high - p.low\n     t1 = torch.log(common_term)\n     t2 = (q.concentration1 - 1) * (_x_log_x(p.high) - _x_log_x(p.low) - common_term) / common_term\n-    t3 = (q.concentration0 - 1) * (_x_log_x((1 - p.high)) - _x_log_x((1 - p.low)) + common_term) / common_term\n+    t3 = (q.concentration0 - 1) * (_x_log_x(1 - p.high) - _x_log_x(1 - p.low) + common_term) / common_term\n     t4 = q.concentration1.lgamma() + q.concentration0.lgamma() - (q.concentration1 + q.concentration0).lgamma()\n     result = t3 + t4 - t1 - t2\n     result[(p.high > q.support.upper_bound) | (p.low < q.support.lower_bound)] = inf\ndiff --git a/torch/distributions/lowrank_multivariate_normal.py b/torch/distributions/lowrank_multivariate_normal.py\nindex f74ea47a7e53a4..5ca125a92dd006 100644\n--- a/torch/distributions/lowrank_multivariate_normal.py\n+++ b/torch/distributions/lowrank_multivariate_normal.py\n@@ -93,7 +93,7 @@ def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n             raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\"\n                              .format(event_shape[0]))\n         if cov_diag.shape[-1:] != event_shape:\n-            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n+            raise ValueError(f\"cov_diag must be a batch of vectors with shape {event_shape}\")\n \n         loc_ = loc.unsqueeze(-1)\n         cov_diag_ = cov_diag.unsqueeze(-1)\ndiff --git a/torch/distributions/mixture_same_family.py b/torch/distributions/mixture_same_family.py\nindex f12bef1da2c54d..a4d7bd6ff4610b 100644\n--- a/torch/distributions/mixture_same_family.py\n+++ b/torch/distributions/mixture_same_family.py\n@@ -71,17 +71,17 @@ def __init__(self,\n         cdbs = self._component_distribution.batch_shape[:-1]\n         for size1, size2 in zip(reversed(mdbs), reversed(cdbs)):\n             if size1 != 1 and size2 != 1 and size1 != size2:\n-                raise ValueError(\"`mixture_distribution.batch_shape` ({0}) is not \"\n+                raise ValueError(\"`mixture_distribution.batch_shape` ({}) is not \"\n                                  \"compatible with `component_distribution.\"\n-                                 \"batch_shape`({1})\".format(mdbs, cdbs))\n+                                 \"batch_shape`({})\".format(mdbs, cdbs))\n \n         # Check that the number of mixture component matches\n         km = self._mixture_distribution.logits.shape[-1]\n         kc = self._component_distribution.batch_shape[-1]\n         if km is not None and kc is not None and km != kc:\n-            raise ValueError(\"`mixture_distribution component` ({0}) does not\"\n+            raise ValueError(\"`mixture_distribution component` ({}) does not\"\n                              \" equal `component_distribution.batch_shape[-1]`\"\n-                             \" ({1})\".format(km, kc))\n+                             \" ({})\".format(km, kc))\n         self._num_component = km\n \n         event_shape = self._component_distribution.event_shape\ndiff --git a/torch/distributions/transformed_distribution.py b/torch/distributions/transformed_distribution.py\nindex d31064210d4ba7..cd7b5f088a99fe 100644\n--- a/torch/distributions/transformed_distribution.py\n+++ b/torch/distributions/transformed_distribution.py\n@@ -51,7 +51,7 @@ def __init__(self, base_distribution, transforms, validate_args=None):\n                 raise ValueError(\"transforms must be a Transform or a list of Transforms\")\n             self.transforms = transforms\n         else:\n-            raise ValueError(\"transforms must be a Transform or list, but was {}\".format(transforms))\n+            raise ValueError(f\"transforms must be a Transform or list, but was {transforms}\")\n \n         # Reshape base_distribution according to transforms.\n         base_shape = base_distribution.batch_shape + base_distribution.event_shape\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 06d21548384e3c..6745d1f6fbd51e 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -135,7 +135,7 @@ def with_cache(self, cache_size=1):\n             return self\n         if type(self).__init__ is Transform.__init__:\n             return type(self)(cache_size=cache_size)\n-        raise NotImplementedError(\"{}.with_cache is not implemented\".format(type(self)))\n+        raise NotImplementedError(f\"{type(self)}.with_cache is not implemented\")\n \n     def __eq__(self, other):\n         return self is other\n@@ -506,7 +506,7 @@ def forward_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.in_shape)\n         if shape[cut:] != self.in_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.in_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.in_shape}\")\n         return shape[:cut] + self.out_shape\n \n     def inverse_shape(self, shape):\n@@ -514,7 +514,7 @@ def inverse_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.out_shape)\n         if shape[cut:] != self.out_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.out_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.out_shape}\")\n         return shape[:cut] + self.in_shape\n \n \ndiff --git a/torch/optim/adadelta.py b/torch/optim/adadelta.py\nindex d4cbd41883af65..a38337426313db 100644\n--- a/torch/optim/adadelta.py\n+++ b/torch/optim/adadelta.py\n@@ -22,13 +22,13 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= rho <= 1.0:\n-            raise ValueError(\"Invalid rho value: {}\".format(rho))\n+            raise ValueError(f\"Invalid rho value: {rho}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adagrad.py b/torch/optim/adagrad.py\nindex 5909818e5bfb6e..1a3e5120004f98 100644\n--- a/torch/optim/adagrad.py\n+++ b/torch/optim/adagrad.py\n@@ -23,11 +23,11 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= lr_decay:\n-            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n+            raise ValueError(f\"Invalid lr_decay value: {lr_decay}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= initial_accumulator_value:\n             raise ValueError(\n                 \"Invalid initial_accumulator_value value: {}\".format(\n@@ -35,7 +35,7 @@ def __init__(\n                 )\n             )\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adam.py b/torch/optim/adam.py\nindex 3c0d550f414b45..c7e4ed45a92156 100644\n--- a/torch/optim/adam.py\n+++ b/torch/optim/adam.py\n@@ -16,15 +16,15 @@ def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                  maximize: bool = False, capturable: bool = False,\n                  differentiable: bool = False, fused: Optional[bool] = None):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, amsgrad=amsgrad,\ndiff --git a/torch/optim/adamax.py b/torch/optim/adamax.py\nindex 9a5bf9131993ad..1ee927274558f1 100644\n--- a/torch/optim/adamax.py\n+++ b/torch/optim/adamax.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adamw.py b/torch/optim/adamw.py\nindex da202d95c2a032..ff8dbef1d46e8a 100644\n--- a/torch/optim/adamw.py\n+++ b/torch/optim/adamw.py\n@@ -26,15 +26,15 @@ def __init__(\n         fused: Optional[bool] = None,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/asgd.py b/torch/optim/asgd.py\nindex 5e5bd759c1d540..e483e1c31fbc7c 100644\n--- a/torch/optim/asgd.py\n+++ b/torch/optim/asgd.py\n@@ -28,9 +28,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/lr_scheduler.py b/torch/optim/lr_scheduler.py\nindex b531f5149d1aff..d0f85a5daea0c8 100644\n--- a/torch/optim/lr_scheduler.py\n+++ b/torch/optim/lr_scheduler.py\n@@ -1366,11 +1366,11 @@ class CosineAnnealingWarmRestarts(LRScheduler):\n \n     def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n         if T_0 <= 0 or not isinstance(T_0, int):\n-            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n+            raise ValueError(f\"Expected positive integer T_0, but got {T_0}\")\n         if T_mult < 1 or not isinstance(T_mult, int):\n-            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n+            raise ValueError(f\"Expected integer T_mult >= 1, but got {T_mult}\")\n         if not isinstance(eta_min, (float, int)):\n-            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))\n+            raise ValueError(f\"Expected float or int eta_min, but got {eta_min} of type {type(eta_min)}\")\n         self.T_0 = T_0\n         self.T_i = T_0\n         self.T_mult = T_mult\n@@ -1425,7 +1425,7 @@ def step(self, epoch=None):\n                 self.T_i = self.T_i * self.T_mult\n         else:\n             if epoch < 0:\n-                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n+                raise ValueError(f\"Expected non-negative epoch, but got {epoch}\")\n             if epoch >= self.T_0:\n                 if self.T_mult == 1:\n                     self.T_cur = epoch % self.T_0\n@@ -1590,13 +1590,13 @@ def __init__(self,\n             raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n         elif total_steps is not None:\n             if total_steps <= 0 or not isinstance(total_steps, int):\n-                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n+                raise ValueError(f\"Expected positive integer total_steps, but got {total_steps}\")\n             self.total_steps = total_steps\n         else:\n             if epochs <= 0 or not isinstance(epochs, int):\n-                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n+                raise ValueError(f\"Expected positive integer epochs, but got {epochs}\")\n             if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n-                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n+                raise ValueError(f\"Expected positive integer steps_per_epoch, but got {steps_per_epoch}\")\n             self.total_steps = epochs * steps_per_epoch\n \n         if three_phase:\n@@ -1643,11 +1643,11 @@ def __init__(self,\n \n         # Validate pct_start\n         if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n-            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n+            raise ValueError(f\"Expected float between 0 and 1 pct_start, but got {pct_start}\")\n \n         # Validate anneal_strategy\n         if anneal_strategy not in ['cos', 'linear']:\n-            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n+            raise ValueError(f\"anneal_strategy must by one of 'cos' or 'linear', instead got {anneal_strategy}\")\n         elif anneal_strategy == 'cos':\n             self.anneal_func = self._annealing_cos\n         elif anneal_strategy == 'linear':\ndiff --git a/torch/optim/nadam.py b/torch/optim/nadam.py\nindex 23fa563f044d0d..aeb3fc8b77dd2c 100644\n--- a/torch/optim/nadam.py\n+++ b/torch/optim/nadam.py\n@@ -11,17 +11,17 @@ def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                  weight_decay=0, momentum_decay=4e-3, *, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= momentum_decay:\n-            raise ValueError(\"Invalid momentum_decay value: {}\".format(momentum_decay))\n+            raise ValueError(f\"Invalid momentum_decay value: {momentum_decay}\")\n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, momentum_decay=momentum_decay,\n                         foreach=foreach, differentiable=differentiable)\ndiff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py\nindex 34d27bdaca6058..2356a073f3719d 100644\n--- a/torch/optim/optimizer.py\n+++ b/torch/optim/optimizer.py\n@@ -246,10 +246,10 @@ def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         for i, group in enumerate(self.param_groups):\n             format_string += '\\n'\n-            format_string += 'Parameter Group {0}\\n'.format(i)\n+            format_string += f'Parameter Group {i}\\n'\n             for key in sorted(group.keys()):\n                 if key != 'params':\n-                    format_string += '    {0}: {1}\\n'.format(key, group[key])\n+                    format_string += f'    {key}: {group[key]}\\n'\n         format_string += ')'\n         return format_string\n \n@@ -304,7 +304,7 @@ def profile_hook_step(func):\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             self, *_ = args\n-            profile_name = \"Optimizer.step#{}.step\".format(self.__class__.__name__)\n+            profile_name = f\"Optimizer.step#{self.__class__.__name__}.step\"\n             with torch.autograd.profiler.record_function(profile_name):\n                 # call optimizer step pre hooks\n                 for pre_hook in chain(_global_optimizer_pre_hooks.values(), self._optimizer_step_pre_hooks.values()):\n@@ -337,7 +337,7 @@ def _group_tensors_by_device_and_dtype(tensorlistlist, with_indices=False):\n             return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)\n \n     def _patch_step_function(self):\n-        self._zero_grad_profile_name = \"Optimizer.zero_grad#{}.zero_grad\".format(self.__class__.__name__)\n+        self._zero_grad_profile_name = f\"Optimizer.zero_grad#{self.__class__.__name__}.zero_grad\"\n         hooked = getattr(self.__class__.step, \"hooked\", None)\n         if not hooked:\n             self.__class__.step = self.profile_hook_step(self.__class__.step)  # type: ignore[method-assign]\n@@ -468,8 +468,8 @@ def load_state_dict(self, state_dict):\n                              \"that doesn't match the size of optimizer's group\")\n \n         # Update the state\n-        id_map = dict(zip(chain.from_iterable((g['params'] for g in saved_groups)),\n-                      chain.from_iterable((g['params'] for g in groups))))\n+        id_map = dict(zip(chain.from_iterable(g['params'] for g in saved_groups),\n+                      chain.from_iterable(g['params'] for g in groups)))\n \n         def cast(param, value, param_id=None, param_groups=None, key=None):\n             r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\ndiff --git a/torch/optim/radam.py b/torch/optim/radam.py\nindex 3078db48cfd2fc..120620ab949cc1 100644\n--- a/torch/optim/radam.py\n+++ b/torch/optim/radam.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/rmsprop.py b/torch/optim/rmsprop.py\nindex 88acf98a1bcbda..cec27d95506840 100644\n--- a/torch/optim/rmsprop.py\n+++ b/torch/optim/rmsprop.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= momentum:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= alpha:\n-            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n+            raise ValueError(f\"Invalid alpha value: {alpha}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/rprop.py b/torch/optim/rprop.py\nindex a0812f5fbc903f..93e7241010500a 100644\n--- a/torch/optim/rprop.py\n+++ b/torch/optim/rprop.py\n@@ -20,9 +20,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < etas[0] < 1.0 < etas[1]:\n-            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n+            raise ValueError(f\"Invalid eta values: {etas[0]}, {etas[1]}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/sgd.py b/torch/optim/sgd.py\nindex c34761d5e48555..d22fb2a697fd41 100644\n--- a/torch/optim/sgd.py\n+++ b/torch/optim/sgd.py\n@@ -11,11 +11,11 @@ def __init__(self, params, lr=required, momentum=0, dampening=0,\n                  weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if lr is not required and lr < 0.0:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if momentum < 0.0:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if weight_decay < 0.0:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                         weight_decay=weight_decay, nesterov=nesterov,\ndiff --git a/torch/optim/sparse_adam.py b/torch/optim/sparse_adam.py\nindex 383b6866e822af..c68441cb389c04 100644\n--- a/torch/optim/sparse_adam.py\n+++ b/torch/optim/sparse_adam.py\n@@ -7,13 +7,13 @@\n class SparseAdam(Optimizer):\n     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, maximize: bool = False):\n         if not 0.0 < lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n \n         params = list(params)\n \ndiff --git a/torch/package/_importlib.py b/torch/package/_importlib.py\nindex 327c79c67ef9d7..011567b89f5d6c 100644\n--- a/torch/package/_importlib.py\n+++ b/torch/package/_importlib.py\n@@ -31,13 +31,13 @@ def _resolve_name(name, package, level):\n     if len(bits) < level:\n         raise ValueError(\"attempted relative import beyond top-level package\")\n     base = bits[0]\n-    return \"{}.{}\".format(base, name) if name else base\n+    return f\"{base}.{name}\" if name else base\n \n \n def _sanity_check(name, package, level):\n     \"\"\"Verify arguments are \"sane\".\"\"\"\n     if not isinstance(name, str):\n-        raise TypeError(\"module name must be str, not {}\".format(type(name)))\n+        raise TypeError(f\"module name must be str, not {type(name)}\")\n     if level < 0:\n         raise ValueError(\"level must be >= 0\")\n     if level > 0:\n@@ -90,6 +90,6 @@ def _normalize_path(path):\n     \"\"\"\n     parent, file_name = os.path.split(path)\n     if parent:\n-        raise ValueError(\"{!r} must be only a file name\".format(path))\n+        raise ValueError(f\"{path!r} must be only a file name\")\n     else:\n         return file_name\ndiff --git a/torch/package/file_structure_representation.py b/torch/package/file_structure_representation.py\nindex 6ea69173ed3f69..cc5f055c1a20ef 100644\n--- a/torch/package/file_structure_representation.py\n+++ b/torch/package/file_structure_representation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Dict, List\n \n from .glob_group import GlobGroup, GlobPattern\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex f9478b66327605..053ce0c0a89552 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -79,7 +79,7 @@ class PackagingErrorReason(Enum):\n     \"\"\"\n \n     def __repr__(self):\n-        return \"<%s.%s>\" % (self.__class__.__name__, self.name)\n+        return f\"<{self.__class__.__name__}.{self.name}>\"\n \n     IS_EXTENSION_MODULE = (\n         \"Module is a C extension module. torch.package supports Python modules only.\"\n@@ -156,14 +156,14 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-                        (\n+\n                             \"      Note: While we usually use modules in the python standard library \"\n                             f\"from the local environment, `{module_name}` has a lot of system \"\n                             \"level access and therefore can pose a security risk. We heavily \"\n                             f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n                             \"is not possible, add it to the extern list by calling \"\n                             f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-                        )\n+\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +173,10 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-                (\n+\n                     \"Set debug=True when invoking PackageExporter for a visualization of where \"\n                     \"broken modules are coming from!\\n\"\n-                )\n+\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\ndiff --git a/torch/package/package_importer.py b/torch/package/package_importer.py\nindex 8369e79e783ad7..2d313c8f14eb45 100644\n--- a/torch/package/package_importer.py\n+++ b/torch/package/package_importer.py\n@@ -539,7 +539,7 @@ def _handle_fromlist(self, module, fromlist, *, recursive=False):\n                     if not recursive and hasattr(module, \"__all__\"):\n                         self._handle_fromlist(module, module.__all__, recursive=True)\n                 elif not hasattr(module, x):\n-                    from_name = \"{}.{}\".format(module_name, x)\n+                    from_name = f\"{module_name}.{x}\"\n                     try:\n                         self._gcd_import(from_name)\n                     except ModuleNotFoundError as exc:\n@@ -587,13 +587,13 @@ def _get_package(self, package):\n         \"\"\"\n         if hasattr(package, \"__spec__\"):\n             if package.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package.__spec__.name))\n+                raise TypeError(f\"{package.__spec__.name!r} is not a package\")\n             else:\n                 return package\n         else:\n             module = self.import_module(package)\n             if module.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package))\n+                raise TypeError(f\"{package!r} is not a package\")\n             else:\n                 return module\n \ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex 7ade85a85caa11..fbbcd4d67b7889 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -738,11 +738,11 @@ def _category_snapshot(self) -> Dict[TensorAndID, Optional[Category]]:\n \n         for node in self._data_flow_graph.flow_nodes:\n             all_tensor_versions.update(((k, v) for k, (_, v) in node.inputs.items()))\n-            all_tensor_versions.update(((key, 0) for key in node.intermediates))\n+            all_tensor_versions.update((key, 0) for key in node.intermediates)\n             all_tensor_versions.update(node.outputs.items())\n \n         for i in self._categories._values.values():\n-            all_tensor_versions.update(((key, 0) for key in i._by_id_keyset))\n+            all_tensor_versions.update((key, 0) for key in i._by_id_keyset)\n \n         return {\n             (key, version): self._categories.get(key, version)\ndiff --git a/torch/profiler/_pattern_matcher.py b/torch/profiler/_pattern_matcher.py\nindex ae95faf0d2bae7..1d85d193ecf894 100644\n--- a/torch/profiler/_pattern_matcher.py\n+++ b/torch/profiler/_pattern_matcher.py\n@@ -642,7 +642,7 @@ def report_all_anti_patterns(prof,\n         json_report_path = os.path.join(json_report_dir,\n                                         \"torchtidy_report.json\")\n         if os.path.exists(json_report_path):\n-            with open(json_report_path, \"r\") as f:\n+            with open(json_report_path) as f:\n                 exisiting_report = json.load(f)\n                 exisiting_report.update(report_dict)\n                 report_dict = exisiting_report\ndiff --git a/torch/signal/windows/windows.py b/torch/signal/windows/windows.py\nindex 1ddfff96228927..d1b8e2529bb97e 100644\n--- a/torch/signal/windows/windows.py\n+++ b/torch/signal/windows/windows.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Optional, Iterable\n \n import torch\ndiff --git a/torch/sparse/semi_structured.py b/torch/sparse/semi_structured.py\nindex 0e4c217a50aafb..d1e4321d66f36b 100644\n--- a/torch/sparse/semi_structured.py\n+++ b/torch/sparse/semi_structured.py\n@@ -136,28 +136,28 @@ def __init__(\n             # check device\n             if not original_tensor.is_cuda:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.device= {original_tensor.device} is not supported! \"\n                         \"Only CUDA tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dim\n             if original_tensor.dim() != 2:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dim = {original_tensor.dim()} is not supported! \"\n                         \"Only 2d tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dtype\n             if original_tensor.dtype not in _DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dtype {original_tensor.dtype} is not a supported dtype! \"\n                         \"dtype must be one of: {_DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG}\"\n-                    )\n+\n                 )\n \n             # check shape\n@@ -167,10 +167,10 @@ def __init__(\n             if m < min_rows or m % min_rows or n < min_cols or n % min_cols:\n                 # TODO in the future we can add in padding to support dimensions that aren't perfect multiples\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.shape {original_tensor.shape} is not supported! \"\n                         \"Both dimensions must be larger or equal than and a multiple of ({min_rows}, {min_cols})\"\n-                    )\n+\n                 )\n \n             # This code calculates the size of the compressed tensor.\n"
  },
  {
    "number": 105395,
    "title": "[BE] Enable ruff's UP rules and autoformat testing/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "cd301362e02aee06d615e6f115d99225784f2e26",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105395",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105395/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105395.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105395.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105395/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105395/comments",
    "labels": [
      "open source",
      "release notes: distributed (rpc)"
    ],
    "_event_time": "2023-07-18T01:12:26.734280Z",
    "state": "closed",
    "patch": "From 770868f5b3897cc0b780c03f7b7fc508b6dd71cc Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:20 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat testing/\n\n[ghstack-poisoned]\n---\n torch/testing/_comparison.py                  | 10 +--\n .../_internal/check_kernel_launches.py        |  2 +-\n .../_internal/codegen/random_topo_test.py     | 16 ++---\n torch/testing/_internal/common_cuda.py        |  2 +-\n torch/testing/_internal/common_device_type.py | 50 +++++++--------\n torch/testing/_internal/common_distributed.py |  4 +-\n torch/testing/_internal/common_fsdp.py        |  2 +-\n .../_internal/common_methods_invocations.py   | 18 +++---\n torch/testing/_internal/common_modules.py     | 64 +++++++++----------\n torch/testing/_internal/common_nn.py          | 12 ++--\n torch/testing/_internal/common_pruning.py     |  1 -\n .../testing/_internal/common_quantization.py  |  5 +-\n torch/testing/_internal/common_utils.py       | 34 +++++-----\n torch/testing/_internal/dist_utils.py         |  4 +-\n .../_internal/distributed/distributed_test.py | 14 ++--\n .../distributed/nn/api/remote_module_test.py  | 20 +++---\n .../distributed/rpc/dist_autograd_test.py     | 22 +++----\n .../reinforcement_learning_rpc_test.py        |  2 +-\n .../distributed/rpc/faulty_agent_rpc_test.py  |  8 +--\n .../rpc/faulty_rpc_agent_test_fixture.py      |  2 +-\n .../_internal/distributed/rpc/jit/rpc_test.py |  2 +-\n .../distributed/rpc/jit/rpc_test_faulty.py    |  8 +--\n .../_internal/distributed/rpc/rpc_test.py     |  4 +-\n .../rpc/tensorpipe_rpc_agent_test_fixture.py  |  2 +-\n .../_internal/jit_metaprogramming_utils.py    | 18 +++---\n torch/testing/_internal/jit_utils.py          | 12 ++--\n torch/testing/_internal/opinfo/core.py        |  9 +--\n .../_internal/opinfo/definitions/linalg.py    |  2 +-\n .../_internal/opinfo/definitions/sparse.py    |  2 +-\n 29 files changed, 172 insertions(+), 179 deletions(-)\n\ndiff --git a/torch/testing/_comparison.py b/torch/testing/_comparison.py\nindex 1ccc7447ed7486..4204a6c0e69441 100644\n--- a/torch/testing/_comparison.py\n+++ b/torch/testing/_comparison.py\n@@ -465,9 +465,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[bool, bool]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_bool(bool_like, id=id) for bool_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_bool(self, bool_like: Any, *, id: Tuple[Any, ...]) -> bool:\n@@ -559,9 +559,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[Union[int, float, complex], Union[int, float, complex]]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_number(number_like, id=id) for number_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_number(\n@@ -675,7 +675,7 @@ def _process_inputs(\n         if not allow_subclasses and type(actual) is not type(expected):\n             self._inputs_not_supported()\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\ndiff --git a/torch/testing/_internal/check_kernel_launches.py b/torch/testing/_internal/check_kernel_launches.py\nindex 667e3412ceaf2c..131ea461ce544a 100644\n--- a/torch/testing/_internal/check_kernel_launches.py\n+++ b/torch/testing/_internal/check_kernel_launches.py\n@@ -111,7 +111,7 @@ def check_file(filename):\n         return 0\n     if should_exclude_file(filename):\n         return 0\n-    with open(filename, \"r\") as fo:\n+    with open(filename) as fo:\n         contents = fo.read()\n         unsafeCount = check_code_for_cuda_kernel_launches(contents, filename)\n     return unsafeCount\ndiff --git a/torch/testing/_internal/codegen/random_topo_test.py b/torch/testing/_internal/codegen/random_topo_test.py\nindex fdb13d4ef139c0..b94f8f60a301d3 100644\n--- a/torch/testing/_internal/codegen/random_topo_test.py\n+++ b/torch/testing/_internal/codegen/random_topo_test.py\n@@ -96,7 +96,7 @@ def get_root(x, dependency_map):\n         out_tensor = None\n \n         if DEBUG_PRINT:\n-            print(\"iteration {0}, num_sets{1}, candidates {2}, tensor_list {3}, lh_index {4}, op_index {5}\".format(\n+            print(\"iteration {}, num_sets{}, candidates {}, tensor_list {}, lh_index {}, op_index {}\".format(\n                 num_operations, num_sets, candidate, len(tensor_list), lh_index, op_index))\n         if num_operations >= 0:\n             num_operations -= 1\n@@ -125,7 +125,7 @@ def get_root(x, dependency_map):\n                     #  right = tensor_list[lh_index]\n                     out_tensor = binary_operations[op_index - u_op_size](left, right)\n                 if DEBUG_PRINT:\n-                    print(\"binary, op_2_index {0}, rh_index ?{1}\".format(op_2_index, rh_index))\n+                    print(f\"binary, op_2_index {op_2_index}, rh_index ?{rh_index}\")\n         else:\n             # binary operation, we just randomly pick two candidates.\n             # this is not the most efficient way to close dependency, as we could have\n@@ -136,7 +136,7 @@ def get_root(x, dependency_map):\n             # [if rh_index: create binary operator output tensor]\n             rh_index = candidate[cand_index]\n             if DEBUG_PRINT:\n-                print(\"binary rh_index ?{0}\".format(rh_index))\n+                print(f\"binary rh_index ?{rh_index}\")\n \n         # update candidate should happen before we remove rh_index\n         candidate[index] = len(tensor_list)\n@@ -185,7 +185,7 @@ def get_root(x, dependency_map):\n             ret_list.append(tensor_list[ind])\n \n     if DEBUG_PRINT:\n-        print(\"ended with tensor_list: {0}\".format(len(tensor_list)))\n+        print(f\"ended with tensor_list: {len(tensor_list)}\")\n \n     return tuple(ret_list)\n \n@@ -248,7 +248,7 @@ def prepareInputTensorsToRandomTopoTest(seed,\n \n \n def reproString(current_seed, args):\n-    repro_str = \"python {0}\".format(__file__)\n+    repro_str = f\"python {__file__}\"\n     if args.cuda_fuser:\n         repro_str += \" --cuda-fuser\"\n     if args.legacy_fuser:\n@@ -259,8 +259,8 @@ def reproString(current_seed, args):\n         repro_str += \" --fp16\"\n     if args.cpu:\n         repro_str += \" --cpu\"\n-    repro_str += \" --max-num-tensor {0} --max-tensor-dim {1} --max-tensor-size {2}\"\\\n-        \" --depth-factor {3} --seed {4} --repro-run\".format(\n+    repro_str += \" --max-num-tensor {} --max-tensor-dim {} --max-tensor-size {}\"\\\n+        \" --depth-factor {} --seed {} --repro-run\".format(\n             args.max_num_tensor, args.max_tensor_dim, args.max_tensor_size,\n             args.depth_factor, current_seed)\n     return repro_str\n@@ -390,7 +390,7 @@ def parse_args():\n         if len(failing_repros) == 0:\n             print(\"test passed\")\n         else:\n-            print(\"{0} out of {1} tests failed;\".format(\n+            print(\"{} out of {} tests failed;\".format(\n                   len(failing_repros), args.iterations))\n             print(\"To repro failing tests, run\\n\")\n             for repro in failing_repros:\ndiff --git a/torch/testing/_internal/common_cuda.py b/torch/testing/_internal/common_cuda.py\nindex c380dd5e6d7250..f427f7b62b9f77 100644\n--- a/torch/testing/_internal/common_cuda.py\n+++ b/torch/testing/_internal/common_cuda.py\n@@ -48,7 +48,7 @@ def initialize_cuda_context_rng():\n     if not __cuda_ctx_rng_initialized:\n         # initialize cuda context and rng for memory tests\n         for i in range(torch.cuda.device_count()):\n-            torch.randn(1, device=\"cuda:{}\".format(i))\n+            torch.randn(1, device=f\"cuda:{i}\")\n         __cuda_ctx_rng_initialized = True\n \n \ndiff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py\nindex d9c362c332479d..891c878cc5f059 100644\n--- a/torch/testing/_internal/common_device_type.py\n+++ b/torch/testing/_internal/common_device_type.py\n@@ -276,9 +276,9 @@ def _dtype_test_suffix(dtypes):\n     if isinstance(dtypes, (list, tuple)):\n         if len(dtypes) == 0:\n             return ''\n-        return '_' + '_'.join((dtype_name(d) for d in dtypes))\n+        return '_' + '_'.join(dtype_name(d) for d in dtypes)\n     elif dtypes:\n-        return '_{}'.format(dtype_name(dtypes))\n+        return f'_{dtype_name(dtypes)}'\n     else:\n         return ''\n \n@@ -286,7 +286,7 @@ def _dtype_test_suffix(dtypes):\n def _update_param_kwargs(param_kwargs, name, value):\n     \"\"\" Adds a kwarg with the specified name and value to the param_kwargs dict. \"\"\"\n     # Make name plural (e.g. devices / dtypes) if the value is composite.\n-    plural_name = '{}s'.format(name)\n+    plural_name = f'{name}s'\n \n     # Clear out old entries of the arg if any.\n     if name in param_kwargs:\n@@ -432,7 +432,7 @@ def instantiated_test(self, param_kwargs=param_kwargs):\n \n                 return result\n \n-            assert not hasattr(cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(cls, name), f\"Redefinition of test {name}\"\n             setattr(cls, name, instantiated_test)\n \n         def default_parametrize_fn(test, generic_cls, device_cls):\n@@ -467,7 +467,7 @@ def dtype_parametrize_fn(test, generic_cls, device_cls, dtypes=dtypes):\n             dtype_kwarg = None\n             if 'dtype' in param_kwargs or 'dtypes' in param_kwargs:\n                 dtype_kwarg = param_kwargs['dtypes'] if 'dtypes' in param_kwargs else param_kwargs['dtype']\n-            test_name = '{}{}{}{}'.format(name, test_suffix, device_suffix, _dtype_test_suffix(dtype_kwarg))\n+            test_name = f'{name}{test_suffix}{device_suffix}{_dtype_test_suffix(dtype_kwarg)}'\n \n             instantiate_test_helper(cls=cls, name=test_name, test=test, param_kwargs=param_kwargs,\n                                     decorator_fn=decorator_fn)\n@@ -523,7 +523,7 @@ def setUpClass(cls):\n         cls.cudnn_version = None if cls.no_cudnn else torch.backends.cudnn.version()\n \n         # Acquires the current device as the primary (test) device\n-        cls.primary_device = 'cuda:{0}'.format(torch.cuda.current_device())\n+        cls.primary_device = f'cuda:{torch.cuda.current_device()}'\n \n # See Note [Lazy Tensor tests in device agnostic testing]\n lazy_ts_backend_init = False\n@@ -589,7 +589,7 @@ def setUpClass(cls):\n         cls.device_mod = getattr(torch, cls.device_type, None)\n         assert cls.device_mod is not None, f'''torch has no module of `{cls.device_type}`, you should register\n                                             a module by `torch._register_device_module`.'''\n-        cls.primary_device = '{device_type}:{id}'.format(device_type=cls.device_type, id=cls.device_mod.current_device())\n+        cls.primary_device = f'{cls.device_type}:{cls.device_mod.current_device()}'\n \n # Adds available device-type-specific test base classes\n def get_device_type_test_bases():\n@@ -744,7 +744,7 @@ def split_if_not_empty(x: str):\n                 else:\n                     device_type_test_class.instantiate_test(name, copy.deepcopy(test))\n             else:  # Ports non-test member\n-                assert name not in device_type_test_class.__dict__, \"Redefinition of directly defined member {0}\".format(name)\n+                assert name not in device_type_test_class.__dict__, f\"Redefinition of directly defined member {name}\"\n                 nontest = getattr(generic_test_class, name)\n                 setattr(device_type_test_class, name, nontest)\n \n@@ -913,7 +913,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for op {1}!\".format(test_name, op.name))\n+                    print(f\"Failed to instantiate {test_name} for op {op.name}!\")\n                     raise ex\n         if op is check_exhausted_iterator:\n             raise ValueError('An empty op_list was passed to @ops. '\n@@ -1034,7 +1034,7 @@ def dep_fn(self, *args, **kwargs):\n             size_bytes = size(self, *args, **kwargs) if callable(size) else size\n             _device = device if device is not None else self.get_primary_device()\n             if not _has_sufficient_memory(_device, size_bytes):\n-                raise unittest.SkipTest('Insufficient {} memory'.format(_device))\n+                raise unittest.SkipTest(f'Insufficient {_device} memory')\n \n             return fn(self, *args, **kwargs)\n         return dep_fn\n@@ -1072,7 +1072,7 @@ def __call__(self, fn):\n         @wraps(fn)\n         def only_fn(slf, *args, **kwargs):\n             if self.device_type != slf.device_type:\n-                reason = \"Only runs on {0}\".format(self.device_type)\n+                reason = f\"Only runs on {self.device_type}\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, *args, **kwargs)\n@@ -1090,13 +1090,13 @@ def __init__(self, num_required_devices):\n         self.num_required_devices = num_required_devices\n \n     def __call__(self, fn):\n-        assert not hasattr(fn, 'num_required_devices'), \"deviceCountAtLeast redefinition for {0}\".format(fn.__name__)\n+        assert not hasattr(fn, 'num_required_devices'), f\"deviceCountAtLeast redefinition for {fn.__name__}\"\n         fn.num_required_devices = self.num_required_devices\n \n         @wraps(fn)\n         def multi_fn(slf, devices, *args, **kwargs):\n             if len(devices) < self.num_required_devices:\n-                reason = \"fewer than {0} devices detected\".format(self.num_required_devices)\n+                reason = f\"fewer than {self.num_required_devices} devices detected\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, devices, *args, **kwargs)\n@@ -1108,7 +1108,7 @@ def onlyNativeDeviceTypes(fn):\n     @wraps(fn)\n     def only_fn(self, *args, **kwargs):\n         if self.device_type not in NATIVE_DEVICES:\n-            reason = \"onlyNativeDeviceTypes: doesn't run on {0}\".format(self.device_type)\n+            reason = f\"onlyNativeDeviceTypes: doesn't run on {self.device_type}\"\n             raise unittest.SkipTest(reason)\n \n         return fn(self, *args, **kwargs)\n@@ -1137,7 +1137,7 @@ class precisionOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"precisionOverride not given a dtype : precision dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"precisionOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"precisionOverride given unknown dtype {dtype}\"\n \n         self.d = d\n \n@@ -1168,7 +1168,7 @@ class toleranceOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"toleranceOverride not given a dtype : tol dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"toleranceOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"toleranceOverride given unknown dtype {dtype}\"\n             assert isinstance(prec, tol), \"toleranceOverride not given a dtype : tol dict!\"\n \n         self.d = d\n@@ -1195,17 +1195,17 @@ def __init__(self, *args, device_type=\"all\"):\n                 assert isinstance(arg, (list, tuple)), \\\n                     \"When one dtype variant is a tuple or list, \" \\\n                     \"all dtype variants must be. \" \\\n-                    \"Received non-list non-tuple dtype {0}\".format(str(arg))\n-                assert all(isinstance(dtype, torch.dtype) for dtype in arg), \"Unknown dtype in {0}\".format(str(arg))\n+                    \"Received non-list non-tuple dtype {}\".format(str(arg))\n+                assert all(isinstance(dtype, torch.dtype) for dtype in arg), f\"Unknown dtype in {str(arg)}\"\n         else:\n-            assert all(isinstance(arg, torch.dtype) for arg in args), \"Unknown dtype in {0}\".format(str(args))\n+            assert all(isinstance(arg, torch.dtype) for arg in args), f\"Unknown dtype in {str(args)}\"\n \n         self.args = args\n         self.device_type = device_type\n \n     def __call__(self, fn):\n         d = getattr(fn, 'dtypes', {})\n-        assert self.device_type not in d, \"dtypes redefinition for {0}\".format(self.device_type)\n+        assert self.device_type not in d, f\"dtypes redefinition for {self.device_type}\"\n         d[self.device_type] = self.args\n         fn.dtypes = d\n         return fn\n@@ -1244,7 +1244,7 @@ def onlyPRIVATEUSE1(fn):\n     device_type = torch._C._get_privateuse1_backend_name()\n     device_mod = getattr(torch, device_type, None)\n     if device_mod is None:\n-        reason = \"Skip as torch has no module of {0}\".format(device_type)\n+        reason = f\"Skip as torch has no module of {device_type}\"\n         return unittest.skip(reason)(fn)\n     return onlyOn(device_type)(fn)\n \n@@ -1358,7 +1358,7 @@ def wrap_fn(self, *args, **kwargs):\n                     raise unittest.SkipTest(reason)\n                 rocm_version_tuple = _get_torch_rocm_version()\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\n@@ -1375,7 +1375,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version in (versions or []):\n-                reason = \"test skipped for CUDA version {0}\".format(version)\n+                reason = f\"test skipped for CUDA version {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1391,7 +1391,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version < versions:\n-                reason = \"test skipped for CUDA versions < {0}\".format(version)\n+                reason = f\"test skipped for CUDA versions < {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1409,7 +1409,7 @@ def wrap_fn(self, *args, **kwargs):\n                     reason = \"cuDNN not available\"\n                     raise unittest.SkipTest(reason)\n                 if self.cudnn_version is None or self.cudnn_version < version:\n-                    reason = \"cuDNN version {0} is available but {1} required\".format(self.cudnn_version, version)\n+                    reason = f\"cuDNN version {self.cudnn_version} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\ndiff --git a/torch/testing/_internal/common_distributed.py b/torch/testing/_internal/common_distributed.py\nindex 8981aa78d06a09..d1cf02749b79ce 100644\n--- a/torch/testing/_internal/common_distributed.py\n+++ b/torch/testing/_internal/common_distributed.py\n@@ -770,7 +770,7 @@ def _check_no_test_errors(self, elapsed_time) -> None:\n         for i, p in enumerate(self.processes):\n             if p.exitcode is None:\n                 raise RuntimeError(\n-                    \"Process {} timed out after {} seconds\".format(i, elapsed_time)\n+                    f\"Process {i} timed out after {elapsed_time} seconds\"\n                 )\n             self.assertNotEqual(self.TEST_ERROR_EXIT_CODE, p.exitcode)\n \n@@ -1102,7 +1102,7 @@ def _check_return_codes(cls, failed_ranks, timeout, fn):\n                     \"Caught exception: \\n%s exiting thread %s\", msg, rank\n                 )\n                 error_msg += (\n-                    \"Thread {} exited with exception:\\n{}\\n\".format(rank, msg)\n+                    f\"Thread {rank} exited with exception:\\n{msg}\\n\"\n                 )\n             elif isinstance(exc, SystemExit):\n                 if type(exc.code) == int and skip_code < 0:\ndiff --git a/torch/testing/_internal/common_fsdp.py b/torch/testing/_internal/common_fsdp.py\nindex 589372eaa8e216..20a35930a7f5f4 100644\n--- a/torch/testing/_internal/common_fsdp.py\n+++ b/torch/testing/_internal/common_fsdp.py\n@@ -881,7 +881,7 @@ def process_group(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     def _check_cpu_offload(self, fsdp_model, cpu_offload):\n         self.assertEqual(cpu_offload, fsdp_model.cpu_offload)\ndiff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py\nindex 3b9d3269853a05..a9f58c76d90f2d 100644\n--- a/torch/testing/_internal/common_methods_invocations.py\n+++ b/torch/testing/_internal/common_methods_invocations.py\n@@ -851,7 +851,7 @@ def error_inputs_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"normal expects std >= 0.0, but found std {}\".format(invalid_std),\n+        error_regex=fr\"normal expects std >= 0.0, but found std {invalid_std}\",\n     )\n \n def sample_inputs_cauchy(op, device, dtype, requires_grad, **kwargs):\n@@ -871,7 +871,7 @@ def error_inputs_cauchy(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_scale,)),\n         error_type=RuntimeError,\n-        error_regex=r\"cauchy_ expects sigma > 0.0, but found sigma={}\".format(invalid_scale),\n+        error_regex=fr\"cauchy_ expects sigma > 0.0, but found sigma={invalid_scale}\",\n     )\n \n \n@@ -893,7 +893,7 @@ def error_inputs_exponential(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(invalid_rate,)),\n         error_type=RuntimeError,\n-        error_regex=r\"exponential_ expects lambda > 0.0, but found lambda={}\".format(invalid_rate),\n+        error_regex=fr\"exponential_ expects lambda > 0.0, but found lambda={invalid_rate}\",\n     )\n \n \n@@ -915,7 +915,7 @@ def error_inputs_geometric(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(neg_prob,)),\n         error_type=RuntimeError,\n-        error_regex=r\"geometric_ expects p to be in \\(0, 1\\), but got p={}\".format(neg_prob),\n+        error_regex=fr\"geometric_ expects p to be in \\(0, 1\\), but got p={neg_prob}\",\n     )\n \n \n@@ -937,7 +937,7 @@ def error_inputs_log_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"log_normal_ expects std > 0.0, but found std={}\".format(invalid_std),\n+        error_regex=fr\"log_normal_ expects std > 0.0, but found std={invalid_std}\",\n     )\n \n \n@@ -1889,9 +1889,9 @@ def sample_inputs_logcumsumexp(self, device, dtype, requires_grad, **kwargs):\n             yield SampleInput(t, dim)\n \n def sample_inputs_trace(self, device, dtype, requires_grad, **kwargs):\n-    yield SampleInput((make_tensor((S, S), dtype=dtype, device=device,\n+    yield SampleInput(make_tensor((S, S), dtype=dtype, device=device,\n                                    low=None, high=None,\n-                                   requires_grad=requires_grad)))\n+                                   requires_grad=requires_grad))\n \n \n def error_inputs_trace(op, device):\n@@ -4020,7 +4020,7 @@ def error_inputs_group_norm(opinfo, device, **kwargs):\n \n     # check that input has minimum number of dimensions\n     err_msg1 = \"Expected at least 2 dimensions for input tensor but received\"\n-    s1 = SampleInput(make_arg((1)), args=(1,))\n+    s1 = SampleInput(make_arg(1), args=(1,))\n     yield ErrorInput(s1, error_regex=err_msg1)\n \n     # check that the channels dimension is compatible with number of groups\n@@ -6950,7 +6950,7 @@ def make_bool_mask(shape):\n \n         if mask_t.sum() == 0:\n             def random_index(shape):\n-                return tuple((random.randrange(0, max_idx) for max_idx in shape))\n+                return tuple(random.randrange(0, max_idx) for max_idx in shape)\n \n             mask_t[random_index(mask_t.shape)] = True\n             return mask_t\ndiff --git a/torch/testing/_internal/common_modules.py b/torch/testing/_internal/common_modules.py\nindex 2119678a33f5ef..c3ac11454ab410 100644\n--- a/torch/testing/_internal/common_modules.py\n+++ b/torch/testing/_internal/common_modules.py\n@@ -123,7 +123,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for module {1}!\".format(test_name, module_info.name))\n+                    print(f\"Failed to instantiate {test_name} for module {module_info.name}!\")\n                     raise ex\n \n \n@@ -252,7 +252,7 @@ def bilinear_reference_fn(m, p, x1, x2, bias=True):\n                     desc='no_bias',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n         ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n-                    forward_input=FunctionInput(make_input((2)), make_input((3))),\n+                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                     desc='no_batch_dim',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n     ]\n@@ -312,9 +312,9 @@ def module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_\n     for desc, constructor_kwargs in cases:\n         module_inputs.append(\n             ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n-                        forward_input=FunctionInput(make_input((3)),\n-                                                    make_target((3)),\n-                                                    make_input((1)).abs()),\n+                        forward_input=FunctionInput(make_input(3),\n+                                                    make_target(3),\n+                                                    make_input(1).abs()),\n                         desc=desc,\n                         reference_fn=no_batch_dim_reference_fn)\n         )\n@@ -454,7 +454,7 @@ def generate_regression_criterion_inputs(make_input):\n             constructor_input=FunctionInput(reduction=reduction),\n             forward_input=FunctionInput(make_input((4, )), make_input(4,)),\n             reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True),\n-            desc='no_batch_dim_{}'.format(reduction)\n+            desc=f'no_batch_dim_{reduction}'\n         ) for reduction in ['none', 'mean', 'sum']]\n \n \n@@ -752,7 +752,7 @@ def module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, tra\n     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n     conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n     kernel_size, C_in, C_out = 3, 4, 5\n-    input_no_batch_shape = (C_in,) + tuple((i + 3 for i in range(N)))\n+    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n     input_batch_shape = (2,) + input_no_batch_shape\n     return [\n         ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n@@ -878,7 +878,7 @@ def module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad,\n         ModuleInput(constructor_input=FunctionInput(),\n                     forward_input=FunctionInput(make_input((3, 2, 5)))),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(0.5),\n@@ -897,10 +897,10 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n \n     return [\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(()))),\n+                    forward_input=FunctionInput(make_input(())),\n                     desc='scalar'),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -908,7 +908,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -916,7 +916,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -924,7 +924,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5, 6)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d_multiparam')]\n \n@@ -1216,11 +1216,11 @@ def module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3, 6, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 6, 5)))),\n+            forward_input=FunctionInput(make_input((4, 6, 5))),\n             desc='1d_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput(3, 12, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 12)))),\n+            forward_input=FunctionInput(make_input((4, 12))),\n             desc='1d_affine_GN'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 6, 1e-3),\n@@ -1334,13 +1334,13 @@ def module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_g\n             constructor_input=(\n                 FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape)))),\n+            forward_input=FunctionInput(make_input(input_batch_shape))),\n         ModuleInput(\n             constructor_input=(\n                 FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                 FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape))),\n+            forward_input=FunctionInput(make_input(input_batch_shape)),\n             desc='tracking_stats'),\n         ModuleInput(\n             constructor_input=(\n@@ -1365,15 +1365,15 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((128, 5, 5)))),\n+            forward_input=FunctionInput(make_input((128, 5, 5))),\n             desc='1d_elementwise_affine_large_batch'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3, False),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_no_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([2, 2, 5], 1e-3),\n@@ -1396,11 +1396,11 @@ def module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, require\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7))),\n             desc='1d'),\n         ModuleInput(\n             constructor_input=FunctionInput(2,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n             desc='2d_uneven_pad'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 1., 0.5, 2.),\n@@ -1415,7 +1415,7 @@ def module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, t\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(1.5, 2),\n-            forward_input=FunctionInput(make_input(((1, 3, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 7))),\n             desc='norm'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, 2, 3),\n@@ -1449,7 +1449,7 @@ def module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(4),\n-            forward_input=FunctionInput(make_input(((2, 10, 4)))),\n+            forward_input=FunctionInput(make_input((2, 10, 4))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput(4, 4),\n@@ -1468,7 +1468,7 @@ def module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n-            forward_input=FunctionInput(make_input(((3, 7, 7)))),\n+            forward_input=FunctionInput(make_input((3, 7, 7))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n@@ -1486,7 +1486,7 @@ def module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2)),\n-            forward_input=FunctionInput(make_input(((2, 3, 5, 5, 5))))),\n+            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n         ModuleInput(\n             constructor_input=FunctionInput(2, (2, 2, 2)),\n             forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n@@ -1511,7 +1511,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n@@ -1521,11 +1521,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((3, 5, 7))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\n@@ -1545,7 +1545,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n@@ -1559,11 +1559,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\ndiff --git a/torch/testing/_internal/common_nn.py b/torch/testing/_internal/common_nn.py\nindex 6bc41ab20f3fcd..85f9e35ac0a6bb 100644\n--- a/torch/testing/_internal/common_nn.py\n+++ b/torch/testing/_internal/common_nn.py\n@@ -2540,7 +2540,7 @@ def unsqueeze_inp(inp):\n         output_size = (2, 3) + tuple(p + 1 for p in padding)  # simplified from `(4 + 2 * p - 3) // 2 + 1`\n         new_module_tests.append(\n             dict(\n-                module_name='Conv{}d'.format(d),\n+                module_name=f'Conv{d}d',\n                 constructor_args=(2, 3, 3, 2, padding, 1, 1, True, padding_mode),\n                 cpp_constructor_args='''torch::nn::Conv{}dOptions(2, 3, 3)\n                                         .stride(2)\n@@ -2552,7 +2552,7 @@ def unsqueeze_inp(inp):\n                 input_size=input_size,\n                 output_size=output_size,\n                 cudnn=True,\n-                desc='{}_stride2_pad2'.format(padding_mode),\n+                desc=f'{padding_mode}_stride2_pad2',\n                 with_tf32=True,\n                 tf32_precision=0.05\n             ),\n@@ -3906,7 +3906,7 @@ def flatten(xs):\n reductions = ['none', 'mean', 'sum']\n for name, reduction in product(regression_criterion_no_batch, reductions):\n     regression_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_size=(3, ),\n         target_size=(3, ),\n@@ -3959,7 +3959,7 @@ def flatten(xs):\n for (name, input_fn, target_fn), reduction in product(classification_criterion_no_batch,\n                                                       reductions):\n     classification_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_fn=lambda f=input_fn: f(),\n         target_fn=lambda f=target_fn: f(),\n@@ -4152,7 +4152,7 @@ def _get_arg(self, name, unpack):\n                 self._arg_cache[name] = self._extra_kwargs[fn_name]()\n             else:\n                 assert size_name in self._extra_kwargs, \\\n-                    \"Missing `{}`, `{}` or `{}` for {}\".format(name, size_name, fn_name, self.get_name())\n+                    f\"Missing `{name}`, `{size_name}` or `{fn_name}` for {self.get_name()}\"\n \n                 def map_tensor_sizes(sizes):\n                     if isinstance(sizes, list):\n@@ -4281,7 +4281,7 @@ def test_cuda(self, test_case):\n         type_map = {torch.double: torch.float}\n         cpu_input_tuple = cpu_input if isinstance(cpu_input, tuple) else (cpu_input,)\n \n-        is_any_input_complex = any((isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple))\n+        is_any_input_complex = any(isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple)\n \n         gpu_input_tuple = to_gpu(cpu_input_tuple, type_map=type_map)\n \ndiff --git a/torch/testing/_internal/common_pruning.py b/torch/testing/_internal/common_pruning.py\nindex 32732818a25b02..b6cbd92105f3f4 100644\n--- a/torch/testing/_internal/common_pruning.py\n+++ b/torch/testing/_internal/common_pruning.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.ao.pruning import BaseSparsifier\ndiff --git a/torch/testing/_internal/common_quantization.py b/torch/testing/_internal/common_quantization.py\nindex 95686be4511264..dcc575c942bc84 100644\n--- a/torch/testing/_internal/common_quantization.py\n+++ b/torch/testing/_internal/common_quantization.py\n@@ -791,8 +791,7 @@ def _get_underlying_op_type(\n                     (exp_type_end_b is act_type_end_b)\n                 self.assertTrue(\n                     types_match,\n-                    'Type mismatch at %s: expected %s, got %s' %\n-                    (k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n+                    'Type mismatch at {}: expected {}, got {}'.format(k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n                         (act_type_start_a, act_type_end_a, act_type_start_b, act_type_end_b))\n                 )\n \n@@ -1601,7 +1600,7 @@ def __init__(self):\n         super().__init__()\n         self.quant = torch.ao.quantization.QuantStub()\n         self.fc1 = torch.nn.Linear(5, 8).to(dtype=torch.float)\n-        self.layer_norm = torch.nn.LayerNorm((8))\n+        self.layer_norm = torch.nn.LayerNorm(8)\n         self.group_norm = torch.nn.GroupNorm(2, 8)\n         self.instance_norm1d = torch.nn.InstanceNorm1d(8)\n         self.instance_norm2d = torch.nn.InstanceNorm2d(8)\ndiff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py\nindex 3ba2331e7cd40e..c96d13bd46ac53 100644\n--- a/torch/testing/_internal/common_utils.py\n+++ b/torch/testing/_internal/common_utils.py\n@@ -203,7 +203,7 @@ def repro_env_var_prefix() -> str:\n \n def maybe_load_json(filename):\n     if os.path.isfile(filename):\n-        with open(filename, 'r') as fp:\n+        with open(filename) as fp:\n             return json.load(fp)\n     log.warning(\"Attempted to load json file '%s' but it does not exist.\", filename)\n     return {}\n@@ -355,12 +355,12 @@ def instantiate_test_helper(cls, name, test, param_kwargs):\n             def instantiated_test(self, param_kwargs=param_kwargs):\n                 test(self, **param_kwargs)\n \n-            assert not hasattr(generic_cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(generic_cls, name), f\"Redefinition of test {name}\"\n             setattr(generic_cls, name, instantiated_test)\n \n         for (test, test_suffix, param_kwargs, decorator_fn) in class_attr.parametrize_fn(\n                 class_attr, generic_cls=generic_cls, device_cls=None):\n-            full_name = '{}_{}'.format(test.__name__, test_suffix)\n+            full_name = f'{test.__name__}_{test_suffix}'\n \n             # Apply decorators based on full param kwargs.\n             for decorator in decorator_fn(param_kwargs):\n@@ -830,7 +830,7 @@ def run_tests(argv=UNITTEST_ARGS):\n     # import test files.\n     if SLOW_TESTS_FILE:\n         if os.path.exists(SLOW_TESTS_FILE):\n-            with open(SLOW_TESTS_FILE, 'r') as fp:\n+            with open(SLOW_TESTS_FILE) as fp:\n                 global slow_tests_dict\n                 slow_tests_dict = json.load(fp)\n                 # use env vars so pytest-xdist subprocesses can still access them\n@@ -839,7 +839,7 @@ def run_tests(argv=UNITTEST_ARGS):\n             warnings.warn(f'slow test file provided but not found: {SLOW_TESTS_FILE}')\n     if DISABLED_TESTS_FILE:\n         if os.path.exists(DISABLED_TESTS_FILE):\n-            with open(DISABLED_TESTS_FILE, 'r') as fp:\n+            with open(DISABLED_TESTS_FILE) as fp:\n                 global disabled_tests_dict\n                 disabled_tests_dict = json.load(fp)\n                 os.environ['DISABLED_TESTS_FILE'] = DISABLED_TESTS_FILE\n@@ -905,7 +905,7 @@ def run_tests(argv=UNITTEST_ARGS):\n         test_batches = chunk_list(get_test_names(test_cases), RUN_PARALLEL)\n         processes = []\n         for i in range(RUN_PARALLEL):\n-            command = [sys.executable] + argv + ['--log-suffix=-shard-{}'.format(i + 1)] + test_batches[i]\n+            command = [sys.executable] + argv + [f'--log-suffix=-shard-{i + 1}'] + test_batches[i]\n             processes.append(subprocess.Popen(command, universal_newlines=True))\n         failed = False\n         for p in processes:\n@@ -1294,7 +1294,7 @@ def wrap_fn(self, *args, **kwargs):\n                 rocm_version = rocm_version.split(\"-\")[0]    # ignore git sha\n                 rocm_version_tuple = tuple(int(x) for x in rocm_version.split(\".\"))\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n         return wrap_fn\n@@ -1672,7 +1672,7 @@ def is_iterable_of_tensors(iterable, include_empty=False):\n     return True\n \n \n-class CudaNonDefaultStream():\n+class CudaNonDefaultStream:\n     def __enter__(self):\n         # Before starting CUDA test save currently active streams on all\n         # CUDA devices and set new non default streams to all CUDA devices\n@@ -1698,7 +1698,7 @@ def __exit__(self, exec_type, exec_value, traceback):\n                                      device_type=self.beforeStreams[d].device_type)\n         torch._C._cuda_setDevice(beforeDevice)\n \n-class CudaMemoryLeakCheck():\n+class CudaMemoryLeakCheck:\n     def __init__(self, testcase, name=None):\n         self.name = testcase.id() if name is None else name\n         self.testcase = testcase\n@@ -2104,7 +2104,7 @@ def __init__(self, actual, expected, *, rtol_override=0.0, atol_override=0.0, **\n     def _process_inputs(self, actual, expected, *, id, allow_subclasses):\n         self._check_inputs_isinstance(actual, expected, cls=(torch.Tensor, np.ndarray))\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\n@@ -2201,7 +2201,7 @@ def set_warn_always_context(new_val: bool):\n         torch.set_warn_always(old_val)\n \n \n-class NoTest():\n+class NoTest:\n     # causes pytest to not recognize this class as a test\n     __test__ = False\n \n@@ -3408,12 +3408,12 @@ def remove_prefix(text, prefix):\n         subname_output = \"\"\n         if subname:\n             expected_file += \"-\" + subname\n-            subname_output = \" ({})\".format(subname)\n+            subname_output = f\" ({subname})\"\n         expected_file += \".expect\"\n         expected = None\n \n         def accept_output(update_type):\n-            print(\"Accepting {} for {}{}:\\n\\n{}\".format(update_type, munged_id, subname_output, s))\n+            print(f\"Accepting {update_type} for {munged_id}{subname_output}:\\n\\n{s}\")\n             with open(expected_file, 'w') as f:\n                 # Adjust for producer_version, leave s unmodified\n                 s_tag = re.sub(r'(producer_version): \"[0-9.]*\"',\n@@ -3423,7 +3423,7 @@ def accept_output(update_type):\n         try:\n             with open(expected_file) as f:\n                 expected = f.read()\n-        except IOError as e:\n+        except OSError as e:\n             if e.errno != errno.ENOENT:\n                 raise\n             elif expecttest.ACCEPT:\n@@ -3442,7 +3442,7 @@ def accept_output(update_type):\n         # Adjust for producer_version\n         expected = expected.replace(\n             'producer_version: \"CURRENT_VERSION\"',\n-            'producer_version: \"{}\"'.format(torch.onnx.producer_version)\n+            f'producer_version: \"{torch.onnx.producer_version}\"'\n         )\n         if expecttest.ACCEPT:\n             if expected != s:\n@@ -3600,7 +3600,7 @@ def download_file(url, binary=True):\n             f.write(data)\n         return path\n     except error.URLError as e:\n-        msg = \"could not download test file '{}'\".format(url)\n+        msg = f\"could not download test file '{url}'\"\n         warnings.warn(msg, RuntimeWarning)\n         raise unittest.SkipTest(msg) from e\n \n@@ -4359,7 +4359,7 @@ def wrap_fn(*args, **kwargs):\n     return wrap_fn\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def get_cycles_per_ms() -> float:\n     \"\"\"Measure and return approximate number of cycles per millisecond for torch.cuda._sleep\n     \"\"\"\ndiff --git a/torch/testing/_internal/dist_utils.py b/torch/testing/_internal/dist_utils.py\nindex 94aafe31773261..daee88e4580cd5 100644\n--- a/torch/testing/_internal/dist_utils.py\n+++ b/torch/testing/_internal/dist_utils.py\n@@ -101,7 +101,7 @@ def wait_until_node_failure(rank: int, expected_error_regex: str = \".*\") -> str:\n     \"\"\"\n     while True:\n         try:\n-            rpc.rpc_sync(\"worker{}\".format(rank), noop, args=())\n+            rpc.rpc_sync(f\"worker{rank}\", noop, args=())\n             time.sleep(0.1)\n         except Exception as e:\n             if re.search(pattern=expected_error_regex, string=str(e)):\n@@ -187,7 +187,7 @@ def initialize_pg(init_method, rank: int, world_size: int) -> None:\n \n \n def worker_name(rank: int) -> str:\n-    return \"worker{}\".format(rank)\n+    return f\"worker{rank}\"\n \n \n def get_function_event(function_events, partial_event_name):\ndiff --git a/torch/testing/_internal/distributed/distributed_test.py b/torch/testing/_internal/distributed/distributed_test.py\nindex 19a0c3f50d1ae4..5fdc796310d44d 100644\n--- a/torch/testing/_internal/distributed/distributed_test.py\n+++ b/torch/testing/_internal/distributed/distributed_test.py\n@@ -517,7 +517,7 @@ def sync(cls, wait_for=None, timeout=10):\n             arrived = 0\n             with _lock():\n                 for f_name in os.listdir(barrier_dir):\n-                    with open(os.path.join(barrier_dir, f_name), \"r\") as f:\n+                    with open(os.path.join(barrier_dir, f_name)) as f:\n                         data = f.read()\n                         if int(data) >= cls.barrier_id:\n                             arrived += 1\n@@ -552,7 +552,7 @@ def tearDown(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     @classmethod\n     def _run(cls, rank, test_name, file_name, pipe):\n@@ -654,7 +654,7 @@ def test_dump_DDP_relevant_env_vars(self):\n                 lines = out.getvalue().splitlines()\n \n             def format_line(var):\n-                return \"env:%s=%s\" % (\n+                return \"env:{}={}\".format(\n                     var,\n                     os.environ[var] if var in os.environ else \"N/A\",\n                 )\n@@ -692,7 +692,7 @@ def test_get_rank(self):\n \n             all_ranks = set()\n             for f_name in os.listdir(test_dir):\n-                with open(os.path.join(test_dir, f_name), \"r\") as f:\n+                with open(os.path.join(test_dir, f_name)) as f:\n                     all_ranks.add(int(f.read()))\n             self.assertEqual(len(all_ranks), num_processes)\n \n@@ -9640,7 +9640,7 @@ def backward(ctx, grad_output):\n \n             class MyModel(nn.Module):\n                 def __init__(self, device):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.error = True\n                     self.fc1 = nn.Linear(10, 10).cuda(device)\n \n@@ -9683,12 +9683,12 @@ def forward(self, inp):\n         def test_ddp_has_finalized(self):\n \n             @dataclass\n-            class MyClass():\n+            class MyClass:\n                 obj: torch.Tensor\n \n             class MyModel(nn.Module):\n                 def __init__(self, rank):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.rank = rank\n                     self.fc1 = nn.Linear(1024, 1024).cuda(rank)\n                     self.fc2 = nn.Linear(1024, 2 * 1024).cuda(rank)\ndiff --git a/torch/testing/_internal/distributed/nn/api/remote_module_test.py b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\nindex f955c0fc1ace1a..4d9f1d9b53ddc4 100644\n--- a/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n+++ b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n@@ -131,7 +131,7 @@ def test_bad_module(self):\n         if self.rank != 0:\n             return\n         dst_worker_name = dist_utils.worker_name((self.rank + 1) % self.world_size)\n-        remote_device = \"{}/cpu\".format(dst_worker_name)\n+        remote_device = f\"{dst_worker_name}/cpu\"\n         args = (1,)\n         kwargs = dict(first_kwarg=2)\n \n@@ -575,7 +575,7 @@ def test_valid_device(self):\n         dst_worker_name = dist_utils.worker_name(dst_rank)\n \n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -585,7 +585,7 @@ def test_valid_device(self):\n \n         # Test rank works as well.\n         for remote_module in self._create_remote_module_iter(\n-            \"rank:{}/cuda:0\".format(dst_rank), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"rank:{dst_rank}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -607,7 +607,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/foo\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/foo\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -618,7 +618,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:100\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:100\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -627,7 +627,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cpu2\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cpu2\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -636,7 +636,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -648,7 +648,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:0/cuda:1\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:0/cuda:1\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -692,7 +692,7 @@ def test_input_moved_to_cuda_device(self):\n \n         # Only test Python nn.Module, because script module methods don't support taking kwargs.\n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             ret_fut = remote_module.forward_async(*args, **kwargs)\n             ret = ret_fut.wait()\n@@ -716,7 +716,7 @@ def test_input_moved_to_cuda_device_script(self):\n \n         scripted_remote_module = next(\n             self._create_remote_module_iter(\n-                \"{}/cuda:0\".format(dst_worker_name),\n+                f\"{dst_worker_name}/cuda:0\",\n                 modes=[ModuleCreationMode.MODULE_CTOR_WITH_INTERFACE],\n             )\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\nindex 8e8c353460e6ff..b08b51c31d9f7d 100644\n--- a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n+++ b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n@@ -226,7 +226,7 @@ def _exec_func_with_dst(self, dst, exec_mode, method, *args):\n             fut = rpc.rpc_async(worker_name(dst), method, args=(args))\n             return fut.wait()\n         else:\n-            raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+            raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n     def _exec_func(self, exec_mode, method, *args):\n         return self._exec_func_with_dst(\n@@ -288,7 +288,7 @@ def _test_graph(self, fn, exec_mode, sparse):\n                     worker_name(dst_rank), fn, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -355,7 +355,7 @@ def _test_graph_for_py_nested_call(self, exec_mode, sparse):\n                     args=(t1, t2, dst_rank, self.world_size, 1),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             # Barrier to ensure all RPCs are done.\n             dist.barrier()\n@@ -449,7 +449,7 @@ def _test_graph_for_py_nested_call_itself(self, exec_mode, sparse):\n                     ),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name((self.rank + 1) % self.world_size),\n@@ -505,7 +505,7 @@ def _test_no_graph_with_tensors_not_require_grad(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.add, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -548,7 +548,7 @@ def _test_rpc_complex_args(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.stack, args=(tensors,)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             self.assertEqual(torch.stack(tensors), ret)\n \n@@ -1292,7 +1292,7 @@ def test_autograd_context(self):\n         for context_id in context_ids:\n             with self.assertRaisesRegex(\n                 RuntimeError,\n-                \"Could not find autograd context with id: {}\".format(context_id),\n+                f\"Could not find autograd context with id: {context_id}\",\n             ):\n                 dist_autograd._retrieve_context(context_id)\n \n@@ -1357,7 +1357,7 @@ def _test_grad_only_on_return_value(self, exec_mode):\n                     worker_name(dst_rank), ret_requires_grad\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             dist_autograd.backward(context_id, [ret.sum()])\n \n@@ -1748,7 +1748,7 @@ def test_backward_without_context(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             res = rpc.rpc_sync(\n                 worker_name(self._next_rank()), torch.add, args=(t1, t2)\n@@ -2031,7 +2031,7 @@ def test_clean_context_during_backward(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             dist_autograd.backward(context_id, [t1.sum()])\n \n@@ -2234,7 +2234,7 @@ def test_multiple_backward_with_errors(self):\n         t2 = torch.rand((3, 3), requires_grad=True)\n         with dist_autograd.context() as context_id:\n             loss = rpc.rpc_sync(\n-                'worker{}'.format(self._next_rank()),\n+                f'worker{self._next_rank()}',\n                 DistAutogradTest._python_udf_with_backward_error,\n                 args=(t1, t2)).sum()\n \ndiff --git a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\nindex 9f8b71911a07cc..98db73d7401845 100644\n--- a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n@@ -225,7 +225,7 @@ def run_agent(agent, n_steps):\n         last_reward = agent.finish_episode()\n \n         if agent.running_reward > agent.reward_threshold:\n-            print(\"Solved! Running reward is now {}!\".format(agent.running_reward))\n+            print(f\"Solved! Running reward is now {agent.running_reward}!\")\n             break\n \n \ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\nindex d050a2138b7922..b7683064dcfd13 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n@@ -70,7 +70,7 @@ def _test_remote_message_dropped_pickle(self, dst=None):\n         if self.rank != 0:\n             return\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -100,7 +100,7 @@ def _test_remote_message_dropped_timeout(self, func, args, dst=None):\n \n         # test the case where rpc.remote() message creation is completely dropped.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -143,7 +143,7 @@ def _test_remote_message_delay_timeout(self, func, args, dst=None):\n         # Test the case where remote message is eventually processed on the owner,\n         # but the future on the creator times out before the response comes back.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, func, args=args, timeout=0.001)\n         # Future corresponding to the remote creation should time out.\n@@ -233,7 +233,7 @@ def test_rref_to_here_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\nindex b08e897ec464af..af73fef4794b06 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n@@ -54,7 +54,7 @@ def get_shutdown_error_regex(self):\n             \"Connection reset by peer\",\n             \"Connection closed by peer\"\n         ]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\nindex fce0b5e8802567..0bb45ddeadb186 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n@@ -310,7 +310,7 @@ def future_return_to_python(\n             dst_rank: int, inputs: Tuple[Tensor, Tensor]\n         ) -> Future[Tensor]:\n             return rpc.rpc_async(\n-                \"worker{}\".format(dst_rank), two_args_two_kwargs, inputs\n+                f\"worker{dst_rank}\", two_args_two_kwargs, inputs\n             )\n \n         fut_res = future_return_to_python(dst_rank, inputs)\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\nindex 96ede7231a9722..2e4eea3a365176 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n@@ -157,7 +157,7 @@ def test_remote_timeout_to_here_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -173,7 +173,7 @@ def test_rref_to_here_timeout_in_jit(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -188,7 +188,7 @@ def test_rref_timeout_pickle_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -205,7 +205,7 @@ def test_rref_timeout_pickle_script_func(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/rpc_test.py b/torch/testing/_internal/distributed/rpc/rpc_test.py\nindex 2d350d06cc6794..47b13a837a0355 100644\n--- a/torch/testing/_internal/distributed/rpc/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/rpc_test.py\n@@ -3153,7 +3153,7 @@ def test_rref_str(self):\n         rref1 = RRef(self.rank)\n         id_class = \"GloballyUniqueId\"\n         self.assertEqual(\n-            \"OwnerRRef({}(created_on={}, local_id=0))\".format(id_class, self.rank), rref1.__str__()\n+            f\"OwnerRRef({id_class}(created_on={self.rank}, local_id=0))\", rref1.__str__()\n         )\n \n         dst_rank = (self.rank + 1) % self.world_size\n@@ -4296,7 +4296,7 @@ def test_rref_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, my_sleep_func, args=(2, ), timeout=0.01)\n         # Future corresponding to the remote creation should time out.\ndiff --git a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\nindex 0f5cb0a4987a8f..191017caad139e 100644\n--- a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n@@ -26,7 +26,7 @@ def get_shutdown_error_regex(self):\n         # FIXME Once we consolidate the error messages returned by the\n         # TensorPipe agent put some more specific regex here.\n         error_regexes = [\".*\"]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/jit_metaprogramming_utils.py b/torch/testing/_internal/jit_metaprogramming_utils.py\nindex 72b7e477c76a49..88137fd1029a18 100644\n--- a/torch/testing/_internal/jit_metaprogramming_utils.py\n+++ b/torch/testing/_internal/jit_metaprogramming_utils.py\n@@ -338,11 +338,11 @@ def get_call(method_name, func_type, args, kwargs):\n     argument_str += kwargs_str\n \n     if func_type == 'functional' or func_type == 'function':\n-        call = 'torch.{}({})'.format(method_name, argument_str)\n+        call = f'torch.{method_name}({argument_str})'\n     elif func_type == 'method':\n-        call = '{}.{}({})'.format(self_arg, method_name, argument_str)\n+        call = f'{self_arg}.{method_name}({argument_str})'\n     elif func_type == 'nn_functional':\n-        call = 'torch.nn.functional.{}({})'.format(method_name, argument_str)\n+        call = f'torch.nn.functional.{method_name}({argument_str})'\n     else:\n         raise TypeError('Unsupported function type')\n \n@@ -361,17 +361,17 @@ def get_script_args(args):\n     actuals: List[str] = []\n     for arg in args:\n         if isinstance(arg, torch.Tensor):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name)\n             actuals.append(name)\n             tensors.append(arg)\n         elif is_iterable_of_tensors(arg):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name + ': List[torch.Tensor]')\n             actuals.append(name)\n             tensors.append(list(arg))\n         elif isinstance(arg, str):\n-            actuals.append(\"'{}'\".format(arg))\n+            actuals.append(f\"'{arg}'\")\n         else:\n             actuals.append(str(get_constant(arg)))\n     return (formals, tensors, actuals)\n@@ -399,7 +399,7 @@ def script_fn(*args, **kwargs):\n         return output\n     return script_fn\n \n-class SplitInputs():\n+class SplitInputs:\n     all_tensors: List[Any]\n     tensor_args: List[Any]\n     nontensor_args: List[Any]\n@@ -584,7 +584,7 @@ def script_module(*args, **kwargs):\n \n         method_args = ', '.join(['self'] + actuals)\n         call_args_str = ', '.join(actuals)\n-        call = \"self.submodule({})\".format(call_args_str)\n+        call = f\"self.submodule({call_args_str})\"\n         script = script_method_template.format(method_args, call)\n \n         submodule_constants = []\n@@ -640,7 +640,7 @@ def get_nn_mod_test_name(**kwargs):\n         test_name = get_nn_module_name_from_kwargs(**kwargs)\n         if 'desc' in kwargs:\n             test_name = \"{}_{}\".format(test_name, kwargs['desc'])\n-    return 'test_nn_{}'.format(test_name)\n+    return f'test_nn_{test_name}'\n \n def get_nn_module_class_from_kwargs(**kwargs):\n     name = get_nn_module_name_from_kwargs(**kwargs)\ndiff --git a/torch/testing/_internal/jit_utils.py b/torch/testing/_internal/jit_utils.py\nindex b72dd5dc1285a6..2f6675234d3e7c 100644\n--- a/torch/testing/_internal/jit_utils.py\n+++ b/torch/testing/_internal/jit_utils.py\n@@ -176,12 +176,12 @@ def get_nodes_and_parents_recursively(block, kind, acc):\n \n         fusion_groups : Dict[torch._C.Block, List[torch._C.Node]] = defaultdict(list)\n         get_nodes_and_parents_recursively(graph, FUSION_GROUP, fusion_groups)\n-        self.assertTrue(len(fusion_groups) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_groups) == 1, f'got {graph}')\n         (graph, fusion_nodes) = list(fusion_groups.items())[0]\n         # the block contains one FUSION_GROUP and the rest of nodes are `allowed_nodes`\n-        self.assertTrue(len(fusion_nodes) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_nodes) == 1, f'got {graph}')\n         self.assertTrue(all(node.kind() in allowed_nodes for node in graph.nodes()),\n-                        'got {}'.format(graph))\n+                        f'got {graph}')\n \n     def _isHookExceptionOk(self, e):\n         se = str(e)\n@@ -294,7 +294,7 @@ def assertGraphContains(self, graph, kind, consider_subgraphs=False):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             self.assertTrue(count > 0)\n             return\n \n@@ -321,7 +321,7 @@ def perform_assert(graph, kind, actual, expected, consider_subgraphs):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             perform_assert(graph, kind, count, num_kind_nodes,\n                            consider_subgraphs)\n             return\n@@ -768,7 +768,7 @@ def _get_py3_code(code, fn_name):\n         fn = getattr(module, fn_name)\n         return fn\n \n-class TensorExprTestOptions():\n+class TensorExprTestOptions:\n     def __init__(self):\n         self.old_profiling_executor = torch._C._jit_set_profiling_executor(True)\n         self.old_profiling_mode = torch._C._get_graph_executor_optimize(True)\ndiff --git a/torch/testing/_internal/opinfo/core.py b/torch/testing/_internal/opinfo/core.py\nindex 8f86cbd06af61a..5e7e1dcd5a49ea 100644\n--- a/torch/testing/_internal/opinfo/core.py\n+++ b/torch/testing/_internal/opinfo/core.py\n@@ -859,9 +859,7 @@ class OpInfo:\n     def __post_init__(self):\n         self._original_opinfo_args = asdict(self).copy()\n \n-        assert self.dtypes is not None, \"OpInfo for {0} has no dtypes!\".format(\n-            self.name\n-        )\n+        assert self.dtypes is not None, \"OpInfo for {} has no dtypes!\".format(self.name)\n \n         dtypes_args = (self.dtypes, self.dtypesIfCUDA, self.dtypesIfROCM)\n \n@@ -874,10 +872,7 @@ def __post_init__(self):\n \n         # Attribute to verify dynamic_dtypes are used.\n         self.dynamic_dtypes = any(\n-            (\n-                isinstance(dtypes, utils._dynamic_dispatch_dtypes)\n-                for dtypes in dtypes_args\n-            )\n+            isinstance(dtypes, utils._dynamic_dispatch_dtypes) for dtypes in dtypes_args\n         )\n \n         if self.dynamic_dtypes:\ndiff --git a/torch/testing/_internal/opinfo/definitions/linalg.py b/torch/testing/_internal/opinfo/definitions/linalg.py\nindex ca84eca5d3d027..a8c29dbf09309d 100644\n--- a/torch/testing/_internal/opinfo/definitions/linalg.py\n+++ b/torch/testing/_internal/opinfo/definitions/linalg.py\n@@ -1007,7 +1007,7 @@ def sample_inputs_linalg_solve(\n         nrhs = [(1,), (3,)]\n \n     for n, batch, rhs in product(ns, batches, nrhs):\n-        yield SampleInput(make_a(*batch, n, n), args=(make_b((batch + (n,) + rhs)),))\n+        yield SampleInput(make_a(*batch, n, n), args=(make_b(batch + (n,) + rhs),))\n \n \n def sample_inputs_linalg_solve_triangular(\ndiff --git a/torch/testing/_internal/opinfo/definitions/sparse.py b/torch/testing/_internal/opinfo/definitions/sparse.py\nindex 6baff3b2f86fea..570b2c546f099a 100644\n--- a/torch/testing/_internal/opinfo/definitions/sparse.py\n+++ b/torch/testing/_internal/opinfo/definitions/sparse.py\n@@ -331,7 +331,7 @@ def _validate_sample_input_sparse_reduction_sum(sample, check_validate=False):\n     }:\n         if (isinstance(dim, int) and (t_inp.dim() != 2 or keepdim)) or (\n             isinstance(dim, (list, tuple))\n-            and (((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim))\n+            and ((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim)\n         ):\n             if layout in {torch.sparse_bsr, torch.sparse_bsc}:\n                 return ErrorInput(\n"
  },
  {
    "number": 105394,
    "title": "[BE] Enable ruff's UP rules and autoformat utils/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "3ec248e4430779004cffdc063a6bb301be7f7fda",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105394",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105394/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105394.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105394.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105394/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105394/comments",
    "labels": [
      "release notes: dataloader",
      "open source"
    ],
    "_event_time": "2023-07-18T01:12:22.271508Z",
    "state": "closed",
    "patch": "From bf983f1f5ae4d8e5eeb6c2ac1dff1f552eab719a Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:15 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat utils/\n\n[ghstack-poisoned]\n---\n torch/utils/_freeze.py                        |  2 +-\n torch/utils/benchmark/examples/end_to_end.py  |  5 ++--\n .../utils/benchmark/examples/op_benchmark.py  |  4 +--\n .../benchmark/examples/sparse/op_benchmark.py |  4 +--\n .../examples/spectral_ops_fuzz_test.py        |  2 +-\n torch/utils/benchmark/utils/common.py         |  4 +--\n torch/utils/benchmark/utils/cpp_jit.py        |  6 ++--\n .../utils/valgrind_wrapper/timer_interface.py | 12 ++++----\n torch/utils/bottleneck/__main__.py            |  6 ++--\n torch/utils/bundled_inputs.py                 |  8 +++---\n torch/utils/checkpoint.py                     |  2 +-\n torch/utils/collect_env.py                    | 26 ++++++++---------\n torch/utils/cpp_extension.py                  |  8 +++---\n torch/utils/data/_utils/pin_memory.py         |  2 +-\n torch/utils/data/_utils/worker.py             |  8 +++---\n torch/utils/data/dataloader.py                |  6 ++--\n torch/utils/data/datapipes/_decorator.py      |  2 +-\n torch/utils/data/datapipes/_typing.py         |  8 +++---\n .../data/datapipes/dataframe/dataframes.py    | 18 ++++++------\n torch/utils/data/datapipes/datapipe.py        | 10 +++----\n torch/utils/data/datapipes/gen_pyi.py         |  2 +-\n torch/utils/data/datapipes/iter/callable.py   |  2 +-\n .../data/datapipes/iter/combinatorics.py      |  4 +--\n torch/utils/data/datapipes/iter/combining.py  |  6 ++--\n torch/utils/data/datapipes/iter/filelister.py |  2 +-\n torch/utils/data/datapipes/iter/fileopener.py |  4 +--\n torch/utils/data/datapipes/iter/grouping.py   |  2 +-\n .../data/datapipes/iter/routeddecoder.py      |  2 +-\n torch/utils/data/datapipes/iter/sharding.py   |  2 +-\n torch/utils/data/datapipes/map/combining.py   |  2 +-\n torch/utils/data/datapipes/map/grouping.py    |  2 +-\n torch/utils/data/datapipes/utils/common.py    |  2 +-\n torch/utils/data/datapipes/utils/decoder.py   |  6 ++--\n torch/utils/data/graph.py                     |  2 +-\n torch/utils/dlpack.py                         |  2 +-\n torch/utils/hipify/cuda_to_hip_mappings.py    |  2 +-\n torch/utils/hipify/hipify_python.py           | 28 +++++++++----------\n torch/utils/jit/log_extract.py                |  2 +-\n torch/utils/mobile_optimizer.py               |  4 +--\n torch/utils/tensorboard/_caffe2_graph.py      |  4 +--\n torch/utils/tensorboard/_embedding.py         |  2 +-\n torch/utils/tensorboard/_pytorch_graph.py     |  6 ++--\n torch/utils/tensorboard/writer.py             |  2 +-\n torch/utils/throughput_benchmark.py           | 10 +++----\n torch/utils/viz/_cycles.py                    | 12 ++++----\n torch/utils/weak.py                           |  5 ++--\n 46 files changed, 131 insertions(+), 131 deletions(-)\n\ndiff --git a/torch/utils/_freeze.py b/torch/utils/_freeze.py\nindex 5245ac011e19ac..6590ff4b769e42 100644\n--- a/torch/utils/_freeze.py\n+++ b/torch/utils/_freeze.py\n@@ -237,7 +237,7 @@ def compile_file(self, path: Path, top_package_path: Path):\n         module_mangled_name = \"__\".join(module_qualname)\n         c_name = \"M_\" + module_mangled_name\n \n-        with open(path, \"r\") as src_file:\n+        with open(path) as src_file:\n             co = self.compile_string(src_file.read())\n \n         bytecode = marshal.dumps(co)\ndiff --git a/torch/utils/benchmark/examples/end_to_end.py b/torch/utils/benchmark/examples/end_to_end.py\nindex 5e0f42712d7c7a..a6d05a91c94253 100644\n--- a/torch/utils/benchmark/examples/end_to_end.py\n+++ b/torch/utils/benchmark/examples/end_to_end.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"End-to-end example to test a PR for regressions:\n \n $ python -m examples.end_to_end --pr 39850\n@@ -111,7 +110,7 @@ def parse_args():\n \n def construct_stmt_and_label(pr, params):\n     if pr == \"39850\":\n-        k0, k1, k2, dim = [params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"]]\n+        k0, k1, k2, dim = (params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"])\n         state = np.random.RandomState(params[\"random_value\"])\n         topk_dim = state.randint(low=0, high=dim)\n         dim_size = [k0, k1, k2][topk_dim]\n@@ -291,7 +290,7 @@ def construct_table(results, device_str, test_variance):\n     )\n \n     _, result_log_file = tempfile.mkstemp(suffix=\".log\")\n-    with open(result_log_file, \"wt\") as f:\n+    with open(result_log_file, \"w\") as f:\n         f.write(f\"{device_str}\\n\\n{column_labels}\\n\")\n         print(f\"\\n{column_labels}\\n[First twenty omitted (these tend to be noisy) ]\")\n         for key, (r_ref, r_pr), rel_diff in results:\ndiff --git a/torch/utils/benchmark/examples/op_benchmark.py b/torch/utils/benchmark/examples/op_benchmark.py\nindex 65b69d84b41f44..b7536b9ec26bb8 100644\n--- a/torch/utils/benchmark/examples/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/op_benchmark.py\n@@ -37,13 +37,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/sparse/op_benchmark.py b/torch/utils/benchmark/examples/sparse/op_benchmark.py\nindex f9ee17d5617e08..d7e97d33cc1101 100644\n--- a/torch/utils/benchmark/examples/sparse/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/sparse/op_benchmark.py\n@@ -32,13 +32,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\nindex d8284ee4187c49..c70395573adb2c 100644\n--- a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n+++ b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n@@ -27,7 +27,7 @@ def run_benchmark(name: str, function: object, dtype: torch.dtype, seed: int, de\n     results = []\n     for tensors, tensor_params, params in spectral_fuzzer.take(samples):\n         shape = [params['k0'], params['k1'], params['k2']][:params['ndim']]\n-        str_shape = ' x '.join([\"{:<4}\".format(s) for s in shape])\n+        str_shape = ' x '.join([f\"{s:<4}\" for s in shape])\n         sub_label = f\"{str_shape} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n         for dim in _dim_options(params['ndim']):\n             for nthreads in (1, 4, 16) if not cuda else (1,):\ndiff --git a/torch/utils/benchmark/utils/common.py b/torch/utils/benchmark/utils/common.py\nindex a8bbef3bfbeb4f..c1636ddb78a2bf 100644\n--- a/torch/utils/benchmark/utils/common.py\n+++ b/torch/utils/benchmark/utils/common.py\n@@ -325,7 +325,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n                 if not os.path.exists(owner_file):\n                     continue\n \n-                with open(owner_file, \"rt\") as f:\n+                with open(owner_file) as f:\n                     owner_pid = int(f.read())\n \n                 if owner_pid == os.getpid():\n@@ -349,7 +349,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n     os.makedirs(path, exist_ok=False)\n \n     if use_dev_shm:\n-        with open(os.path.join(path, \"owner.pid\"), \"wt\") as f:\n+        with open(os.path.join(path, \"owner.pid\"), \"w\") as f:\n             f.write(str(os.getpid()))\n \n     return path\ndiff --git a/torch/utils/benchmark/utils/cpp_jit.py b/torch/utils/benchmark/utils/cpp_jit.py\nindex 65b8c70ee43e6c..a09f1a00aace6f 100644\n--- a/torch/utils/benchmark/utils/cpp_jit.py\n+++ b/torch/utils/benchmark/utils/cpp_jit.py\n@@ -137,7 +137,7 @@ def _compile_template(\n         os.makedirs(build_dir, exist_ok=True)\n \n         src_path = os.path.join(build_dir, \"timer_src.cpp\")\n-        with open(src_path, \"wt\") as f:\n+        with open(src_path, \"w\") as f:\n             f.write(src)\n \n     # `cpp_extension` has its own locking scheme, so we don't need our lock.\n@@ -154,7 +154,7 @@ def _compile_template(\n \n def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> TimeitModuleType:\n     template_path: str = os.path.join(SOURCE_ROOT, \"timeit_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     module = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)\n@@ -164,7 +164,7 @@ def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> Time\n \n def compile_callgrind_template(*, stmt: str, setup: str, global_setup: str) -> str:\n     template_path: str = os.path.join(SOURCE_ROOT, \"valgrind_wrapper\", \"timer_callgrind_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     target = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 71753bd59548ae..11ce6d90fc47f3 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -28,7 +28,9 @@\n     CompletedProcessType = subprocess.CompletedProcess\n \n \n-FunctionCount = NamedTuple(\"FunctionCount\", [(\"count\", int), (\"function\", str)])\n+class FunctionCount(NamedTuple):\n+    count: int\n+    function: str\n \n \n @dataclasses.dataclass(repr=False, eq=False, frozen=True)\n@@ -598,7 +600,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     stderr=subprocess.STDOUT,\n                     **kwargs,\n                 )\n-                with open(stdout_stderr_log, \"rt\") as f:\n+                with open(stdout_stderr_log) as f:\n                     return invocation, f.read()\n             finally:\n                 f_stdout_stderr.close()\n@@ -612,7 +614,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     )\n \n                 script_file = os.path.join(working_dir, \"timer_callgrind.py\")\n-                with open(script_file, \"wt\") as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(self._construct_script(\n                         task_spec,\n                         globals=GlobalsBridge(globals, data_dir),\n@@ -652,7 +654,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n             if valgrind_invocation.returncode:\n                 error_report = \"\"\n                 if os.path.exists(error_log):\n-                    with open(error_log, \"rt\") as f:\n+                    with open(error_log) as f:\n                         error_report = f.read()\n                 if not error_report:\n                     error_report = \"Unknown error.\\n\" + valgrind_invocation_output\n@@ -724,7 +726,7 @@ def read_results(i: int) -> Tuple[FunctionCounts, FunctionCounts, Optional[str]]\n                 fpath = f\"{callgrind_out}.{i + 1}\"  # Callgrind one-indexes files.\n                 callgrind_out_contents: Optional[str] = None\n                 if retain_out_file:\n-                    with open(fpath, \"rt\") as f:\n+                    with open(fpath) as f:\n                         callgrind_out_contents = f.read()\n \n                 return (\ndiff --git a/torch/utils/bottleneck/__main__.py b/torch/utils/bottleneck/__main__.py\nindex 86c1af04baa0e6..f7fd209e1438fa 100644\n--- a/torch/utils/bottleneck/__main__.py\n+++ b/torch/utils/bottleneck/__main__.py\n@@ -16,7 +16,7 @@ def redirect_argv(new_argv):\n \n def compiled_with_cuda(sysinfo):\n     if sysinfo.cuda_compiled_version:\n-        return 'compiled w/ CUDA {}'.format(sysinfo.cuda_compiled_version)\n+        return f'compiled w/ CUDA {sysinfo.cuda_compiled_version}'\n     return 'not compiled w/ CUDA'\n \n \n@@ -59,7 +59,7 @@ def run_env_analysis():\n         'debug_str': debug_str,\n         'pytorch_version': info.torch_version,\n         'cuda_compiled': compiled_with_cuda(info),\n-        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'py_version': f'{sys.version_info[0]}.{sys.version_info[1]}',\n         'cuda_runtime': cuda_avail,\n         'pip_version': pip_version,\n         'pip_list_output': pip_list_output,\n@@ -138,7 +138,7 @@ def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\n \n     result = {\n         'mode': mode,\n-        'description': 'top {} events sorted by {}'.format(topk, sortby),\n+        'description': f'top {topk} events sorted by {sortby}',\n         'output': torch.autograd.profiler_util._build_table(topk_events),\n         'cuda_warning': cuda_warning\n     }\ndiff --git a/torch/utils/bundled_inputs.py b/torch/utils/bundled_inputs.py\nindex 4ae39733ff2e4b..ad34e15e6bfa17 100644\n--- a/torch/utils/bundled_inputs.py\n+++ b/torch/utils/bundled_inputs.py\n@@ -261,11 +261,11 @@ def augment_many_model_functions_with_bundled_inputs(\n \n \n         if input_list is not None and not isinstance(input_list, Sequence):\n-            raise TypeError(\"Error inputs for function {0} is not a Sequence\".format(function_name))\n+            raise TypeError(f\"Error inputs for function {function_name} is not a Sequence\")\n \n         function_arg_types = [arg.type for arg in function.schema.arguments[1:]]  # type: ignore[attr-defined]\n         deflated_inputs_type: ListType = ListType(TupleType(function_arg_types))\n-        model._c._register_attribute(\"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs_type, [])\n+        model._c._register_attribute(f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs_type, [])\n \n         if hasattr(model, \"_generate_bundled_inputs_for_\" + function_name):\n             if input_list is not None:\n@@ -290,7 +290,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             for inp_idx, args in enumerate(input_list):\n                 if not isinstance(args, Tuple) and not isinstance(args, List):  # type: ignore[arg-type]\n                     raise TypeError(\n-                        \"Error bundled input for function {0} idx: {1} is not a Tuple or a List\".format(function_name, inp_idx)\n+                        f\"Error bundled input for function {function_name} idx: {inp_idx} is not a Tuple or a List\"\n                     )\n                 deflated_args = []\n                 parts.append(\"(\")\n@@ -314,7 +314,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             # Back-channel return this expr for debugging.\n             if _receive_inflate_expr is not None:\n                 _receive_inflate_expr.append(expr)\n-            setattr(model, \"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs)\n+            setattr(model, f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs)\n             definition = textwrap.dedent(\"\"\"\n                 def _generate_bundled_inputs_for_{name}(self):\n                     deflated = self._bundled_inputs_deflated_{name}\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..4da281d32ac3bd 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -66,7 +66,7 @@ def _get_device_module(device=\"cuda\"):\n     return device_module\n \n \n-class DefaultDeviceType(object):\n+class DefaultDeviceType:\n     r\"\"\"\n     A class that manages the default device type for checkpointing.\n     If no non-CPU tensors are present, the default device type will\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex de03564a2b75d1..2266d64c1944d5 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n+    out = run_and_read_all(run_lambda, f\"{conda} list\")\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n+        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return 'Probably one of the following:\\n{}'.format(result)\n+    return f'Probably one of the following:\\n{result}'\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = '\"{}\"'.format(candidate_smi)\n+                smi = f'\"{candidate_smi}\"'\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n+    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return 'macOS {} ({})'.format(version, machine())\n+        return f'macOS {version} ({machine()})'\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n-        return '{} ({})'.format(platform, machine())\n+        return f'{platform} ({machine()})'\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n+        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return '\\n{}\\n'.format(string)\n+            return f'\\n{string}\\n'\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               '[{}] '.format(envinfo.pip_version))\n+                                               f'[{envinfo.pip_version}] ')\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n+            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..e847f4e30915b9 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -150,10 +150,10 @@ def _join_rocm_home(*paths) -> str:\n     only once we need to get any ROCm-specific path.\n     '''\n     if ROCM_HOME is None:\n-        raise EnvironmentError('ROCM_HOME environment variable is not set. '\n+        raise OSError('ROCM_HOME environment variable is not set. '\n                                'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n-        raise EnvironmentError('Building PyTorch extensions using '\n+        raise OSError('Building PyTorch extensions using '\n                                'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n@@ -264,7 +264,7 @@ def _maybe_write(filename, new_content):\n     if it already had the right content (to avoid triggering recompile).\n     '''\n     if os.path.exists(filename):\n-        with open(filename, 'r') as f:\n+        with open(filename) as f:\n             content = f.read()\n \n         if content == new_content:\n@@ -2247,7 +2247,7 @@ def _join_cuda_home(*paths) -> str:\n     only once we need to get any CUDA-specific path.\n     '''\n     if CUDA_HOME is None:\n-        raise EnvironmentError('CUDA_HOME environment variable is not set. '\n+        raise OSError('CUDA_HOME environment variable is not set. '\n                                'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \ndiff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py\nindex 074b89b624b9d3..cdd53c2d9ea2b1 100644\n--- a/torch/utils/data/_utils/pin_memory.py\n+++ b/torch/utils/data/_utils/pin_memory.py\n@@ -37,7 +37,7 @@ def do_one_step():\n                 data = pin_memory(data, device)\n             except Exception:\n                 data = ExceptionWrapper(\n-                    where=\"in pin memory thread for device {}\".format(device_id))\n+                    where=f\"in pin memory thread for device {device_id}\")\n             r = (idx, data)\n         while not done_event.is_set():\n             try:\ndiff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py\nindex b4fc8e0748f0f1..0d43f63a6a2f20 100644\n--- a/torch/utils/data/_utils/worker.py\n+++ b/torch/utils/data/_utils/worker.py\n@@ -76,13 +76,13 @@ def __init__(self, **kwargs):\n \n     def __setattr__(self, key, val):\n         if self.__initialized:\n-            raise RuntimeError(\"Cannot assign attributes to {} objects\".format(self.__class__.__name__))\n+            raise RuntimeError(f\"Cannot assign attributes to {self.__class__.__name__} objects\")\n         return super().__setattr__(key, val)\n \n     def __repr__(self):\n         items = []\n         for k in self.__keys:\n-            items.append('{}={}'.format(k, getattr(self, k)))\n+            items.append(f'{k}={getattr(self, k)}')\n         return '{}({})'.format(self.__class__.__name__, ', '.join(items))\n \n \n@@ -252,7 +252,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n             fetcher = _DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)\n         except Exception:\n             init_exception = ExceptionWrapper(\n-                where=\"in DataLoader worker process {}\".format(worker_id))\n+                where=f\"in DataLoader worker process {worker_id}\")\n \n         # When using Iterable mode, some worker can exit earlier than others due\n         # to the IterableDataset behaving differently for different workers.\n@@ -318,7 +318,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n                         # `ExceptionWrapper` does the correct thing.\n                         # See NOTE [ Python Traceback Reference Cycle Problem ]\n                         data = ExceptionWrapper(\n-                            where=\"in DataLoader worker process {}\".format(worker_id))\n+                            where=f\"in DataLoader worker process {worker_id}\")\n             data_queue.put((idx, data))\n             del data, idx, index, r  # save memory\n     except KeyboardInterrupt:\ndiff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py\nindex ec86f778023ba6..1c33592f02f146 100644\n--- a/torch/utils/data/dataloader.py\n+++ b/torch/utils/data/dataloader.py\n@@ -604,7 +604,7 @@ def __init__(self, loader: DataLoader) -> None:\n         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n         self._persistent_workers = loader.persistent_workers\n         self._num_yielded = 0\n-        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n+        self._profile_name = f\"enumerate(DataLoader)#{self.__class__.__name__}.__next__\"\n \n     def __iter__(self) -> '_BaseDataLoaderIter':\n         return self\n@@ -1145,7 +1145,7 @@ def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n                     self._mark_worker_as_unavailable(worker_id)\n             if len(failed_workers) > 0:\n                 pids_str = ', '.join(str(w.pid) for w in failed_workers)\n-                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n+                raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n             if isinstance(e, queue.Empty):\n                 return (False, None)\n             import tempfile\n@@ -1281,7 +1281,7 @@ def _get_data(self):\n             if success:\n                 return data\n             else:\n-                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n+                raise RuntimeError(f'DataLoader timed out after {self._timeout} seconds')\n         elif self._pin_memory:\n             while self._pin_memory_thread.is_alive():\n                 success, data = self._try_get_data()\ndiff --git a/torch/utils/data/datapipes/_decorator.py b/torch/utils/data/datapipes/_decorator.py\nindex e4cc9e4e59365d..96b7e00e076f02 100644\n--- a/torch/utils/data/datapipes/_decorator.py\n+++ b/torch/utils/data/datapipes/_decorator.py\n@@ -80,7 +80,7 @@ def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:\n         elif isinstance(arg, Callable):  # type:ignore[arg-type]\n             self.deterministic_fn = arg  # type: ignore[assignment, misc]\n         else:\n-            raise TypeError(\"{} can not be decorated by non_deterministic\".format(arg))\n+            raise TypeError(f\"{arg} can not be decorated by non_deterministic\")\n \n     def __call__(self, *args, **kwargs):\n         global _determinism\ndiff --git a/torch/utils/data/datapipes/_typing.py b/torch/utils/data/datapipes/_typing.py\nindex 6377a2ec940860..68049ba30d9018 100644\n--- a/torch/utils/data/datapipes/_typing.py\n+++ b/torch/utils/data/datapipes/_typing.py\n@@ -234,7 +234,7 @@ def issubtype(self, other):\n             return issubtype(self.param, other.param)\n         if isinstance(other, type):\n             return issubtype(self.param, other)\n-        raise TypeError(\"Expected '_DataPipeType' or 'type', but found {}\".format(type(other)))\n+        raise TypeError(f\"Expected '_DataPipeType' or 'type', but found {type(other)}\")\n \n     def issubtype_of_instance(self, other):\n         return issubinstance(other, self.param)\n@@ -279,13 +279,13 @@ def __init__(self, name, bases, namespace, **kwargs):\n     @_tp_cache\n     def _getitem_(self, params):\n         if params is None:\n-            raise TypeError('{}[t]: t can not be None'.format(self.__name__))\n+            raise TypeError(f'{self.__name__}[t]: t can not be None')\n         if isinstance(params, str):\n             params = ForwardRef(params)\n         if not isinstance(params, tuple):\n             params = (params, )\n \n-        msg = \"{}[t]: t must be a type\".format(self.__name__)\n+        msg = f\"{self.__name__}[t]: t must be a type\"\n         params = tuple(_type_check(p, msg) for p in params)\n \n         if isinstance(self.type.param, _GenericAlias):\n@@ -303,7 +303,7 @@ def _getitem_(self, params):\n                                        '__type_class__': True})\n \n         if len(params) > 1:\n-            raise TypeError('Too many parameters for {} actual {}, expected 1'.format(self, len(params)))\n+            raise TypeError(f'Too many parameters for {self} actual {len(params)}, expected 1')\n \n         t = _DataPipeType(params[0])\n \ndiff --git a/torch/utils/data/datapipes/dataframe/dataframes.py b/torch/utils/data/datapipes/dataframe/dataframes.py\nindex 06029e07851685..72d93cde66c3cb 100644\n--- a/torch/utils/data/datapipes/dataframe/dataframes.py\n+++ b/torch/utils/data/datapipes/dataframe/dataframes.py\n@@ -36,7 +36,7 @@ def disable_capture():\n     CaptureControl.disabled = True\n \n \n-class CaptureControl():\n+class CaptureControl:\n     disabled = False\n \n \n@@ -184,7 +184,7 @@ def execute(self):\n         return value\n \n \n-class CaptureLikeMock():\n+class CaptureLikeMock:\n     def __init__(self, name):\n         import unittest.mock as mock\n         # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n@@ -232,7 +232,7 @@ class CaptureVariableAssign(CaptureF):\n     def __str__(self):\n         variable = self.kwargs['variable']\n         value = self.kwargs['value']\n-        return \"{variable} = {value}\".format(variable=variable, value=value)\n+        return f\"{variable} = {value}\"\n \n     def execute(self):\n         self.kwargs['variable'].calculated_value = self.kwargs['value'].execute()\n@@ -272,7 +272,7 @@ def __init__(self, left, key, ctx):\n         self.key = key\n \n     def __str__(self):\n-        return \"%s[%s]\" % (self.left, get_val(self.key))\n+        return f\"{self.left}[{get_val(self.key)}]\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -287,7 +287,7 @@ def __init__(self, left, key, value, ctx):\n         self.value = value\n \n     def __str__(self):\n-        return \"%s[%s] = %s\" % (self.left, get_val(self.key), self.value)\n+        return f\"{self.left}[{get_val(self.key)}] = {self.value}\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -302,7 +302,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s + %s\" % (self.left, self.right)\n+        return f\"{self.left} + {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) + get_val(self.right)\n@@ -315,7 +315,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s * %s\" % (self.left, self.right)\n+        return f\"{self.left} * {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) * get_val(self.right)\n@@ -328,7 +328,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s - %s\" % (self.left, self.right)\n+        return f\"{self.left} - {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) - get_val(self.right)\n@@ -341,7 +341,7 @@ def __init__(self, src, name, ctx):\n         self.name = name\n \n     def __str__(self):\n-        return \"%s.%s\" % (self.src, self.name)\n+        return f\"{self.src}.{self.name}\"\n \n     def execute(self):\n         val = get_val(self.src)\ndiff --git a/torch/utils/data/datapipes/datapipe.py b/torch/utils/data/datapipes/datapipe.py\nindex 445400ecb59c32..1017b52af0fbce 100644\n--- a/torch/utils/data/datapipes/datapipe.py\n+++ b/torch/utils/data/datapipes/datapipe.py\n@@ -126,7 +126,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -135,7 +135,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register, enable_df_api_tracing=False):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, enable_df_api_tracing, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -265,7 +265,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -274,7 +274,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -363,7 +363,7 @@ def __len__(self):\n             return len(self._datapipe)\n         except Exception as e:\n             raise TypeError(\n-                \"{} instance doesn't have valid length\".format(type(self).__name__)\n+                f\"{type(self).__name__} instance doesn't have valid length\"\n             ) from e\n \n \ndiff --git a/torch/utils/data/datapipes/gen_pyi.py b/torch/utils/data/datapipes/gen_pyi.py\nindex 1b77fbfecf0290..ed3e75bc5da12e 100644\n--- a/torch/utils/data/datapipes/gen_pyi.py\n+++ b/torch/utils/data/datapipes/gen_pyi.py\n@@ -19,7 +19,7 @@ def gen_from_template(dir: str, template_name: str, output_name: str, replacemen\n     template_path = os.path.join(dir, template_name)\n     output_path = os.path.join(dir, output_name)\n \n-    with open(template_path, \"r\") as f:\n+    with open(template_path) as f:\n         content = f.read()\n     for placeholder, lines, indentation in replacements:\n         with open(output_path, \"w\") as f:\ndiff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py\nindex 4e3dce4b82d1dd..9916b094e408d1 100644\n--- a/torch/utils/data/datapipes/iter/callable.py\n+++ b/torch/utils/data/datapipes/iter/callable.py\n@@ -126,7 +126,7 @@ def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n         raise TypeError(\n-            \"{} instance doesn't have valid length\".format(type(self).__name__)\n+            f\"{type(self).__name__} instance doesn't have valid length\"\n         )\n \n \ndiff --git a/torch/utils/data/datapipes/iter/combinatorics.py b/torch/utils/data/datapipes/iter/combinatorics.py\nindex 30b569e329b654..4d2973bbc5a2e9 100644\n--- a/torch/utils/data/datapipes/iter/combinatorics.py\n+++ b/torch/utils/data/datapipes/iter/combinatorics.py\n@@ -48,7 +48,7 @@ def __len__(self) -> int:\n         # Dataset has been tested as `Sized`\n         if isinstance(self.sampler, Sized):\n             return len(self.sampler)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('shuffle')\n@@ -137,7 +137,7 @@ def __iter__(self) -> Iterator[T_co]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self._buffer = []\ndiff --git a/torch/utils/data/datapipes/iter/combining.py b/torch/utils/data/datapipes/iter/combining.py\nindex 7c76e986b230d4..4fe05ea717cf16 100644\n--- a/torch/utils/data/datapipes/iter/combining.py\n+++ b/torch/utils/data/datapipes/iter/combining.py\n@@ -56,7 +56,7 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return sum(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('fork')\n@@ -567,7 +567,7 @@ def __len__(self):\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes) * len(self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self.buffer = []\n@@ -627,4 +627,4 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/filelister.py b/torch/utils/data/datapipes/iter/filelister.py\nindex b2ecd71b5ce9c9..22e2cd432d6a3a 100644\n--- a/torch/utils/data/datapipes/iter/filelister.py\n+++ b/torch/utils/data/datapipes/iter/filelister.py\n@@ -61,5 +61,5 @@ def __iter__(self) -> Iterator[str] :\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/fileopener.py b/torch/utils/data/datapipes/iter/fileopener.py\nindex 03d5761a9f164c..50737d9b587b25 100644\n--- a/torch/utils/data/datapipes/iter/fileopener.py\n+++ b/torch/utils/data/datapipes/iter/fileopener.py\n@@ -51,7 +51,7 @@ def __init__(\n         self.encoding: Optional[str] = encoding\n \n         if self.mode not in ('b', 't', 'rb', 'rt', 'r'):\n-            raise ValueError(\"Invalid mode {}\".format(mode))\n+            raise ValueError(f\"Invalid mode {mode}\")\n         # TODO: enforce typing for each instance based on mode, otherwise\n         #       `argument_validation` with this DataPipe may be potentially broken\n \n@@ -68,5 +68,5 @@ def __iter__(self):\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/grouping.py b/torch/utils/data/datapipes/iter/grouping.py\nindex c83bd2748b78fb..b26847d7319740 100644\n--- a/torch/utils/data/datapipes/iter/grouping.py\n+++ b/torch/utils/data/datapipes/iter/grouping.py\n@@ -83,7 +83,7 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('unbatch')\ndiff --git a/torch/utils/data/datapipes/iter/routeddecoder.py b/torch/utils/data/datapipes/iter/routeddecoder.py\nindex 8bfbe1442180ab..5e68ae133e05ab 100644\n--- a/torch/utils/data/datapipes/iter/routeddecoder.py\n+++ b/torch/utils/data/datapipes/iter/routeddecoder.py\n@@ -62,4 +62,4 @@ def __iter__(self) -> Iterator[Tuple[str, Any]]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/sharding.py b/torch/utils/data/datapipes/iter/sharding.py\nindex 730caeaf7d4da3..1f4a3a291bd11f 100644\n--- a/torch/utils/data/datapipes/iter/sharding.py\n+++ b/torch/utils/data/datapipes/iter/sharding.py\n@@ -80,4 +80,4 @@ def __len__(self):\n         if isinstance(self.source_datapipe, Sized):\n             return len(self.source_datapipe) // self.num_of_instances +\\\n                 (1 if (self.instance_id < len(self.source_datapipe) % self.num_of_instances) else 0)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/map/combining.py b/torch/utils/data/datapipes/map/combining.py\nindex 85146f8345cbdc..4a4a785eff78e5 100644\n--- a/torch/utils/data/datapipes/map/combining.py\n+++ b/torch/utils/data/datapipes/map/combining.py\n@@ -47,7 +47,7 @@ def __getitem__(self, index) -> T_co:  # type: ignore[type-var]\n                 return dp[index - offset]\n             else:\n                 offset += len(dp)\n-        raise IndexError(\"Index {} is out of range.\".format(index))\n+        raise IndexError(f\"Index {index} is out of range.\")\n \n     def __len__(self) -> int:\n         return sum(len(dp) for dp in self.datapipes)\ndiff --git a/torch/utils/data/datapipes/map/grouping.py b/torch/utils/data/datapipes/map/grouping.py\nindex da3cf5688a1bb0..65b30d8eba1f40 100644\n--- a/torch/utils/data/datapipes/map/grouping.py\n+++ b/torch/utils/data/datapipes/map/grouping.py\n@@ -64,4 +64,4 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/utils/common.py b/torch/utils/data/datapipes/utils/common.py\nindex e39d67ee6c81b9..99ae0cb4cbd024 100644\n--- a/torch/utils/data/datapipes/utils/common.py\n+++ b/torch/utils/data/datapipes/utils/common.py\n@@ -305,7 +305,7 @@ def __init__(self, file_obj, parent_stream=None, name=None):\n         self.closed = False\n         if parent_stream is not None:\n             if not isinstance(parent_stream, StreamWrapper):\n-                raise RuntimeError('Parent stream should be StreamWrapper, {} was given'.format(type(parent_stream)))\n+                raise RuntimeError(f'Parent stream should be StreamWrapper, {type(parent_stream)} was given')\n             parent_stream.child_counter += 1\n             self.parent_stream = parent_stream\n         if StreamWrapper.debug_unclosed_streams:\ndiff --git a/torch/utils/data/datapipes/utils/decoder.py b/torch/utils/data/datapipes/utils/decoder.py\nindex 4da810c3276684..8a7cb71b619de4 100644\n--- a/torch/utils/data/datapipes/utils/decoder.py\n+++ b/torch/utils/data/datapipes/utils/decoder.py\n@@ -137,7 +137,7 @@ class ImageHandler:\n     - pilrgba: pil None rgba\n     \"\"\"\n     def __init__(self, imagespec):\n-        assert imagespec in list(imagespecs.keys()), \"unknown image specification: {}\".format(imagespec)\n+        assert imagespec in list(imagespecs.keys()), f\"unknown image specification: {imagespec}\"\n         self.imagespec = imagespec.lower()\n \n     def __call__(self, extension, data):\n@@ -167,14 +167,14 @@ def __call__(self, extension, data):\n                 return img\n             elif atype == \"numpy\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n                 if etype == \"uint8\":\n                     return result\n                 else:\n                     return result.astype(\"f\") / 255.0\n             elif atype == \"torch\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n \n                 if etype == \"uint8\":\n                     result = np.array(result.transpose(2, 0, 1))\ndiff --git a/torch/utils/data/graph.py b/torch/utils/data/graph.py\nindex 2769e326c03e3b..7fc95d58fa2198 100644\n--- a/torch/utils/data/graph.py\n+++ b/torch/utils/data/graph.py\n@@ -130,7 +130,7 @@ def traverse(datapipe: DataPipe, only_datapipe: Optional[bool] = None) -> DataPi\n # Add cache here to prevent infinite recursion on DataPipe\n def _traverse_helper(datapipe: DataPipe, only_datapipe: bool, cache: Set[int]) -> DataPipeGraph:\n     if not isinstance(datapipe, (IterDataPipe, MapDataPipe)):\n-        raise RuntimeError(\"Expected `IterDataPipe` or `MapDataPipe`, but {} is found\".format(type(datapipe)))\n+        raise RuntimeError(f\"Expected `IterDataPipe` or `MapDataPipe`, but {type(datapipe)} is found\")\n \n     dp_id = id(datapipe)\n     if dp_id in cache:\ndiff --git a/torch/utils/dlpack.py b/torch/utils/dlpack.py\nindex f903de94eb67b2..a987bca6dcd51b 100644\n--- a/torch/utils/dlpack.py\n+++ b/torch/utils/dlpack.py\n@@ -102,7 +102,7 @@ def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n         # device is either CUDA or ROCm, we need to pass the current\n         # stream\n         if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n-            stream = torch.cuda.current_stream('cuda:{}'.format(device[1]))\n+            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n             # cuda_stream is the pointer to the stream and it is a public\n             # attribute, but it is not documented\n             # The array API specify that the default legacy stream must be passed\ndiff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py\nindex 3b583dbf790109..163f3649c41279 100644\n--- a/torch/utils/hipify/cuda_to_hip_mappings.py\n+++ b/torch/utils/hipify/cuda_to_hip_mappings.py\n@@ -46,7 +46,7 @@\n     RE_MINOR = re.compile(r\"#define\\s+ROCM_VERSION_MINOR\\s+(\\d+)\")\n     RE_PATCH = re.compile(r\"#define\\s+ROCM_VERSION_PATCH\\s+(\\d+)\")\n     major, minor, patch = 0, 0, 0\n-    for line in open(rocm_version_h, \"r\"):\n+    for line in open(rocm_version_h):\n         match = RE_MAJOR.search(line)\n         if match:\n             major = int(match.group(1))\ndiff --git a/torch/utils/hipify/hipify_python.py b/torch/utils/hipify/hipify_python.py\nindex 34a066750e1cdc..fa800659595bd7 100755\n--- a/torch/utils/hipify/hipify_python.py\n+++ b/torch/utils/hipify/hipify_python.py\n@@ -219,13 +219,13 @@ def compute_stats(stats):\n     unsupported_calls = {cuda_call for (cuda_call, _filepath) in stats[\"unsupported_calls\"]}\n \n     # Print the number of unsupported calls\n-    print(\"Total number of unsupported CUDA function calls: {0:d}\".format(len(unsupported_calls)))\n+    print(f\"Total number of unsupported CUDA function calls: {len(unsupported_calls):d}\")\n \n     # Print the list of unsupported calls\n     print(\", \".join(unsupported_calls))\n \n     # Print the number of kernel launches\n-    print(\"\\nTotal number of replaced kernel launches: {0:d}\".format(len(stats[\"kernel_launches\"])))\n+    print(\"\\nTotal number of replaced kernel launches: {:d}\".format(len(stats[\"kernel_launches\"])))\n \n \n def add_dim3(kernel_string, cuda_kernel):\n@@ -254,8 +254,8 @@ def add_dim3(kernel_string, cuda_kernel):\n     first_arg_clean = kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace(\"\\n\", \"\").strip(\" \")\n     second_arg_clean = kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace(\"\\n\", \"\").strip(\" \")\n \n-    first_arg_dim3 = \"dim3({})\".format(first_arg_clean)\n-    second_arg_dim3 = \"dim3({})\".format(second_arg_clean)\n+    first_arg_dim3 = f\"dim3({first_arg_clean})\"\n+    second_arg_dim3 = f\"dim3({second_arg_clean})\"\n \n     first_arg_raw_dim3 = first_arg_raw.replace(first_arg_clean, first_arg_dim3)\n     second_arg_raw_dim3 = second_arg_raw.replace(second_arg_clean, second_arg_dim3)\n@@ -269,7 +269,7 @@ def add_dim3(kernel_string, cuda_kernel):\n def processKernelLaunches(string, stats):\n     \"\"\" Replace the CUDA style Kernel launches with the HIP style kernel launches.\"\"\"\n     # Concat the namespace with the kernel names. (Find cleaner way of doing this later).\n-    string = RE_KERNEL_LAUNCH.sub(lambda inp: \"{0}{1}::\".format(inp.group(1), inp.group(2)), string)\n+    string = RE_KERNEL_LAUNCH.sub(lambda inp: f\"{inp.group(1)}{inp.group(2)}::\", string)\n \n     def grab_method_and_template(in_kernel):\n         # The positions for relevant kernel components.\n@@ -482,7 +482,7 @@ def replace_math_functions(input_string):\n     \"\"\"\n     output_string = input_string\n     for func in MATH_TRANSPILATIONS:\n-        output_string = output_string.replace(r'{}('.format(func), '{}('.format(MATH_TRANSPILATIONS[func]))\n+        output_string = output_string.replace(fr'{func}(', f'{MATH_TRANSPILATIONS[func]}(')\n \n     return output_string\n \n@@ -531,7 +531,7 @@ def replace_extern_shared(input_string):\n     \"\"\"\n     output_string = input_string\n     output_string = RE_EXTERN_SHARED.sub(\n-        lambda inp: \"HIP_DYNAMIC_SHARED({0} {1}, {2})\".format(\n+        lambda inp: \"HIP_DYNAMIC_SHARED({} {}, {})\".format(\n             inp.group(1) or \"\", inp.group(2), inp.group(3)), output_string)\n \n     return output_string\n@@ -657,7 +657,7 @@ def is_caffe2_gpu_file(rel_filepath):\n \n \n # Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n-class Trie():\n+class Trie:\n     \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n     The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n \n@@ -750,7 +750,7 @@ def pattern(self):\n             CAFFE2_TRIE.add(src)\n             CAFFE2_MAP[src] = dst\n RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.pattern())\n-RE_PYTORCH_PREPROCESSOR = re.compile(r'(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))\n+RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\\W)({PYTORCH_TRIE.pattern()})(?=\\W)')\n \n RE_QUOTE_HEADER = re.compile(r'#include \"([^\"]+)\"')\n RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')\n@@ -789,7 +789,7 @@ def preprocessor(\n \n     rel_filepath = os.path.relpath(filepath, output_directory)\n \n-    with open(fin_path, 'r', encoding='utf-8') as fin:\n+    with open(fin_path, encoding='utf-8') as fin:\n         if fin.readline() == HIPIFY_C_BREADCRUMB:\n             hipify_result.hipified_path = None\n             hipify_result.status = \"[ignored, input is hipified output]\"\n@@ -929,7 +929,7 @@ def repl(m):\n \n     do_write = True\n     if os.path.exists(fout_path):\n-        with open(fout_path, 'r', encoding='utf-8') as fout_old:\n+        with open(fout_path, encoding='utf-8') as fout_old:\n             do_write = fout_old.read() != output_source\n     if do_write:\n         try:\n@@ -956,7 +956,7 @@ def file_specific_replacement(filepath, search_string, replace_string, strict=Fa\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if strict:\n-            contents = re.sub(r'\\b({0})\\b'.format(re.escape(search_string)), lambda x: replace_string, contents)\n+            contents = re.sub(fr'\\b({re.escape(search_string)})\\b', lambda x: replace_string, contents)\n         else:\n             contents = contents.replace(search_string, replace_string)\n         f.seek(0)\n@@ -968,8 +968,8 @@ def file_add_header(filepath, header):\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if header[0] != \"<\" and header[-1] != \">\":\n-            header = '\"{0}\"'.format(header)\n-        contents = ('#include {0} \\n'.format(header)) + contents\n+            header = f'\"{header}\"'\n+        contents = (f'#include {header} \\n') + contents\n         f.seek(0)\n         f.write(contents)\n         f.truncate()\ndiff --git a/torch/utils/jit/log_extract.py b/torch/utils/jit/log_extract.py\nindex d9d0e442c1dbf6..2e89a769eff0c8 100644\n--- a/torch/utils/jit/log_extract.py\n+++ b/torch/utils/jit/log_extract.py\n@@ -11,7 +11,7 @@ def extract_ir(filename: str) -> List[str]:\n     pfx = None\n     current = \"\"\n     graphs = []\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         split_strs = f.read().split(BEGIN)\n         for i, split_str in enumerate(split_strs):\n             if i == 0:\ndiff --git a/torch/utils/mobile_optimizer.py b/torch/utils/mobile_optimizer.py\nindex ec200423e10c5b..66d57a2372baf9 100644\n--- a/torch/utils/mobile_optimizer.py\n+++ b/torch/utils/mobile_optimizer.py\n@@ -31,7 +31,7 @@ def optimize_for_mobile(\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     if optimization_blocklist is None:\n         optimization_blocklist = set()\n@@ -86,7 +86,7 @@ def generate_mobile_module_lints(script_module: torch.jit.ScriptModule):\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     lint_list = []\n \ndiff --git a/torch/utils/tensorboard/_caffe2_graph.py b/torch/utils/tensorboard/_caffe2_graph.py\nindex 8bba2aeffddef2..2aa162af7ad5c3 100644\n--- a/torch/utils/tensorboard/_caffe2_graph.py\n+++ b/torch/utils/tensorboard/_caffe2_graph.py\n@@ -232,7 +232,7 @@ def _add_gradient_scope(shapes, blob_name_tracker, ops):\n \n     def f(name):\n         if \"_grad\" in name:\n-            return \"GRADIENTS/{}\".format(name)\n+            return f\"GRADIENTS/{name}\"\n         else:\n             return name\n \n@@ -317,7 +317,7 @@ def _tf_device(device_option):\n     ):\n         return \"/cpu:*\"\n     if device_option.device_type == caffe2_pb2.CUDA:\n-        return \"/gpu:{}\".format(device_option.device_id)\n+        return f\"/gpu:{device_option.device_id}\"\n     raise Exception(\"Unhandled device\", device_option)\n \n \ndiff --git a/torch/utils/tensorboard/_embedding.py b/torch/utils/tensorboard/_embedding.py\nindex f172e092608337..afbe68191aa98f 100644\n--- a/torch/utils/tensorboard/_embedding.py\n+++ b/torch/utils/tensorboard/_embedding.py\n@@ -62,7 +62,7 @@ def make_sprite(label_img, save_path):\n \n def get_embedding_info(metadata, label_img, subdir, global_step, tag):\n     info = EmbeddingInfo()\n-    info.tensor_name = \"{}:{}\".format(tag, str(global_step).zfill(5))\n+    info.tensor_name = f\"{tag}:{str(global_step).zfill(5)}\"\n     info.tensor_path = _gfile_join(subdir, \"tensors.tsv\")\n     if metadata is not None:\n         info.metadata_path = _gfile_join(subdir, \"metadata.tsv\")\ndiff --git a/torch/utils/tensorboard/_pytorch_graph.py b/torch/utils/tensorboard/_pytorch_graph.py\nindex f03812b603e1c0..280b503c515c0b 100644\n--- a/torch/utils/tensorboard/_pytorch_graph.py\n+++ b/torch/utils/tensorboard/_pytorch_graph.py\n@@ -275,7 +275,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n                     parent_scope, attr_scope, attr_name\n                 )\n             else:\n-                attr_to_scope[attr_key] = \"__module.{}\".format(attr_name)\n+                attr_to_scope[attr_key] = f\"__module.{attr_name}\"\n             # We don't need classtype nodes; scope will provide this information\n             if node.output().type().kind() != CLASSTYPE_KIND:\n                 node_py = NodePyOP(node)\n@@ -286,7 +286,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n \n     for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n         node_pyio = NodePyIO(node, \"output\")\n-        node_pyio.debugName = \"output.{}\".format(i + 1)\n+        node_pyio.debugName = f\"output.{i + 1}\"\n         node_pyio.inputs = [node.debugName()]\n         nodes_py.append(node_pyio)\n \n@@ -302,7 +302,7 @@ def parse_traced_name(module):\n     for name, module in trace.named_modules(prefix=\"__module\"):\n         mod_name = parse_traced_name(module)\n         attr_name = name.split(\".\")[-1]\n-        alias_to_name[name] = \"{}[{}]\".format(mod_name, attr_name)\n+        alias_to_name[name] = f\"{mod_name}[{attr_name}]\"\n \n     for node in nodes_py.nodes_op:\n         module_aliases = node.scopeName.split(\"/\")\ndiff --git a/torch/utils/tensorboard/writer.py b/torch/utils/tensorboard/writer.py\nindex 2fa34d05e727d9..b592707df2c1e1 100644\n--- a/torch/utils/tensorboard/writer.py\n+++ b/torch/utils/tensorboard/writer.py\n@@ -953,7 +953,7 @@ def add_embedding(\n \n         # Maybe we should encode the tag so slashes don't trip us up?\n         # I don't think this will mess us up, but better safe than sorry.\n-        subdir = \"%s/%s\" % (str(global_step).zfill(5), self._encode(tag))\n+        subdir = f\"{str(global_step).zfill(5)}/{self._encode(tag)}\"\n         save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n \n         fs = tf.io.gfile\ndiff --git a/torch/utils/throughput_benchmark.py b/torch/utils/throughput_benchmark.py\nindex 8b2fd1a76ca8a4..2dc3ce8543a9b6 100644\n--- a/torch/utils/throughput_benchmark.py\n+++ b/torch/utils/throughput_benchmark.py\n@@ -18,10 +18,10 @@ def format_time(time_us=None, time_ms=None, time_s=None):\n             raise AssertionError(\"Shouldn't reach here :)\")\n \n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n \n class ExecutionStats:\n@@ -52,8 +52,8 @@ def total_time_seconds(self):\n     def __str__(self):\n         return '\\n'.join([\n             \"Average latency per example: \" + format_time(time_ms=self.latency_avg_ms),\n-            \"Total number of iterations: {}\".format(self.num_iters),\n-            \"Total number of iterations per second (across all threads): {:.2f}\".format(self.iters_per_second),\n+            f\"Total number of iterations: {self.num_iters}\",\n+            f\"Total number of iterations per second (across all threads): {self.iters_per_second:.2f}\",\n             \"Total time: \" + format_time(time_s=self.total_time_seconds)\n         ])\n \ndiff --git a/torch/utils/viz/_cycles.py b/torch/utils/viz/_cycles.py\nindex a64d5e9c35830a..13a425cd1b8285 100644\n--- a/torch/utils/viz/_cycles.py\n+++ b/torch/utils/viz/_cycles.py\n@@ -220,29 +220,29 @@ def format_sequence(obj):\n     if isinstance(obj, BASE_TYPES):\n         return repr(obj)\n     if type(obj).__name__ == 'function':\n-        return \"function\\n{}\".format(obj.__name__)\n+        return f\"function\\n{obj.__name__}\"\n     elif isinstance(obj, types.MethodType):\n         try:\n             func_name = obj.__func__.__qualname__\n         except AttributeError:\n             func_name = \"<anonymous>\"\n-        return \"instancemethod\\n{}\".format(func_name)\n+        return f\"instancemethod\\n{func_name}\"\n     elif isinstance(obj, list):\n         return f\"[{format_sequence(obj)}]\"\n     elif isinstance(obj, tuple):\n         return f\"({format_sequence(obj)})\"\n     elif isinstance(obj, dict):\n-        return \"dict[{}]\".format(len(obj))\n+        return f\"dict[{len(obj)}]\"\n     elif isinstance(obj, types.ModuleType):\n-        return \"module\\n{}\".format(obj.__name__)\n+        return f\"module\\n{obj.__name__}\"\n     elif isinstance(obj, type):\n-        return \"type\\n{}\".format(obj.__name__)\n+        return f\"type\\n{obj.__name__}\"\n     elif isinstance(obj, weakref.ref):\n         referent = obj()\n         if referent is None:\n             return \"weakref (dead referent)\"\n         else:\n-            return \"weakref to id 0x{:x}\".format(id(referent))\n+            return f\"weakref to id 0x{id(referent):x}\"\n     elif isinstance(obj, types.FrameType):\n         filename = obj.f_code.co_filename\n         if len(filename) > FRAME_FILENAME_LIMIT:\ndiff --git a/torch/utils/weak.py b/torch/utils/weak.py\nindex 2a7d597c4f2a06..bcd3025bc68e3a 100644\n--- a/torch/utils/weak.py\n+++ b/torch/utils/weak.py\n@@ -4,7 +4,6 @@\n from weakref import ref\n from _weakrefset import _IterationGuard  # type: ignore[attr-defined]\n from collections.abc import MutableMapping, Mapping\n-from typing import Dict\n from torch import Tensor\n import collections.abc as _collections_abc\n \n@@ -83,7 +82,7 @@ def __eq__(self, other):\n \n # This is directly adapted from cpython/Lib/weakref.py\n class WeakIdKeyDictionary(MutableMapping):\n-    data: Dict[WeakIdRef, object]\n+    data: dict[WeakIdRef, object]\n \n     def __init__(self, dict=None):\n         self.data = {}\n@@ -144,7 +143,7 @@ def __len__(self):\n         return len(self.data) - len(self._pending_removals)\n \n     def __repr__(self):\n-        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n+        return f\"<{self.__class__.__name__} at {id(self):#x}>\"\n \n     def __setitem__(self, key, value):\n         self.data[WeakIdRef(key, self._remove)] = value  # CHANGED\n"
  },
  {
    "number": 105393,
    "title": "[BE] Enable ruff's UP rules and autoformat torchgen/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "a27bbeea3a282030ec1323929427b16761b42dad",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105393",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105393/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105393.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105393.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105393/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105393/comments",
    "labels": [
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:12:17.864089Z",
    "state": "closed",
    "patch": "From 4c67dc2ec567e4c353c1de486f44186cbb0d5cce Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:12:10 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat torchgen/\n\n[ghstack-poisoned]\n---\n torchgen/api/python.py               | 16 ++++++++--------\n torchgen/code_template.py            |  2 +-\n torchgen/executorch/parse.py         |  2 +-\n torchgen/gen.py                      |  4 ++--\n torchgen/gen_backend_stubs.py        |  6 +++---\n torchgen/gen_executorch.py           |  6 +++---\n torchgen/gen_lazy_tensor.py          |  6 +++---\n torchgen/model.py                    |  2 +-\n torchgen/selective_build/operator.py |  2 +-\n torchgen/selective_build/selector.py |  4 ++--\n torchgen/utils.py                    | 10 +++++-----\n 11 files changed, 30 insertions(+), 30 deletions(-)\n\ndiff --git a/torchgen/api/python.py b/torchgen/api/python.py\nindex b4da5d1113dce6..96aa43be1060b5 100644\n--- a/torchgen/api/python.py\n+++ b/torchgen/api/python.py\n@@ -315,7 +315,7 @@ def from_outputs(\n                 outputs=outputs,\n             )\n         elif size > 1:\n-            if any((not a.type.is_tensor_like() for a in outputs)):\n+            if any(not a.type.is_tensor_like() for a in outputs):\n                 raise RuntimeError(f\"Unsupported output type: {outputs}\")\n             return PythonOutArgument(\n                 name=\"out\",\n@@ -882,10 +882,10 @@ def topt_default_init(name: str) -> Optional[str]:\n \n \n def namedtuple_fieldnames(returns: Tuple[Return, ...]) -> List[str]:\n-    if len(returns) <= 1 or all((r.name is None for r in returns)):\n+    if len(returns) <= 1 or all(r.name is None for r in returns):\n         return []\n     else:\n-        if any((r.name is None for r in returns)):\n+        if any(r.name is None for r in returns):\n             # When building on Windows, `PyStructSequence_UnnamedField` could not be\n             # resolved by the linker for some reason, which cause error in building:\n             #\n@@ -1163,7 +1163,7 @@ def dispatch_lambda_return_str(f: NativeFunction) -> str:\n     # mutable reference to temporary.  Maybe we could assign it to a\n     # variable itself.)\n     returns_without_annotation = tuple(\n-        (Return(r.name, r.type, None) for r in f.func.returns)\n+        Return(r.name, r.type, None) for r in f.func.returns\n     )\n     return_str = cpp.returns_type(returns_without_annotation, symint=True).cpp_type()\n     if return_str not in SUPPORTED_RETURN_TYPES:\n@@ -1195,7 +1195,7 @@ def cpp_dispatch_exprs(\n     exprs: Tuple[str, ...] = tuple()\n     if not isinstance(python_signature, PythonSignatureDeprecated):\n         # By default the exprs are consistent with the C++ signature.\n-        exprs = tuple((a.name for a in cpp_args))\n+        exprs = tuple(a.name for a in cpp_args)\n     else:\n         # For deprecated python signature we may need fill in some constants.\n         exprs = tuple(\n@@ -1426,7 +1426,7 @@ def dispatch_lambda_exprs(\n                     f\"{f.func}: unrecognized type '{str(a.type)}' for tensor options field '{a.name}'\"\n                 )\n         if not all(\n-            (a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys())\n+            a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys()\n         ):\n             raise RuntimeError(\n                 f\"{f.func}: incomplete tensor options args: {tensor_options_args_names}\"\n@@ -1454,7 +1454,7 @@ def dispatch_lambda_exprs(\n                 raise RuntimeError(\n                     f\"{f.func}: dtype in tensor_options_args without output arg\"\n                 )\n-            if not all((a in tensor_options_args_names for a in (\"layout\", \"device\"))):\n+            if not all(a in tensor_options_args_names for a in (\"layout\", \"device\")):\n                 raise RuntimeError(\n                     f\"{f.func}: incomplete tensor options for output check\"\n                 )\n@@ -1473,6 +1473,6 @@ def dispatch_lambda_exprs(\n             )\n \n     return DispatchLambdaArgumentExprs(\n-        exprs=tuple((lambda_args_exprs[a.name] for a in lambda_args)),\n+        exprs=tuple(lambda_args_exprs[a.name] for a in lambda_args),\n         inits=inits,\n     )\ndiff --git a/torchgen/code_template.py b/torchgen/code_template.py\nindex 9f877771afe9be..b932a94ecc9192 100644\n--- a/torchgen/code_template.py\n+++ b/torchgen/code_template.py\n@@ -20,7 +20,7 @@ class CodeTemplate:\n \n     @staticmethod\n     def from_file(filename: str) -> \"CodeTemplate\":\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             return CodeTemplate(f.read(), filename)\n \n     def __init__(self, pattern: str, filename: str = \"\") -> None:\ndiff --git a/torchgen/executorch/parse.py b/torchgen/executorch/parse.py\nindex f6f30b4554aafb..89b4b93558a6a2 100644\n--- a/torchgen/executorch/parse.py\n+++ b/torchgen/executorch/parse.py\n@@ -124,7 +124,7 @@ def parse_et_yaml(\n     \"\"\"Parse native_functions.yaml into NativeFunctions and an Operator Indexed Dict\n     of fields to persist from native_functions.yaml to functions.yaml\n     \"\"\"\n-    with open(path, \"r\") as f:\n+    with open(path) as f:\n         es = yaml.load(f, Loader=LineLoader)\n \n     et_kernel = extract_kernel_fields(es)\ndiff --git a/torchgen/gen.py b/torchgen/gen.py\nindex dcdd0945dff019..9766c8af5bc0f5 100644\n--- a/torchgen/gen.py\n+++ b/torchgen/gen.py\n@@ -212,7 +212,7 @@ def parse_tags_yaml_struct(es: object, path: str = \"<stdin>\") -> Set[str]:\n def parse_tags_yaml(path: str) -> Set[str]:\n     global _GLOBAL_PARSE_TAGS_YAML_CACHE\n     if path not in _GLOBAL_PARSE_TAGS_YAML_CACHE:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n             _GLOBAL_PARSE_TAGS_YAML_CACHE[path] = parse_tags_yaml_struct(es, path=path)\n \n@@ -233,7 +233,7 @@ def parse_native_yaml(\n \n         # if a loaded yaml is provided, use that instead of reading from path\n         if loaded_yaml is None:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 es = yaml.load(f, Loader=LineLoader)\n         else:\n             es = loaded_yaml\ndiff --git a/torchgen/gen_backend_stubs.py b/torchgen/gen_backend_stubs.py\nindex 7322daa5dc7602..ff23aa9be39713 100644\n--- a/torchgen/gen_backend_stubs.py\n+++ b/torchgen/gen_backend_stubs.py\n@@ -47,7 +47,7 @@ def parse_backend_yaml(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -253,9 +253,9 @@ def error_on_missing_kernels(\n     full_codegen: Optional[List[OperatorName]] = None,\n ) -> None:\n     try:\n-        with open(kernel_defn_file_path, \"r\") as f:\n+        with open(kernel_defn_file_path) as f:\n             backend_defns = f.read()\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified impl_path file: {kernel_defn_file_path}\"\n         ) from e\ndiff --git a/torchgen/gen_executorch.py b/torchgen/gen_executorch.py\nindex bfd42a7985e49d..6f5df46944f0f6 100644\n--- a/torchgen/gen_executorch.py\n+++ b/torchgen/gen_executorch.py\n@@ -575,7 +575,7 @@ def translate_native_yaml(\n         None\n     \"\"\"\n     if use_aten_lib:\n-        with open(aten_yaml_path, \"r\") as aten_yaml:\n+        with open(aten_yaml_path) as aten_yaml:\n             out_file.writelines(aten_yaml.readlines())\n         return\n \n@@ -604,7 +604,7 @@ def translate_native_yaml(\n         or os.stat(native_yaml_path).st_size == 0\n     ):\n         return\n-    with open(native_yaml_path, \"r\") as native_yaml:\n+    with open(native_yaml_path) as native_yaml:\n         native_es = yaml.load(native_yaml, Loader=LineLoader)\n         if not native_es:\n             return\n@@ -641,7 +641,7 @@ def parse_yaml(\n     Union[Dict[DispatchKey, Dict[OperatorName, BackendMetadata]], ETKernelIndex],\n ]:\n     if path and os.path.exists(path) and os.stat(path).st_size > 0:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n \n         # Check for kernel index structure\ndiff --git a/torchgen/gen_lazy_tensor.py b/torchgen/gen_lazy_tensor.py\nindex f995bdb2619838..3e4e4b0414277c 100644\n--- a/torchgen/gen_lazy_tensor.py\n+++ b/torchgen/gen_lazy_tensor.py\n@@ -115,7 +115,7 @@ def parse_native_functions_keys(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -134,10 +134,10 @@ def validate_shape_inference_header(\n     shape_inference_hdr: str, expected_shape_infr_decls: List[str]\n ) -> None:\n     try:\n-        with open(shape_inference_hdr, \"r\") as f:\n+        with open(shape_inference_hdr) as f:\n             shape_infr_decls = f.read()\n             shape_infr_decl_lines = set(shape_infr_decls.split(\"\\n\"))\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified shape_inference_hdr file: {shape_inference_hdr}\"\n         ) from e\ndiff --git a/torchgen/model.py b/torchgen/model.py\nindex 151fb02bb2c908..0b44732455ea2e 100644\n--- a/torchgen/model.py\n+++ b/torchgen/model.py\n@@ -40,7 +40,7 @@ class Location:\n     line: int\n \n     def __str__(self) -> str:\n-        return \"{}:{}\".format(self.file, self.line)\n+        return f\"{self.file}:{self.line}\"\n \n \n # Valid values of the 'variants' field in native_functions.yaml\ndiff --git a/torchgen/selective_build/operator.py b/torchgen/selective_build/operator.py\nindex 52fdcb74fca84b..d7f5c56f63a60d 100644\n--- a/torchgen/selective_build/operator.py\n+++ b/torchgen/selective_build/operator.py\n@@ -83,7 +83,7 @@ def from_yaml_dict(\n         if \"debug_info\" in op_info:\n             di_list = op_info[\"debug_info\"]\n             assert isinstance(di_list, list)\n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         return SelectiveBuildOperator(\n             name=op_name,\ndiff --git a/torchgen/selective_build/selector.py b/torchgen/selective_build/selector.py\nindex 1d4a00e5968950..4fdc513534444d 100644\n--- a/torchgen/selective_build/selector.py\n+++ b/torchgen/selective_build/selector.py\n@@ -93,7 +93,7 @@ def from_yaml_dict(data: Dict[str, object]) -> \"SelectiveBuilder\":\n             di_list = data[\"debug_info\"]\n             assert isinstance(di_list, list)\n \n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         operators = {}\n         operators_dict = data.get(\"operators\", {})\n@@ -141,7 +141,7 @@ def from_yaml_str(config_contents: str) -> \"SelectiveBuilder\":\n \n     @staticmethod\n     def from_yaml_path(config_path: str) -> \"SelectiveBuilder\":\n-        with open(config_path, \"r\") as f:\n+        with open(config_path) as f:\n             contents = yaml.safe_load(f)\n             return SelectiveBuilder.from_yaml_dict(contents)\n \ndiff --git a/torchgen/utils.py b/torchgen/utils.py\nindex dd187c737c93ed..0729645ef10b92 100644\n--- a/torchgen/utils.py\n+++ b/torchgen/utils.py\n@@ -105,7 +105,7 @@ def context(msg_fn: Callable[[], str]) -> Iterator[None]:\n # for getting mypy to do exhaustiveness checking\n # TODO: put this somewhere else, maybe\n def assert_never(x: NoReturn) -> NoReturn:\n-    raise AssertionError(\"Unhandled type: {}\".format(type(x).__name__))\n+    raise AssertionError(f\"Unhandled type: {type(x).__name__}\")\n \n \n @functools.lru_cache(maxsize=None)\n@@ -137,9 +137,9 @@ def __init__(self, install_dir: str, template_dir: str, dry_run: bool) -> None:\n     def _write_if_changed(self, filename: str, contents: str) -> None:\n         old_contents: Optional[str]\n         try:\n-            with open(filename, \"r\") as f:\n+            with open(filename) as f:\n                 old_contents = f.read()\n-        except IOError:\n+        except OSError:\n             old_contents = None\n         if contents != old_contents:\n             # Create output directory if it doesn't exist\n@@ -157,7 +157,7 @@ def substitute_with_template(\n             # TODO: Update the comment reference to the correct location\n             if \"generated_comment\" not in env:\n                 comment = \"@\" + \"generated by torchgen/gen.py\"\n-                comment += \" from {}\".format(os.path.basename(template_path))\n+                comment += f\" from {os.path.basename(template_path)}\"\n                 env[\"generated_comment\"] = comment\n             template = _read_template(template_path)\n             return template.substitute(env)\n@@ -172,7 +172,7 @@ def write_with_template(\n         template_fn: str,\n         env_callable: Callable[[], Union[str, Dict[str, Any]]],\n     ) -> None:\n-        filename = \"{}/{}\".format(self.install_dir, filename)\n+        filename = f\"{self.install_dir}/{filename}\"\n         assert filename not in self.filenames, \"duplicate file write {filename}\"\n         self.filenames.add(filename)\n         if not self.dry_run:\n"
  },
  {
    "number": 105392,
    "title": "[BE] Enable ruff's UP rules and autoformat dynamo / functorch and refs",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "8760206490c142e56dc120da0af2fc91b1839912",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105392",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105392/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105392.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105392.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105392/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105392/comments",
    "labels": [
      "open source",
      "release notes: fx",
      "ciflow/inductor",
      "module: dynamo",
      "module: export"
    ],
    "_event_time": "2023-07-18T01:11:49.799580Z",
    "state": "closed",
    "patch": "From 4fecdf031a9fbbabe40dc76eac9ff7addf475d85 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:42 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat dynamo / functorch\n and refs\n\n[ghstack-poisoned]\n---\n functorch/benchmarks/operator_authoring.py    |  2 +-\n functorch/einops/rearrange.py                 |  4 +-\n .../examples/compilation/linear_train.py      |  2 +-\n .../maml_omniglot/support/omniglot_loaders.py |  4 +-\n functorch/op_analysis/gen_data.py             | 10 +--\n test/dynamo/test_autograd_function.py         |  4 +-\n test/dynamo/test_compile.py                   |  2 +-\n test/dynamo/test_logging.py                   |  2 +-\n test/dynamo/test_misc.py                      | 34 +++++-----\n test/dynamo/test_modules.py                   | 12 ++--\n test/dynamo/test_profiler.py                  |  4 +-\n test/dynamo/test_repros.py                    |  2 +-\n test/error_messages/storage.py                |  2 +-\n test/export/test_db.py                        |  6 +-\n test/export/test_serialize.py                 |  2 +-\n .../check_forward_backward_compatibility.py   |  2 +-\n test/functorch/test_aotdispatch.py            |  4 +-\n test/functorch/test_vmap.py                   |  2 +-\n test/fx/test_future.py                        |  4 +-\n test/fx/test_gradual_type.py                  |  8 +--\n torch/_decomp/decompositions.py               | 14 ++---\n torch/_dynamo/debug_utils.py                  | 16 ++---\n torch/_dynamo/eval_frame.py                   |  4 +-\n torch/_dynamo/output_graph.py                 |  2 +-\n torch/_dynamo/resume_execution.py             |  6 +-\n torch/_dynamo/symbolic_convert.py             |  4 +-\n torch/_dynamo/test_minifier_common.py         |  6 +-\n torch/_dynamo/variables/builder.py            | 10 ++-\n torch/_dynamo/variables/misc.py               |  2 +-\n torch/_export/verifier.py                     |  8 +--\n torch/_functorch/eager_transforms.py          |  2 +-\n torch/_functorch/pytree_hacks.py              |  2 +-\n torch/_prims/__init__.py                      | 62 +++++++++----------\n torch/_prims/executor.py                      |  2 +-\n torch/_prims/nvfuser_executor.py              |  4 +-\n torch/_prims_common/__init__.py               | 28 ++++-----\n torch/_prims_common/wrappers.py               |  8 +--\n torch/_refs/__init__.py                       | 46 +++++++-------\n torch/_refs/nn/functional/__init__.py         | 16 ++---\n .../migrate_gradual_types/constraint.py       |  1 -\n .../constraint_transformation.py              |  4 +-\n .../migrate_gradual_types/operation.py        |  1 -\n torch/fx/experimental/symbolic_shapes.py      |  4 +-\n .../multipledispatch/dispatcher.py            | 15 ++---\n torch/fx/interpreter.py                       |  2 +-\n torch/fx/passes/utils/matcher_utils.py        |  2 +-\n torch/fx/passes/utils/source_matcher_utils.py |  2 +-\n 47 files changed, 181 insertions(+), 204 deletions(-)\n\ndiff --git a/functorch/benchmarks/operator_authoring.py b/functorch/benchmarks/operator_authoring.py\nindex cbd816e2ad1324..456f5040d759f2 100644\n--- a/functorch/benchmarks/operator_authoring.py\n+++ b/functorch/benchmarks/operator_authoring.py\n@@ -113,7 +113,7 @@ def out_setup(n):\n def test_backwards(make_args, nnc=nnc_add, aten=torch.add):\n     def backwards_setup(n):\n         args = make_args(n)\n-        (grad_var,) = [a for a in args if a.requires_grad]\n+        (grad_var,) = (a for a in args if a.requires_grad)\n         aten(*args).sum().backward()\n         correct = grad_var.grad.clone()\n         grad_var.grad.zero_()\ndiff --git a/functorch/einops/rearrange.py b/functorch/einops/rearrange.py\nindex c45d2063c7114a..f8f60c4917b766 100644\n--- a/functorch/einops/rearrange.py\n+++ b/functorch/einops/rearrange.py\n@@ -108,7 +108,7 @@ class dims.\"\"\"\n \n     custom_rearrange_callable_name = \"do_rearrange\"\n     custom_rearrange_callable_code = (\n-        (\n+\n             f\"def {custom_rearrange_callable_name}(tensor):\\n\"\n             f\"    {comma_separate(first_class_dims)} = dims({n_dims})\\n\"\n             + (\n@@ -120,7 +120,7 @@ class dims.\"\"\"\n                 f\"    return tensor.sum({comma_separate([anon_dims])}, keepdim=False)\\n\"\n                 if anon_dims else \"    return tensor\\n\"\n             )\n-        )\n+\n     )\n \n     exec(custom_rearrange_callable_code)\ndiff --git a/functorch/examples/compilation/linear_train.py b/functorch/examples/compilation/linear_train.py\nindex 2d5f9d7dd37b44..ee84347470835b 100644\n--- a/functorch/examples/compilation/linear_train.py\n+++ b/functorch/examples/compilation/linear_train.py\n@@ -18,7 +18,7 @@ def bench(f, iters=100, warmup=10):\n     begin = time.time()\n     for _ in range(iters):\n         f()\n-    print((time.time() - begin))\n+    print(time.time() - begin)\n \n \n class Foo(nn.Module):\ndiff --git a/functorch/examples/maml_omniglot/support/omniglot_loaders.py b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\nindex ce636ecca0b1b2..6a4369ba4b208f 100644\n--- a/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n+++ b/functorch/examples/maml_omniglot/support/omniglot_loaders.py\n@@ -276,10 +276,10 @@ def load_data_cache(self, data_pack):\n             x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n             y_qrys = np.array(y_qrys).astype(int).reshape(self.batchsz, querysz)\n \n-            x_spts, y_spts, x_qrys, y_qrys = [\n+            x_spts, y_spts, x_qrys, y_qrys = (\n                 torch.from_numpy(z).to(self.device) for z in\n                 [x_spts, y_spts, x_qrys, y_qrys]\n-            ]\n+            )\n \n             data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n \ndiff --git a/functorch/op_analysis/gen_data.py b/functorch/op_analysis/gen_data.py\nindex a9cc84e6f9362c..ab1f3a79125c20 100644\n--- a/functorch/op_analysis/gen_data.py\n+++ b/functorch/op_analysis/gen_data.py\n@@ -23,7 +23,7 @@ def gen_data(special_op_lists, analysis_name):\n     composite_ops = get_ops_for_key('CompositeImplicitAutograd')\n     noncomposite_ops = all_ops - composite_ops\n \n-    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)\n+    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml').read(), Loader=yaml.CLoader)\n \n     annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}\n     from collections import defaultdict\n@@ -132,19 +132,19 @@ def remove_prefix(input_string, prefix):\n \n \n if True:\n-    with open('run_ops.txt', 'r') as f:\n+    with open('run_ops.txt') as f:\n         opinfo_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n-    with open('count_ops.txt', 'r') as f:\n+    with open('count_ops.txt') as f:\n         opinfo_counts = [i.strip() for i in f.readlines()]\n         opinfo_counts = defaultdict(int, dict(zip(opinfo_ops, opinfo_counts)))\n \n     def count_fn(x):\n         return opinfo_counts[x['full_name']]\n \n-    with open('run_decompositions.txt', 'r') as f:\n+    with open('run_decompositions.txt') as f:\n         decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]\n \n-    with open('public_api', 'r') as f:\n+    with open('public_api') as f:\n         ref_api = [i.strip() for i in f.readlines()]\n \n     def has_ref_impl(x):\ndiff --git a/test/dynamo/test_autograd_function.py b/test/dynamo/test_autograd_function.py\nindex 55165edd61a41d..7de264e5051743 100644\n--- a/test/dynamo/test_autograd_function.py\n+++ b/test/dynamo/test_autograd_function.py\n@@ -207,7 +207,7 @@ def backward(ctx, grad_output):\n \n class ModuleWithGradFunc(torch.nn.Module):\n     def __init__(self, func):\n-        super(ModuleWithGradFunc, self).__init__()\n+        super().__init__()\n         self.f = func.apply\n \n     def forward(self, x):\n@@ -336,7 +336,7 @@ def backward(ctx, grad_output):\n \n         class MyMod(torch.nn.Module):\n             def __init__(self):\n-                super(MyMod, self).__init__()\n+                super().__init__()\n                 self.gamma = torch.nn.Parameter(torch.rand([4, 128, 32, 32]))\n \n             def forward(self, x):\ndiff --git a/test/dynamo/test_compile.py b/test/dynamo/test_compile.py\nindex e3847cbb2ae121..5b2de2b7b3867f 100644\n--- a/test/dynamo/test_compile.py\n+++ b/test/dynamo/test_compile.py\n@@ -11,7 +11,7 @@\n \n class ToyModel(torch.nn.Module):\n     def __init__(self):\n-        super(ToyModel, self).__init__()\n+        super().__init__()\n         self.linear = torch.nn.Linear(10, 10)\n         self.relu = torch.nn.ReLU()\n \ndiff --git a/test/dynamo/test_logging.py b/test/dynamo/test_logging.py\nindex 183910f6db3510..eed99681e2c04e 100644\n--- a/test/dynamo/test_logging.py\n+++ b/test/dynamo/test_logging.py\n@@ -157,7 +157,7 @@ def throw(x):\n     def test_ddp_graphs(self, records):\n         class ToyModel(torch.nn.Module):\n             def __init__(self):\n-                super(ToyModel, self).__init__()\n+                super().__init__()\n                 self.layers = torch.nn.Sequential(\n                     torch.nn.Linear(1024, 1024),\n                     torch.nn.Linear(1024, 1024),\ndiff --git a/test/dynamo/test_misc.py b/test/dynamo/test_misc.py\nindex 6b020adc0d8007..6ec0acf8054fd3 100644\n--- a/test/dynamo/test_misc.py\n+++ b/test/dynamo/test_misc.py\n@@ -823,7 +823,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 3)\n@@ -837,7 +837,7 @@ def fn(a, b):\n         v2 = torch.randn((10, 10))\n         correct = fn(v1, v2)\n         cnts = torch._dynamo.testing.CompileCounter()\n-        opt_fn = torch._dynamo.optimize((cnts))(fn)\n+        opt_fn = torch._dynamo.optimize(cnts)(fn)\n         self.assertEqual(opt_fn(v1, v2), correct)\n         self.assertEqual(cnts.frame_count, 1)\n         self.assertEqual(cnts.op_count, 2)\n@@ -2202,7 +2202,7 @@ def fn():\n def fn():\n     foo.bar(1, 2, 3)\n {str(chr(10)).join(' ' * 4 + 'x' + str(i) + ' = 1' for i in range(1 << 9))}\n-    l = [{str(' ').join('x' + str(i) + ',' for i in range(1 << 9))}]\n+    l = [{' '.join('x' + str(i) + ',' for i in range(1 << 9))}]\n         \"\"\"\n         locals = {}\n         exec(fn_str, {}, locals)\n@@ -3087,7 +3087,7 @@ def foo(self, memo=None, prefix=\"\", remove_duplicate=False):\n                     memo=memo, prefix=prefix, remove_duplicate=remove_duplicate\n                 ):\n                     for pn, p in self.named_parameters():\n-                        fpn = \"%s.%s\" % (mn, pn) if mn else pn\n+                        fpn = f\"{mn}.{pn}\" if mn else pn\n                         self.names.append(fpn)\n \n         # Test plain recurse\n@@ -5032,11 +5032,11 @@ def test_compute_exception_table_nested(self):\n             (15, 16, 7),\n             (17, 17, 6),\n         ]\n-        self.assertEquals(len(tab), len(expected))\n+        self.assertEqual(len(tab), len(expected))\n         for entry, exp in zip(tab, expected):\n-            self.assertEquals(entry.start, exp[0] * 2)\n-            self.assertEquals(entry.end, exp[1] * 2)\n-            self.assertEquals(entry.target, exp[2] * 2)\n+            self.assertEqual(entry.start, exp[0] * 2)\n+            self.assertEqual(entry.end, exp[1] * 2)\n+            self.assertEqual(entry.target, exp[2] * 2)\n \n     @skipIfNotPy311\n     def test_remove_dead_code_with_exn_table_entries(self):\n@@ -5060,17 +5060,17 @@ def test_remove_dead_code_with_exn_table_entries(self):\n         )\n         bytecode_transformation.propagate_inst_exn_table_entries(insts)\n         insts = bytecode_analysis.remove_dead_code(insts)\n-        self.assertEquals(len(insts), 5)\n+        self.assertEqual(len(insts), 5)\n         self.assertNotIn(exn_start, insts)\n         self.assertNotIn(exn_end, insts)\n         self.assertIn(target2, insts)\n         self.assertIn(target3, insts)\n         bytecode_transformation.update_offsets(insts)\n         tab = bytecode_transformation.compute_exception_table(insts)\n-        self.assertEquals(len(tab), 1)\n-        self.assertEquals(tab[0].start, 2)\n-        self.assertEquals(tab[0].end, 4)\n-        self.assertEquals(tab[0].target, 6)\n+        self.assertEqual(len(tab), 1)\n+        self.assertEqual(tab[0].start, 2)\n+        self.assertEqual(tab[0].end, 4)\n+        self.assertEqual(tab[0].target, 6)\n \n     def test_unhandled_exception_in_dynamo(self):\n         # traceback.format_exc() approximates an unhandled exception\n@@ -5757,7 +5757,7 @@ def guard(L):\n     def test_dynamo_compiling_fake_tensor_to_vararg_int(self):\n         class MyModule(torch.nn.Module):\n             def __init__(self):\n-                super(MyModule, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 # use numpy int so it's wrapped as fake tensor in dynamo\n@@ -5776,7 +5776,7 @@ def forward(self, x):\n     def test_scalar_tensor_is_equivalent_to_symint_argument(self):\n         class GumbelTopKSampler(torch.nn.Module):\n             def __init__(self, T, k):\n-                super(GumbelTopKSampler, self).__init__()\n+                super().__init__()\n                 self.T = torch.nn.Parameter(\n                     torch.tensor(T, dtype=torch.float32), requires_grad=False\n                 )\n@@ -5803,7 +5803,7 @@ def forward(self, logits):\n     def test_scalar_tensor_is_equivalent_to_symint_list_argument(self):\n         class Jitter(torch.nn.Module):\n             def __init__(self, jitter_val):\n-                super(Jitter, self).__init__()\n+                super().__init__()\n                 self.jitter_val = jitter_val\n \n             def roll_tensor(self, input):\n@@ -5986,7 +5986,7 @@ def _prepare_for_translation_validator(self):\n \n         # Z3 symbols.\n         [validator.add_var(s, int) for s in (s0, s1, s2)]\n-        z0, z1, z2 = [validator.z3var(s) for s in (s0, s1, s2)]\n+        z0, z1, z2 = (validator.z3var(s) for s in (s0, s1, s2))\n \n         return (s0, s1, s2), (z0, z1, z2), validator\n \ndiff --git a/test/dynamo/test_modules.py b/test/dynamo/test_modules.py\nindex 03ef4f07305454..5a881d0053ec1d 100644\n--- a/test/dynamo/test_modules.py\n+++ b/test/dynamo/test_modules.py\n@@ -762,7 +762,7 @@ def forward(self, x):\n \n class ConvCallSuperForwardDirectly(torch.nn.Conv1d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -770,13 +770,13 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n         )\n \n     def forward(self, inputs, mask=None):\n-        outputs = super(ConvCallSuperForwardDirectly, self).forward(inputs)\n+        outputs = super().forward(inputs)\n         return outputs\n \n \n class ConvTransposeCallSuperForwardDirectly(torch.nn.ConvTranspose2d):\n     def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n-        super(ConvTransposeCallSuperForwardDirectly, self).__init__(\n+        super().__init__(\n             in_channels=in_channels,\n             out_channels=out_channels,\n             kernel_size=kernel_size,\n@@ -785,7 +785,7 @@ def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n \n     def forward(self, x):\n         if x.numel() > 0:\n-            return super(ConvTransposeCallSuperForwardDirectly, self).forward(x)\n+            return super().forward(x)\n         output_shape = [\n             ((i - 1) * d - 2 * p + (di * (k - 1) + 1) + op)\n             for i, p, di, k, d, op in zip(\n@@ -923,7 +923,7 @@ def forward(self, x):\n class SequentialWithDuplicatedModule(torch.nn.Module):\n     # Sequential module(self.layer) contains three duplicated ReLU module.\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             torch.nn.Linear(10, 20),\n@@ -940,7 +940,7 @@ def forward(self, x):\n \n class SequentialWithDuplicatedModule2(torch.nn.Module):\n     def __init__(self):\n-        super(SequentialWithDuplicatedModule2, self).__init__()\n+        super().__init__()\n         self.relu = torch.nn.ReLU()\n         self.layer = torch.nn.Sequential(\n             collections.OrderedDict(\ndiff --git a/test/dynamo/test_profiler.py b/test/dynamo/test_profiler.py\nindex 7f58d99863d093..bec7adb33eda98 100644\n--- a/test/dynamo/test_profiler.py\n+++ b/test/dynamo/test_profiler.py\n@@ -20,7 +20,7 @@ def inner_fn(x):\n         def outer_fn(x, y):\n             return inner_fn(x) * y\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             outer_fn(x, y)\n@@ -40,7 +40,7 @@ def test_dynamo_timed_profiling_backend_compile(self):\n         def fn(x, y):\n             return x.sin() * y.cos()\n \n-        x, y = [torch.rand((2, 2)) for _ in range(2)]\n+        x, y = (torch.rand((2, 2)) for _ in range(2))\n \n         with torch.profiler.profile(with_stack=False) as prof:\n             torch._dynamo.optimize(\"aot_eager\")(fn)(x, y)\ndiff --git a/test/dynamo/test_repros.py b/test/dynamo/test_repros.py\nindex 2e84776ea76580..77d8859541472c 100644\n--- a/test/dynamo/test_repros.py\n+++ b/test/dynamo/test_repros.py\n@@ -2632,7 +2632,7 @@ def test_error_return_without_exception_set(self):\n         # https://github.com/pytorch/pytorch/issues/93781\n         @torch.compile\n         def f():\n-            _generator_type = type((_ for _ in ()))\n+            _generator_type = type(_ for _ in ())\n \n         self.assertNoUnraisable(f)\n \ndiff --git a/test/error_messages/storage.py b/test/error_messages/storage.py\nindex f3053d862a220c..b33b86e0908a95 100644\n--- a/test/error_messages/storage.py\n+++ b/test/error_messages/storage.py\n@@ -14,7 +14,7 @@ def check_error(desc, fn, *required_substrings):\n         for sub in required_substrings:\n             assert sub in error_message\n         return\n-    raise AssertionError(\"given function ({}) didn't raise an error\".format(desc))\n+    raise AssertionError(f\"given function ({desc}) didn't raise an error\")\n \n check_error(\n     'Wrong argument types',\ndiff --git a/test/export/test_db.py b/test/export/test_db.py\nindex 10d149e096be9d..bfa57baf214c8f 100644\n--- a/test/export/test_db.py\n+++ b/test/export/test_db.py\n@@ -23,7 +23,7 @@ class ExampleTests(TestCase):\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.SUPPORTED).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -51,7 +51,7 @@ def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n     @parametrize(\n         \"name,case\",\n         filter_examples_by_support_level(SupportLevel.NOT_SUPPORTED_YET).items(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\n@@ -73,7 +73,7 @@ def test_exportdb_not_supported(self, name: str, case: ExportCase) -> None:\n             ).items()\n             for rewrite_case in get_rewrite_cases(case)\n         ],\n-        name_fn=lambda name, case: \"case_{}_{}\".format(name, case.name),\n+        name_fn=lambda name, case: f\"case_{name}_{case.name}\",\n     )\n     def test_exportdb_not_supported_rewrite(\n         self, name: str, rewrite_case: ExportCase\ndiff --git a/test/export/test_serialize.py b/test/export/test_serialize.py\nindex 01bb32ad2c791b..c3942936e2bc55 100644\n--- a/test/export/test_serialize.py\n+++ b/test/export/test_serialize.py\n@@ -361,7 +361,7 @@ def f(x, y):\n     @parametrize(\n         \"name,case\",\n         get_filtered_export_db_tests(),\n-        name_fn=lambda name, case: \"case_{}\".format(name),\n+        name_fn=lambda name, case: f\"case_{name}\",\n     )\n     def test_exportdb_supported(self, name: str, case: ExportCase) -> None:\n         model = case.model\ndiff --git a/test/forward_backward_compatibility/check_forward_backward_compatibility.py b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\nindex 886ad32a24bafb..cf4ce8def1adb7 100644\n--- a/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n+++ b/test/forward_backward_compatibility/check_forward_backward_compatibility.py\n@@ -501,7 +501,7 @@ def check_fc(existing_schemas):\n     args = parser.parse_args()\n     existing_schema_dict = {}\n     slist = []\n-    with open(args.existing_schemas, \"r\") as f:\n+    with open(args.existing_schemas) as f:\n         while True:\n             line = f.readline()\n             if not line:\ndiff --git a/test/functorch/test_aotdispatch.py b/test/functorch/test_aotdispatch.py\nindex e4357ce0bcafd8..ca92201fedc5ea 100644\n--- a/test/functorch/test_aotdispatch.py\n+++ b/test/functorch/test_aotdispatch.py\n@@ -1805,8 +1805,8 @@ def test_batch_norm_amp(self):\n         device = \"cuda\"\n         input_dtype = torch.float16\n         param_dtype = torch.float32\n-        weight, bias = [torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2)]\n-        running_mean, running_var = [torch.ones(64, device=device, dtype=param_dtype) for _ in range(2)]\n+        weight, bias = (torch.ones(64, device=device, dtype=param_dtype, requires_grad=True) for _ in range(2))\n+        running_mean, running_var = (torch.ones(64, device=device, dtype=param_dtype) for _ in range(2))\n \n         def bn(x):\n             return torch.ops.aten.cudnn_batch_norm(\ndiff --git a/test/functorch/test_vmap.py b/test/functorch/test_vmap.py\nindex a0f6e077004344..81b980edd6d006 100644\n--- a/test/functorch/test_vmap.py\n+++ b/test/functorch/test_vmap.py\n@@ -3438,7 +3438,7 @@ def test():\n             check_shape_only = op.name in ('empty_like', 'new_empty')\n             for sample_input in sample_inputs_itr:\n                 args = (sample_input.input,) + sample_input.args\n-                if not any((isinstance(arg, torch.Tensor) for arg in args)):\n+                if not any(isinstance(arg, torch.Tensor) for arg in args):\n                     # Atleast one tensor required for vmap.\n                     continue\n                 kwargs = sample_input.kwargs\ndiff --git a/test/fx/test_future.py b/test/fx/test_future.py\nindex 4f093de54b4f84..4525f678eaeb6c 100644\n--- a/test/fx/test_future.py\n+++ b/test/fx/test_future.py\n@@ -16,7 +16,7 @@ def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n \n # Forward references\n class M2(torch.nn.Module):\n-    def forward(self, x: 'torch.Tensor', a: 'A') -> 'torch.Tensor':\n+    def forward(self, x: torch.Tensor, a: A) -> torch.Tensor:\n         return a(x)\n \n # Non-torch annotation with no internal forward references\n@@ -26,7 +26,7 @@ def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n \n # Non-torch annotation with internal forward references\n class M4(torch.nn.Module):\n-    def forward(self, x: typing.List['torch.Tensor'], a: A) -> 'torch.Tensor':\n+    def forward(self, x: typing.List[torch.Tensor], a: A) -> torch.Tensor:\n         return a(x[0])\n \n x = torch.rand(2, 3)\ndiff --git a/test/fx/test_gradual_type.py b/test/fx/test_gradual_type.py\nindex 23c6496b3a294f..e3f83756eb2668 100644\n--- a/test/fx/test_gradual_type.py\n+++ b/test/fx/test_gradual_type.py\n@@ -990,12 +990,12 @@ def forward(self, x : TensorType((4, 3, Dyn, Dyn))):\n \n         for n in traced.graph.nodes:\n             if n.target == 'conv1':\n-                assert n.type == TensorType((4, 6, sympy.floor((sympy.symbols('~0') - 4)),\n-                                             sympy.floor((sympy.symbols('~1') - 4))))\n+                assert n.type == TensorType((4, 6, sympy.floor(sympy.symbols('~0') - 4),\n+                                             sympy.floor(sympy.symbols('~1') - 4)))\n \n             elif n.target == 'conv2':\n-                assert n.type == TensorType((4, 16, sympy.floor((sympy.symbols('~4') - 4)),\n-                                             sympy.floor((sympy.symbols('~5') - 4))))\n+                assert n.type == TensorType((4, 16, sympy.floor(sympy.symbols('~4') - 4),\n+                                             sympy.floor(sympy.symbols('~5') - 4)))\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/torch/_decomp/decompositions.py b/torch/_decomp/decompositions.py\nindex 70c69ff5cef47a..f6b268abc72a3f 100644\n--- a/torch/_decomp/decompositions.py\n+++ b/torch/_decomp/decompositions.py\n@@ -1331,10 +1331,10 @@ def native_layer_norm_backward(\n     input_shape = input.shape\n     input_ndim = input.dim()\n     computation_dtype = utils.get_computation_dtype(input.dtype)\n-    grad_out_cast, input_cast, weight_cast, bias_cast = [\n+    grad_out_cast, input_cast, weight_cast, bias_cast = (\n         x.to(computation_dtype).contiguous() if x is not None else x\n         for x in (grad_out, input, weight, bias)\n-    ]\n+    )\n     assert grad_out_cast is not None\n \n     axis = input_ndim - len(normalized_shape)\n@@ -1745,7 +1745,7 @@ def native_batch_norm_backward(\n         running_var_cast,\n         save_mean_cast,\n         save_invstd_cast,\n-    ) = [\n+    ) = (\n         x.to(computation_dtype) if x is not None else x\n         for x in (\n             grad_out,\n@@ -1756,7 +1756,7 @@ def native_batch_norm_backward(\n             save_mean,\n             save_invstd,\n         )\n-    ]\n+    )\n     input_shape = input.shape\n     input_rank = input.dim()\n     assert input_rank >= 2, \"rank of the input must be at least 2\"\n@@ -3123,7 +3123,7 @@ def get_coeff(ofs: int) -> Tensor:\n             )\n             return _upsample_cubic_interp1d(cs, tx.unsqueeze(1))\n \n-        coeffs = tuple((get_coeff(ofs) for ofs in range(4)))\n+        coeffs = tuple(get_coeff(ofs) for ofs in range(4))\n         return _upsample_cubic_interp1d(coeffs, ty.unsqueeze(1))\n \n \n@@ -3371,10 +3371,10 @@ def load_bounded(ys, xs):\n         return aten._unsafe_index(a, [N_idx, C_idx, y_idx, x_idx])\n \n     def get_x_interp(y):\n-        coeffs_x = tuple((load_bounded(y, x_ofs) for x_ofs in ixs_ofs))\n+        coeffs_x = tuple(load_bounded(y, x_ofs) for x_ofs in ixs_ofs)\n         return _upsample_cubic_interp1d(coeffs_x, t_x)\n \n-    coeffs_y = tuple((get_x_interp(y_ofs) for y_ofs in iys_ofs))\n+    coeffs_y = tuple(get_x_interp(y_ofs) for y_ofs in iys_ofs)\n     result = _upsample_cubic_interp1d(coeffs_y, t_y)\n \n     # convert output to correct memory format, if necessary\ndiff --git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py\nindex 4d7c98aa222536..bf7a61b0793654 100644\n--- a/torch/_dynamo/debug_utils.py\n+++ b/torch/_dynamo/debug_utils.py\n@@ -385,11 +385,9 @@ def same_two_models(\n         # This means that the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return True.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph.\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph.\"\n         )\n         return True\n \n@@ -465,11 +463,9 @@ def backend_accuracy_fails(\n         # This means that the the minified graph is bad/exposes a different problem.\n         # As we are checking accuracy here, lets log the exception and return False.\n         log.exception(\n-            (\n-                \"While minifying the program in accuracy minification mode, \"\n-                \"ran into a runtime exception which is likely an unrelated issue.\"\n-                \" Skipping this graph\"\n-            )\n+            \"While minifying the program in accuracy minification mode, \"\n+            \"ran into a runtime exception which is likely an unrelated issue.\"\n+            \" Skipping this graph\"\n         )\n         return False\n \ndiff --git a/torch/_dynamo/eval_frame.py b/torch/_dynamo/eval_frame.py\nindex f55606186a8b7a..0bb1dbf7342a44 100644\n--- a/torch/_dynamo/eval_frame.py\n+++ b/torch/_dynamo/eval_frame.py\n@@ -750,9 +750,7 @@ def __init__(\n \n         self.new_args = []\n         for i in range(0, len(flat_args)):\n-            arg = super(FlattenInputOutputSignature, self).placeholder(\n-                f\"arg{i}\", (), {}\n-            )\n+            arg = super().placeholder(f\"arg{i}\", (), {})\n             if i in matched_input_elements_to_fake:\n                 arg.node.meta[\"val\"] = matched_input_elements_to_fake[i]\n             else:\ndiff --git a/torch/_dynamo/output_graph.py b/torch/_dynamo/output_graph.py\nindex f6527f9b2de356..667b2363cff694 100644\n--- a/torch/_dynamo/output_graph.py\n+++ b/torch/_dynamo/output_graph.py\n@@ -1073,7 +1073,7 @@ class SubgraphTracer(fx.Tracer):\n     \"\"\"\n \n     def __init__(self, output_graph, parent=None):\n-        super(SubgraphTracer, self).__init__()\n+        super().__init__()\n         self.output_graph = weakref.proxy(output_graph)\n         self.graph = torch.fx.Graph()\n         # Map from graph input name to its placeholder proxy object, where the\ndiff --git a/torch/_dynamo/resume_execution.py b/torch/_dynamo/resume_execution.py\nindex a3344f3d69bca8..f3eafba1979470 100644\n--- a/torch/_dynamo/resume_execution.py\n+++ b/torch/_dynamo/resume_execution.py\n@@ -490,13 +490,13 @@ def find_new_offset(\n             instructions: List[Instruction], code_options: Dict[str, Any]\n         ):\n             nonlocal new_offset\n-            (target,) = [i for i in instructions if i.offset == offset]\n+            (target,) = (i for i in instructions if i.offset == offset)\n             # match the functions starting at the last instruction as we have added a prefix\n-            (new_target,) = [\n+            (new_target,) = (\n                 i2\n                 for i1, i2 in zip(reversed(instructions), reversed(meta.instructions))\n                 if i1 is target\n-            ]\n+            )\n             assert target.opcode == new_target.opcode\n             new_offset = new_target.offset\n \ndiff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py\nindex 75a44965b63a3e..46d7526ebbc57d 100644\n--- a/torch/_dynamo/symbolic_convert.py\n+++ b/torch/_dynamo/symbolic_convert.py\n@@ -888,7 +888,7 @@ def resolve_name(self, name, package, level):\n         if len(bits) < level:\n             raise ImportError(\"attempted relative import beyond top-level package\")\n         base = bits[0]\n-        return \"{}.{}\".format(base, name) if name else base\n+        return f\"{base}.{name}\" if name else base\n \n     def calc_package(self):\n         \"\"\"\n@@ -1840,7 +1840,7 @@ def format_frame_summary(self, additional_stack_frames=None):\n             additional_stack_frames = []\n         return \"\".join(\n             traceback.format_list(\n-                ([self.frame_summary()] + list(reversed(additional_stack_frames)))\n+                [self.frame_summary()] + list(reversed(additional_stack_frames))\n             )\n         )\n \ndiff --git a/torch/_dynamo/test_minifier_common.py b/torch/_dynamo/test_minifier_common.py\nindex 757e92d2f23b51..e1eadd6da8a595 100644\n--- a/torch/_dynamo/test_minifier_common.py\n+++ b/torch/_dynamo/test_minifier_common.py\n@@ -86,7 +86,7 @@ def _maybe_subprocess_run(self, args, *, isolate, cwd=None):\n                 args = [\"-c\"]\n             else:\n                 assert len(args) >= 2, args\n-                with open(args[1], \"r\") as f:\n+                with open(args[1]) as f:\n                     code = f.read()\n                 args = args[1:]\n \n@@ -156,7 +156,7 @@ def _run_test_code(self, code, *, isolate):\n     def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n         self.assertIsNotNone(repro_dir)\n         launch_file = os.path.join(repro_dir, \"minifier_launcher.py\")\n-        with open(launch_file, \"r\") as f:\n+        with open(launch_file) as f:\n             launch_code = f.read()\n         self.assertTrue(os.path.exists(launch_file))\n \n@@ -175,7 +175,7 @@ def _run_minifier_launcher(self, repro_dir, isolate, *, minifier_args=()):\n     def _run_repro(self, repro_dir, *, isolate=True):\n         self.assertIsNotNone(repro_dir)\n         repro_file = os.path.join(repro_dir, \"repro.py\")\n-        with open(repro_file, \"r\") as f:\n+        with open(repro_file) as f:\n             repro_code = f.read()\n         self.assertTrue(os.path.exists(repro_file))\n \ndiff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py\nindex d2aab5a65bd4d4..c720a0e637c478 100644\n--- a/torch/_dynamo/variables/builder.py\n+++ b/torch/_dynamo/variables/builder.py\n@@ -368,12 +368,10 @@ def _wrap(self, value):\n         elif istype(\n             value, (dict, collections.defaultdict, collections.OrderedDict)\n         ) and all(\n-            (\n-                ConstantVariable.is_literal(k)\n-                or self.tensor_can_be_dict_key(k)\n-                or isinstance(k, enum.Enum)\n-                for k in value.keys()\n-            )\n+            ConstantVariable.is_literal(k)\n+            or self.tensor_can_be_dict_key(k)\n+            or isinstance(k, enum.Enum)\n+            for k in value.keys()\n         ):\n             if not value and self.get_source().is_nn_module():\n                 # It is faster to guard on 'false' property than to guard\ndiff --git a/torch/_dynamo/variables/misc.py b/torch/_dynamo/variables/misc.py\nindex b260eab0af0fdb..e049ccfb25269d 100644\n--- a/torch/_dynamo/variables/misc.py\n+++ b/torch/_dynamo/variables/misc.py\n@@ -880,7 +880,7 @@ def as_proxy(self):\n # Used to keep track of NULLs pushed on the stack for Python 3.11 function calls\n class NullVariable(VariableTracker):\n     def __init__(self, **kwargs):\n-        super(NullVariable, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n     def __str__(self):\n         return \"NullVariable\"\ndiff --git a/torch/_export/verifier.py b/torch/_export/verifier.py\nindex 41888230e02242..73906bb6a0d7a8 100644\n--- a/torch/_export/verifier.py\n+++ b/torch/_export/verifier.py\n@@ -38,7 +38,7 @@ def _check_is_fake_tensor(val):\n \n     val = node.meta.get(\"val\", None)\n     if val is None or not _check_is_fake_tensor(val):\n-        raise SpecViolationError(\"Node.meta {} is missing val field.\".format(node.name))\n+        raise SpecViolationError(f\"Node.meta {node.name} is missing val field.\")\n \n \n @compatibility(is_backward_compatible=False)\n@@ -71,7 +71,7 @@ def check_valid_op(self, op):\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         # All ops functional\n@@ -87,7 +87,7 @@ def check_valid(self, gm: GraphModule) -> None:  # noqa: C901\n             # TODO(T140410192): should have fake tensor for all dialects\n             if node.op in {\"call_module\", \"call_method\"}:\n                 raise SpecViolationError(\n-                    \"call_module is not valid: got a class '{}' \".format(node.target),\n+                    f\"call_module is not valid: got a class '{node.target}' \",\n                 )\n \n             if node.op == \"call_function\":\n@@ -122,7 +122,7 @@ def check_valid_op(self, op) -> None:\n \n         if not isinstance(op, OpOverload):\n             raise SpecViolationError(\n-                \"Operator '{}' is not a registered Op\".format(op_name),\n+                f\"Operator '{op_name}' is not a registered Op\",\n             )\n \n         if (\ndiff --git a/torch/_functorch/eager_transforms.py b/torch/_functorch/eager_transforms.py\nindex a25a7bc456bec1..4ba72eb2152c6b 100644\n--- a/torch/_functorch/eager_transforms.py\n+++ b/torch/_functorch/eager_transforms.py\n@@ -548,7 +548,7 @@ def compute_jacobian_stacked():\n             # Iterate and concat the jacobians of different\n             # inputs.\n             for idx in range(len(flat_primals)):\n-                r = tuple((r_[idx] for r_ in chunked_results))\n+                r = tuple(r_[idx] for r_ in chunked_results)\n                 flat_results.append(torch.cat(r, 0))\n \n             return flat_results\ndiff --git a/torch/_functorch/pytree_hacks.py b/torch/_functorch/pytree_hacks.py\nindex 3694a53d7debb0..61bcdfbbf38b16 100644\n--- a/torch/_functorch/pytree_hacks.py\n+++ b/torch/_functorch/pytree_hacks.py\n@@ -13,7 +13,7 @@ def tree_map_(fn_, pytree):\n     return pytree\n \n \n-class PlaceHolder():\n+class PlaceHolder:\n     def __repr__(self):\n         return '*'\n \ndiff --git a/torch/_prims/__init__.py b/torch/_prims/__init__.py\nindex 7e8a37da76b06d..ac447dab410a07 100644\n--- a/torch/_prims/__init__.py\n+++ b/torch/_prims/__init__.py\n@@ -1437,7 +1437,7 @@ def expand_dims(\n     else:\n         dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]\n     if len(set(dims)) != len(dims):\n-        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))\n+        msg = f\"Received duplicate dimensions to expand in {str(dimensions)}\"\n         raise ValueError(msg)\n \n     new_shape = list(a.shape)\n@@ -1463,35 +1463,33 @@ def _slice_meta(\n     _strides = strides if strides is not None else [1] * len(start_indices)\n \n     if a.ndim != len(start_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with start_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with start_indices of length {}!\".format(\n             a.ndim, len(start_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(limit_indices):\n-        msg = \"Attempting to slice tensor of rank {0} with limit_indices of length {1}!\".format(\n+        msg = \"Attempting to slice tensor of rank {} with limit_indices of length {}!\".format(\n             a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     if a.ndim != len(_strides):\n-        msg = (\n-            \"Attempting to slice tensor of rank {0} with strides of length {1}!\".format(\n-                a.ndim, len(limit_indices)\n-            )\n+        msg = \"Attempting to slice tensor of rank {} with strides of length {}!\".format(\n+            a.ndim, len(limit_indices)\n         )\n         raise ValueError(msg)\n \n     for x, y in zip(start_indices, a.shape):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative start index of {0}!\".format(\n+            msg = \"Attempting to slice a tensor with a negative start index of {}!\".format(\n                 x\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than\"\n-                \" the length of its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a start index in {} is greater than\"\n+                \" the length of its corresponding dimension in shape {}\".format(\n                     start_indices, a.shape\n                 )\n             )\n@@ -1499,30 +1497,30 @@ def _slice_meta(\n \n     for x, y, z in zip(limit_indices, a.shape, start_indices):\n         if x < 0:\n-            msg = \"Attempting to slice a tensor with a negative stop index of {0}!\".format(\n-                x\n+            msg = (\n+                \"Attempting to slice a tensor with a negative stop index of {}!\".format(\n+                    x\n+                )\n             )\n             raise ValueError(msg)\n         if x > y:\n             msg = (\n-                \"Attempting to slice a tensor but a stop index in {0} is greater than the length of \"\n-                \" its corresponding dimension in shape {1}\".format(\n+                \"Attempting to slice a tensor but a stop index in {} is greater than the length of \"\n+                \" its corresponding dimension in shape {}\".format(\n                     limit_indices, a.shape\n                 )\n             )\n             raise ValueError(msg)\n         if x < z:\n             msg = (\n-                \"Attempting to slice a tensor but a start index in {0} is greater than \"\n-                \" its corresponding stop index {1}\".format(x, z)\n+                \"Attempting to slice a tensor but a start index in {} is greater than \"\n+                \" its corresponding stop index {}\".format(x, z)\n             )\n \n     for x in _strides:\n         if x <= 0:\n-            msg = (\n-                \"Attempting to slice a tensor with a non-positive step of {0}!\".format(\n-                    x\n-                )\n+            msg = \"Attempting to slice a tensor with a non-positive step of {}!\".format(\n+                x\n             )\n             raise ValueError(msg)\n \n@@ -1581,38 +1579,38 @@ def _slice_in_dim_meta(\n     axis: int = 0,\n ) -> TensorLikeType:\n     if axis < 0:\n-        msg = \"slice_in_dim: received a negative axis {0}\".format(axis)\n+        msg = f\"slice_in_dim: received a negative axis {axis}\"\n         raise ValueError(msg)\n     if axis >= a.ndim:\n-        msg = \"slice_in_dim: axis {0} is greater or equal to the rank {1} of the tensor\".format(\n+        msg = \"slice_in_dim: axis {} is greater or equal to the rank {} of the tensor\".format(\n             axis, a.ndim\n         )\n         raise ValueError(msg)\n \n     if start_index < 0:\n-        msg = \"slice_in_dim: received a negative start_index {0}\".format(start_index)\n+        msg = f\"slice_in_dim: received a negative start_index {start_index}\"\n         raise ValueError(msg)\n \n     if start_index > a.shape[axis]:\n-        msg = \"slice_in_dim: start_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: start_index is greater than the length {} of dimension {}\".format(\n             start_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index > a.shape[axis]:\n-        msg = \"slice_in_dim: limit_index is greater than the length {0} of dimension {1}\".format(\n+        msg = \"slice_in_dim: limit_index is greater than the length {} of dimension {}\".format(\n             limit_index, axis\n         )\n         raise ValueError(msg)\n \n     if limit_index < start_index:\n-        msg = \"slice_in_dim: received a limit_index {0} less than the start_index {1}\".format(\n+        msg = \"slice_in_dim: received a limit_index {} less than the start_index {}\".format(\n             limit_index, start_index\n         )\n         raise ValueError(msg)\n \n     if stride < 0:\n-        msg = \"slice_in_dim: received a non-positive stride of {0}!\".format(stride)\n+        msg = f\"slice_in_dim: received a non-positive stride of {stride}!\"\n         raise ValueError(msg)\n \n     start_indices = [0] * a.ndim\n@@ -1667,7 +1665,7 @@ def _split_dim_meta(a: TensorLikeType, dim: int, outer_length: int) -> TensorLik\n     inner_length = a.shape[dim] // outer_length\n \n     if (a.shape[dim] % outer_length) != 0:\n-        msg = \"Attempting to split dimension of length {0}, but outer length of {1} divides it with a remainder!\".format(\n+        msg = \"Attempting to split dimension of length {}, but outer length of {} divides it with a remainder!\".format(\n             a.shape[dim], outer_length\n         )\n         raise ValueError(msg)\n@@ -1746,13 +1744,13 @@ def _squeeze_meta(a: TensorLikeType, dimensions: Sequence) -> TensorLikeType:\n \n def _transpose_meta(a: TensorLikeType, permutation: DimsSequenceType) -> TensorLikeType:\n     if a.ndim != len(permutation):\n-        msg = \"Attempting to permute a tensor of rank {0}, but received a permutation of length {1}!\".format(\n+        msg = \"Attempting to permute a tensor of rank {}, but received a permutation of length {}!\".format(\n             a.ndim, len(permutation)\n         )\n         raise ValueError(msg)\n \n     if not utils.is_valid_permutation(a.ndim, permutation):\n-        msg = \"Received an invalid permutation, {0}!\".format(permutation)\n+        msg = f\"Received an invalid permutation, {permutation}!\"\n         raise ValueError(msg)\n \n     new_shape = [0] * a.ndim\n@@ -1938,7 +1936,7 @@ def _reshape_meta(a: TensorLikeType, shape: ShapeType):\n     # same number of elements\n     numel = reduce(operator.mul, shape)\n     if numel != a.numel():\n-        msg = \"Attempting to reshape a tensor with {0} elements to a shape with {1} elements!\".format(\n+        msg = \"Attempting to reshape a tensor with {} elements to a shape with {} elements!\".format(\n             a.numel(), numel\n         )\n         raise ValueError(msg)\n@@ -2190,7 +2188,7 @@ def _copy_to_meta(a: TensorLikeType, b: TensorLikeType):\n \n     # Validates the tensors have the same number of elements\n     if a.numel() != b.numel():\n-        msg = \"Attempting to copy {0} elements to a tensor with {1} elements!\".format(\n+        msg = \"Attempting to copy {} elements to a tensor with {} elements!\".format(\n             b.numel(), a.numel()\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_prims/executor.py b/torch/_prims/executor.py\nindex 2d8d815f063809..325ac67a665cc3 100644\n--- a/torch/_prims/executor.py\n+++ b/torch/_prims/executor.py\n@@ -28,7 +28,7 @@ def execute(\n     elif executor == \"strictly_nvfuser\":\n         return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)\n \n-    msg = \"Received unexpected value for 'executor': {0}. Allowed values are: aten, nvfuser.\".format(\n+    msg = \"Received unexpected value for 'executor': {}. Allowed values are: aten, nvfuser.\".format(\n         executor\n     )\n     raise ValueError(msg)\ndiff --git a/torch/_prims/nvfuser_executor.py b/torch/_prims/nvfuser_executor.py\nindex d0f51e928650c4..c1e61c1bb72f1f 100644\n--- a/torch/_prims/nvfuser_executor.py\n+++ b/torch/_prims/nvfuser_executor.py\n@@ -282,7 +282,7 @@ def nvfuser_execute(gm: GraphModule, *args, executor_parameters=None):\n \n         if get_nvprim_dump_nvtx():\n             torch.cuda.nvtx.range_push(\n-                \"fusion: {0}, graph: {1}\".format(\n+                \"fusion: {}, graph: {}\".format(\n                     fusion.id(),\n                     str(\n                         [\n@@ -475,7 +475,7 @@ def maybe_partition_graph(\n class NVTXInterpreter(torch.fx.Interpreter):\n     def run_node(self, n):\n         torch.cuda.nvtx.range_push(\n-            \"name: {0}, args: {1}, op: {2}, kwargs: {3}\".format(\n+            \"name: {}, args: {}, op: {}, kwargs: {}\".format(\n                 n.name, n.args, n.op, n.kwargs\n             )\n         )\ndiff --git a/torch/_prims_common/__init__.py b/torch/_prims_common/__init__.py\nindex f8033bb5780f74..4800966f3e2ff5 100644\n--- a/torch/_prims_common/__init__.py\n+++ b/torch/_prims_common/__init__.py\n@@ -120,11 +120,11 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n     assert isinstance(b, TensorLike)\n \n     if not same_shape(a.shape, b.shape):\n-        msg = \"Shapes {0} and {1} are not equal!\".format(a.shape, b.shape)\n+        msg = f\"Shapes {a.shape} and {b.shape} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.dtype != b.dtype:\n-        msg = \"Dtypes {0} and {1} are not equal!\".format(a.dtype, b.dtype)\n+        msg = f\"Dtypes {a.dtype} and {b.dtype} are not equal!\"\n         raise AssertionError(msg)\n \n     if a.device != b.device:\n@@ -135,7 +135,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         ):\n             pass\n         else:\n-            msg = \"Devices {0} and {1} are not equal!\".format(a.device, b.device)\n+            msg = f\"Devices {a.device} and {b.device} are not equal!\"\n             raise AssertionError(msg)\n \n     # Stride checking is currently disabled, see https://github.com/pytorch/pytorch/issues/78050\n@@ -143,7 +143,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n         same_strides, idx = check_significant_strides(a, b)\n         if not same_strides:\n             msg = (\n-                \"Stride mismatch! Strides are {0} and {1} (mismatched at {2})!\".format(\n+                \"Stride mismatch! Strides are {} and {} (mismatched at {})!\".format(\n                     a.stride(), b.stride(), idx\n                 )\n             )\n@@ -151,7 +151,7 @@ def compare_tensor_meta(a: TensorLikeType, b: TensorLikeType, check_strides=Fals\n \n         if a.storage_offset() != b.storage_offset():\n             msg = (\n-                \"Storage offset mismatch! Storage offsets are {0} and {1}!\".format(\n+                \"Storage offset mismatch! Storage offsets are {} and {}!\".format(\n                     a.storage_offset(), b.storage_offset()\n                 )\n             )\n@@ -584,7 +584,7 @@ def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int:\n \n     if _idx < 0 or _idx >= rank:\n         # Same error message as in aten/src/ATen/WrapDimUtils.h:49\n-        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(\n+        msg = \"Dimension out of range (expected to be in range of [{}, {}], but got {})\".format(\n             -rank, rank - 1, idx\n         )\n         raise IndexError(msg)\n@@ -710,7 +710,7 @@ def check_same_shape(*args, allow_cpu_scalar_tensors: bool):\n                 shape = arg.shape\n \n             if not is_same_shape(shape, arg.shape):\n-                msg = \"Shape {0} is not the expected shape {1}!\".format(\n+                msg = \"Shape {} is not the expected shape {}!\".format(\n                     arg.shape, shape\n                 )\n                 raise RuntimeError(msg)\n@@ -1102,7 +1102,7 @@ def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool:\n         if fn(cast_from):\n             return False\n \n-    raise ValueError(\"Received unknown dtypes {0}, {1}!\".format(cast_to, cast_from))\n+    raise ValueError(f\"Received unknown dtypes {cast_to}, {cast_from}!\")\n \n \n def check_same_dtype(*args):\n@@ -1340,7 +1340,7 @@ def elementwise_dtypes(\n     for x in args:\n         if not isinstance(x, (Number, TensorLike, sympy.Symbol)):\n             msg = (\n-                \"Unexpected type {0} when computing elementwise type promotion!\".format(\n+                \"Unexpected type {} when computing elementwise type promotion!\".format(\n                     str(type(x))\n                 )\n             )\n@@ -1424,7 +1424,7 @@ def _find_highest_dtype_filtered(\n         return get_computation_dtype(result_dtype), torch.bool\n     else:\n         raise ValueError(\n-            \"Unknown type promotion kind {0}\".format(str(type_promotion_kind))\n+            f\"Unknown type promotion kind {str(type_promotion_kind)}\"\n         )\n \n \n@@ -1648,8 +1648,8 @@ def check_in_bounds_for_storage(\n     required_length = compute_required_storage_length(shape, strides, storage_offset)\n     if a.size() < required_length:\n         msg = (\n-            \"Can't view a storage of size {0} with an offset of {1}, shape of {2}, and strides of {3}, \"\n-            \"which requires a storage of size {4}\".format(\n+            \"Can't view a storage of size {} with an offset of {}, shape of {}, and strides of {}, \"\n+            \"which requires a storage of size {}\".format(\n                 a.size(), storage_offset, str(shape), str(strides), required_length\n             )\n         )\n@@ -1671,9 +1671,9 @@ def check(\n     .. note:: This function is planned for removal in the future. Please use\n         `torch._check*` functions instead.\n     \"\"\"\n-    warnings.warn(DeprecationWarning((\n+    warnings.warn(DeprecationWarning(\n         \"'torch._prims_common.check' will be removed in the future. Please use \"\n-        \"'torch._check*' functions instead\")))\n+        \"'torch._check*' functions instead\"))\n     torch._check_with(exc_type, b, s)\n \n \ndiff --git a/torch/_prims_common/wrappers.py b/torch/_prims_common/wrappers.py\nindex 938465cac36318..c9755de3e0da63 100644\n--- a/torch/_prims_common/wrappers.py\n+++ b/torch/_prims_common/wrappers.py\n@@ -48,16 +48,16 @@ def _maybe_convert_to_dtype(a, dtype):\n         return None\n \n     raise ValueError(\n-        \"Received type {0} that is neither a tensor or a number!\".format(type(a))\n+        f\"Received type {type(a)} that is neither a tensor or a number!\"\n     )\n \n \n def _maybe_convert_to_type(a: NumberType, typ: type) -> NumberType:\n     if not isinstance(a, Number):\n-        msg = \"Found unknown type {0} when trying to convert scalars!\".format(type(a))\n+        msg = f\"Found unknown type {type(a)} when trying to convert scalars!\"\n         raise ValueError(msg)\n     if not utils.is_weakly_lesser_type(type(a), typ):\n-        msg = \"Scalar {0} of type {1} cannot be safely cast to type {2}!\".format(\n+        msg = \"Scalar {} of type {} cannot be safely cast to type {}!\".format(\n             a, type(a), typ\n         )\n         raise ValueError(msg)\n@@ -169,7 +169,7 @@ def _safe_copy_out(\n ):\n     # Checks same device\n     if copy_from.device != copy_to.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             copy_from.device, copy_to.device\n         )\n         raise RuntimeError(msg)\ndiff --git a/torch/_refs/__init__.py b/torch/_refs/__init__.py\nindex 611c6e3b9a625a..53851d62a5eb44 100644\n--- a/torch/_refs/__init__.py\n+++ b/torch/_refs/__init__.py\n@@ -597,7 +597,7 @@ def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType:\n \n     python_type = utils.dtype_to_type(a.dtype)\n     if not utils.is_weakly_lesser_type(type(value), python_type):\n-        msg = \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+        msg = \"value argument of type {} cannot be safely cast to type {}!\".format(\n             type(value), python_type\n         )\n         raise ValueError(msg)\n@@ -997,10 +997,8 @@ def add(\n         if python_type != bool and not utils.is_weakly_lesser_type(\n             type(alpha), python_type\n         ):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         b = prims.mul(b, alpha)\n@@ -1069,7 +1067,7 @@ def copysign(\n     if isinstance(b, Number) and isinstance(a, Tensor):\n         b = scalar_tensor(b, dtype=a.dtype, device=a.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n-        msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+        msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n             a.device, b.device\n         )\n         raise RuntimeError(msg)\n@@ -1100,7 +1098,7 @@ def div(\n     else:\n         msg = (\n             \"div expected rounding_mode to be one of None, 'trunc', or 'floor' \"\n-            \"but found {0}.\".format(rounding_mode)\n+            \"but found {}.\".format(rounding_mode)\n         )\n         raise ValueError(msg)\n \n@@ -1218,7 +1216,7 @@ def floor_divide(\n         a = scalar_tensor(a, dtype=b.dtype, device=b.device)\n     elif isinstance(a, Tensor) and isinstance(b, Tensor) and a.device != b.device:\n         if a.device == torch.device(\"cpu\"):\n-            msg = \"Expected divisor (b) to be on the same device ({0}) as dividend (a), but it is found on {1}!\".format(\n+            msg = \"Expected divisor (b) to be on the same device ({}) as dividend (a), but it is found on {}!\".format(\n                 a.device, b.device\n             )\n             raise RuntimeError(msg)\n@@ -1378,19 +1376,19 @@ def _check_close_args(\n ) -> None:\n     torch._check_value(\n         a.dtype == b.dtype,\n-        lambda: \"{0}: Attempting to compare tensors of different dtypes {1} and {2}!\".format(\n+        lambda: \"{}: Attempting to compare tensors of different dtypes {} and {}!\".format(\n             name, a.dtype, b.dtype\n         ),\n     )\n     torch._check(\n         rtol >= 0,\n-        lambda: \"{0}: rtol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: rtol must be greater than or equal to zero, but got {}!\".format(\n             name, rtol\n         ),\n     )\n     torch._check(\n         atol >= 0,\n-        lambda: \"{0}: atol must be greater than or equal to zero, but got {1}!\".format(\n+        lambda: \"{}: atol must be greater than or equal to zero, but got {}!\".format(\n             name, atol\n         ),\n     )\n@@ -1664,10 +1662,8 @@ def sub(\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         if isinstance(b, torch.Tensor):\n@@ -1759,7 +1755,7 @@ def addcdiv(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1788,7 +1784,7 @@ def addcmul(\n         python_type = utils.dtype_to_type(dtype)\n         torch._check_value(\n             utils.is_weakly_lesser_type(type(value), python_type),\n-            lambda: \"value argument of type {0} cannot be safely cast to type {1}!\".format(\n+            lambda: \"value argument of type {} cannot be safely cast to type {}!\".format(\n                 type(value), python_type\n             ),\n         )\n@@ -1892,7 +1888,7 @@ def clone(\n \n def copy_to(a: Tensor, b: Tensor, *, allow_cross_device=True):\n     if not allow_cross_device and a.device != b.device:\n-        msg = \"Attempting to copy from device {0} to device {1}, but cross-device copies are not allowed!\".format(\n+        msg = \"Attempting to copy from device {} to device {}, but cross-device copies are not allowed!\".format(\n             b.device, a.device\n         )\n         raise RuntimeError(msg)\n@@ -2098,7 +2094,7 @@ def _reduction(\n     assert isinstance(a, TensorLike)\n     if a.ndim > 64:\n         raise RuntimeError(\n-            \"Received a tensor with {0} dimensions, but only tensors with up to 64 dims are supported!\".format(\n+            \"Received a tensor with {} dimensions, but only tensors with up to 64 dims are supported!\".format(\n                 a.ndim\n             )\n         )\n@@ -2864,7 +2860,7 @@ def expand_as(a: Tensor, b: Tensor) -> Tensor:\n \n def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> Tuple[TensorLikeType, ...]:\n     if chunks <= 0:\n-        msg = \"Expected at least one chunk, but got {0}!\".format(chunks)\n+        msg = f\"Expected at least one chunk, but got {chunks}!\"\n         raise ValueError(msg)\n \n     dim = utils.canonicalize_dim(a.ndim, dim)\n@@ -3346,7 +3342,7 @@ def _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorL\n                 if allow_copy:\n                     return prims.reshape(a, shape)\n \n-                msg = \"Cannot view a tensor with shape {0} and strides {1} as a tensor with shape {2}!\".format(\n+                msg = \"Cannot view a tensor with shape {} and strides {} as a tensor with shape {}!\".format(\n                     a.shape, a.stride(), shape\n                 )\n                 raise ValueError(msg)\n@@ -3704,13 +3700,13 @@ def tensor_split(\n     # If indices_or_sections is a tensor, it must be a CPU Long tensor\n     if isinstance(indices_or_sections, TensorLike):\n         if not indices_or_sections.device.type == \"cpu\":\n-            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {0}\".format(\n+            msg = \"tensor_split: if indices_or_sections is a tensor it must be on the CPU, but received one on {}\".format(\n                 indices_or_sections.device\n             )\n             raise ValueError(msg)\n         if indices_or_sections.dtype != torch.long:\n             msg = \"tensor_split: if indices_or_sections is a tensor it must have long dtype, \"\n-            \" but received one with dtype {0}\".format(indices_or_sections.dtype)\n+            f\" but received one with dtype {indices_or_sections.dtype}\"\n             raise ValueError(msg)\n \n     # Case 0 -- indices_or_sections is an integer or a scalar tensor n and a is split along dim into n parts of equal-ish length\n@@ -3724,7 +3720,7 @@ def tensor_split(\n         )\n \n         if sections <= 0:\n-            msg = \"tensor_split: number of sections must be greater than 0, but was {0}\".format(\n+            msg = \"tensor_split: number of sections must be greater than 0, but was {}\".format(\n                 sections\n             )\n             raise ValueError(msg)\n@@ -3751,7 +3747,7 @@ def tensor_split(\n         if isinstance(indices_or_sections, TensorLike):\n             if indices_or_sections.ndim != 1:\n                 msg = \"tensor_split: non-scalar indices_or_sections tensors must have only one dimension, \"\n-                \"but received a tensor with {0} dimensions\".format(\n+                \"but received a tensor with {} dimensions\".format(\n                     indices_or_sections.ndim\n                 )\n                 raise ValueError(msg)\ndiff --git a/torch/_refs/nn/functional/__init__.py b/torch/_refs/nn/functional/__init__.py\nindex eaa6618379f356..ba00179c4b2d6f 100644\n--- a/torch/_refs/nn/functional/__init__.py\n+++ b/torch/_refs/nn/functional/__init__.py\n@@ -167,10 +167,8 @@ def celu(\n     if alpha is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(alpha), python_type):\n-            msg = (\n-                \"alpha argument of type {0} cannot be safely cast to type {1}!\".format(\n-                    type(alpha), python_type\n-                )\n+            msg = \"alpha argument of type {} cannot be safely cast to type {}!\".format(\n+                type(alpha), python_type\n             )\n             raise ValueError(msg)\n         rhs = alpha * torch.expm1(torch.true_divide(a, alpha))  # type: ignore[arg-type]\n@@ -437,7 +435,7 @@ def softplus(\n     if beta is not None:\n         python_type = utils.dtype_to_type(a.dtype)\n         if not utils.is_weakly_lesser_type(type(beta), python_type):\n-            msg = \"beta argument of type {0} cannot be safely cast to type {1}!\".format(\n+            msg = \"beta argument of type {} cannot be safely cast to type {}!\".format(\n                 type(beta), python_type\n             )\n             raise ValueError(msg)\n@@ -610,11 +608,9 @@ def margin_ranking_loss(\n     # loss_without_reduction = max(0, \u2212target * (input1 \u2212 input2) + margin)\n     if input1.ndim != input2.ndim or input1.ndim != target.ndim:\n         raise RuntimeError(\n-            (\n-                \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n-                \"input1: {}, input2: {}, target: {} \".format(\n-                    input1.shape, input2.shape, target.shape\n-                )\n+            \"margin_ranking_loss : All input tensors should have same dimension but got sizes: \"\n+            \"input1: {}, input2: {}, target: {} \".format(\n+                input1.shape, input2.shape, target.shape\n             )\n         )\n     _check_reduction_value(reduction)\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint.py b/torch/fx/experimental/migrate_gradual_types/constraint.py\nindex bab7c62347bbd9..0f0d23d0187490 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from torch.fx.experimental.migrate_gradual_types.operation import op_add, op_sub, op_mul, op_div, \\\n     op_mod, op_gt, op_lt, op_neq, op_eq\n from torch.fx.tensor_type import TensorType, Dyn\ndiff --git a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\nindex fc1fae790d8300..153a8407fc4113 100644\n--- a/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/constraint_transformation.py\n@@ -75,7 +75,7 @@ def transform_index_select(constraint, counter):\n     # if the index is valid then replace the input dimension with the new dimension\n     # otherwise the dimension will not be replaced and the clause will contain False\n     if is_valid_index == T():\n-        new_dims = copy.deepcopy((dims))\n+        new_dims = copy.deepcopy(dims)\n         new_dims[constraint.index] = constraint.dim_replace\n \n     transformed_constraint = Conj([BinConstraintT(constraint.input_var, TensorType(dims), op_eq),\n@@ -803,7 +803,7 @@ def apply_padding(e1_var: TVar,\n         broadcast_padding = []\n \n         # for every padding size, we also consider broadcasting\n-        for j in range((len(d2) - i)):\n+        for j in range(len(d2) - i):\n             broadcast_padding.append(broadcast_dim(simulate_padding, d2, d11, d12, j, True))\n \n         # we consider the possibilities for broadcasting for every dimension. Since we already\ndiff --git a/torch/fx/experimental/migrate_gradual_types/operation.py b/torch/fx/experimental/migrate_gradual_types/operation.py\nindex 68bba2d59a7608..ec2cb91bbcc179 100644\n--- a/torch/fx/experimental/migrate_gradual_types/operation.py\n+++ b/torch/fx/experimental/migrate_gradual_types/operation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n op_add = '+'\n op_sub = '-'\n op_mul = '*'\ndiff --git a/torch/fx/experimental/symbolic_shapes.py b/torch/fx/experimental/symbolic_shapes.py\nindex 90a3c519b3c44b..70762cf1f81b8c 100644\n--- a/torch/fx/experimental/symbolic_shapes.py\n+++ b/torch/fx/experimental/symbolic_shapes.py\n@@ -1488,7 +1488,7 @@ def _print_Symbol(self, expr) -> str:\n         return self.print_source(self.symbol_to_source[expr][0])\n \n     def _print_Relational(self, expr):\n-        return '%s %s %s' % (\n+        return '{} {} {}'.format(\n             self.parenthesize(expr.lhs, precedence(expr)),\n             expr.rel_op,\n             self.parenthesize(expr.rhs, precedence(expr))\n@@ -1887,7 +1887,7 @@ def print_results(grouped, indent, result_fn):\n class ShapeEnvLoggerAdapter(logging.LoggerAdapter):\n     def process(self, msg, kwargs):\n         # TODO: Maybe suppress the envid if not DEBUG?\n-        return '%s: %s' % (self.extra['envid'], msg), kwargs\n+        return '{}: {}'.format(self.extra['envid'], msg), kwargs\n \n \n ENV_COUNTER = collections.Counter()\ndiff --git a/torch/fx/experimental/unification/multipledispatch/dispatcher.py b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\nindex c76d8c60b097c7..ac8bc7d8dd159c 100644\n--- a/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n+++ b/torch/fx/experimental/unification/multipledispatch/dispatcher.py\n@@ -205,10 +205,9 @@ def add(self, signature, func):\n             if not isinstance(typ, (type, list)):\n                 str_sig = ', '.join(c.__name__ if isinstance(c, type)\n                                     else str(c) for c in signature)\n-                raise TypeError(\"Tried to dispatch on non-type: %s\\n\"\n-                                \"In signature: <%s>\\n\"\n-                                \"In function: %s\" %\n-                                (typ, str_sig, self.name))\n+                raise TypeError(\"Tried to dispatch on non-type: {}\\n\"\n+                                \"In signature: <{}>\\n\"\n+                                \"In function: {}\".format(typ, str_sig, self.name))\n \n             # handle variadic signatures\n             if isinstance(typ, list):\n@@ -257,8 +256,7 @@ def __call__(self, *args, **kwargs):\n             func = self.dispatch(*types)\n             if not func:\n                 raise NotImplementedError(\n-                    'Could not find signature for %s: <%s>' %\n-                    (self.name, str_signature(types))) from e\n+                    f'Could not find signature for {self.name}: <{str_signature(types)}>') from e\n             self._cache[types] = func\n         try:\n             return func(*args, **kwargs)\n@@ -274,7 +272,7 @@ def __call__(self, *args, **kwargs):\n \n             raise NotImplementedError(\n                 \"Matching functions for \"\n-                \"%s: <%s> found, but none completed successfully\" % (\n+                \"{}: <{}> found, but none completed successfully\".format(\n                     self.name, str_signature(types),),) from e\n \n     def __str__(self):\n@@ -408,8 +406,7 @@ def __call__(self, *args, **kwargs):\n         types = tuple([type(arg) for arg in args])\n         func = self.dispatch(*types)\n         if not func:\n-            raise NotImplementedError('Could not find signature for %s: <%s>' %\n-                                      (self.name, str_signature(types)))\n+            raise NotImplementedError(f'Could not find signature for {self.name}: <{str_signature(types)}>')\n         return func(self.obj, *args, **kwargs)\n \n \ndiff --git a/torch/fx/interpreter.py b/torch/fx/interpreter.py\nindex 7bc5e55288b03c..6ee5706f92ee34 100644\n--- a/torch/fx/interpreter.py\n+++ b/torch/fx/interpreter.py\n@@ -139,7 +139,7 @@ def run(self, *args, initial_env : Optional[Dict[Node, Any]] = None, enable_io_p\n             except Exception as e:\n                 if self.extra_traceback:\n                     msg = f\"While executing {node.format_node()}\"\n-                    msg = '{}\\n\\n{}'.format(e.args[0], msg) if e.args else str(msg)\n+                    msg = f'{e.args[0]}\\n\\n{msg}' if e.args else str(msg)\n                     msg += f\"\\nOriginal traceback:\\n{node.stack_trace}\"\n                     e.args = (msg,) + e.args[1:]\n                     if isinstance(e, KeyError):\ndiff --git a/torch/fx/passes/utils/matcher_utils.py b/torch/fx/passes/utils/matcher_utils.py\nindex 1037b48d8eb61d..8b66561a6e280d 100644\n--- a/torch/fx/passes/utils/matcher_utils.py\n+++ b/torch/fx/passes/utils/matcher_utils.py\n@@ -30,7 +30,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class InternalMatch():\n+class InternalMatch:\n     # Nodes from which the match was found\n     anchors: List[Node]\n     # Maps nodes in the pattern subgraph to nodes in the larger graph\ndiff --git a/torch/fx/passes/utils/source_matcher_utils.py b/torch/fx/passes/utils/source_matcher_utils.py\nindex e00b9695742e36..da8cf9f0f168c4 100644\n--- a/torch/fx/passes/utils/source_matcher_utils.py\n+++ b/torch/fx/passes/utils/source_matcher_utils.py\n@@ -29,7 +29,7 @@ def _init_logger():\n \n @compatibility(is_backward_compatible=False)\n @dataclass\n-class SourcePartition():\n+class SourcePartition:\n     # Nodes in a particular partition\n     nodes: List[Node]\n \n"
  },
  {
    "number": 105391,
    "title": "[BE] Enable ruff's UP rules and autoformat inductor/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "25730618e5d5e9962c5685b1016975f57642bcaa",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105391",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105391/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105391.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105391.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105391/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105391/comments",
    "labels": [
      "open source",
      "module: inductor",
      "ciflow/inductor"
    ],
    "_event_time": "2023-07-18T01:11:44.593156Z",
    "state": "closed",
    "patch": "From 83b4affe1bc05e389fea76d933727692232293e4 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:37 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat inductor/\n\n[ghstack-poisoned]\n---\n test/inductor/test_cpu_repro.py                    |  2 +-\n test/inductor/test_cuda_repro.py                   |  2 +-\n test/inductor/test_cudagraph_trees.py              |  2 +-\n test/inductor/test_fused_attention.py              |  4 ++--\n test/inductor/test_mkldnn_pattern_matcher.py       |  8 ++++----\n test/inductor/test_profiler.py                     |  4 ++--\n test/inductor/test_standalone_compile.py           |  2 +-\n test/inductor/test_torchinductor.py                |  6 +++---\n .../test_torchinductor_codegen_dynamic_shapes.py   | 14 +++++++-------\n torch/_inductor/codecache.py                       |  6 +++---\n torch/_inductor/codegen/cpp.py                     |  2 +-\n torch/_inductor/ir.py                              | 10 +++++-----\n torch/_inductor/lowering.py                        |  6 +++---\n torch/_inductor/triton_heuristics.py               |  6 ++----\n torch/_inductor/utils.py                           |  2 +-\n 15 files changed, 37 insertions(+), 39 deletions(-)\n\ndiff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py\nindex 89ea4315b1de6c..2074d8451be9cc 100644\n--- a/test/inductor/test_cpu_repro.py\n+++ b/test/inductor/test_cpu_repro.py\n@@ -93,7 +93,7 @@ def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n     def test_conv2d_bn_mixed_dtype(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     3,\n                     16,\ndiff --git a/test/inductor/test_cuda_repro.py b/test/inductor/test_cuda_repro.py\nindex 30f9274875cf4c..7df54f0d35a146 100644\n--- a/test/inductor/test_cuda_repro.py\n+++ b/test/inductor/test_cuda_repro.py\n@@ -783,7 +783,7 @@ def forward(inductor_seeds, mul_4, view_15):\n     def test_issue100806(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.linear1 = torch.nn.Linear(10, 20)\n                 self.linear2 = torch.nn.Linear(20, 30)\n                 self.relu = torch.nn.ReLU()\ndiff --git a/test/inductor/test_cudagraph_trees.py b/test/inductor/test_cudagraph_trees.py\nindex 85e44c1e272f9d..8181484c419b58 100644\n--- a/test/inductor/test_cudagraph_trees.py\n+++ b/test/inductor/test_cudagraph_trees.py\n@@ -137,7 +137,7 @@ def tearDown(self):\n \n         def get_manager(self, device_index=None):\n             return torch._inductor.cudagraph_trees.get_container(\n-                (self.device_idx if not device_index else device_index)\n+                self.device_idx if not device_index else device_index\n             ).tree_manager\n \n         def get_roots(self):\ndiff --git a/test/inductor/test_fused_attention.py b/test/inductor/test_fused_attention.py\nindex e509c97b0cb04a..7163ac24de5cf3 100644\n--- a/test/inductor/test_fused_attention.py\n+++ b/test/inductor/test_fused_attention.py\n@@ -297,7 +297,7 @@ def test_pattern_fails_with_tensor_factor(self):\n         # https://github.com/pytorch/pytorch/issues/99124\n         class Model(torch.nn.Module):\n             def __init__(self, is_inv_factor):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.is_inv_factor = is_inv_factor\n \n             def forward(self, query, key, value, scale_factor) -> torch.Tensor:\n@@ -328,7 +328,7 @@ class Model(torch.nn.Module):\n             def __init__(\n                 self,\n             ):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, query, key, value, attn_mask) -> torch.Tensor:\n                 attn_weight = torch.softmax(\ndiff --git a/test/inductor/test_mkldnn_pattern_matcher.py b/test/inductor/test_mkldnn_pattern_matcher.py\nindex 3e07c0181994cf..9337a70b2f857e 100644\n--- a/test/inductor/test_mkldnn_pattern_matcher.py\n+++ b/test/inductor/test_mkldnn_pattern_matcher.py\n@@ -374,7 +374,7 @@ def forward(self, x, negative_slope):\n     def test_conv2d_add_scalar(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1\n                 )\n@@ -476,7 +476,7 @@ def forward(self, x, other, alpha):\n         # we can't do the fusion when add's inputs are same tensor.\n         class Model2(torch.nn.Module):\n             def __init__(self):\n-                super(Model2, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -490,7 +490,7 @@ def forward(self, x):\n         # we can't do the fusion when add's inputs are mixed dtype.\n         class Model3(torch.nn.Module):\n             def __init__(self):\n-                super(Model3, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(\n                     in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1\n                 )\n@@ -526,7 +526,7 @@ def forward(self, x):\n     def test_reproduce_99842_issue(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.conv = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n \n             def forward(self, input_tensor):\ndiff --git a/test/inductor/test_profiler.py b/test/inductor/test_profiler.py\nindex eebe884da46b33..68f44df8b9f052 100644\n--- a/test/inductor/test_profiler.py\n+++ b/test/inductor/test_profiler.py\n@@ -21,14 +21,14 @@ def test_inductor_profiling_triton_launch(self):\n         def fn(x, y):\n             return (x + y).sin().cos()\n \n-        x, y = [torch.rand((4, 4), device=\"cuda\") for _ in range(2)]\n+        x, y = (torch.rand((4, 4), device=\"cuda\") for _ in range(2))\n \n         with torch.profiler.profile() as prof:\n             fn(x, y)\n \n         with TemporaryFileName(mode=\"w+\") as fname:\n             prof.export_chrome_trace(fname)\n-            with open(fname, \"r\") as f:\n+            with open(fname) as f:\n                 trace_json = json.load(f)\n \n         self.assertTrue(\"traceEvents\" in trace_json)\ndiff --git a/test/inductor/test_standalone_compile.py b/test/inductor/test_standalone_compile.py\nindex c424c76244cc0e..88c528c891a4fc 100644\n--- a/test/inductor/test_standalone_compile.py\n+++ b/test/inductor/test_standalone_compile.py\n@@ -100,7 +100,7 @@ def test_inductor_via_export2(self):\n     def test_inductor_via_op_with_multiple_outputs(self):\n         x1 = torch.randn((2, 512, 128))\n         x2 = [128]\n-        x3 = torch.randn((128))\n+        x3 = torch.randn(128)\n         x4 = torch.randn((128,))\n         x5 = 1e-6\n         mod, inp = gen_gm_and_inputs(\ndiff --git a/test/inductor/test_torchinductor.py b/test/inductor/test_torchinductor.py\nindex 520837d3865a8d..45ee70cd8067ee 100644\n--- a/test/inductor/test_torchinductor.py\n+++ b/test/inductor/test_torchinductor.py\n@@ -2354,7 +2354,7 @@ def fn(x):\n     def test_adaptive_avg_pool2d_low_prec(self):\n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n                 self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n \n             def forward(self, x):\n@@ -5995,7 +5995,7 @@ def fn(x, y):\n \n         self.common(\n             fn,\n-            [torch.randn((4, 2)), torch.randn((4))],\n+            [torch.randn((4, 2)), torch.randn(4)],\n         )\n \n     # Shape padding causes the inputs to all get specialized, so the codegen\n@@ -6047,7 +6047,7 @@ def test_sqrt_dynamic_shapes(self):\n \n         class Model(torch.nn.Module):\n             def __init__(self):\n-                super(Model, self).__init__()\n+                super().__init__()\n \n             def forward(self, x):\n                 B, N, C = x.shape\ndiff --git a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\nindex 8f4086af099e64..0cdfbd54ffbe3b 100644\n--- a/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n+++ b/test/inductor/test_torchinductor_codegen_dynamic_shapes.py\n@@ -124,14 +124,14 @@ def run(*ex, **kwargs):\n     \"test_expand_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_glu_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_isinf2_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_layer_norm_dynamic_shapes\": TestFailure((\"cuda\")),\n+    \"test_layer_norm_dynamic_shapes\": TestFailure(\"cuda\"),\n     \"test_linspace1_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_reflection_pad2d_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_stack_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor2_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_tensor3_dynamic_shapes\": TestFailure((\"cpu\",)),\n-    \"test_to_device_constant_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_to_device_constant_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_upsample_nearest2d_backward_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views3_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_views4_dynamic_shapes\": TestFailure((\"cpu\",)),\n@@ -161,9 +161,9 @@ def run(*ex, **kwargs):\n     \"test_empty2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_empty_strided_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_index3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_inductor_bucketize_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure((\"cpu\")),\n-    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_inductor_bucketize_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_default_kwargs_dynamic_shapes\": TestFailure(\"cpu\"),\n+    \"test_inductor_bucketize_int_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_like_rands_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace2_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_linspace3_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n@@ -194,7 +194,7 @@ def run(*ex, **kwargs):\n     \"test_views6_dynamic_shapes\": TestFailure((\"cpu\",)),\n     \"test_view_detach_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n     \"test_view_on_aliased_dynamic_shapes\": TestFailure((\"cpu\", \"cuda\")),\n-    \"test_linear_float64_dynamic_shapes\": TestFailure((\"cpu\")),\n+    \"test_linear_float64_dynamic_shapes\": TestFailure(\"cpu\"),\n     \"test_adaptive_avg_pool_with_output_size_0_dynamic_shapes\": TestFailure(\n         (\"cpu\", \"cuda\")\n     ),\n@@ -288,7 +288,7 @@ def run(*ex, **kwargs):\n \n if TEST_WITH_ROCM:\n     # aten.miopen_batch_norm is not registered for lowering\n-    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure((\"cuda\"))\n+    test_failures[\"test_batch_norm_2d_dynamic_shapes\"] = TestFailure(\"cuda\")\n \n DynamicShapesCodegenCommonTemplate = make_dynamic_cls(\n     CommonTemplate, xfail_prop=\"_expected_failure_codegen_dynamic\"\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex fd1f4228c9d29a..8ac73569fe1c2a 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -158,7 +158,7 @@ def __init__(self):\n     def get_local_cache(self):\n         if not self.local_cache_path.is_file():\n             return {}\n-        with open(self.local_cache_path, \"r\") as local_cache_fp:\n+        with open(self.local_cache_path) as local_cache_fp:\n             local_cache = json.load(local_cache_fp)\n         return local_cache[\"cache\"]\n \n@@ -201,7 +201,7 @@ class PersistentCache(CacheBase):\n     def get_global_cache(self):\n         if self.global_cache_path is None or not self.global_cache_path.is_file():\n             return {}\n-        with open(self.global_cache_path, \"r\") as global_cache_fp:\n+        with open(self.global_cache_path) as global_cache_fp:\n             global_cache = json.load(global_cache_fp)\n         return global_cache[\"cache\"]\n \n@@ -844,7 +844,7 @@ def wrapper_call(*args):\n # - valid_vec_isa_list()\n # - VecISA.__bool__() <-- takes out a lock\n # - compile_file() <-- imports cpp_prefix_path from cpp, which causes us to try to take out the same lock.\n-@functools.lru_cache()\n+@functools.lru_cache\n def cpp_prefix_path():\n     path = Path(__file__).parent / \"codegen/cpp_prefix.h\"\n     with path.open() as f:\ndiff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py\nindex bb2469ebffd33d..f844160b7255c0 100644\n--- a/torch/_inductor/codegen/cpp.py\n+++ b/torch/_inductor/codegen/cpp.py\n@@ -278,7 +278,7 @@ def parallel_num_threads():\n     return threads\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def stride_at(var: sympy.Symbol, index: sympy.Expr):\n     replacement = {var: var + 1}\n     new_index = sympy_subs(index, replacement)\ndiff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py\nindex 869a0fb60861f6..c8172ff36b3dcf 100644\n--- a/torch/_inductor/ir.py\n+++ b/torch/_inductor/ir.py\n@@ -3042,7 +3042,7 @@ class InplaceBernoulliFallback(ExternKernel):\n     kernel = \"aten.bernoulli_\"\n \n     def codegen(self, wrapper):\n-        (x,) = [t.codegen_reference() for t in self.inputs]\n+        (x,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(\n             f\"{self.kernel}({x}, {', '.join(map(repr, self.constant_args))})\"\n         )\n@@ -3073,9 +3073,9 @@ class ScatterFallback(ExternKernel):\n \n     def codegen(self, wrapper):\n         if self.src_is_tensor:\n-            (x, index, src) = [t.codegen_reference() for t in self.inputs]\n+            (x, index, src) = (t.codegen_reference() for t in self.inputs)\n         else:\n-            (x, index) = [t.codegen_reference() for t in self.inputs]\n+            (x, index) = (t.codegen_reference() for t in self.inputs)\n             src = self.constant_args[1]\n         wrapper.generate_scatter_fallback(\n             x,\n@@ -3156,7 +3156,7 @@ class IndexPutFallback(ExternKernel):\n     \"\"\"\n \n     def codegen(self, wrapper):\n-        (x, values, *valid_indices) = [t.codegen_reference() for t in self.inputs]\n+        (x, values, *valid_indices) = (t.codegen_reference() for t in self.inputs)\n         indices = []\n         iter_valid_indices = iter(valid_indices)\n         for i, _ in enumerate(self.indices):\n@@ -4694,7 +4694,7 @@ def codegen(self, wrapper):\n         wrapper.add_import_once(\n             \"from torch.distributed._functional_collectives_impl import _wait_tensor\"\n         )\n-        (input_collective,) = [t.codegen_reference() for t in self.inputs]\n+        (input_collective,) = (t.codegen_reference() for t in self.inputs)\n         wrapper.writeline(f\"{input_collective} = _wait_tensor({input_collective})\")\n \n         # wait op still needs to produce a 'buffer' that represents the tensor output.\ndiff --git a/torch/_inductor/lowering.py b/torch/_inductor/lowering.py\nindex 3f17db1e5fed3a..14430ace100534 100644\n--- a/torch/_inductor/lowering.py\n+++ b/torch/_inductor/lowering.py\n@@ -2871,11 +2871,11 @@ def load_bounded(fy, fx):\n \n         iy = ops.to_dtype(in_y, get_int_dtype(iH + 1))\n         ix = ops.to_dtype(in_x, get_int_dtype(iW + 1))\n-        iys_ofs = tuple((ops.add(iy, ofs) for ofs in (-1, 0, 1, 2)))\n-        ixs_ofs = tuple((ops.add(ix, ofs) for ofs in (-1, 0, 1, 2)))\n+        iys_ofs = tuple(ops.add(iy, ofs) for ofs in (-1, 0, 1, 2))\n+        ixs_ofs = tuple(ops.add(ix, ofs) for ofs in (-1, 0, 1, 2))\n \n         def get_x_interp(y):\n-            coeffs_x = tuple((load_bounded(y, x) for x in ixs_ofs))\n+            coeffs_x = tuple(load_bounded(y, x) for x in ixs_ofs)\n             return cubic_interp1d(coeffs_x, t_x)\n \n         coeffs_y = tuple(get_x_interp(y) for y in iys_ofs)\ndiff --git a/torch/_inductor/triton_heuristics.py b/torch/_inductor/triton_heuristics.py\nindex 61027661111e5a..88fa275f8b0102 100644\n--- a/torch/_inductor/triton_heuristics.py\n+++ b/torch/_inductor/triton_heuristics.py\n@@ -482,9 +482,7 @@ def hash_configs(configs: List[Config]):\n     hasher = hashlib.sha256()\n     for cfg in configs:\n         hasher.update(\n-            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode(\n-                \"utf-8\"\n-            )\n+            f\"{sorted(cfg.kwargs.items())} {cfg.num_warps} {cfg.num_stages}\\n\".encode()\n         )\n     return hasher.hexdigest()\n \n@@ -498,7 +496,7 @@ def load_cached_autotuning(\n     if not os.path.exists(cache_filename):\n         return None\n \n-    with open(cache_filename, \"r\") as fd:\n+    with open(cache_filename) as fd:\n         best_config = json.loads(fd.read())\n     if best_config.pop(\"configs_hash\", None) != configs_hash:\n         return None\ndiff --git a/torch/_inductor/utils.py b/torch/_inductor/utils.py\nindex 538d0a2040fb93..c604c45d53d32a 100644\n--- a/torch/_inductor/utils.py\n+++ b/torch/_inductor/utils.py\n@@ -688,7 +688,7 @@ def run_and_get_code(fn, *args, **kwargs):\n \n     def patched_compile_to_module(self):\n         mod = compile_to_module(self)\n-        with open(mod.__file__, \"r\") as f:\n+        with open(mod.__file__) as f:\n             source_codes.append(f.read())\n         return mod\n \n"
  },
  {
    "number": 105390,
    "title": "[BE] Enable ruff's UP rules and autoformat ao/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "b3f1e6f0de6138113056863a48702aac578cc251",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105390",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105390/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105390.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105390.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105390/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105390/comments",
    "labels": [
      "release notes: AO frontend",
      "open source",
      "release notes: quantization"
    ],
    "_event_time": "2023-07-18T01:11:40.042650Z",
    "state": "closed",
    "patch": "From 96bd5708513a317e015b40499eb6e0cd291a3f04 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:33 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat ao/\n\n[ghstack-poisoned]\n---\n .../ao/sparsity/test_activation_sparsifier.py |  1 -\n test/ao/sparsity/test_composability.py        |  1 -\n test/ao/sparsity/test_data_scheduler.py       |  1 -\n test/ao/sparsity/test_data_sparsifier.py      |  1 -\n test/ao/sparsity/test_kernels.py              |  1 -\n test/ao/sparsity/test_parametrization.py      |  1 -\n test/ao/sparsity/test_scheduler.py            |  1 -\n test/ao/sparsity/test_sparsifier.py           |  1 -\n test/ao/sparsity/test_sparsity_utils.py       |  1 -\n .../ao/sparsity/test_structured_sparsifier.py |  1 -\n torch/ao/nn/intrinsic/modules/fused.py        |  2 +-\n .../ao/nn/intrinsic/qat/modules/conv_fused.py | 12 +++----\n .../nn/intrinsic/qat/modules/linear_relu.py   |  2 +-\n .../quantized/dynamic/modules/linear_relu.py  |  2 +-\n .../nn/intrinsic/quantized/modules/bn_relu.py |  4 +--\n .../intrinsic/quantized/modules/conv_relu.py  |  6 ++--\n .../quantized/modules/linear_relu.py          |  2 +-\n torch/ao/nn/quantizable/modules/activation.py |  6 ++--\n torch/ao/nn/quantized/dynamic/modules/conv.py |  1 -\n .../ao/nn/quantized/dynamic/modules/linear.py |  2 +-\n torch/ao/nn/quantized/dynamic/modules/rnn.py  | 32 +++++++++----------\n torch/ao/nn/quantized/modules/__init__.py     |  2 +-\n torch/ao/nn/quantized/modules/conv.py         |  5 ++-\n torch/ao/nn/quantized/modules/linear.py       |  2 +-\n .../ao/nn/quantized/reference/modules/rnn.py  |  2 +-\n .../ao/nn/sparse/quantized/dynamic/linear.py  |  2 +-\n torch/ao/ns/fx/ns_types.py                    |  8 ++---\n .../data_scheduler/base_data_scheduler.py     |  4 +--\n torch/ao/pruning/scheduler/base_scheduler.py  |  4 +--\n torch/ao/pruning/scheduler/cubic_scheduler.py |  1 -\n .../quantization/_learnable_fake_quantize.py  |  4 +--\n .../backend_config/backend_config.py          |  3 +-\n .../ao/quantization/backend_config/onednn.py  | 12 +++----\n .../quantization/experimental/APoT_tensor.py  |  2 +-\n .../ao/quantization/experimental/quantizer.py |  2 +-\n torch/ao/quantization/fake_quantize.py        |  2 +-\n torch/ao/quantization/fuse_modules.py         |  2 +-\n .../ao/quantization/fuser_method_mappings.py  | 10 +++---\n torch/ao/quantization/fx/_equalize.py         |  2 +-\n .../fx/_lower_to_native_backend.py            |  2 +-\n .../quantization/fx/_model_report/detector.py |  8 ++---\n .../_model_report/model_report_visualizer.py  |  2 +-\n torch/ao/quantization/fx/convert.py           |  2 +-\n torch/ao/quantization/fx/custom_config.py     |  9 ++----\n torch/ao/quantization/fx/prepare.py           |  2 +-\n torch/ao/quantization/fx/utils.py             | 24 ++++++--------\n torch/ao/quantization/observer.py             | 18 +++++------\n torch/ao/quantization/pt2e/prepare.py         |  4 +--\n .../quantization/pt2e/quantizer/quantizer.py  | 21 ++----------\n torch/ao/quantization/qconfig.py              |  8 ++---\n .../ao/quantization/quantization_mappings.py  | 10 +++---\n torch/ao/quantization/quantize.py             |  2 +-\n torch/ao/quantization/utils.py                |  2 +-\n 53 files changed, 114 insertions(+), 150 deletions(-)\n\ndiff --git a/test/ao/sparsity/test_activation_sparsifier.py b/test/ao/sparsity/test_activation_sparsifier.py\nindex 573a40762c31cc..01bdfa045da9d1 100644\n--- a/test/ao/sparsity/test_activation_sparsifier.py\n+++ b/test/ao/sparsity/test_activation_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import copy\ndiff --git a/test/ao/sparsity/test_composability.py b/test/ao/sparsity/test_composability.py\nindex 85d78c49ea54ae..cb799f714ca17b 100644\n--- a/test/ao/sparsity/test_composability.py\n+++ b/test/ao/sparsity/test_composability.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_data_scheduler.py b/test/ao/sparsity/test_data_scheduler.py\nindex 9c33a160e76836..ab7c051c21077a 100644\n--- a/test/ao/sparsity/test_data_scheduler.py\n+++ b/test/ao/sparsity/test_data_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_data_sparsifier.py b/test/ao/sparsity/test_data_sparsifier.py\nindex 81a899f6932a0d..9248a371826ebd 100644\n--- a/test/ao/sparsity/test_data_sparsifier.py\n+++ b/test/ao/sparsity/test_data_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import logging\ndiff --git a/test/ao/sparsity/test_kernels.py b/test/ao/sparsity/test_kernels.py\nindex 4786557ceb3be7..111d51465be109 100644\n--- a/test/ao/sparsity/test_kernels.py\n+++ b/test/ao/sparsity/test_kernels.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.testing._internal.common_utils import run_tests\ndiff --git a/test/ao/sparsity/test_parametrization.py b/test/ao/sparsity/test_parametrization.py\nindex 54b6f778d9fa8f..02f7cc6db7fddf 100644\n--- a/test/ao/sparsity/test_parametrization.py\n+++ b/test/ao/sparsity/test_parametrization.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_scheduler.py b/test/ao/sparsity/test_scheduler.py\nindex 52eb54cb9ecb96..835c5143f18bc2 100644\n--- a/test/ao/sparsity/test_scheduler.py\n+++ b/test/ao/sparsity/test_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch import nn\ndiff --git a/test/ao/sparsity/test_sparsifier.py b/test/ao/sparsity/test_sparsifier.py\nindex 4c79416a78dd69..c9309d4b81fe5b 100644\n--- a/test/ao/sparsity/test_sparsifier.py\n+++ b/test/ao/sparsity/test_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n import itertools\ndiff --git a/test/ao/sparsity/test_sparsity_utils.py b/test/ao/sparsity/test_sparsity_utils.py\nindex 90aad10ab18db6..9a4fc79e6c454e 100644\n--- a/test/ao/sparsity/test_sparsity_utils.py\n+++ b/test/ao/sparsity/test_sparsity_utils.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n \ndiff --git a/test/ao/sparsity/test_structured_sparsifier.py b/test/ao/sparsity/test_structured_sparsifier.py\nindex f50420c89a199d..13ab245a2efc27 100644\n--- a/test/ao/sparsity/test_structured_sparsifier.py\n+++ b/test/ao/sparsity/test_structured_sparsifier.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n import copy\n import logging\ndiff --git a/torch/ao/nn/intrinsic/modules/fused.py b/torch/ao/nn/intrinsic/modules/fused.py\nindex f70a5430e65c21..7c87154b0e6225 100644\n--- a/torch/ao/nn/intrinsic/modules/fused.py\n+++ b/torch/ao/nn/intrinsic/modules/fused.py\n@@ -125,7 +125,7 @@ class LinearBn1d(_FusedModule):\n     During quantization this will be replaced with the corresponding fused module.\"\"\"\n     def __init__(self, linear, bn):\n         assert type_before_parametrizations(linear) == Linear and type_before_parametrizations(bn) == BatchNorm1d, \\\n-            'Incorrect types for input modules{}{}'.format(type_before_parametrizations(linear), type_before_parametrizations(bn))\n+            f'Incorrect types for input modules{type_before_parametrizations(linear)}{type_before_parametrizations(bn)}'\n         super().__init__(linear, bn)\n \n class LinearLeakyReLU(_FusedModule):\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\nindex 3f457ad5917eba..161280ca079d53 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/conv_fused.py\n@@ -453,7 +453,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU1d(nnqat.Conv1d, nni._FusedModule):\n     r\"\"\"A ConvReLU1d module is a fused module of Conv1d and ReLU, attached with\n@@ -490,7 +490,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn2d(_ConvBnNd, nn.Conv2d):\n     r\"\"\"\n@@ -585,7 +585,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU2d(nnqat.Conv2d, nni._FusedModule):\n     r\"\"\"A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\n@@ -622,7 +622,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvBn3d(_ConvBnNd, nn.Conv3d):\n     r\"\"\"\n@@ -758,7 +758,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvBnReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n class ConvReLU3d(nnqat.Conv3d, nni._FusedModule):\n     r\"\"\"A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with\n@@ -813,7 +813,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n def update_bn_stats(mod):\n     if type(mod) in {ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvBn1d, ConvBn2d, ConvBn3d}:\ndiff --git a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\nindex 93b19537083427..11d11047c2c723 100644\n--- a/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/qat/modules/linear_relu.py\n@@ -37,7 +37,7 @@ def forward(self, input):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     def to_float(self):\n         linear = torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)\ndiff --git a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\nindex 9a6502d546641b..a0bccdc0e3d3d4 100644\n--- a/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\n@@ -48,7 +48,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qlinear_relu):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\nindex 5cd2ed8a757cee..856fa43aac9941 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\n@@ -39,7 +39,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU2d\n-        return super(BNReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\n@@ -75,7 +75,7 @@ def _get_name(self):\n     @classmethod\n     def from_float(cls, mod):\n         # TODO: Add qat support for BNReLU3d\n-        return super(BNReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, bn_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\nindex 7a88a7b8f92d3b..30d00474e4a5ad 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/conv_relu.py\n@@ -58,7 +58,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU1d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -107,7 +107,7 @@ def from_float(cls, mod):\n             mod.weight, mod.bias = fuse_conv_bn_weights(\n                 mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var,\n                 mod.bn.eps, mod.bn.weight, mod.bn.bias)\n-        return super(ConvReLU2d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\n@@ -163,7 +163,7 @@ def from_float(cls, mod):\n                 mod.bn.weight,\n                 mod.bn.bias,\n             )\n-        return super(ConvReLU3d, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_qconv, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\nindex 9c3a7bcd3b4a0c..17cb48f80fda91 100644\n--- a/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n+++ b/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py\n@@ -41,7 +41,7 @@ def _get_name(self):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LinearReLU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_linear_relu, output_scale, output_zero_point):\ndiff --git a/torch/ao/nn/quantizable/modules/activation.py b/torch/ao/nn/quantizable/modules/activation.py\nindex b7ba9dd8dc72c2..6a25d0f591021d 100644\n--- a/torch/ao/nn/quantizable/modules/activation.py\n+++ b/torch/ao/nn/quantizable/modules/activation.py\n@@ -317,7 +317,7 @@ def _forward_impl(self,\n             raise AssertionError(\"causal mask not supported by AO MHA module\")\n \n         if self.batch_first:\n-            query, key, value = [x.transpose(0, 1) for x in (query, key, value)]\n+            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n \n         tgt_len, bsz, embed_dim_to_check = query.size()\n         assert self.embed_dim == embed_dim_to_check\n@@ -339,7 +339,7 @@ def _forward_impl(self,\n                 warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n                 attn_mask = attn_mask.to(torch.bool)\n             assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n-                'Only float and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n+                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n \n             if attn_mask.dim() == 2:\n                 attn_mask = attn_mask.unsqueeze(0)\n@@ -349,7 +349,7 @@ def _forward_impl(self,\n                 if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n                     raise RuntimeError('The size of the 3D attn_mask is not correct.')\n             else:\n-                raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n+                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n             # attn_mask's dim is 3 now.\n \n         # convert ByteTensor key_padding_mask to bool\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/conv.py b/torch/ao/nn/quantized/dynamic/modules/conv.py\nindex 125b48edaacde5..f1af7796413655 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/conv.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Dynamically quantized convolution modules.\"\"\"\n \n import torch\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/linear.py b/torch/ao/nn/quantized/dynamic/modules/linear.py\nindex 78e459f9bc63c5..22f483f32fd7a8 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/linear.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/linear.py\n@@ -68,7 +68,7 @@ def extra_repr(self):\n             self.in_features, self.out_features, self._packed_params.dtype\n         )\n         if self._packed_params.dtype == torch.qint8:\n-            extra_repr_str += ', qscheme={}'.format(self.weight().qscheme())\n+            extra_repr_str += f', qscheme={self.weight().qscheme()}'\n         return extra_repr_str\n \n     def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\ndiff --git a/torch/ao/nn/quantized/dynamic/modules/rnn.py b/torch/ao/nn/quantized/dynamic/modules/rnn.py\nindex 3e78948b5447b2..47c8a9ac2fb43c 100644\n--- a/torch/ao/nn/quantized/dynamic/modules/rnn.py\n+++ b/torch/ao/nn/quantized/dynamic/modules/rnn.py\n@@ -231,8 +231,8 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n     def set_weight_bias(self, weight_bias_dict):\n \n         def weight_bias_name(ihhh, layer, suffix):\n-            weight_name = \"weight_{}_l{}{}\".format(ihhh, layer, suffix)\n-            bias_name = \"bias_{}_l{}{}\".format(ihhh, layer, suffix)\n+            weight_name = f\"weight_{ihhh}_l{layer}{suffix}\"\n+            bias_name = f\"bias_{ihhh}_l{layer}{suffix}\"\n             return weight_name, bias_name\n \n         num_directions = 2 if self.bidirectional else 1\n@@ -286,7 +286,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n         # RNNBase can be either LSTM or GRU\n         qRNNBase: Union[LSTM, GRU]\n         if mod.mode == 'LSTM':\n@@ -308,8 +308,8 @@ def from_float(cls, mod):\n                 suffix = '_reverse' if direction == 1 else ''\n \n                 def retrieve_weight_bias(ihhh):\n-                    weight_name = 'weight_{}_l{}{}'.format(ihhh, layer, suffix)\n-                    bias_name = 'bias_{}_l{}{}'.format(ihhh, layer, suffix)\n+                    weight_name = f'weight_{ihhh}_l{layer}{suffix}'\n+                    bias_name = f'bias_{ihhh}_l{layer}{suffix}'\n                     weight = getattr(mod, weight_name)\n                     bias = getattr(mod, bias_name)\n                     return weight, bias\n@@ -358,15 +358,15 @@ def _weight_bias(self):\n         for layer in range(self.num_layers):\n             for direction in range(num_directions):\n                 suffix = '_reverse' if direction == 1 else ''\n-                key_name1 = 'weight_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'weight_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'weight_ih_l{layer}{suffix}'\n+                key_name2 = f'weight_hh_l{layer}{suffix}'\n                 # packed weights are part of torchbind class, CellParamsSerializationType\n                 # Within the packed weight class, the weight and bias are accessible as Tensors\n                 packed_weight_bias = self._all_weight_values[count].param.__getstate__()[0][4]\n                 weight_bias_dict['weight'][key_name1] = packed_weight_bias[0].__getstate__()[0][0]\n                 weight_bias_dict['weight'][key_name2] = packed_weight_bias[1].__getstate__()[0][0]\n-                key_name1 = 'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n-                key_name2 = 'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)\n+                key_name1 = f'bias_ih_l{layer}{suffix}'\n+                key_name2 = f'bias_hh_l{layer}{suffix}'\n                 weight_bias_dict['bias'][key_name1] = packed_weight_bias[0].__getstate__()[0][1]\n                 weight_bias_dict['bias'][key_name2] = packed_weight_bias[1].__getstate__()[0][1]\n                 count = count + 1\n@@ -494,7 +494,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTM, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -746,7 +746,7 @@ def forward(self, input, hx=None):\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRU, cls).from_float(mod)\n+        return super().from_float(mod)\n \n     @classmethod\n     def from_reference(cls, ref_mod):\n@@ -860,7 +860,7 @@ def from_float(cls, mod):\n         dtype = weight_observer_method().dtype\n         supported_scalar_types = [torch.qint8, torch.float16]\n         if dtype not in supported_scalar_types:\n-            raise RuntimeError('Unsupported dtype for dynamic RNN quantization: {}'.format(dtype))\n+            raise RuntimeError(f'Unsupported dtype for dynamic RNN quantization: {dtype}')\n \n         qRNNCellBase: Union[LSTMCell, GRUCell, RNNCell]\n \n@@ -1009,12 +1009,12 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n         return ret\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(RNNCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class LSTMCell(RNNCellBase):\n@@ -1057,7 +1057,7 @@ def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None) ->\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(LSTMCell, cls).from_float(mod)\n+        return super().from_float(mod)\n \n \n class GRUCell(RNNCellBase):\n@@ -1098,4 +1098,4 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n \n     @classmethod\n     def from_float(cls, mod):\n-        return super(GRUCell, cls).from_float(mod)\n+        return super().from_float(mod)\ndiff --git a/torch/ao/nn/quantized/modules/__init__.py b/torch/ao/nn/quantized/modules/__init__.py\nindex 05866f6da4066a..668f765fe3ef0a 100644\n--- a/torch/ao/nn/quantized/modules/__init__.py\n+++ b/torch/ao/nn/quantized/modules/__init__.py\n@@ -104,7 +104,7 @@ def from_float(mod):\n         return Quantize(scale.float().item(), zero_point.long().item(), mod.activation_post_process.dtype)\n \n     def extra_repr(self):\n-        return 'scale={}, zero_point={}, dtype={}'.format(self.scale, self.zero_point, self.dtype)\n+        return f'scale={self.scale}, zero_point={self.zero_point}, dtype={self.dtype}'\n \n \n class DeQuantize(torch.nn.Module):\ndiff --git a/torch/ao/nn/quantized/modules/conv.py b/torch/ao/nn/quantized/modules/conv.py\nindex 727447841ca43c..22a11014375948 100644\n--- a/torch/ao/nn/quantized/modules/conv.py\n+++ b/torch/ao/nn/quantized/modules/conv.py\n@@ -1,4 +1,3 @@\n-# coding=utf-8\n r\"\"\"Quantized convolution modules.\"\"\"\n \n from typing import Optional, List, TypeVar\n@@ -64,7 +63,7 @@ def _init(self, in_channels, out_channels, kernel_size, stride,\n         self.output_padding = output_padding\n         self.groups = groups\n         if padding_mode not in _SUPPORTED_PADDING:\n-            raise ValueError(\"'padding_mode' {} is not supported by quantized convolution\".format(padding_mode))\n+            raise ValueError(f\"'padding_mode' {padding_mode} is not supported by quantized convolution\")\n         self.padding_mode = padding_mode\n         # Initialize as NCHW. set_weight will internally transpose to NHWC.\n         if self.transposed:\n@@ -593,7 +592,7 @@ def __init__(self, in_channels, out_channels, kernel_size, stride,\n                  padding, dilation, transposed, output_padding,\n                  groups, bias, padding_mode, device=None, dtype=None):\n         if padding_mode != 'zeros':\n-            raise ValueError('Only \"zeros\" padding mode is supported for {}'.format(self.__class__.__name__))\n+            raise ValueError(f'Only \"zeros\" padding mode is supported for {self.__class__.__name__}')\n         factory_kwargs = {'device': device, 'dtype': dtype}\n         # Subclasses of _ConvNd need to call _init rather than __init__. See\n         # discussion on PR #49702\ndiff --git a/torch/ao/nn/quantized/modules/linear.py b/torch/ao/nn/quantized/modules/linear.py\nindex e592c5f9b4d015..213934e62962a0 100644\n--- a/torch/ao/nn/quantized/modules/linear.py\n+++ b/torch/ao/nn/quantized/modules/linear.py\n@@ -262,7 +262,7 @@ def from_float(cls, mod):\n             if not isinstance(cls._FLOAT_MODULE, Iterable):\n                 cls._FLOAT_MODULE = [cls._FLOAT_MODULE]  # type: ignore[assignment]\n             supported_modules = ', '.join([float_mod.__name__ for float_mod in cls._FLOAT_MODULE])  # type: ignore[attr-defined]\n-            error_msg = 'nnq.{}.from_float only works for {}, but got: {}'.format(cls.__name__, supported_modules, type(mod))\n+            error_msg = f'nnq.{cls.__name__}.from_float only works for {supported_modules}, but got: {type(mod)}'\n             assert type_before_parametrizations(mod) in cls._FLOAT_MODULE, error_msg.format()  # type: ignore[attr-defined]\n             assert hasattr(mod, 'qconfig'), 'Input float module must have qconfig defined'\n             activation_post_process = mod.activation_post_process\ndiff --git a/torch/ao/nn/quantized/reference/modules/rnn.py b/torch/ao/nn/quantized/reference/modules/rnn.py\nindex 566642832a544d..9f44667c270b56 100644\n--- a/torch/ao/nn/quantized/reference/modules/rnn.py\n+++ b/torch/ao/nn/quantized/reference/modules/rnn.py\n@@ -152,7 +152,7 @@ def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n         else:\n             ret = input  # TODO: remove when jit supports exception flow\n             raise RuntimeError(\n-                \"Unknown nonlinearity: {}\".format(self.nonlinearity))\n+                f\"Unknown nonlinearity: {self.nonlinearity}\")\n \n         if not is_batched:\n             ret = ret.squeeze(0)\ndiff --git a/torch/ao/nn/sparse/quantized/dynamic/linear.py b/torch/ao/nn/sparse/quantized/dynamic/linear.py\nindex 87d174db8098ac..4190ebe38c2f93 100644\n--- a/torch/ao/nn/sparse/quantized/dynamic/linear.py\n+++ b/torch/ao/nn/sparse/quantized/dynamic/linear.py\n@@ -60,7 +60,7 @@ def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                               missing_keys, unexpected_keys, error_msgs):\n         op_type = int(state_dict[prefix + 'op_type'])\n         assert op_type == 'sparse', \\\n-            \"Cannot load from op_type [{}], expecting [{}]\".format(op_type, self._op_type)\n+            f\"Cannot load from op_type [{op_type}], expecting [{self._op_type}]\"\n         state_dict.pop(prefix + 'op_type')\n \n         version = local_metadata.get('version', None)\ndiff --git a/torch/ao/ns/fx/ns_types.py b/torch/ao/ns/fx/ns_types.py\nindex cf0451a155dd44..5c3c422dd4ae9d 100644\n--- a/torch/ao/ns/fx/ns_types.py\n+++ b/torch/ao/ns/fx/ns_types.py\n@@ -10,10 +10,10 @@ class NSSingleResultValuesType(str, enum.Enum):\n     NODE_OUTPUT = 'node_output'\n     NODE_INPUT = 'node_input'\n \n-NSSubgraph = NamedTuple(\n-    'NSSubgraph',\n-    [('start_node', Node), ('end_node', Node), ('base_op_node', Node)]\n-)\n+class NSSubgraph(NamedTuple):\n+    start_node: Node\n+    end_node: Node\n+    base_op_node: Node\n \n # TODO(future PR): see if we can use typing_extensions's TypedDict instead\n # to properly type the various keys\ndiff --git a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\nindex 26da2146952ffd..0e4060f95435b6 100644\n--- a/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n+++ b/torch/ao/pruning/_experimental/data_scheduler/base_data_scheduler.py\n@@ -103,8 +103,8 @@ def get_schedule_param(self):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Data Sparsifier {0}\\n'.format(self.data_sparsifier)\n-        format_string += '    {0}: {1}\\n'.format(self.schedule_param, self.base_param)\n+        format_string += f'Data Sparsifier {self.data_sparsifier}\\n'\n+        format_string += f'    {self.schedule_param}: {self.base_param}\\n'\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/base_scheduler.py b/torch/ao/pruning/scheduler/base_scheduler.py\nindex 8986d5bbdf630f..66863b31c5f8d8 100644\n--- a/torch/ao/pruning/scheduler/base_scheduler.py\n+++ b/torch/ao/pruning/scheduler/base_scheduler.py\n@@ -106,8 +106,8 @@ def print_sl(self, is_verbose, group, sl, epoch=None):\n     def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         format_string += '\\n'\n-        format_string += 'Sparsifier {0}\\n'.format(self.sparsifier)\n-        format_string += '    {0}: {1}\\n'.format('base_sl', self.base_sl)\n+        format_string += f'Sparsifier {self.sparsifier}\\n'\n+        format_string += '    {}: {}\\n'.format('base_sl', self.base_sl)\n         format_string += ')'\n         return format_string\n \ndiff --git a/torch/ao/pruning/scheduler/cubic_scheduler.py b/torch/ao/pruning/scheduler/cubic_scheduler.py\nindex 49ee9f51b42ae6..76fc61daa288a6 100644\n--- a/torch/ao/pruning/scheduler/cubic_scheduler.py\n+++ b/torch/ao/pruning/scheduler/cubic_scheduler.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import warnings\n \n from .base_scheduler import BaseScheduler\ndiff --git a/torch/ao/quantization/_learnable_fake_quantize.py b/torch/ao/quantization/_learnable_fake_quantize.py\nindex df86cd50a2a775..f21fedeb3bc28f 100644\n--- a/torch/ao/quantization/_learnable_fake_quantize.py\n+++ b/torch/ao/quantization/_learnable_fake_quantize.py\n@@ -114,8 +114,8 @@ def toggle_fake_quant(self, enabled=True):\n \n     @torch.jit.export\n     def observe_quant_params(self):\n-        print('_LearnableFakeQuantize Scale: {}'.format(self.scale.detach()))\n-        print('_LearnableFakeQuantize Zero Point: {}'.format(self.zero_point.detach()))\n+        print(f'_LearnableFakeQuantize Scale: {self.scale.detach()}')\n+        print(f'_LearnableFakeQuantize Zero Point: {self.zero_point.detach()}')\n \n     @torch.jit.export\n     def calculate_qparams(self):\ndiff --git a/torch/ao/quantization/backend_config/backend_config.py b/torch/ao/quantization/backend_config/backend_config.py\nindex ef31166b5cdab1..32abcc42e402fb 100644\n--- a/torch/ao/quantization/backend_config/backend_config.py\n+++ b/torch/ao/quantization/backend_config/backend_config.py\n@@ -599,8 +599,7 @@ def _get_dtype_config(obj: Any) -> DTypeConfig:\n                 return obj\n             if isinstance(obj, Dict):\n                 return DTypeConfig.from_dict(obj)\n-            raise ValueError(\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (DTYPE_CONFIGS_DICT_KEY, type(obj)))\n+            raise ValueError(f\"Expected a list of DTypeConfigs in backend_pattern_config_dict[\\\"{DTYPE_CONFIGS_DICT_KEY}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         if PATTERN_DICT_KEY in backend_pattern_config_dict:\ndiff --git a/torch/ao/quantization/backend_config/onednn.py b/torch/ao/quantization/backend_config/onednn.py\nindex 6a896608c9b5a8..8c14637ae3d3f7 100644\n--- a/torch/ao/quantization/backend_config/onednn.py\n+++ b/torch/ao/quantization/backend_config/onednn.py\n@@ -89,7 +89,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n         \"Linear, BN and LeakyReLU all must be in the same mode (train or eval).\"\n \n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((linear, bn, leaky_relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(linear, bn, leaky_relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Linear: nni.LinearLeakyReLU,\n@@ -100,7 +100,7 @@ def _fuse_linear_bn_leaky_relu(is_qat, linear, bn, leaky_relu):\n             fm = fused_module(fused_linear, leaky_relu)\n             return fm\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((linear, bn, leaky_relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(linear, bn, leaky_relu)}\")\n \n # ======================\n # |  CONFIGS FOR CONV  |\n@@ -144,7 +144,7 @@ def _conv_add_extra_inputs_getter_left(pattern):\n def _fuse_conv_bn_add_left(is_qat, add, bn_conv, _):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -216,7 +216,7 @@ def _conv_add_extra_inputs_getter_right(pattern):\n def _fuse_conv_bn_add_right(is_qat, add, _, bn_conv):\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAdd2d(fused_conv, add)\n@@ -305,7 +305,7 @@ def _fuse_conv_bn_add_relu_left(is_qat, relu, add_pattern):\n     add, bn_conv, _ = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\n@@ -387,7 +387,7 @@ def _fuse_conv_bn_add_relu_right(is_qat, relu, add_pattern):\n     add, _, bn_conv = add_pattern\n     bn, conv = bn_conv\n     if is_qat:\n-        raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, add, relu)))\n+        raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, add, relu)}\")\n     else:\n         fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n         return nni.ConvAddReLU2d(fused_conv, add, relu)\ndiff --git a/torch/ao/quantization/experimental/APoT_tensor.py b/torch/ao/quantization/experimental/APoT_tensor.py\nindex f780e204154147..debda7aea8c0d1 100644\n--- a/torch/ao/quantization/experimental/APoT_tensor.py\n+++ b/torch/ao/quantization/experimental/APoT_tensor.py\n@@ -2,7 +2,7 @@\n from torch.ao.quantization.experimental.quantizer import APoTQuantizer\n \n # class to store APoT quantized tensor\n-class TensorAPoT():\n+class TensorAPoT:\n     quantizer: APoTQuantizer\n     data: torch.Tensor\n \ndiff --git a/torch/ao/quantization/experimental/quantizer.py b/torch/ao/quantization/experimental/quantizer.py\nindex e7e6048fb00e08..df9c0f27847e13 100644\n--- a/torch/ao/quantization/experimental/quantizer.py\n+++ b/torch/ao/quantization/experimental/quantizer.py\n@@ -5,7 +5,7 @@\n \n # class to store APoT quantizer and\n # implement quantize and dequantize\n-class APoTQuantizer():\n+class APoTQuantizer:\n     alpha: torch.Tensor\n     gamma: torch.Tensor\n     quantization_levels: torch.Tensor\ndiff --git a/torch/ao/quantization/fake_quantize.py b/torch/ao/quantization/fake_quantize.py\nindex 881d431dcccb18..0da19e9f09b5a8 100644\n--- a/torch/ao/quantization/fake_quantize.py\n+++ b/torch/ao/quantization/fake_quantize.py\n@@ -268,7 +268,7 @@ class FixedQParamsFakeQuantize(FakeQuantize):\n     def __init__(self, observer):\n         super().__init__(observer=observer)\n         assert type(self.activation_post_process) == FixedQParamsObserver,\\\n-            \"%s's observer must be a %s\" % (self.__class__.__name__, FixedQParamsObserver.__name__)\n+            f\"{self.__class__.__name__}'s observer must be a {FixedQParamsObserver.__name__}\"\n         self._observer_ctr = observer\n         self.scale = self.activation_post_process.scale\n         self.zero_point = self.activation_post_process.zero_point\ndiff --git a/torch/ao/quantization/fuse_modules.py b/torch/ao/quantization/fuse_modules.py\nindex 80c3933ddc06a1..7c7ef1a88e83a7 100644\n--- a/torch/ao/quantization/fuse_modules.py\n+++ b/torch/ao/quantization/fuse_modules.py\n@@ -51,7 +51,7 @@ def fuse_known_modules(mod_list, is_qat, additional_fuser_method_mapping=None):\n     types = tuple(type_before_parametrizations(m) for m in mod_list)\n     fuser_method = get_fuser_method(types, additional_fuser_method_mapping)\n     if fuser_method is None:\n-        raise NotImplementedError(\"Cannot fuse modules: {}\".format(types))\n+        raise NotImplementedError(f\"Cannot fuse modules: {types}\")\n     new_mod : List[Optional[nn.Module]] = [None] * len(mod_list)\n     fused = fuser_method(is_qat, *mod_list)\n     # NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\ndiff --git a/torch/ao/quantization/fuser_method_mappings.py b/torch/ao/quantization/fuser_method_mappings.py\nindex 9971326de1d102..3140f13008ac3d 100644\n--- a/torch/ao/quantization/fuser_method_mappings.py\n+++ b/torch/ao/quantization/fuser_method_mappings.py\n@@ -47,7 +47,7 @@ def fuse_conv_bn(is_qat, conv, bn):\n         if fused_module_class is not None:\n             return fused_module_class(conv, bn)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn)}\")\n     else:\n         return nn.utils.fuse_conv_bn_eval(conv, bn)\n \n@@ -84,7 +84,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n         if fused_module is not None:\n             return fused_module(conv, bn, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse train modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse train modules: {(conv, bn, relu)}\")\n     else:\n         map_to_fused_module_eval = {\n             nn.Conv1d: nni.ConvReLU1d,\n@@ -96,7 +96,7 @@ def fuse_conv_bn_relu(is_qat, conv, bn, relu):\n             fused_conv = nn.utils.fusion.fuse_conv_bn_eval(conv, bn)\n             return fused_module(fused_conv, relu)\n         else:\n-            raise NotImplementedError(\"Cannot fuse eval modules: {}\".format((conv, bn, relu)))\n+            raise NotImplementedError(f\"Cannot fuse eval modules: {(conv, bn, relu)}\")\n \n def fuse_linear_bn(is_qat, linear, bn):\n     r\"\"\"Given the linear and bn modules, fuses them and returns the fused module\n@@ -187,7 +187,7 @@ def get_fuser_method(op_list, additional_fuser_method_mapping=None):\n     all_mappings = get_combined_dict(_DEFAULT_OP_LIST_TO_FUSER_METHOD,\n                                      additional_fuser_method_mapping)\n     fuser_method = all_mappings.get(op_list, None)\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_list)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_list} \"\n     return fuser_method\n \n def _reverse2(f):\n@@ -244,5 +244,5 @@ def get_fuser_method_new(\n         fuser_method = fuser_method_mapping.get(op_pattern, None)\n         if fuser_method is not None:\n             break\n-    assert fuser_method is not None, \"did not find fuser method for: {} \".format(op_pattern)\n+    assert fuser_method is not None, f\"did not find fuser method for: {op_pattern} \"\n     return fuser_method\ndiff --git a/torch/ao/quantization/fx/_equalize.py b/torch/ao/quantization/fx/_equalize.py\nindex 357db6454032e7..883ddf19682f08 100644\n--- a/torch/ao/quantization/fx/_equalize.py\n+++ b/torch/ao/quantization/fx/_equalize.py\n@@ -227,7 +227,7 @@ def __new__(cls, input_activation=torch.nn.Identity, weight=torch.nn.Identity):\n         if isinstance(input_activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"EqualizationQConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        self = super(EqualizationQConfig, cls).__new__(cls, input_activation, weight)\n+        self = super().__new__(cls, input_activation, weight)\n         return self\n \n \ndiff --git a/torch/ao/quantization/fx/_lower_to_native_backend.py b/torch/ao/quantization/fx/_lower_to_native_backend.py\nindex b897a7e80ab7cf..f1e9c81896f6c5 100644\n--- a/torch/ao/quantization/fx/_lower_to_native_backend.py\n+++ b/torch/ao/quantization/fx/_lower_to_native_backend.py\n@@ -514,7 +514,7 @@ def _match_static_pattern(\n     matched_dequantize = False\n     for i in dequantize_node_arg_indices:\n         assert i < len(ref_node.args),\\\n-            \"Dequantize index %s exceeded reference node's arg length %s\" % (i, len(ref_node.args))\n+            f\"Dequantize index {i} exceeded reference node's arg length {len(ref_node.args)}\"\n         arg = ref_node.args[i]\n         if is_dequantize_node(arg):\n             matched_dequantize = True\ndiff --git a/torch/ao/quantization/fx/_model_report/detector.py b/torch/ao/quantization/fx/_model_report/detector.py\nindex 60c9b26ffacecd..2e3cdc0a316611 100644\n--- a/torch/ao/quantization/fx/_model_report/detector.py\n+++ b/torch/ao/quantization/fx/_model_report/detector.py\n@@ -32,7 +32,7 @@\n DETECTOR_OBS_ARGS_KEY = \"observer_args\"\n \n # Mapping related code\n-class DetectorQConfigInfo():\n+class DetectorQConfigInfo:\n     r\"\"\"\n     This class contains the QConfig information for a single module.\n     The list of variables / values this contains can grow depending on the\n@@ -234,7 +234,7 @@ def __init__(self, backend: str = torch.backends.quantized.engine):\n         if self.backend_chosen in self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES:\n             self.supported_modules = self.DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES[self.backend_chosen]\n         else:\n-            raise ValueError(\"Not configured to work with {}. Try a different default backend\".format(self.backend_chosen))\n+            raise ValueError(f\"Not configured to work with {self.backend_chosen}. Try a different default backend\")\n \n     def get_detector_name(self) -> str:\n         r\"\"\" returns the string name of this detector\"\"\"\n@@ -352,7 +352,7 @@ def generate_detector_report(self, model: nn.Module) -> Tuple[str, Dict[str, Any\n         per_channel_info = self._detect_per_channel_helper(model)\n \n         # String to let the user know of further optimizations\n-        further_optims_str = \"Further Optimizations for backend {}: \\n\".format(self.backend_chosen)\n+        further_optims_str = f\"Further Optimizations for backend {self.backend_chosen}: \\n\"\n \n         optimizations_possible = False\n         for fqn in per_channel_info:\n@@ -1019,7 +1019,7 @@ def _generate_comparison_values(self, input_info: Dict, weight_info: Dict) -> Di\n \n             # raise error if not in weight info\n             if module_fqn not in weight_info:\n-                raise KeyError(\"Unable to find weight range stats for module {}\".format(module_fqn))\n+                raise KeyError(f\"Unable to find weight range stats for module {module_fqn}\")\n \n             # calculate the ratios of the weight info and input info\n             weight_ratio = self._calculate_range_ratio(weight_info[module_fqn], self.WEIGHT_STR, module_fqn)\ndiff --git a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\nindex f1f17e80982e54..8e04338446dab1 100644\n--- a/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n+++ b/torch/ao/quantization/fx/_model_report/model_report_visualizer.py\n@@ -587,7 +587,7 @@ def generate_plot_visualization(self, feature_filter: str, module_fqn_filter: st\n             avg_vals = [sum(y_data[:][index]) / num_channels for index in range(num_modules)]\n \n             # plot the three things we measured\n-            ax.plot(x_data, avg_vals, label=\"Average Value Across {} Channels\".format(num_channels))\n+            ax.plot(x_data, avg_vals, label=f\"Average Value Across {num_channels} Channels\")\n             ax.legend(loc='upper right')\n         else:\n             ax.set_xlabel(\"idx\")\ndiff --git a/torch/ao/quantization/fx/convert.py b/torch/ao/quantization/fx/convert.py\nindex 14e6ec094ede48..687342cc0ed321 100644\n--- a/torch/ao/quantization/fx/convert.py\n+++ b/torch/ao/quantization/fx/convert.py\n@@ -970,7 +970,7 @@ def convert(\n         # all the values either match what was set in prepare node_name_to_qconfig\n         # or are set to None in the convert_node_name_to_qconfig.\n         for k, v in node_name_to_qconfig.items():\n-            assert k in convert_node_name_to_qconfig, 'Expected key {} in convert node_name_to_qconfig'.format(k)\n+            assert k in convert_node_name_to_qconfig, f'Expected key {k} in convert node_name_to_qconfig'\n             if convert_node_name_to_qconfig[k] is not None:\n                 assert qconfig_equals(v, convert_node_name_to_qconfig[k]), \\\n                     \"Expected k {} to have the same value in prepare and convert QConfigMappings, \" \\\ndiff --git a/torch/ao/quantization/fx/custom_config.py b/torch/ao/quantization/fx/custom_config.py\nindex ef29061796d3a3..4fb2c3a28cb0a5 100644\n--- a/torch/ao/quantization/fx/custom_config.py\n+++ b/torch/ao/quantization/fx/custom_config.py\n@@ -197,8 +197,7 @@ def _get_qconfig_mapping(obj: Any, dict_key: str) -> Optional[QConfigMapping]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return QConfigMapping.from_dict(obj)\n-            raise ValueError(\"Expected QConfigMapping in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected QConfigMapping in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCustomConfig]:\n             \"\"\"\n@@ -208,8 +207,7 @@ def _get_prepare_custom_config(obj: Any, dict_key: str) -> Optional[PrepareCusto\n                 return obj\n             if isinstance(obj, Dict):\n                 return PrepareCustomConfig.from_dict(obj)\n-            raise ValueError(\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected PrepareCustomConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n             \"\"\"\n@@ -219,8 +217,7 @@ def _get_backend_config(obj: Any, dict_key: str) -> Optional[BackendConfig]:\n                 return obj\n             if isinstance(obj, Dict):\n                 return BackendConfig.from_dict(obj)\n-            raise ValueError(\"Expected BackendConfig in prepare_custom_config_dict[\\\"%s\\\"], got '%s'\" %\n-                             (dict_key, type(obj)))\n+            raise ValueError(f\"Expected BackendConfig in prepare_custom_config_dict[\\\"{dict_key}\\\"], got '{type(obj)}'\")\n \n         conf = cls()\n         for (module_name, qconfig_dict, example_inputs, _prepare_custom_config_dict, backend_config_dict) in\\\ndiff --git a/torch/ao/quantization/fx/prepare.py b/torch/ao/quantization/fx/prepare.py\nindex d14bf444ccd8c6..aa9f1f7467f932 100644\n--- a/torch/ao/quantization/fx/prepare.py\n+++ b/torch/ao/quantization/fx/prepare.py\n@@ -238,7 +238,7 @@ def _needs_obs_or_fq(\n     # be converted to choose_qparams -> q -> dq in convert step\n     if cur_target_is_dynamic:\n         assert cur_target_dtype in _OBS_DTYPE_LIST, \\\n-            \"Expected cur_target_dtype to be torch.float, but got: {}\".format(cur_target_dtype)\n+            f\"Expected cur_target_dtype to be torch.float, but got: {cur_target_dtype}\"\n         assert prev_output_dtype not in _DO_NOT_OBS_DTYPE_LIST\n         return is_zeroth_arg\n     if reuse_input_obs_or_fq:\ndiff --git a/torch/ao/quantization/fx/utils.py b/torch/ao/quantization/fx/utils.py\nindex 2e0b6bbb130530..0942fd9462b0a1 100644\n--- a/torch/ao/quantization/fx/utils.py\n+++ b/torch/ao/quantization/fx/utils.py\n@@ -149,7 +149,7 @@ def get_qconv_prepack_op(conv_op: Callable) -> Callable:\n         torch.nn.functional.conv_transpose3d: torch.ops.quantized.conv_transpose3d_prepack,\n     }\n     prepack_op = prepack_ops.get(conv_op, None)\n-    assert prepack_op, \"Didn't find prepack op for {}\".format(conv_op)\n+    assert prepack_op, f\"Didn't find prepack op for {conv_op}\"\n     return prepack_op\n \n # Returns a function that can get a new attribute name for module with given\n@@ -811,24 +811,21 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n         # check quantization ranges\n         if backend_quant_min is not None and backend_quant_max is not None:\n             if app_quant_min is None or app_quant_max is None:\n-                warnings.warn(\"QConfig %s must specify 'quant_min' and 'quant_max', ignoring %s\" %\n-                              (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'quant_min' and 'quant_max', ignoring {qconfig}\")\n                 return False\n             elif app_quant_min < backend_quant_min or app_quant_max > backend_quant_max:\n-                warnings.warn((\"QConfig %s quantization range must fall within the backend's:\\n\"\n-                              \"QConfig range = (%s, %s), BackendConfig range = (%s, %s), ignoring %s\") %\n-                              (debug_string, app_quant_min, app_quant_max,\n+                warnings.warn((\"QConfig {} quantization range must fall within the backend's:\\n\"\n+                              \"QConfig range = ({}, {}), BackendConfig range = ({}, {}), ignoring {}\").format(debug_string, app_quant_min, app_quant_max,\n                               backend_quant_min, backend_quant_max, qconfig))\n                 return False\n         # check scale min\n         if backend_scale_min is not None:\n             if app_scale_min is None:\n-                warnings.warn(\"QConfig %s must specify 'eps', ignoring %s\" % (debug_string, qconfig))\n+                warnings.warn(f\"QConfig {debug_string} must specify 'eps', ignoring {qconfig}\")\n                 return False\n             elif app_scale_min < backend_scale_min:\n-                warnings.warn((\"QConfig %s eps (%s) must be greater than or equal to \"\n-                              \"the backend's min scale value (%s), ignoring %s\") %\n-                              (debug_string, app_scale_min, backend_scale_min, qconfig))\n+                warnings.warn((\"QConfig {} eps ({}) must be greater than or equal to \"\n+                              \"the backend's min scale value ({}), ignoring {}\").format(debug_string, app_scale_min, backend_scale_min, qconfig))\n                 return False\n         # check fixed scale and zero point\n         if backend_scale_exact_match is not None and backend_zero_point_exact_match is not None:\n@@ -846,12 +843,11 @@ def _activation_post_process_satisfies_dtype_config_constraints(\n             if not isinstance(activation_post_process, FixedQParamsObserver) and \\\n                     not isinstance(activation_post_process, FixedQParamsFakeQuantize):\n                 warnings.warn((\"QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize \"\n-                              \"for fixed qparams ops, ignoring %s.\\n%s\") % (qconfig, suggestion_str))\n+                              \"for fixed qparams ops, ignoring {}.\\n{}\").format(qconfig, suggestion_str))\n                 return False\n             if observer.scale != backend_scale_exact_match or observer.zero_point != backend_zero_point_exact_match:\n-                warnings.warn((\"QConfig fixed scale (%s) and zero point (%s) do not match the backend's \"\n-                              \"(%s and %s), ignoring %s.\\n%s\") %\n-                              (observer.scale, observer.zero_point, backend_scale_exact_match,\n+                warnings.warn((\"QConfig fixed scale ({}) and zero point ({}) do not match the backend's \"\n+                              \"({} and {}), ignoring {}.\\n{}\").format(observer.scale, observer.zero_point, backend_scale_exact_match,\n                               backend_zero_point_exact_match, qconfig, suggestion_str))\n                 return False\n         return True\ndiff --git a/torch/ao/quantization/observer.py b/torch/ao/quantization/observer.py\nindex 3263ae11564129..f5d24c45787265 100644\n--- a/torch/ao/quantization/observer.py\n+++ b/torch/ao/quantization/observer.py\n@@ -118,7 +118,7 @@ def _with_callable_args(cls_or_self, **kwargs):\n     return r.with_callable_args(**kwargs)\n \n \n-ABC: Any = ABCMeta(str(\"ABC\"), (object,), {})  # compatible with Python 2 *and* 3:\n+ABC: Any = ABCMeta(\"ABC\", (object,), {})  # compatible with Python 2 *and* 3:\n \n \n class ObserverBase(ABC, nn.Module):\n@@ -509,7 +509,7 @@ def calculate_qparams(self):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     @torch.jit.export\n     def reset_min_max_vals(self):\n@@ -712,7 +712,7 @@ def calculate_qparams(self):\n         return self._calculate_qparams(self.min_val, self.max_val)\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n     def _load_from_state_dict(\n         self,\n@@ -746,7 +746,7 @@ def _load_from_state_dict(\n                 elif name == expected_max_name:\n                     self.max_val.resize_(val.shape)\n                 else:\n-                    warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                    warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n                 # For torchscript module we need to update the attributes here since we do not\n                 # call the `_load_from_state_dict` function defined module.py\n                 if torch.jit.is_scripting():\n@@ -755,7 +755,7 @@ def _load_from_state_dict(\n                     elif name == expected_max_name:\n                         self.max_val.copy_(val)\n                     else:\n-                        warnings.warn(\"Observer load_from_state_dict got unexpected name {}\".format(name))\n+                        warnings.warn(f\"Observer load_from_state_dict got unexpected name {name}\")\n             elif strict:\n                 missing_keys.append(key)\n \n@@ -1265,7 +1265,7 @@ def _load_from_state_dict(\n         )\n \n     def extra_repr(self):\n-        return \"min_val={}, max_val={}\".format(self.min_val, self.max_val)\n+        return f\"min_val={self.min_val}, max_val={self.max_val}\"\n \n \n class FixedQParamsObserver(ObserverBase):\n@@ -1363,7 +1363,7 @@ def forward(self, x):\n \n     @torch.jit.export\n     def extra_repr(self):\n-        return \"dtype={}, is_dynamic={}\".format(self.dtype, self.is_dynamic)\n+        return f\"dtype={self.dtype}, is_dynamic={self.is_dynamic}\"\n \n     @torch.jit.export\n     def calculate_qparams(self):\n@@ -1518,10 +1518,10 @@ def load_observer_state_dict(mod, obs_dict):\n                 )\n     for k in missing_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Missing keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Missing keys for observer {k} in state_dict\")\n     for k in unexpected_keys:\n         if \"observer\" in k or \"activation_post_process\" in k:\n-            raise Exception(\"Unexpected keys for observer {} in state_dict\".format(k))\n+            raise Exception(f\"Unexpected keys for observer {k} in state_dict\")\n \n \n # Restrict activations to be in the range (0,127)\ndiff --git a/torch/ao/quantization/pt2e/prepare.py b/torch/ao/quantization/pt2e/prepare.py\nindex 2a29a92a986ced..13f73acca354b4 100644\n--- a/torch/ao/quantization/pt2e/prepare.py\n+++ b/torch/ao/quantization/pt2e/prepare.py\n@@ -68,9 +68,9 @@ def _maybe_insert_input_observer_for_arg_or_kwarg(\n             assert _is_activation_post_process_node(arg, named_modules)\n             assert arg_as_input_act_obs_or_fq is not None\n             observed_arg = arg.args[0]\n-            assert isinstance(observed_arg, Node), \"expect observed argument to be a Node, but got: {}\".format(type(observed_arg))\n+            assert isinstance(observed_arg, Node), f\"expect observed argument to be a Node, but got: {type(observed_arg)}\"\n             assert observed_arg in obs_or_fq_map, \\\n-                \"can't refer to a node that does not have observer/fake_quant inserted yet: {}\".format(observed_arg)\n+                f\"can't refer to a node that does not have observer/fake_quant inserted yet: {observed_arg}\"\n             arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]\n             new_arg = arg\n             obs_or_fq_map[(observed_arg, node)] = arg_as_input_act_obs_or_fq\ndiff --git a/torch/ao/quantization/pt2e/quantizer/quantizer.py b/torch/ao/quantization/pt2e/quantizer/quantizer.py\nindex 3f7531c33a4e8b..46d7286756d17c 100644\n--- a/torch/ao/quantization/pt2e/quantizer/quantizer.py\n+++ b/torch/ao/quantization/pt2e/quantizer/quantizer.py\n@@ -128,24 +128,9 @@ class QuantizationConfig:\n OperatorPatternType = List[Callable]\n OperatorPatternType.__module__ = \"torch.ao.quantization.pt2e.quantizer.quantizer\"\n \n-OperatorConfig = NamedTuple(\n-    \"OperatorConfig\",\n-    # fix List[str] with List[List[Union[nn.Module, FunctionType, BuiltinFunctionType]]]\n-    # Basically we are mapping a quantization config to some list of patterns.\n-    # a pattern is defined as a list of nn module, function or builtin function names\n-    # e.g. [nn.Conv2d, torch.relu, torch.add]\n-    # We have not resolved whether fusion can be considered internal details of the\n-    # quantizer hence it does not need communication to user.\n-    # Note this pattern is not really informative since it does not really\n-    # tell us the graph structure resulting from the list of ops.\n-    [\n-        (\"config\", QuantizationConfig),\n-        (\n-            \"operators\",\n-            List[OperatorPatternType],\n-        ),\n-    ],\n-)\n+class OperatorConfig(NamedTuple):\n+    config: QuantizationConfig\n+    operators: List[OperatorPatternType]\n \n @dataclass\n class QuantizationAnnotation:\ndiff --git a/torch/ao/quantization/qconfig.py b/torch/ao/quantization/qconfig.py\nindex f7489c1ed4d05a..dc8353d6172990 100644\n--- a/torch/ao/quantization/qconfig.py\n+++ b/torch/ao/quantization/qconfig.py\n@@ -103,7 +103,7 @@ def __new__(cls, activation, weight):\n         if isinstance(activation, nn.Module) or isinstance(weight, nn.Module):\n             raise ValueError(\"QConfig received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n-        return super(QConfig, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n class QConfigDynamic(namedtuple('QConfigDynamic', ['activation', 'weight'])):\n@@ -128,7 +128,7 @@ def __new__(cls, activation=torch.nn.Identity, weight=torch.nn.Identity):\n             raise ValueError(\"QConfigDynamic received observer instance, please pass observer class instead. \" +\n                              \"Use MyObserver.with_args(x=1) to override arguments to constructor if needed\")\n         warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n-        return super(QConfigDynamic, cls).__new__(cls, activation, weight)\n+        return super().__new__(cls, activation, weight)\n \n \n default_qconfig = QConfig(activation=default_observer,\n@@ -236,7 +236,7 @@ def get_default_qconfig(backend='x86', version=0):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     if version == 0:\n@@ -326,7 +326,7 @@ def get_default_qat_qconfig(backend='x86', version=1):\n     if backend not in supported_backends:\n         raise AssertionError(\n             \"backend: \" + str(backend) +\n-            \" not supported. backend must be one of {}\".format(supported_backends)\n+            f\" not supported. backend must be one of {supported_backends}\"\n         )\n \n     # Histogram observer is too slow for quantization aware training\ndiff --git a/torch/ao/quantization/quantization_mappings.py b/torch/ao/quantization/quantization_mappings.py\nindex 96db52624acd34..4b3f4d26b2ac09 100644\n--- a/torch/ao/quantization/quantization_mappings.py\n+++ b/torch/ao/quantization/quantization_mappings.py\n@@ -251,7 +251,7 @@ def get_static_quant_module_class(\n         else DEFAULT_STATIC_QUANT_MODULE_MAPPINGS, additional_static_quant_mapping)\n     static_quant_module_class = all_mappings.get(float_module_class, None)\n     assert static_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(static_quant_module_class)\n \n@@ -266,7 +266,7 @@ def get_dynamic_quant_module_class(\n     all_mappings = get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping)\n     dynamic_quant_module_class = all_mappings.get(float_module_class, None)\n     assert dynamic_quant_module_class is not None, \\\n-        \"Floating point module class {}\".format(str(float_module_class)) + \\\n+        f\"Floating point module class {str(float_module_class)}\" + \\\n         \" does not have a corresponding quantized module class\"\n     return copy.deepcopy(dynamic_quant_module_class)\n \n@@ -300,10 +300,10 @@ def get_default_qconfig_propagation_list() -> Set[Callable]:\n     attribute to in prepare\n     '''\n     QCONFIG_PROPAGATE_MODULE_CLASS_LIST = (\n-        (set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n+        set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |\n          set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |\n-         _INCLUDE_QCONFIG_PROPAGATE_LIST)\n+         _INCLUDE_QCONFIG_PROPAGATE_LIST\n     )\n     return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n \n@@ -332,7 +332,7 @@ def get_quantized_operator(float_op: Union[Callable, str]) -> Callable:\n     '''\n     quantized_op = DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS.get(float_op, None)\n     assert quantized_op is not None, \\\n-        'Operator {} does not have corresponding quantized op'.format(str(float_op))\n+        f'Operator {str(float_op)} does not have corresponding quantized op'\n     return quantized_op\n \n def _get_special_act_post_process(module: torch.nn.Module) -> Optional[Callable]:\ndiff --git a/torch/ao/quantization/quantize.py b/torch/ao/quantization/quantize.py\nindex ca60475fc95a79..23c234c3f35103 100644\n--- a/torch/ao/quantization/quantize.py\n+++ b/torch/ao/quantization/quantize.py\n@@ -442,7 +442,7 @@ def quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8,\n             }\n         else:\n             raise ValueError(\n-                \"Don't know how to quantize with default settings for {}. Provide full qconfig please\".format(dtype))\n+                f\"Don't know how to quantize with default settings for {dtype}. Provide full qconfig please\")\n     elif isinstance(qconfig_spec, set):\n         if dtype is torch.qint8:\n             default_qconfig = default_dynamic_qconfig\ndiff --git a/torch/ao/quantization/utils.py b/torch/ao/quantization/utils.py\nindex 1a82c0fcf432ae..d97b76fb253389 100644\n--- a/torch/ao/quantization/utils.py\n+++ b/torch/ao/quantization/utils.py\n@@ -327,7 +327,7 @@ def check_min_max_valid(min_val: torch.Tensor, max_val: torch.Tensor) -> bool:\n     else:\n         assert torch.all(\n             min_val <= max_val\n-        ), \"min {} should be less than max {}\".format(min_val, max_val)\n+        ), f\"min {min_val} should be less than max {max_val}\"\n \n     return True\n \n"
  },
  {
    "number": 105389,
    "title": "[BE] Enable ruff's UP rules and autoformat benchmarks/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @aakhundov",
    "merge_commit_sha": "1039693734cbbb3eea8127e2ddc134dd64cdc4ea",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105389",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105389/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105389.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105389.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105389/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105389/comments",
    "labels": [
      "module: dynamo",
      "open source",
      "ciflow/inductor",
      "release notes: distributed (ddp)"
    ],
    "_event_time": "2023-07-18T01:11:36.210298Z",
    "state": "closed",
    "patch": "From 46440ce5cba24fbc8a711519336def356813fb48 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:28 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat benchmarks/\n\n[ghstack-poisoned]\n---\n benchmarks/compare-fastrnn-results.py              |  4 ++--\n benchmarks/cpp/tensorexpr/bench_ops.py             |  4 ++--\n benchmarks/distributed/ddp/benchmark.py            | 14 +++++++-------\n benchmarks/distributed/ddp/diff.py                 | 10 +++++-----\n benchmarks/distributed/pipeline/pipe.py            |  4 ++--\n .../distributed/rpc/parameter_server/launcher.py   |  2 +-\n benchmarks/distributed/rpc/rl/coordinator.py       |  2 +-\n benchmarks/dynamo/_onnx/reporter.py                | 12 ++++++------\n benchmarks/dynamo/benchmarks.py                    |  2 +-\n benchmarks/dynamo/combine_csv.py                   |  2 +-\n benchmarks/dynamo/common.py                        |  4 ++--\n .../dynamo/microbenchmarks/operator_inp_utils.py   |  2 +-\n benchmarks/dynamo/parse_logs.py                    |  2 +-\n benchmarks/dynamo/runner.py                        | 10 +++++-----\n benchmarks/dynamo/timm_models.py                   |  4 ++--\n benchmarks/fastrnns/bench.py                       |  2 +-\n benchmarks/fastrnns/profile.py                     |  8 ++++----\n benchmarks/fastrnns/runner.py                      |  6 +++---\n benchmarks/fastrnns/test.py                        |  4 ++--\n .../framework_overhead_benchmark/C2Module.py       |  4 ++--\n .../framework_overhead_benchmark.py                |  6 +++---\n .../pt_wrapper_module.py                           |  4 ++--\n benchmarks/framework_overhead_benchmark/utils.py   |  2 +-\n .../functional_autograd_benchmark/compare.py       |  4 ++--\n .../functional_autograd_benchmark.py               |  6 +++---\n benchmarks/functional_autograd_benchmark/utils.py  |  2 +-\n benchmarks/instruction_counts/applications/ci.py   |  2 +-\n benchmarks/instruction_counts/core/expand.py       |  2 +-\n benchmarks/operator_benchmark/benchmark_caffe2.py  |  6 +++---\n benchmarks/operator_benchmark/benchmark_core.py    |  8 ++++----\n benchmarks/operator_benchmark/benchmark_pytorch.py |  2 +-\n .../operator_benchmark/common/repeat_benchmark.py  |  2 +-\n benchmarks/overrides_benchmark/bench.py            |  4 ++--\n benchmarks/sparse/dlmc/matmul_bench.py             |  4 ++--\n benchmarks/sparse/dlmc/utils.py                    |  8 ++++----\n benchmarks/tensorexpr/__main__.py                  |  9 ++++-----\n benchmarks/tensorexpr/benchmark.py                 |  4 ++--\n benchmarks/tensorexpr/reduction.py                 |  2 +-\n benchmarks/upload_scribe.py                        |  2 +-\n 39 files changed, 90 insertions(+), 91 deletions(-)\n\ndiff --git a/benchmarks/compare-fastrnn-results.py b/benchmarks/compare-fastrnn-results.py\nindex bc4a55688b17ed..45961ca78de53b 100644\n--- a/benchmarks/compare-fastrnn-results.py\n+++ b/benchmarks/compare-fastrnn-results.py\n@@ -23,9 +23,9 @@ def get_times(json_data):\n parser.add_argument('--format', default='md', type=str, help='output format (csv, md, json, table)')\n args = parser.parse_args()\n \n-with open(args.base, \"r\") as base:\n+with open(args.base) as base:\n     base_times = get_times(json.load(base))\n-with open(args.diff, \"r\") as diff:\n+with open(args.diff) as diff:\n     diff_times = get_times(json.load(diff))\n \n all_keys = set(base_times.keys()).union(diff_times.keys())\ndiff --git a/benchmarks/cpp/tensorexpr/bench_ops.py b/benchmarks/cpp/tensorexpr/bench_ops.py\nindex 12d766ae74862c..fef18f912e75c0 100644\n--- a/benchmarks/cpp/tensorexpr/bench_ops.py\n+++ b/benchmarks/cpp/tensorexpr/bench_ops.py\n@@ -83,8 +83,8 @@ def test_batch_norm():\n         [5, 512, 7, 7]]\n     for n, c, h, w in batch_norm_shapes:\n         x = torch.rand((n, c, h, w))\n-        y = torch.rand((c))\n-        z = torch.rand((c))\n+        y = torch.rand(c)\n+        z = torch.rand(c)\n         traced = torch.jit.trace(lambda x, y, z: op(x, y, z), (x, y, z))\n \n         # Warmup.\ndiff --git a/benchmarks/distributed/ddp/benchmark.py b/benchmarks/distributed/ddp/benchmark.py\nindex c72e3e6a27d961..db592cf6bd32d6 100644\n--- a/benchmarks/distributed/ddp/benchmark.py\n+++ b/benchmarks/distributed/ddp/benchmark.py\n@@ -181,7 +181,7 @@ def __init__(self, device, distributed_backend, bucket_size, model):\n         self.model = model\n \n     def __str__(self):\n-        return \"{} with batch size {}\".format(self.model, self.batch_size)\n+        return f\"{self.model} with batch size {self.batch_size}\"\n \n     def create_model(self):\n         return torchvision.models.__dict__[self.model]().to(self.device)\n@@ -212,7 +212,7 @@ def main():\n     # metadata, like measurements. Not for benchmarking itself.\n     dist.init_process_group(\n         backend=\"gloo\",\n-        init_method=\"tcp://{}:{}\".format(args.master_addr, args.master_port),\n+        init_method=f\"tcp://{args.master_addr}:{args.master_port}\",\n         rank=args.rank,\n         world_size=args.world_size,\n     )\n@@ -227,10 +227,10 @@ def main():\n         print(\"PyTorch distributed benchmark suite\")\n         print(\"-----------------------------------\")\n         print(\"\")\n-        print(\"* PyTorch version: {}\".format(torch.__version__))\n-        print(\"* CUDA version: {}\".format(torch.version.cuda))\n-        print(\"* Distributed backend: {}\".format(args.distributed_backend))\n-        print(\"* Maximum bucket size: {}MB\".format(args.bucket_size))\n+        print(f\"* PyTorch version: {torch.__version__}\")\n+        print(f\"* CUDA version: {torch.version.cuda}\")\n+        print(f\"* Distributed backend: {args.distributed_backend}\")\n+        print(f\"* Maximum bucket size: {args.bucket_size}MB\")\n         print(\"\")\n         print(\"--- nvidia-smi topo -m ---\")\n         print(\"\")\n@@ -261,7 +261,7 @@ def main():\n     benchmark_results = []\n     for benchmark in benchmarks:\n         if args.rank == 0:\n-            print(\"\\nBenchmark: {}\".format(str(benchmark)))\n+            print(f\"\\nBenchmark: {str(benchmark)}\")\n         result = sweep(benchmark)\n         benchmark_results.append({\n             \"model\": benchmark.model,\ndiff --git a/benchmarks/distributed/ddp/diff.py b/benchmarks/distributed/ddp/diff.py\nindex d427a5b29d9199..bce7a8db56c13e 100644\n--- a/benchmarks/distributed/ddp/diff.py\n+++ b/benchmarks/distributed/ddp/diff.py\n@@ -10,7 +10,7 @@\n \n \n def load(path):\n-    with open(path, 'r') as f:\n+    with open(path) as f:\n         return json.load(f)\n \n \n@@ -44,8 +44,8 @@ def main():\n \n         model = ra[\"model\"]\n         batch_size = int(ra[\"batch_size\"])\n-        name = \"{} with batch size {}\".format(model, batch_size)\n-        print(\"Benchmark: {}\".format(name))\n+        name = f\"{model} with batch size {batch_size}\"\n+        print(f\"Benchmark: {name}\")\n \n         # Print header\n         print(\"\")\n@@ -66,13 +66,13 @@ def main():\n             ngpus = len(xa[\"ranks\"])\n             ma = sorted(xa[\"measurements\"])\n             mb = sorted(xb[\"measurements\"])\n-            print(\"{:>4d} GPUs:\".format(ngpus), end='')  # noqa: E999\n+            print(f\"{ngpus:>4d} GPUs:\", end='')  # noqa: E999\n             for p in [75, 95]:\n                 va = np.percentile(ma, p)\n                 vb = np.percentile(mb, p)\n                 # We're measuring time, so lower is better (hence the negation)\n                 delta = -100 * ((vb - va) / va)\n-                print(\"  p{:02d}: {:8.3f}s {:7d}/s {:+8.1f}%\".format(p, vb, int(batch_size / vb), delta), end='')  # noqa: E999\n+                print(f\"  p{p:02d}: {vb:8.3f}s {int(batch_size / vb):7d}/s {delta:+8.1f}%\", end='')  # noqa: E999\n             print(\"\")\n         print(\"\")\n \ndiff --git a/benchmarks/distributed/pipeline/pipe.py b/benchmarks/distributed/pipeline/pipe.py\nindex 8a08d25ca4c940..58f2850e34868e 100644\n--- a/benchmarks/distributed/pipeline/pipe.py\n+++ b/benchmarks/distributed/pipeline/pipe.py\n@@ -16,7 +16,7 @@\n def sizeof_fmt(num, suffix='B'):\n     for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti']:\n         if abs(num) < 1024.0:\n-            return \"%3.2f%sB\" % (num, unit)\n+            return f\"{num:3.2f}{unit}B\"\n         num /= 1024.0\n \n \n@@ -146,7 +146,7 @@ def get_last_device(model):\n             return torch.cuda.current_device()\n \n \n-    print('Number of parameters for model: {}'.format(sum(p.numel() for p in model.parameters())))\n+    print(f'Number of parameters for model: {sum(p.numel() for p in model.parameters())}')\n     for i, batch in enumerate(lm_dataloader):\n         bi = batch[\"input\"]\n         if args.max_batch and i > args.max_batch:\ndiff --git a/benchmarks/distributed/rpc/parameter_server/launcher.py b/benchmarks/distributed/rpc/parameter_server/launcher.py\nindex a4c13cdb29b696..ec6559c8508f9c 100644\n--- a/benchmarks/distributed/rpc/parameter_server/launcher.py\n+++ b/benchmarks/distributed/rpc/parameter_server/launcher.py\n@@ -362,7 +362,7 @@ def get_json_config(file_name, id):\n         file_name (str): name of configuration file to load\n         id (str): configuration that will be loaded\n     \"\"\"\n-    with open(os.path.join(Path(__file__).parent, file_name), \"r\") as f:\n+    with open(os.path.join(Path(__file__).parent, file_name)) as f:\n         json_config = json.load(f)[id]\n     return json_config\n \ndiff --git a/benchmarks/distributed/rpc/rl/coordinator.py b/benchmarks/distributed/rpc/rl/coordinator.py\nindex b488378d5aee58..8b6d246f0861f6 100644\n--- a/benchmarks/distributed/rpc/rl/coordinator.py\n+++ b/benchmarks/distributed/rpc/rl/coordinator.py\n@@ -102,7 +102,7 @@ def run_coordinator(self, episodes, episode_steps, queue):\n                              'observer throughput': {}}\n \n \n-        print(\"For batch size {0}\".format(self.batch_size))\n+        print(f\"For batch size {self.batch_size}\")\n         print(\"\\nAgent Latency - \", len(agent_latency_final))\n         agent_latency_final = sorted(agent_latency_final)\n         for p in [50, 75, 90, 95]:\ndiff --git a/benchmarks/dynamo/_onnx/reporter.py b/benchmarks/dynamo/_onnx/reporter.py\nindex f10b50c4e323df..9318b7a6dc2e47 100644\n--- a/benchmarks/dynamo/_onnx/reporter.py\n+++ b/benchmarks/dynamo/_onnx/reporter.py\n@@ -22,7 +22,7 @@\n _COMPACT_ERROR_GROUP = False\n \n \n-class ErrorAggregator(object):\n+class ErrorAggregator:\n     \"\"\"\n     Collect and group error messages for report at the end.\n \n@@ -47,7 +47,7 @@ class ErrorAggregator(object):\n     ]\n \n     def __init__(self, log: Optional[logging.Logger] = None):\n-        super(ErrorAggregator, self).__init__()\n+        super().__init__()\n         self.error_groups = []\n         self.bigram_to_group_ids = collections.defaultdict(list)\n         self.log = log or logging.getLogger(__name__)\n@@ -141,7 +141,7 @@ def __len__(self):\n         return sum(map(len, self.error_groups))\n \n \n-class ErrorAggregatorDict(object):\n+class ErrorAggregatorDict:\n     \"\"\"\n     Collect error types and individually group their error messages for a debug report at the end.\n \n@@ -152,7 +152,7 @@ class ErrorAggregatorDict(object):\n     \"\"\"\n \n     def __init__(self):\n-        super(ErrorAggregatorDict, self).__init__()\n+        super().__init__()\n         self.aggregator: Dict[str, ErrorAggregator] = dict()\n \n     def __getitem__(self, item: str):\n@@ -179,7 +179,7 @@ def record(self, error_type: str, error: str, module: str):\n             log.exception(\"%s error from %s\", error_type, module)\n \n \n-class ExportErrorCsvParser(object):\n+class ExportErrorCsvParser:\n     \"\"\"Parses `*_export_error.csv` produced by onnxbench, aggregates errors and produces report.\n \n     Two types of aggregations are performed.\n@@ -310,7 +310,7 @@ def row(self) -> List[str]:\n         return [getattr(self, field.name) for field in dataclasses.fields(self)]\n \n \n-class ExportErrorParser(object):\n+class ExportErrorParser:\n     def __init__(self, device: str, model_name: str, batch_size: int):\n         self.device = device\n         self.model_name = model_name\ndiff --git a/benchmarks/dynamo/benchmarks.py b/benchmarks/dynamo/benchmarks.py\nindex 36aaf33df96b1f..cb4cc84867ca9b 100755\n--- a/benchmarks/dynamo/benchmarks.py\n+++ b/benchmarks/dynamo/benchmarks.py\n@@ -9,7 +9,7 @@\n # TOOD(voz): Someday, consolidate all the files into one runner instead of a shim like this...\n def model_names(filename: str) -> Set[str]:\n     names = set()\n-    with open(filename, \"r\") as fh:\n+    with open(filename) as fh:\n         lines = fh.readlines()\n         lines = [line.rstrip() for line in lines]\n         for line in lines:\ndiff --git a/benchmarks/dynamo/combine_csv.py b/benchmarks/dynamo/combine_csv.py\nindex b579e0a1bbbd5c..560b8a3cf2405a 100644\n--- a/benchmarks/dynamo/combine_csv.py\n+++ b/benchmarks/dynamo/combine_csv.py\n@@ -11,7 +11,7 @@\n RESULTS = defaultdict(dict)\n \n for side, f in zip([\"static\", \"dynamic\"], sys.argv[1:]):\n-    with open(f, \"r\") as f:\n+    with open(f) as f:\n         reader = csv.DictReader(f)\n         for row in reader:\n             RESULTS[(row[\"bench\"], row[\"name\"])][side] = row\ndiff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py\nindex cabc18f35697a7..0e1ce36461a122 100644\n--- a/benchmarks/dynamo/common.py\n+++ b/benchmarks/dynamo/common.py\n@@ -341,7 +341,7 @@ def load_model_from_path(path_and_class_str):\n \n def output_csv(filename, headers, row):\n     if os.path.exists(filename):\n-        with open(filename, \"r\") as fd:\n+        with open(filename) as fd:\n             lines = list(csv.reader(fd)) or [[]]\n             if headers and len(headers) > len(lines[0]):\n                 # if prior results failed the header might not be filled in yet\n@@ -1417,7 +1417,7 @@ def read_batch_size_from_file(args, filename, model_name):\n     if os.path.exists(\"benchmarks\"):\n         filename = os.path.join(\"benchmarks\", filename)\n     assert os.path.exists(filename), filename\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n         lines = [i.split(\",\") for i in lines if len(i.strip()) > 0]\n         for val in lines:\ndiff --git a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\nindex 997624f583b4c8..f83568c4db6cc6 100644\n--- a/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n+++ b/benchmarks/dynamo/microbenchmarks/operator_inp_utils.py\n@@ -240,7 +240,7 @@ class OperatorInputsLoader:\n     def __init__(self, json_file_path):\n         self.operator_db = defaultdict(Counter)\n \n-        with open(json_file_path, \"r\") as f:\n+        with open(json_file_path) as f:\n             lines = f.readlines()\n \n         i = 0\ndiff --git a/benchmarks/dynamo/parse_logs.py b/benchmarks/dynamo/parse_logs.py\nindex aeb72c231c5462..8ae272897903f9 100644\n--- a/benchmarks/dynamo/parse_logs.py\n+++ b/benchmarks/dynamo/parse_logs.py\n@@ -15,7 +15,7 @@\n \n assert len(sys.argv) == 2\n \n-full_log = open(sys.argv[1], \"r\").read()\n+full_log = open(sys.argv[1]).read()\n \n # If the log contains a gist URL, extract it so we can include it in the CSV\n gist_url = \"\"\ndiff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py\nindex c3c3e8bc046c8e..5a544914ca74bd 100755\n--- a/benchmarks/dynamo/runner.py\n+++ b/benchmarks/dynamo/runner.py\n@@ -608,7 +608,7 @@ def __init__(\n \n     def has_header(self, output_filename):\n         header_present = False\n-        with open(output_filename, \"r\") as f:\n+        with open(output_filename) as f:\n             line = f.readline()\n             if \"dev\" in line:\n                 header_present = True\n@@ -1026,7 +1026,7 @@ def __init__(self, args):\n         assert os.path.exists(self.lookup_file)\n \n     def generate_diff(self, last2, filename, caption):\n-        df_cur, df_prev = [pd.read_csv(os.path.join(path, filename)) for path in last2]\n+        df_cur, df_prev = (pd.read_csv(os.path.join(path, filename)) for path in last2)\n         df_merge = df_cur.merge(df_prev, on=\"Compiler\", suffixes=(\"_cur\", \"_prev\"))\n         data = {col: [] for col in (\"compiler\", \"suite\", \"prev_value\", \"cur_value\")}\n         for _, row in df_merge.iterrows():\n@@ -1145,10 +1145,10 @@ def generate_comment(self):\n                     if last2[compiler] is None:\n                         continue\n \n-                    df_cur, df_prev = [\n+                    df_cur, df_prev = (\n                         last2[compiler][i].untouched_parsed_frames[suite][metric]\n                         for i in (0, 1)\n-                    ]\n+                    )\n                     df_merge = df_cur.merge(\n                         df_prev, on=\"name\", suffixes=(\"_cur\", \"_prev\")\n                     )\n@@ -1367,7 +1367,7 @@ def gen_comment(self):\n         all_lines = []\n         for f in files:\n             try:\n-                with open(os.path.join(self.output_dir, f), \"r\") as fh:\n+                with open(os.path.join(self.output_dir, f)) as fh:\n                     all_lines.extend(fh.readlines())\n             except FileNotFoundError:\n                 pass\ndiff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py\nindex 75769f7cb6c50a..587dbb93683f3f 100755\n--- a/benchmarks/dynamo/timm_models.py\n+++ b/benchmarks/dynamo/timm_models.py\n@@ -31,7 +31,7 @@ def pip_install(package):\n TIMM_MODELS = dict()\n filename = os.path.join(os.path.dirname(__file__), \"timm_models_list.txt\")\n \n-with open(filename, \"r\") as fh:\n+with open(filename) as fh:\n     lines = fh.readlines()\n     lines = [line.rstrip() for line in lines]\n     for line in lines:\n@@ -92,7 +92,7 @@ def read_models_from_docs():\n         models = set()\n         # TODO - set the path to pytorch-image-models repo\n         for fn in glob.glob(\"../pytorch-image-models/docs/models/*.md\"):\n-            with open(fn, \"r\") as f:\n+            with open(fn) as f:\n                 while True:\n                     line = f.readline()\n                     if not line:\ndiff --git a/benchmarks/fastrnns/bench.py b/benchmarks/fastrnns/bench.py\nindex d4b70ff78b7a72..f0e9679b80f275 100644\n--- a/benchmarks/fastrnns/bench.py\n+++ b/benchmarks/fastrnns/bench.py\n@@ -187,7 +187,7 @@ def bench(rnn_runners, group_name, print_json=False, sep=' ', **params):\n \n \n def bench_group(model_list, bench_name, bench_group, bench_args):\n-    print_stderr('Benchmarking {}s...'.format(bench_name))\n+    print_stderr(f'Benchmarking {bench_name}s...')\n     nn_results = bench(get_nn_runners(*model_list), bench_group, **bench_args)\n     print_stderr('')\n     return nn_results\ndiff --git a/benchmarks/fastrnns/profile.py b/benchmarks/fastrnns/profile.py\nindex 7f3de61ef9c39f..10707fab986bc7 100644\n--- a/benchmarks/fastrnns/profile.py\n+++ b/benchmarks/fastrnns/profile.py\n@@ -54,7 +54,7 @@ def profile(rnns, sleep_between_seconds=1, nloops=5,\n \n def system(command):\n     \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n-    print('[system] {}'.format(command))\n+    print(f'[system] {command}')\n     p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                          stderr=subprocess.PIPE, shell=True)\n     output, err = p.communicate()\n@@ -87,13 +87,13 @@ def nvprof_output_filename(rnns, **params):\n \n \n def nvprof(cmd, outpath):\n-    return system('nvprof -o {} {}'.format(outpath, cmd))\n+    return system(f'nvprof -o {outpath} {cmd}')\n \n \n def full_profile(rnns, **args):\n     profile_args = []\n     for k, v in args.items():\n-        profile_args.append('--{}={}'.format(k, v))\n+        profile_args.append(f'--{k}={v}')\n     profile_args.append('--rnns {}'.format(' '.join(rnns)))\n     profile_args.append('--internal-run')\n \n@@ -103,7 +103,7 @@ def full_profile(rnns, **args):\n         sys.executable, ' '.join(profile_args))\n     rc, stdout, stderr = nvprof(cmd, outpath)\n     if rc != 0:\n-        raise RuntimeError('stderr: {}\\nstdout: {}'.format(stderr, stdout))\n+        raise RuntimeError(f'stderr: {stderr}\\nstdout: {stdout}')\n \n \n if __name__ == '__main__':\ndiff --git a/benchmarks/fastrnns/runner.py b/benchmarks/fastrnns/runner.py\nindex c6bf3727f38071..c33f3c92ad0f04 100644\n--- a/benchmarks/fastrnns/runner.py\n+++ b/benchmarks/fastrnns/runner.py\n@@ -11,7 +11,7 @@\n                       varlen_lstm_creator, varlen_pytorch_lstm_creator)\n \n \n-class DisableCuDNN():\n+class DisableCuDNN:\n     def __enter__(self):\n         self.saved = torch.backends.cudnn.enabled\n         torch.backends.cudnn.enabled = False\n@@ -20,7 +20,7 @@ def __exit__(self, *args, **kwargs):\n         torch.backends.cudnn.enabled = self.saved\n \n \n-class DummyContext():\n+class DummyContext:\n     def __enter__(self):\n         pass\n \n@@ -28,7 +28,7 @@ def __exit__(self, *args, **kwargs):\n         pass\n \n \n-class AssertNoJIT():\n+class AssertNoJIT:\n     def __enter__(self):\n         import os\n         enabled = os.environ.get('PYTORCH_JIT', 1)\ndiff --git a/benchmarks/fastrnns/test.py b/benchmarks/fastrnns/test.py\nindex a56cf928fd7add..640af10b95c042 100644\n--- a/benchmarks/fastrnns/test.py\n+++ b/benchmarks/fastrnns/test.py\n@@ -71,7 +71,7 @@ def test_vl_py(**test_args):\n     control_creator = varlen_pytorch_lstm_creator\n     name, experim_creator, context = get_nn_runners('vl_py')[0]\n     with context():\n-        print('testing {}...'.format(name))\n+        print(f'testing {name}...')\n         creator_keys = [\n             'seqLength', 'numLayers', 'inputSize',\n             'hiddenSize', 'miniBatch', 'device', 'seed'\n@@ -154,5 +154,5 @@ def test_vl_py(**test_args):\n \n     for name, creator, context in rnn_runners:\n         with context():\n-            print('testing {}...'.format(name))\n+            print(f'testing {name}...')\n             test_rnns(creator, pytorch_lstm_creator, **test_args)\ndiff --git a/benchmarks/framework_overhead_benchmark/C2Module.py b/benchmarks/framework_overhead_benchmark/C2Module.py\nindex dfc5e6e79098a6..b6b80e83db07d0 100644\n--- a/benchmarks/framework_overhead_benchmark/C2Module.py\n+++ b/benchmarks/framework_overhead_benchmark/C2Module.py\n@@ -20,14 +20,14 @@ class C2SimpleNet:\n     def __init__(self, op_name, num_inputs=1, debug=False):\n         self.input_names = []\n         self.net = core.Net(\"framework_benchmark_net\")\n-        self.input_names = [\"in_{}\".format(i) for i in range(num_inputs)]\n+        self.input_names = [f\"in_{i}\" for i in range(num_inputs)]\n         for i in range(num_inputs):\n             add_blob(workspace, self.input_names[i], [1])\n         self.net.AddExternalInputs(self.input_names)\n         op_constructor = getattr(self.net, op_name)\n         op_constructor(self.input_names)\n         self.output_name = self.net._net.op[-1].output\n-        print(\"Benchmarking op {}:\".format(op_name))\n+        print(f\"Benchmarking op {op_name}:\")\n         for _ in range(NUM_LOOP_ITERS):\n             output_name = self.net._net.op[-1].output\n             self.input_names[-1] = output_name[0]\ndiff --git a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\nindex 727b78197b39bc..fd02a00c43655d 100644\n--- a/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n+++ b/benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py\n@@ -31,7 +31,7 @@ def parse_op_args(op):\n def print_results(result):\n     print(\"===================================\")\n     for key, value in result.items():\n-        print(\"{}, latency per iter (us):{}\".format(key, ms_to_us(value)))\n+        print(f\"{key}, latency per iter (us):{ms_to_us(value)}\")\n     print(\"===================================\")\n \n def benchmark_simple_fn(args, config, module_config, module_type, result):\n@@ -46,7 +46,7 @@ def benchmark_simple_fn(args, config, module_config, module_type, result):\n         result:         dictionary instance to be populated with the benchmark result (latency per iter).\n     \"\"\"\n     benchmark_c2_net = args.benchmark_c2_net\n-    print(\"Benchmarking {}\".format(module_type.__name__))\n+    print(f\"Benchmarking {module_type.__name__}\")\n     if benchmark_c2_net:\n         op_name = module_config.c2_op\n         num_inputs = module_config.num_params\n@@ -86,7 +86,7 @@ def main():\n     args = parser.parse_args()\n \n     if args.op not in SUPPORTED_OPS:\n-        print(\"Op {} is not supported: Supported ops are:{}\".format(args.op, SUPPORTED_OPS))\n+        print(f\"Op {args.op} is not supported: Supported ops are:{SUPPORTED_OPS}\")\n         return\n     assert not (args.benchmark_c2_net and args.use_throughput_benchmark), \\\n         \"Benchmarking of C2 net via throughput benchmarking is not yet supported\"\ndiff --git a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\nindex 154564f1c6d799..19f2471cbbfaaa 100644\n--- a/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n+++ b/benchmarks/framework_overhead_benchmark/pt_wrapper_module.py\n@@ -31,8 +31,8 @@ def __init__(self, wrapped_type, module_config, debug, save=False):\n             if save:\n                 file_name = self.module_name + \"_\" + pt_fn.__name__ + \".pt\"\n                 torch.jit.save(self.module, file_name)\n-                print(\"Generated graph is saved in {}\".format(file_name))\n-        print(\"Benchmarking module {} with fn {}: Graph mode:{}\".format(self.module_name, pt_fn.__name__, module_config.graph_mode))\n+                print(f\"Generated graph is saved in {file_name}\")\n+        print(f\"Benchmarking module {self.module_name} with fn {pt_fn.__name__}: Graph mode:{module_config.graph_mode}\")\n         if (debug and isinstance(self.module, torch.jit.ScriptModule)):\n             print(self.module.graph)\n             print(self.module.code)\ndiff --git a/benchmarks/framework_overhead_benchmark/utils.py b/benchmarks/framework_overhead_benchmark/utils.py\nindex 9e760d404339ed..2efb67a51f7887 100644\n--- a/benchmarks/framework_overhead_benchmark/utils.py\n+++ b/benchmarks/framework_overhead_benchmark/utils.py\n@@ -26,7 +26,7 @@ def benchmark_module(config, module, use_throughput_benchmark=False):\n     if use_throughput_benchmark:\n         return benchmark_using_throughput_benchmark(config, module)\n     module.forward(config.num_warmup_iters)\n-    print(\"Running module for {} iterations\".format(config.num_iters))\n+    print(f\"Running module for {config.num_iters} iterations\")\n     start = time.time()\n     module.forward(config.num_iters)\n     end = time.time()\ndiff --git a/benchmarks/functional_autograd_benchmark/compare.py b/benchmarks/functional_autograd_benchmark/compare.py\nindex c2c4ef6c95d5be..65a4a3afcea881 100644\n--- a/benchmarks/functional_autograd_benchmark/compare.py\n+++ b/benchmarks/functional_autograd_benchmark/compare.py\n@@ -10,11 +10,11 @@ def main():\n     parser.add_argument(\"--output\", type=str, default=\"\", help=\"Text file where to write the output\")\n     args = parser.parse_args()\n \n-    with open(args.before, \"r\") as f:\n+    with open(args.before) as f:\n         content = f.read()\n     res_before = from_markdown_table(content)\n \n-    with open(args.after, \"r\") as f:\n+    with open(args.after) as f:\n         content = f.read()\n     res_after = from_markdown_table(content)\n \ndiff --git a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\nindex 1b0ef20902da9b..76c447f04a496f 100644\n--- a/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n+++ b/benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py\n@@ -198,7 +198,7 @@ def noop():\n             pass\n         do_sync = noop\n     else:\n-        device = torch.device(\"cuda:{}\".format(args.gpu))\n+        device = torch.device(f\"cuda:{args.gpu}\")\n         do_sync = torch.cuda.synchronize\n \n     model, inp = model_getter(device)\n@@ -257,7 +257,7 @@ def main():\n             runtimes = torch.tensor(runtimes)\n             mean, var = runtimes.mean(), runtimes.var()\n             results[name][task] = (mean.item(), var.item())\n-            print(\"Results for model {} on task {}: {}s (var: {})\".format(name, task, mean, var))\n+            print(f\"Results for model {name} on task {task}: {mean}s (var: {var})\")\n \n             if has_functorch:\n                 try:\n@@ -269,7 +269,7 @@ def main():\n                 runtimes = torch.tensor(runtimes)\n                 mean, var = runtimes.mean(), runtimes.var()\n                 results[name][f\"functorch {task}\"] = (mean.item(), var.item())\n-                print(\"Results for model {} on task {} using Functorch: {}s (var: {})\".format(name, task, mean, var))\n+                print(f\"Results for model {name} on task {task} using Functorch: {mean}s (var: {var})\")\n \n     if args.output:\n         with open(args.output, \"w\") as f:\ndiff --git a/benchmarks/functional_autograd_benchmark/utils.py b/benchmarks/functional_autograd_benchmark/utils.py\nindex dcf03e7a28d085..23f3481cbde117 100644\n--- a/benchmarks/functional_autograd_benchmark/utils.py\n+++ b/benchmarks/functional_autograd_benchmark/utils.py\n@@ -97,7 +97,7 @@ def from_markdown_table(data: str) -> TimingResultType:\n     res = defaultdict(defaultdict)\n \n     for line in out:\n-        model, task, mean, var = [f.strip() for f in line.strip().split(\"|\") if f]\n+        model, task, mean, var = (f.strip() for f in line.strip().split(\"|\") if f)\n         res[model][task] = (float(mean), float(var))\n \n     return res\ndiff --git a/benchmarks/instruction_counts/applications/ci.py b/benchmarks/instruction_counts/applications/ci.py\nindex 85ee9881d83b55..c34c60b094a857 100644\n--- a/benchmarks/instruction_counts/applications/ci.py\n+++ b/benchmarks/instruction_counts/applications/ci.py\n@@ -70,7 +70,7 @@ def main(argv: List[str]) -> None:\n     }\n \n     if args.destination:\n-        with open(args.destination, \"wt\") as f:\n+        with open(args.destination, \"w\") as f:\n             json.dump(final_results, f)\n \n     if in_debug_mode:\ndiff --git a/benchmarks/instruction_counts/core/expand.py b/benchmarks/instruction_counts/core/expand.py\nindex f6713ee65cb93c..c60925d9e14e91 100644\n--- a/benchmarks/instruction_counts/core/expand.py\n+++ b/benchmarks/instruction_counts/core/expand.py\n@@ -58,7 +58,7 @@ def _generate_torchscript_file(model_src: str, name: str) -> Optional[str]:\n         # to confirm.\n         raise ValueError(f\"File {module_path} already exists.\")\n \n-    with open(module_path, \"wt\") as f:\n+    with open(module_path, \"w\") as f:\n         f.write(model_src)\n \n     # Import magic to actually load our function.\ndiff --git a/benchmarks/operator_benchmark/benchmark_caffe2.py b/benchmarks/operator_benchmark/benchmark_caffe2.py\nindex d5939030d03c1a..df27a172739bf6 100644\n--- a/benchmarks/operator_benchmark/benchmark_caffe2.py\n+++ b/benchmarks/operator_benchmark/benchmark_caffe2.py\n@@ -122,7 +122,7 @@ def run_forward(self, num_runs, print_per_iter=False, cuda_sync=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.forward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator test case: {self.test_name}\")\n \n     def run_backward(self, num_runs, print_per_iter=False):\n         \"\"\" Run the backward path of an operator in a loop\n@@ -130,7 +130,7 @@ def run_backward(self, num_runs, print_per_iter=False):\n         with core.DeviceScope(self.op_bench.dev):\n             op = self.op_bench.backward()\n         if not workspace.RunOperatorMultiple(op, num_runs):\n-            raise ValueError(\"Unable to run operator gradient test case: {}\".format(self.test_name))\n+            raise ValueError(f\"Unable to run operator gradient test case: {self.test_name}\")\n \n     def _print_per_iter(self):\n         pass\n@@ -140,7 +140,7 @@ def create_caffe2_op_test_case(op_bench, test_config):\n     test_case = Caffe2OperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\n \n \ndiff --git a/benchmarks/operator_benchmark/benchmark_core.py b/benchmarks/operator_benchmark/benchmark_core.py\nindex d6fd00f0522b38..3dfb6e3b00d42c 100644\n--- a/benchmarks/operator_benchmark/benchmark_core.py\n+++ b/benchmarks/operator_benchmark/benchmark_core.py\n@@ -197,7 +197,7 @@ def _print_header(self):\n             print(\"# List of Operators to run:\")\n             self.printed_ops_list = set()\n             if self.args.operators:\n-                print(\"# {}\".format(self.args.operators))\n+                print(f\"# {self.args.operators}\")\n \n     def _print_perf_result(self, reported_run_time_us, test_case):\n         if self.args.report_aibench:\n@@ -206,7 +206,7 @@ def _print_perf_result(self, reported_run_time_us, test_case):\n             return\n             test_name = '_'.join([test_case.framework, test_case.test_config.test_name])\n             for run in range(self.num_runs):\n-                print(\"{}Observer \".format(test_case.framework) + json.dumps(\n+                print(f\"{test_case.framework}Observer \" + json.dumps(\n                     {\n                         \"type\": test_name,\n                         \"metric\": \"latency\",\n@@ -349,14 +349,14 @@ def _keep_test(self, test_case):\n     def _print_test_case_info(self, test_case):\n         # Print out the test name and skip the real execution\n         if self.args.list_tests:\n-            print(\"# {}\".format(test_case.test_config.test_name))\n+            print(f\"# {test_case.test_config.test_name}\")\n             return True\n         elif self.args.list_ops:\n             if self.args.operators is None:\n                 op_name = test_case.op_bench.module_name()\n \n                 if op_name not in self.printed_ops_list:\n-                    print(\"# {}\".format(op_name))\n+                    print(f\"# {op_name}\")\n                     self.printed_ops_list.add(op_name)\n             return True\n \ndiff --git a/benchmarks/operator_benchmark/benchmark_pytorch.py b/benchmarks/operator_benchmark/benchmark_pytorch.py\nindex e9a9b3c5de42ad..c4a82dff2ba5c1 100644\n--- a/benchmarks/operator_benchmark/benchmark_pytorch.py\n+++ b/benchmarks/operator_benchmark/benchmark_pytorch.py\n@@ -192,5 +192,5 @@ def create_pytorch_op_test_case(op_bench, test_config):\n     test_case = PyTorchOperatorTestCase(op_bench, test_config)\n     test_config = test_case.test_config\n     op = test_case.op_bench\n-    func_name = \"{}{}{}\".format(op.module_name(), test_case.framework, str(test_config))\n+    func_name = f\"{op.module_name()}{test_case.framework}{str(test_config)}\"\n     return (func_name, test_case)\ndiff --git a/benchmarks/operator_benchmark/common/repeat_benchmark.py b/benchmarks/operator_benchmark/common/repeat_benchmark.py\nindex b744a95d217363..337bf525b29caf 100644\n--- a/benchmarks/operator_benchmark/common/repeat_benchmark.py\n+++ b/benchmarks/operator_benchmark/common/repeat_benchmark.py\n@@ -54,4 +54,4 @@ def pt_repeat_n_times(niters):\n     total_time_s = (time.time() - s)\n     total_time_per_iter_s = total_time_s / NUM_BENCHMARK_ITERS\n     achieved_bandwidth = (total_bytes * BYTES_TO_MB) / total_time_per_iter_s\n-    print(\"Time:{} Achieved Bandwidth:{} MB/s\".format(total_time_per_iter_s, achieved_bandwidth))\n+    print(f\"Time:{total_time_per_iter_s} Achieved Bandwidth:{achieved_bandwidth} MB/s\")\ndiff --git a/benchmarks/overrides_benchmark/bench.py b/benchmarks/overrides_benchmark/bench.py\nindex b6dbd0c2f8d64c..2c591a6e569793 100644\n--- a/benchmarks/overrides_benchmark/bench.py\n+++ b/benchmarks/overrides_benchmark/bench.py\n@@ -56,8 +56,8 @@ def main():\n \n         bench_min, bench_std = bench(tensor_1, tensor_2)\n         print(\n-            \"Type {0} had a minimum time of {1} us\"\n-            \" and a standard deviation of {2} us.\".format(\n+            \"Type {} had a minimum time of {} us\"\n+            \" and a standard deviation of {} us.\".format(\n                 t.__name__, (10 ** 6 * bench_min), (10 ** 6) * bench_std\n             )\n         )\ndiff --git a/benchmarks/sparse/dlmc/matmul_bench.py b/benchmarks/sparse/dlmc/matmul_bench.py\nindex 6b896ddf34a635..8d37d3242dd1bf 100644\n--- a/benchmarks/sparse/dlmc/matmul_bench.py\n+++ b/benchmarks/sparse/dlmc/matmul_bench.py\n@@ -62,8 +62,8 @@ def filter_ops(operation):\n             test_name = device + \":matmul-forward\"\n             return list(filter(None, [\n                 (test_name, device, \"torch:\" + operation.replace(\"sparse\", \"dense\"),\n-                 \"{}(dx, dy)\".format(OPS_MAP[operation])),\n-                (test_name, device, \"torch:\" + operation, \"{}(x, y)\".format(OPS_MAP[operation])),\n+                 f\"{OPS_MAP[operation]}(dx, dy)\"),\n+                (test_name, device, \"torch:\" + operation, f\"{OPS_MAP[operation]}(x, y)\"),\n                 (test_name, device, \"scipy:\" + operation, \"scipy_matmul(sx, sy)\") if device == \"cpu\" else None\n             ]))\n \ndiff --git a/benchmarks/sparse/dlmc/utils.py b/benchmarks/sparse/dlmc/utils.py\nindex 3079abf6e1dff2..8fad391f63f83d 100644\n--- a/benchmarks/sparse/dlmc/utils.py\n+++ b/benchmarks/sparse/dlmc/utils.py\n@@ -21,7 +21,7 @@ def sparse_grad_output(a, b):\n \n \n def read_matrix_params(path):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         line = file.readline()\n         nrows, ncols, nnz = (int(el) for el in line.split(', '))\n         return (nrows, ncols), nnz\n@@ -38,7 +38,7 @@ def csr_to_coo(indices, indptr, shape):\n \n \n def load_sparse_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -51,7 +51,7 @@ def load_sparse_matrix(path, device):\n \n \n def gen_vector(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\n@@ -59,7 +59,7 @@ def gen_vector(path, device):\n \n \n def gen_matrix(path, device):\n-    with open(path, 'r') as file:\n+    with open(path) as file:\n         nrows, ncols, nnz = (int(el) for el in file.readline().split(', '))\n         index_pointers = (int(el) for el in file.readline().split())\n         indices = (int(el) for el in file.readline().split())\ndiff --git a/benchmarks/tensorexpr/__main__.py b/benchmarks/tensorexpr/__main__.py\nindex ed632e966b2cb4..fa81ea6bb1c611 100644\n--- a/benchmarks/tensorexpr/__main__.py\n+++ b/benchmarks/tensorexpr/__main__.py\n@@ -157,7 +157,7 @@ def main():\n         torch._C._jit_set_nvfuser_enabled(True)\n         torch._C._get_graph_executor_optimize(True)\n     else :\n-        raise ValueError(\"Undefined fuser: {}\".format(args.cuda_fuser))\n+        raise ValueError(f\"Undefined fuser: {args.cuda_fuser}\")\n \n     if args.cpu_fusion:\n         import torch\n@@ -207,7 +207,7 @@ def set_global_threads(num_threads):\n     for index, dtype in enumerate(datatypes):\n         datatypes[index] = getattr(torch, dtype)\n         if not datatypes[index] :\n-            raise AttributeError(\"DataType: {} is not valid!\".format(dtype))\n+            raise AttributeError(f\"DataType: {dtype} is not valid!\")\n \n     tensor_engine.set_engine_mode(args.engine)\n \n@@ -282,7 +282,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                         run_with_input_iter(bench_cls, args.input_iter, allow_skip=True)\n                     else :\n                         if args.input_iter is not None :\n-                            print(\"WARNING: Incompatible benchmark class called with input_iter arg: {}\".format(name))\n+                            print(f\"WARNING: Incompatible benchmark class called with input_iter arg: {name}\")\n                         run_default_configs(bench_cls, allow_skip=True)\n \n             if match_class_name:\n@@ -321,8 +321,7 @@ def run_with_input_iter(bench_cls, input_iter, allow_skip=True):\n                     [bench_cls.module() for bench_cls in benchmark_classes]\n                 )\n                 raise ValueError(\n-                    \"invalid name: %s\\nAvailable benchmark classes:\\n%s\"\n-                    % (name, available_classes)\n+                    f\"invalid name: {name}\\nAvailable benchmark classes:\\n{available_classes}\"\n                 )\n \n \ndiff --git a/benchmarks/tensorexpr/benchmark.py b/benchmarks/tensorexpr/benchmark.py\nindex 7a5b255da904aa..2730a1be24784b 100644\n--- a/benchmarks/tensorexpr/benchmark.py\n+++ b/benchmarks/tensorexpr/benchmark.py\n@@ -66,7 +66,7 @@ def desc(self):\n         if \"NNC_NUM_THREADS\" in os.environ:\n             num_threads_str = os.environ[\"NNC_NUM_THREADS\"]\n             device += num_threads_str\n-        return \"%s: %s_%s_%s_%s\" % (\n+        return \"{}: {}_{}_{}_{}\".format(\n             self.engine.mode,\n             self.module(),\n             self.mode,\n@@ -203,7 +203,7 @@ def dump_result(self, result_dict):\n         if self.output_type == \"json\":\n             print(json.dumps(result_dict))\n         elif self.output_type == \"stdout\":\n-            msg = \"%s: %.2f us, SOL %.2f GB/s, algorithmic %.2f GB/s\" % (\n+            msg = \"{}: {:.2f} us, SOL {:.2f} GB/s, algorithmic {:.2f} GB/s\".format(\n                 result_dict[\"desc\"],\n                 result_dict[\"us\"],\n                 result_dict[\"sol\"],\ndiff --git a/benchmarks/tensorexpr/reduction.py b/benchmarks/tensorexpr/reduction.py\nindex 77d64074eb81d1..3613001667746d 100644\n--- a/benchmarks/tensorexpr/reduction.py\n+++ b/benchmarks/tensorexpr/reduction.py\n@@ -139,7 +139,7 @@ def __init__(self, mode, device, dtype, red_dim, dim0, dim1):\n         )]\n \n         if red_dim != 0 and red_dim != 1 :\n-            raise ValueError(\"invalid reduction dimension: {}\".format(red_dim))\n+            raise ValueError(f\"invalid reduction dimension: {red_dim}\")\n \n     def forward(self, inputs):\n         x = self.add(inputs, 0.001)\ndiff --git a/benchmarks/upload_scribe.py b/benchmarks/upload_scribe.py\nindex d476ade1b8df4d..551544b2d288ae 100644\n--- a/benchmarks/upload_scribe.py\n+++ b/benchmarks/upload_scribe.py\n@@ -95,7 +95,7 @@ def post_pytest_benchmarks(self, pytest_json):\n         for b in pytest_json['benchmarks']:\n             test = b['name'].split('[')[0]\n             net_name = b['params']['net_name']\n-            benchmark_name = '{}[{}]'.format(test, net_name)\n+            benchmark_name = f'{test}[{net_name}]'\n             executor = b['params']['executor']\n             fuser = b['params']['fuser']\n             m = self.format_message({\n"
  },
  {
    "number": 105388,
    "title": "[BE] Enable ruff's UP rules and autoformat tools and scripts",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "6b26bfef4360e3c9b42c726fdacd93c3fcaeb1eb",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105388",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105388/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105388.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105388.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105388/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105388/comments",
    "labels": [
      "open source",
      "release notes: releng"
    ],
    "_event_time": "2023-07-18T01:11:30.398251Z",
    "state": "closed",
    "patch": "From ef2ee0564c20a640b92cd88d1695a8d5515cd588 Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:24 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat tools and scripts\n\n[ghstack-poisoned]\n---\n .ci/pytorch/create_test_cert.py               |  4 +-\n .../perf_test/compare_with_baseline.py        |  4 +-\n .../cimodel/data/binary_build_definitions.py  |  2 +-\n .../cimodel/data/pytorch_build_definitions.py |  4 +-\n .circleci/generate_config_yml.py              |  4 +-\n .github/scripts/ensure_actions_will_cancel.py |  4 +-\n .github/scripts/file_io_utils.py              |  2 +-\n .github/scripts/filter_test_configs.py        |  2 +-\n .github/scripts/generate_pytorch_version.py   |  2 +-\n .github/scripts/label_utils.py                |  2 +-\n .github/scripts/lint_native_functions.py      |  2 +-\n .github/scripts/run_torchbench.py             |  4 +-\n .github/scripts/test_check_labels.py          |  2 +-\n .github/scripts/test_trymerge.py              |  2 +-\n .github/scripts/trymerge.py                   |  6 +--\n .github/scripts/trymerge_explainer.py         |  2 +-\n docs/caffe2/process.py                        |  4 +-\n docs/cpp/source/conf.py                       |  3 +-\n docs/source/conf.py                           |  3 +-\n .../scripts/exportdb/generate_example_rst.py  |  4 +-\n setup.py                                      | 38 +++++++++----------\n tools/amd_build/build_amd.py                  |  8 ++--\n tools/autograd/gen_python_functions.py        |  8 ++--\n tools/autograd/load_derivatives.py            |  2 +-\n .../gen_op_registration_allowlist.py          |  4 +-\n tools/code_analyzer/gen_oplist.py             |  2 +-\n tools/coverage_plugins_package/setup.py       |  2 +-\n tools/download_mnist.py                       | 10 ++---\n tools/gen_vulkan_spv.py                       | 20 +++++-----\n tools/generate_torch_version.py               | 12 +++---\n tools/jit/gen_unboxing.py                     |  2 +-\n tools/linter/adapters/constexpr_linter.py     |  2 +-\n tools/linter/adapters/grep_linter.py          |  2 +-\n tools/nightly.py                              |  8 ++--\n tools/onnx/gen_diagnostics.py                 |  2 +-\n tools/onnx/update_default_opset_version.py    |  2 +-\n tools/pyi/gen_pyi.py                          | 20 +++++-----\n tools/setup_helpers/cmake.py                  | 18 ++++-----\n tools/setup_helpers/cmake_utils.py            |  2 +-\n tools/setup_helpers/generate_code.py          |  2 +-\n tools/stats/import_test_stats.py              |  2 +-\n tools/stats/upload_stats_lib.py               |  4 +-\n tools/substitute.py                           |  2 +-\n tools/test/test_executorch_gen.py             |  4 +-\n tools/test/test_executorch_signatures.py      |  8 ++--\n tools/test/test_vulkan_codegen.py             |  4 +-\n tools/testing/explicit_ci_jobs.py             |  2 +-\n tools/testing/test_selections.py              |  2 +-\n torch/package/package_exporter.py             | 20 ++++------\n 49 files changed, 134 insertions(+), 142 deletions(-)\n\ndiff --git a/.ci/pytorch/create_test_cert.py b/.ci/pytorch/create_test_cert.py\nindex d3ead7ae259434..4e31f97878f41b 100644\n--- a/.ci/pytorch/create_test_cert.py\n+++ b/.ci/pytorch/create_test_cert.py\n@@ -88,9 +88,9 @@ def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n \n \n ca_key = genrsa(temp_dir + \"/ca.key\")\n-ca_cert = create_cert(temp_dir + \"/ca.pem\", u\"US\", u\"New York\", u\"New York\", u\"Gloo Certificate Authority\", ca_key)\n+ca_cert = create_cert(temp_dir + \"/ca.pem\", \"US\", \"New York\", \"New York\", \"Gloo Certificate Authority\", ca_key)\n \n pkey = genrsa(temp_dir + \"/pkey.key\")\n-csr = create_req(temp_dir + \"/csr.csr\", u\"US\", u\"California\", u\"San Francisco\", u\"Gloo Testing Company\", pkey)\n+csr = create_req(temp_dir + \"/csr.csr\", \"US\", \"California\", \"San Francisco\", \"Gloo Testing Company\", pkey)\n \n cert = sign_certificate_request(temp_dir + \"/cert.pem\", csr, ca_cert, ca_key)\ndiff --git a/.ci/pytorch/perf_test/compare_with_baseline.py b/.ci/pytorch/perf_test/compare_with_baseline.py\nindex 6d2839ac1db412..f7b962632cd79f 100644\n--- a/.ci/pytorch/perf_test/compare_with_baseline.py\n+++ b/.ci/pytorch/perf_test/compare_with_baseline.py\n@@ -19,7 +19,7 @@\n elif 'gpu' in test_name:\n     backend = 'gpu'\n \n-data_file_path = '../{}_runtime.json'.format(backend)\n+data_file_path = f'../{backend}_runtime.json'\n \n with open(data_file_path) as data_file:\n     data = json.load(data_file)\n@@ -69,7 +69,7 @@\n     print(\"z-value < 3, no perf regression detected.\")\n     if args.update:\n         print(\"We will use these numbers as new baseline.\")\n-        new_data_file_path = '../new_{}_runtime.json'.format(backend)\n+        new_data_file_path = f'../new_{backend}_runtime.json'\n         with open(new_data_file_path) as new_data_file:\n             new_data = json.load(new_data_file)\n         new_data[test_name] = {}\ndiff --git a/.circleci/cimodel/data/binary_build_definitions.py b/.circleci/cimodel/data/binary_build_definitions.py\nindex 45981e8e9ea77b..7dccbdc7cbf689 100644\n--- a/.circleci/cimodel/data/binary_build_definitions.py\n+++ b/.circleci/cimodel/data/binary_build_definitions.py\n@@ -5,7 +5,7 @@\n import cimodel.lib.conf_tree as conf_tree\n import cimodel.lib.miniutils as miniutils\n \n-class Conf(object):\n+class Conf:\n     def __init__(self, os, gpu_version, pydistro, parms, smoke, libtorch_variant, gcc_config_variant, libtorch_config_variant):\n \n         self.os = os\ndiff --git a/.circleci/cimodel/data/pytorch_build_definitions.py b/.circleci/cimodel/data/pytorch_build_definitions.py\nindex 76e87b07c1889f..e6e44bd2b5aeb0 100644\n--- a/.circleci/cimodel/data/pytorch_build_definitions.py\n+++ b/.circleci/cimodel/data/pytorch_build_definitions.py\n@@ -143,7 +143,7 @@ def gen_workflow_job(self, phase):\n \n \n # TODO This is a hack to special case some configs just for the workflow list\n-class HiddenConf(object):\n+class HiddenConf:\n     def __init__(self, name, parent_build=None, filters=None):\n         self.name = name\n         self.parent_build = parent_build\n@@ -160,7 +160,7 @@ def gen_workflow_job(self, phase):\n     def gen_build_name(self, _):\n         return self.name\n \n-class DocPushConf(object):\n+class DocPushConf:\n     def __init__(self, name, parent_build=None, branch=\"master\"):\n         self.name = name\n         self.parent_build = parent_build\ndiff --git a/.circleci/generate_config_yml.py b/.circleci/generate_config_yml.py\nindex b3e47eed8b4317..d1ef439941d4b2 100755\n--- a/.circleci/generate_config_yml.py\n+++ b/.circleci/generate_config_yml.py\n@@ -18,7 +18,7 @@\n import cimodel.lib.miniyaml as miniyaml\n \n \n-class File(object):\n+class File:\n     \"\"\"\n     Verbatim copy the contents of a file into config.yml\n     \"\"\"\n@@ -57,7 +57,7 @@ def horizontal_rule():\n     return \"\".join(\"#\" * 78)\n \n \n-class Header(object):\n+class Header:\n     def __init__(self, title, summary=None):\n         self.title = title\n         self.summary_lines = summary or []\ndiff --git a/.github/scripts/ensure_actions_will_cancel.py b/.github/scripts/ensure_actions_will_cancel.py\nindex 92eb3441acd3cf..8d53f2bed5e18b 100755\n--- a/.github/scripts/ensure_actions_will_cancel.py\n+++ b/.github/scripts/ensure_actions_will_cancel.py\n@@ -17,7 +17,7 @@\n \n \n def should_check(filename: Path) -> bool:\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         content = f.read()\n \n     data = yaml.safe_load(content)\n@@ -37,7 +37,7 @@ def should_check(filename: Path) -> bool:\n     files = [f for f in files if should_check(f)]\n     names = set()\n     for filename in files:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             data = yaml.safe_load(f)\n \n         name = data.get(\"name\")\ndiff --git a/.github/scripts/file_io_utils.py b/.github/scripts/file_io_utils.py\nindex 097b092bc904af..faba9f06d2ac65 100644\n--- a/.github/scripts/file_io_utils.py\n+++ b/.github/scripts/file_io_utils.py\n@@ -44,7 +44,7 @@ def load_json_file(file_path: Path) -> Any:\n     \"\"\"\n     Returns the deserialized json object\n     \"\"\"\n-    with open(file_path, \"r\") as f:\n+    with open(file_path) as f:\n         return json.load(f)\n \n \ndiff --git a/.github/scripts/filter_test_configs.py b/.github/scripts/filter_test_configs.py\nindex 9d1f39a833c571..92968179702273 100755\n--- a/.github/scripts/filter_test_configs.py\n+++ b/.github/scripts/filter_test_configs.py\n@@ -319,7 +319,7 @@ def process_jobs(\n     try:\n         # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking\n         # it into its two components first\n-        current_platform, _ = [n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n]\n+        current_platform, _ = (n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n)\n     except ValueError as error:\n         warnings.warn(f\"Invalid job name {job_name}, returning\")\n         return test_matrix\ndiff --git a/.github/scripts/generate_pytorch_version.py b/.github/scripts/generate_pytorch_version.py\nindex f9a6d49505b308..e70b97165e61d7 100755\n--- a/.github/scripts/generate_pytorch_version.py\n+++ b/.github/scripts/generate_pytorch_version.py\n@@ -50,7 +50,7 @@ def get_tag() -> str:\n \n def get_base_version() -> str:\n     root = get_pytorch_root()\n-    dirty_version = open(root / \"version.txt\", \"r\").read().strip()\n+    dirty_version = open(root / \"version.txt\").read().strip()\n     # Strips trailing a0 from version.txt, not too sure why it's there in the\n     # first place\n     return re.sub(LEGACY_BASE_VERSION_SUFFIX_PATTERN, \"\", dirty_version)\ndiff --git a/.github/scripts/label_utils.py b/.github/scripts/label_utils.py\nindex 812c33b426f441..e3ce0f52fe853f 100644\n--- a/.github/scripts/label_utils.py\n+++ b/.github/scripts/label_utils.py\n@@ -51,7 +51,7 @@ def get_last_page_num_from_header(header: Any) -> int:\n     )\n \n \n-@lru_cache()\n+@lru_cache\n def gh_get_labels(org: str, repo: str) -> List[str]:\n     prefix = f\"https://api.github.com/repos/{org}/{repo}/labels?per_page=100\"\n     header, info = request_for_labels(prefix + \"&page=1\")\ndiff --git a/.github/scripts/lint_native_functions.py b/.github/scripts/lint_native_functions.py\nindex 9bde9e8d84e5f9..4dfe9fd63e2e4e 100755\n--- a/.github/scripts/lint_native_functions.py\n+++ b/.github/scripts/lint_native_functions.py\n@@ -26,7 +26,7 @@ def fn(base: str) -> str:\n     return str(base / Path(\"aten/src/ATen/native/native_functions.yaml\"))\n \n \n-with open(Path(__file__).parent.parent.parent / fn(\".\"), \"r\") as f:\n+with open(Path(__file__).parent.parent.parent / fn(\".\")) as f:\n     contents = f.read()\n \n yaml = ruamel.yaml.YAML()  # type: ignore[attr-defined]\ndiff --git a/.github/scripts/run_torchbench.py b/.github/scripts/run_torchbench.py\nindex 3a80ebbeb9970a..e5e3c7a03dea07 100644\n--- a/.github/scripts/run_torchbench.py\n+++ b/.github/scripts/run_torchbench.py\n@@ -129,7 +129,7 @@ def extract_models_from_pr(\n     model_list = []\n     userbenchmark_list = []\n     pr_list = []\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(filter(lambda x: x.startswith(MAGIC_PREFIX), lines))\n         if magic_lines:\n@@ -157,7 +157,7 @@ def extract_models_from_pr(\n \n def find_torchbench_branch(prbody_file: str) -> str:\n     branch_name: str = \"\"\n-    with open(prbody_file, \"r\") as pf:\n+    with open(prbody_file) as pf:\n         lines = (x.strip() for x in pf.read().splitlines())\n         magic_lines = list(\n             filter(lambda x: x.startswith(MAGIC_TORCHBENCH_PREFIX), lines)\ndiff --git a/.github/scripts/test_check_labels.py b/.github/scripts/test_check_labels.py\nindex 17d33158f2efb4..2b2cd7b6c5204b 100644\n--- a/.github/scripts/test_check_labels.py\n+++ b/.github/scripts/test_check_labels.py\n@@ -15,7 +15,7 @@\n \n \n def mock_parse_args() -> object:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.pr_num = 76123\n \ndiff --git a/.github/scripts/test_trymerge.py b/.github/scripts/test_trymerge.py\nindex 7cf70882ed3d40..a46ef9032a6459 100755\n--- a/.github/scripts/test_trymerge.py\n+++ b/.github/scripts/test_trymerge.py\n@@ -114,7 +114,7 @@ def mocked_rockset_results(head_sha: str, merge_base: str, num_retries: int = 3)\n \n \n def mock_parse_args(revert: bool = False, force: bool = False) -> Any:\n-    class Object(object):\n+    class Object:\n         def __init__(self) -> None:\n             self.revert = revert\n             self.force = force\ndiff --git a/.github/scripts/trymerge.py b/.github/scripts/trymerge.py\nindex 3cffcaa14efaf8..cc253f36cbd1b7 100755\n--- a/.github/scripts/trymerge.py\n+++ b/.github/scripts/trymerge.py\n@@ -1628,10 +1628,8 @@ def validate_revert(\n         allowed_reverters.append(\"CONTRIBUTOR\")\n     if author_association not in allowed_reverters:\n         raise PostCommentError(\n-            (\n-                f\"Will not revert as @{author_login} is not one of \"\n-                f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n-            )\n+            f\"Will not revert as @{author_login} is not one of \"\n+            f\"[{', '.join(allowed_reverters)}], but instead is {author_association}.\"\n         )\n     skip_internal_checks = can_skip_internal_checks(pr, comment_id)\n \ndiff --git a/.github/scripts/trymerge_explainer.py b/.github/scripts/trymerge_explainer.py\nindex ebc74cf63eb833..8aa6ab59b94cbc 100644\n--- a/.github/scripts/trymerge_explainer.py\n+++ b/.github/scripts/trymerge_explainer.py\n@@ -17,7 +17,7 @@ def has_label(labels: List[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:\n     return len(list(filter(pattern.match, labels))) > 0\n \n \n-class TryMergeExplainer(object):\n+class TryMergeExplainer:\n     force: bool\n     labels: List[str]\n     pr_num: int\ndiff --git a/docs/caffe2/process.py b/docs/caffe2/process.py\nindex 3b94b9d38502a2..4a59eec388d90b 100644\n--- a/docs/caffe2/process.py\n+++ b/docs/caffe2/process.py\n@@ -8,7 +8,7 @@\n \n # Module caffe2...caffe2.python.control_test\n def insert(originalfile, first_line, description):\n-    with open(originalfile, 'r') as f:\n+    with open(originalfile) as f:\n         f1 = f.readline()\n         if(f1.find(first_line) < 0):\n             docs = first_line + description + f1\n@@ -30,7 +30,7 @@ def insert(originalfile, first_line, description):\n     for file in files:\n         if (file.endswith(\".py\") and not file.endswith(\"_test.py\") and not file.endswith(\"__.py\")):\n             filepath = os.path.join(root, file)\n-            print((\"filepath: \" + filepath))\n+            print(\"filepath: \" + filepath)\n             directory = os.path.dirname(filepath)[2:]\n             directory = directory.replace(\"/\", \".\")\n             print(\"directory: \" + directory)\ndiff --git a/docs/cpp/source/conf.py b/docs/cpp/source/conf.py\nindex 88648787fa8c8e..2b94cfdb5058fb 100644\n--- a/docs/cpp/source/conf.py\n+++ b/docs/cpp/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -99,7 +98,7 @@\n     ############################################################################\n     # Main library page layout example configuration.                          #\n     ############################################################################\n-    \"afterTitleDescription\": textwrap.dedent(u'''\n+    \"afterTitleDescription\": textwrap.dedent('''\n         Welcome to the developer reference for the PyTorch C++ API.\n     '''),\n }\ndiff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 8bbb189853eec7..8fec5f16f9852c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n #\n # PyTorch documentation build configuration file, created by\n # sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n@@ -624,7 +623,7 @@ def visit_reference(self, node):\n                 anchor = ref_anchor[1]\n                 txt = node.parent.astext()\n                 if txt == anchor or txt == anchor.split('.')[-1]:\n-                    self.body.append('<p id=\"{}\"/>'.format(ref_anchor[1]))\n+                    self.body.append(f'<p id=\"{ref_anchor[1]}\"/>')\n         return old_call(self, node)\n     Klass.visit_reference = visit_reference\n \ndiff --git a/docs/source/scripts/exportdb/generate_example_rst.py b/docs/source/scripts/exportdb/generate_example_rst.py\nindex 58dca5f31ea73d..38f71b905245c3 100644\n--- a/docs/source/scripts/exportdb/generate_example_rst.py\n+++ b/docs/source/scripts/exportdb/generate_example_rst.py\n@@ -31,7 +31,7 @@ def generate_example_rst(example_case: ExportCase):\n         if isinstance(model, torch.nn.Module)\n         else inspect.getfile(model)\n     )\n-    with open(source_file, \"r\") as file:\n+    with open(source_file) as file:\n         source_code = file.read()\n     source_code = re.sub(r\"from torch\\._export\\.db\\.case import .*\\n\", \"\", source_code)\n     source_code = re.sub(r\"@export_case\\((.|\\n)*?\\)\\n\", \"\", source_code)\n@@ -114,7 +114,7 @@ def generate_index_rst(example_cases, tag_to_modules, support_level_to_modules):\n \n     tag_names = \"\\n    \".join(t for t in tag_to_modules.keys())\n \n-    with open(os.path.join(PWD, \"blurb.txt\"), \"r\") as file:\n+    with open(os.path.join(PWD, \"blurb.txt\")) as file:\n         blurb = file.read()\n \n     # Generate contents of the .rst file\ndiff --git a/setup.py b/setup.py\nindex 5f0180cb0c8ac6..d454b2e62f33b4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -323,7 +323,7 @@ def report(*args):\n package_name = os.getenv('TORCH_PACKAGE_NAME', 'torch')\n package_type = os.getenv('PACKAGE_TYPE', 'wheel')\n version = get_torch_version()\n-report(\"Building wheel {}-{}\".format(package_name, version))\n+report(f\"Building wheel {package_name}-{version}\")\n \n cmake = CMake()\n \n@@ -361,7 +361,7 @@ def not_exists_or_empty(folder):\n             start = time.time()\n             subprocess.check_call([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd)\n             end = time.time()\n-            print(' --- Submodule initialization took {:.2f} sec'.format(end - start))\n+            print(f' --- Submodule initialization took {end - start:.2f} sec')\n         except Exception:\n             print(' --- Submodule initalization failed')\n             print('Please run:\\n\\tgit submodule update --init --recursive')\n@@ -616,16 +616,16 @@ def build_extensions(self):\n                 continue\n             fullname = self.get_ext_fullname(ext.name)\n             filename = self.get_ext_filename(fullname)\n-            report(\"\\nCopying extension {}\".format(ext.name))\n+            report(f\"\\nCopying extension {ext.name}\")\n \n             relative_site_packages = sysconfig.get_path('purelib').replace(sysconfig.get_path('data'), '').lstrip(os.path.sep)\n             src = os.path.join(\"torch\", relative_site_packages, filename)\n             if not os.path.exists(src):\n-                report(\"{} does not exist\".format(src))\n+                report(f\"{src} does not exist\")\n                 del self.extensions[i]\n             else:\n                 dst = os.path.join(os.path.realpath(self.build_lib), filename)\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -642,7 +642,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -658,7 +658,7 @@ def build_extensions(self):\n             src = os.path.join(os.path.dirname(filename), \"nvfuser\" + fileext)\n             dst = os.path.join(os.path.realpath(self.build_lib), filename)\n             if os.path.exists(src):\n-                report(\"Copying {} from {} to {}\".format(ext.name, src, dst))\n+                report(f\"Copying {ext.name} from {src} to {dst}\")\n                 dst_dir = os.path.dirname(dst)\n                 if not os.path.exists(dst_dir):\n                     os.makedirs(dst_dir)\n@@ -670,7 +670,7 @@ def build_extensions(self):\n     def get_outputs(self):\n         outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n         outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n-        report(\"setup.py::get_outputs returning {}\".format(outputs))\n+        report(f\"setup.py::get_outputs returning {outputs}\")\n         return outputs\n \n     def create_compile_commands(self):\n@@ -694,13 +694,13 @@ def load(filename):\n         new_contents = json.dumps(all_commands, indent=2)\n         contents = ''\n         if os.path.exists('compile_commands.json'):\n-            with open('compile_commands.json', 'r') as f:\n+            with open('compile_commands.json') as f:\n                 contents = f.read()\n         if contents != new_contents:\n             with open('compile_commands.json', 'w') as f:\n                 f.write(new_contents)\n \n-class concat_license_files():\n+class concat_license_files:\n     \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n \n     LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n@@ -723,7 +723,7 @@ def __enter__(self):\n         finally:\n             sys.path = old_path\n \n-        with open(self.f1, 'r') as f1:\n+        with open(self.f1) as f1:\n             self.bsd_text = f1.read()\n \n         with open(self.f1, 'a') as f1:\n@@ -771,7 +771,7 @@ def finalize_options(self):\n     def run(self):\n         import glob\n         import re\n-        with open('.gitignore', 'r') as f:\n+        with open('.gitignore') as f:\n             ignores = f.read()\n             pat = re.compile(r'^#( BEGIN NOT-CLEAN-FILES )?')\n             for wildcard in filter(None, ignores.split('\\n')):\n@@ -934,31 +934,31 @@ def make_relative_rpath_args(path):\n     if cmake_cache_vars['BUILD_CAFFE2']:\n         extensions.append(\n             Extension(\n-                name=str('caffe2.python.caffe2_pybind11_state'),\n+                name='caffe2.python.caffe2_pybind11_state',\n                 sources=[]),\n         )\n         if cmake_cache_vars['USE_CUDA']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_gpu'),\n+                    name='caffe2.python.caffe2_pybind11_state_gpu',\n                     sources=[]),\n             )\n         if cmake_cache_vars['USE_ROCM']:\n             extensions.append(\n                 Extension(\n-                    name=str('caffe2.python.caffe2_pybind11_state_hip'),\n+                    name='caffe2.python.caffe2_pybind11_state_hip',\n                     sources=[]),\n             )\n     if cmake_cache_vars['BUILD_FUNCTORCH']:\n         extensions.append(\n             Extension(\n-                name=str('functorch._C'),\n+                name='functorch._C',\n                 sources=[]),\n         )\n     if cmake_cache_vars['BUILD_NVFUSER']:\n         extensions.append(\n             Extension(\n-                name=str('nvfuser._C'),\n+                name='nvfuser._C',\n                 sources=[]),\n         )\n \n@@ -1272,7 +1272,7 @@ def main():\n         download_url='https://github.com/pytorch/pytorch/tags',\n         author='PyTorch Team',\n         author_email='packages@pytorch.org',\n-        python_requires='>={}'.format(python_min_version_str),\n+        python_requires=f'>={python_min_version_str}',\n         # PyPI package information.\n         classifiers=[\n             'Development Status :: 5 - Production/Stable',\n@@ -1288,7 +1288,7 @@ def main():\n             'Topic :: Software Development :: Libraries :: Python Modules',\n             'Programming Language :: C++',\n             'Programming Language :: Python :: 3',\n-        ] + ['Programming Language :: Python :: 3.{}'.format(i) for i in range(python_min_version[1], version_range_max)],\n+        ] + [f'Programming Language :: Python :: 3.{i}' for i in range(python_min_version[1], version_range_max)],\n         license='BSD-3',\n         keywords='pytorch, machine learning',\n     )\ndiff --git a/tools/amd_build/build_amd.py b/tools/amd_build/build_amd.py\nindex 59f806b361102e..5d14e9266f3b4a 100755\n--- a/tools/amd_build/build_amd.py\n+++ b/tools/amd_build/build_amd.py\n@@ -140,7 +140,7 @@ def is_hip_clang() -> bool:\n         hip_path = os.getenv(\"HIP_PATH\", \"/opt/rocm/hip\")\n         with open(hip_path + \"/lib/.hipInfo\") as f:\n             return \"HIP_COMPILER=clang\" in f.read()\n-    except IOError:\n+    except OSError:\n         return False\n \n \n@@ -149,7 +149,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Hip.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\" hip_hcc \", \" amdhip64 \") for line in lines]\n         if lines == newlines:\n@@ -163,7 +163,7 @@ def is_hip_clang() -> bool:\n gloo_cmake_file = \"third_party/gloo/cmake/Modules/Findrccl.cmake\"\n if os.path.exists(gloo_cmake_file):\n     do_write = False\n-    with open(gloo_cmake_file, \"r\") as sources:\n+    with open(gloo_cmake_file) as sources:\n         lines = sources.readlines()\n     newlines = [line.replace(\"RCCL_LIBRARY\", \"RCCL_LIB_PATH\") for line in lines]\n     if lines == newlines:\n@@ -179,7 +179,7 @@ def is_hip_clang() -> bool:\n     gloo_cmake_file = \"third_party/gloo/cmake/Dependencies.cmake\"\n     do_write = False\n     if os.path.exists(gloo_cmake_file):\n-        with open(gloo_cmake_file, \"r\") as sources:\n+        with open(gloo_cmake_file) as sources:\n             lines = sources.readlines()\n         newlines = [line.replace(\"HIP_HCC_FLAGS\", \"HIP_CLANG_FLAGS\") for line in lines]\n         if lines == newlines:\ndiff --git a/tools/autograd/gen_python_functions.py b/tools/autograd/gen_python_functions.py\nindex 211aacafaa8728..d1e9d60737defc 100644\n--- a/tools/autograd/gen_python_functions.py\n+++ b/tools/autograd/gen_python_functions.py\n@@ -553,7 +553,7 @@ def load_deprecated_signatures(\n     # find matching original signatures for each deprecated signature\n     results: List[PythonSignatureNativeFunctionPair] = []\n \n-    with open(deprecated_yaml_path, \"r\") as f:\n+    with open(deprecated_yaml_path) as f:\n         deprecated_defs = yaml.load(f, Loader=YamlLoader)\n \n     for deprecated in deprecated_defs:\n@@ -873,7 +873,7 @@ def method_impl(\n         name=name,\n         pycname=pycname,\n         method_header=method_header,\n-        max_args=max((o.signature.arguments_count() for o in overloads)),\n+        max_args=max(o.signature.arguments_count() for o in overloads),\n         signatures=signatures,\n         traceable=traceable,\n         check_has_torch_function=gen_has_torch_function_check(\n@@ -1255,10 +1255,10 @@ def go(f: NativeFunction) -> str:\n         # dispatch lambda signature\n         name = cpp.name(f.func)\n         lambda_formals = \", \".join(\n-            (\n+\n                 f\"{a.type_str} {a.name}\"\n                 for a in dispatch_lambda_args(ps, f, symint=symint)\n-            )\n+\n         )\n         lambda_return = dispatch_lambda_return_str(f)\n \ndiff --git a/tools/autograd/load_derivatives.py b/tools/autograd/load_derivatives.py\nindex b51b625b2ea28f..b846892b0e3ed4 100644\n--- a/tools/autograd/load_derivatives.py\n+++ b/tools/autograd/load_derivatives.py\n@@ -98,7 +98,7 @@ def load_derivatives(\n     global _GLOBAL_LOAD_DERIVATIVE_CACHE\n     key = (derivatives_yaml_path, native_yaml_path)\n     if key not in _GLOBAL_LOAD_DERIVATIVE_CACHE:\n-        with open(derivatives_yaml_path, \"r\") as f:\n+        with open(derivatives_yaml_path) as f:\n             definitions = yaml.load(f, Loader=YamlLoader)\n \n         funcs = parse_native_yaml(native_yaml_path, tags_yaml_path).native_functions\ndiff --git a/tools/code_analyzer/gen_op_registration_allowlist.py b/tools/code_analyzer/gen_op_registration_allowlist.py\nindex b01142c872f1c2..b5d15ca1ae8417 100644\n--- a/tools/code_analyzer/gen_op_registration_allowlist.py\n+++ b/tools/code_analyzer/gen_op_registration_allowlist.py\n@@ -24,7 +24,7 @@ def canonical_name(opname: str) -> str:\n \n \n def load_op_dep_graph(fname: str) -> DepGraph:\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         result = defaultdict(set)\n         for op in yaml.safe_load(stream):\n             op_name = canonical_name(op[\"name\"])\n@@ -36,7 +36,7 @@ def load_op_dep_graph(fname: str) -> DepGraph:\n \n def load_root_ops(fname: str) -> List[str]:\n     result = []\n-    with open(fname, \"r\") as stream:\n+    with open(fname) as stream:\n         for op in yaml.safe_load(stream):\n             result.append(canonical_name(op))\n     return result\ndiff --git a/tools/code_analyzer/gen_oplist.py b/tools/code_analyzer/gen_oplist.py\nindex 7c1764deda5b2f..0a9e2a1539b6a7 100644\n--- a/tools/code_analyzer/gen_oplist.py\n+++ b/tools/code_analyzer/gen_oplist.py\n@@ -79,7 +79,7 @@ def gen_supported_mobile_models(model_dicts: List[Any], output_dir: str) -> None\n \n     supported_hashes = \"\"\n     for md5 in md5_hashes:\n-        supported_hashes += '\"{}\",\\n'.format(md5)\n+        supported_hashes += f'\"{md5}\",\\n'\n     with open(\n         os.path.join(output_dir, \"SupportedMobileModelsRegistration.cpp\"), \"wb\"\n     ) as out_file:\ndiff --git a/tools/coverage_plugins_package/setup.py b/tools/coverage_plugins_package/setup.py\nindex 01250694550423..e3e88067cb08db 100644\n--- a/tools/coverage_plugins_package/setup.py\n+++ b/tools/coverage_plugins_package/setup.py\n@@ -1,6 +1,6 @@\n import setuptools  # type: ignore[import]\n \n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n+with open(\"README.md\", encoding=\"utf-8\") as fh:\n     long_description = fh.read()\n \n setuptools.setup(\ndiff --git a/tools/download_mnist.py b/tools/download_mnist.py\nindex 52fa411eda9f88..ac9c049bdeedb6 100644\n--- a/tools/download_mnist.py\n+++ b/tools/download_mnist.py\n@@ -32,16 +32,16 @@ def report_download_progress(\n def download(destination_path: str, resource: str, quiet: bool) -> None:\n     if os.path.exists(destination_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ...\".format(destination_path))\n+            print(f\"{destination_path} already exists, skipping ...\")\n     else:\n         for mirror in MIRRORS:\n             url = mirror + resource\n-            print(\"Downloading {} ...\".format(url))\n+            print(f\"Downloading {url} ...\")\n             try:\n                 hook = None if quiet else report_download_progress\n                 urlretrieve(url, destination_path, reporthook=hook)\n             except (URLError, ConnectionError) as e:\n-                print(\"Failed to download (trying next):\\n{}\".format(e))\n+                print(f\"Failed to download (trying next):\\n{e}\")\n                 continue\n             finally:\n                 if not quiet:\n@@ -56,13 +56,13 @@ def unzip(zipped_path: str, quiet: bool) -> None:\n     unzipped_path = os.path.splitext(zipped_path)[0]\n     if os.path.exists(unzipped_path):\n         if not quiet:\n-            print(\"{} already exists, skipping ... \".format(unzipped_path))\n+            print(f\"{unzipped_path} already exists, skipping ... \")\n         return\n     with gzip.open(zipped_path, \"rb\") as zipped_file:\n         with open(unzipped_path, \"wb\") as unzipped_file:\n             unzipped_file.write(zipped_file.read())\n             if not quiet:\n-                print(\"Unzipped {} ...\".format(zipped_path))\n+                print(f\"Unzipped {zipped_path} ...\")\n \n \n def main() -> None:\ndiff --git a/tools/gen_vulkan_spv.py b/tools/gen_vulkan_spv.py\nindex 02c9c39270b107..8d38dfa0e82bcb 100644\n--- a/tools/gen_vulkan_spv.py\n+++ b/tools/gen_vulkan_spv.py\n@@ -74,7 +74,7 @@ def __init__(self: \"VulkanShaderGenerator\") -> None:\n \n     def add_params_yaml(self, parameters_yaml_file):  # type: ignore[no-untyped-def]\n         all_template_params = OrderedDict()\n-        with open(parameters_yaml_file, \"r\") as f:\n+        with open(parameters_yaml_file) as f:\n             contents = yaml.load(f, Loader=UniqueKeyLoader)\n             for key in contents:\n                 all_template_params[key] = contents[key]\n@@ -205,7 +205,7 @@ def determineDescriptorType(lineStr: str) -> str:\n \n def getShaderInfo(srcFilePath: str) -> ShaderInfo:\n     shader_info = ShaderInfo([], [], \"\")\n-    with open(srcFilePath, 'r') as srcFile:\n+    with open(srcFilePath) as srcFile:\n         for line in srcFile:\n             if isDescriptorLine(line):\n                 shader_info.layouts.append(determineDescriptorType(line))\n@@ -272,13 +272,13 @@ def genCppH(\n         if len(f) > 1:\n             templateSrcPaths.append(f)\n             templateSrcPaths.sort()\n-    print(\"templateSrcPaths:{}\".format(templateSrcPaths))\n+    print(f\"templateSrcPaths:{templateSrcPaths}\")\n \n     spvPaths = {}\n     for templateSrcPath in templateSrcPaths:\n-        print(\"templateSrcPath {}\".format(templateSrcPath))\n+        print(f\"templateSrcPath {templateSrcPath}\")\n         name = getName(templateSrcPath).replace(\"_glsl\", \"\")\n-        print(\"name {}\".format(name))\n+        print(f\"name {name}\")\n \n         codeTemplate = CodeTemplate.from_file(templateSrcPath)\n         srcPath = tmpDirPath + \"/\" + name + \".glsl\"\n@@ -287,7 +287,7 @@ def genCppH(\n             fw.write(content)\n \n         spvPath = tmpDirPath + \"/\" + name + \".spv\"\n-        print(\"spvPath {}\".format(spvPath))\n+        print(f\"spvPath {spvPath}\")\n \n         cmd = [\n             glslcPath, \"-fshader-stage=compute\",\n@@ -328,7 +328,7 @@ def genCppH(\n     h += nsend\n \n     cpp = \"#include <ATen/native/vulkan/api/Shader.h>\\n\"\n-    cpp += \"#include <ATen/native/vulkan/{}>\\n\".format(H_NAME)\n+    cpp += f\"#include <ATen/native/vulkan/{H_NAME}>\\n\"\n     cpp += \"#include <stdint.h>\\n\"\n     cpp += \"#include <vector>\\n\"\n     cpp += nsbegin\n@@ -340,7 +340,7 @@ def genCppH(\n     for spvPath, srcPath in spvPaths.items():\n         name = getName(spvPath).replace(\"_spv\", \"\")\n \n-        print(\"spvPath:{}\".format(spvPath))\n+        print(f\"spvPath:{spvPath}\")\n         with open(spvPath, 'rb') as fr:\n             next_bin = array.array('I', fr.read())\n             sizeBytes = 4 * len(next_bin)\n@@ -362,8 +362,8 @@ def genCppH(\n         shader_info_layouts = \"{{{}}}\".format(\",\\n \".join(shader_info.layouts))\n \n         shader_info_args = [\n-            \"\\\"vulkan.{}\\\"\".format(name),\n-            \"{}_bin\".format(name),\n+            f\"\\\"vulkan.{name}\\\"\",\n+            f\"{name}_bin\",\n             str(sizeBytes),\n             shader_info_layouts,\n             tile_size,\ndiff --git a/tools/generate_torch_version.py b/tools/generate_torch_version.py\nindex 9e9f73b031f810..d90d3646ab1910 100644\n--- a/tools/generate_torch_version.py\n+++ b/tools/generate_torch_version.py\n@@ -41,7 +41,7 @@ def get_tag(pytorch_root: Union[str, Path]) -> str:\n \n def get_torch_version(sha: Optional[str] = None) -> str:\n     pytorch_root = Path(__file__).parent.parent\n-    version = open(pytorch_root / \"version.txt\", \"r\").read().strip()\n+    version = open(pytorch_root / \"version.txt\").read().strip()\n \n     if os.getenv(\"PYTORCH_BUILD_VERSION\"):\n         assert os.getenv(\"PYTORCH_BUILD_NUMBER\") is not None\n@@ -86,11 +86,11 @@ def get_torch_version(sha: Optional[str] = None) -> str:\n         version = tagged_version\n \n     with open(version_path, \"w\") as f:\n-        f.write(\"__version__ = '{}'\\n\".format(version))\n+        f.write(f\"__version__ = '{version}'\\n\")\n         # NB: This is not 100% accurate, because you could have built the\n         # library code with DEBUG, but csrc without DEBUG (in which case\n         # this would claim to be a release build when it's not.)\n-        f.write(\"debug = {}\\n\".format(repr(bool(args.is_debug))))\n-        f.write(\"cuda = {}\\n\".format(repr(args.cuda_version)))\n-        f.write(\"git_version = {}\\n\".format(repr(sha)))\n-        f.write(\"hip = {}\\n\".format(repr(args.hip_version)))\n+        f.write(f\"debug = {repr(bool(args.is_debug))}\\n\")\n+        f.write(f\"cuda = {repr(args.cuda_version)}\\n\")\n+        f.write(f\"git_version = {repr(sha)}\\n\")\n+        f.write(f\"hip = {repr(args.hip_version)}\\n\")\ndiff --git a/tools/jit/gen_unboxing.py b/tools/jit/gen_unboxing.py\nindex 6179d6afe482ff..ee4e2fc2ddb188 100644\n--- a/tools/jit/gen_unboxing.py\n+++ b/tools/jit/gen_unboxing.py\n@@ -250,7 +250,7 @@ def main(args: List[str]) -> None:\n     if options.op_registration_allowlist:\n         op_registration_allowlist = options.op_registration_allowlist\n     elif options.TEST_ONLY_op_registration_allowlist_yaml_path:\n-        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path, \"r\") as f:\n+        with open(options.TEST_ONLY_op_registration_allowlist_yaml_path) as f:\n             op_registration_allowlist = yaml.safe_load(f)\n     else:\n         op_registration_allowlist = None\ndiff --git a/tools/linter/adapters/constexpr_linter.py b/tools/linter/adapters/constexpr_linter.py\nindex 8992f30ac46b29..24ecc83b238e30 100644\n--- a/tools/linter/adapters/constexpr_linter.py\n+++ b/tools/linter/adapters/constexpr_linter.py\n@@ -35,7 +35,7 @@ class LintMessage(NamedTuple):\n def check_file(filename: str) -> Optional[LintMessage]:\n     logging.debug(\"Checking file %s\", filename)\n \n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         lines = f.readlines()\n \n     for idx, line in enumerate(lines):\ndiff --git a/tools/linter/adapters/grep_linter.py b/tools/linter/adapters/grep_linter.py\nindex 21c8a210b2b697..64dac4cdc079cd 100644\n--- a/tools/linter/adapters/grep_linter.py\n+++ b/tools/linter/adapters/grep_linter.py\n@@ -108,7 +108,7 @@ def lint_file(\n     original = None\n     replacement = None\n     if replace_pattern:\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             original = f.read()\n \n         try:\ndiff --git a/tools/nightly.py b/tools/nightly.py\nindex 1544eb0692b661..28a8c6eb2331e7 100755\n--- a/tools/nightly.py\n+++ b/tools/nightly.py\n@@ -105,7 +105,7 @@ def redact(self, needle: str, replace: str = \"<REDACTED>\") -> None:\n         self.redactions[needle] = replace\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_base_dir() -> str:\n     meta_dir = os.getcwd()\n     base_dir = os.path.join(meta_dir, \"nightly\", \"log\")\n@@ -113,17 +113,17 @@ def logging_base_dir() -> str:\n     return base_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_run_dir() -> str:\n     cur_dir = os.path.join(\n         logging_base_dir(),\n-        \"{}_{}\".format(datetime.datetime.now().strftime(DATETIME_FORMAT), uuid.uuid1()),\n+        f\"{datetime.datetime.now().strftime(DATETIME_FORMAT)}_{uuid.uuid1()}\",\n     )\n     os.makedirs(cur_dir, exist_ok=True)\n     return cur_dir\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def logging_record_argv() -> None:\n     s = subprocess.list2cmdline(sys.argv)\n     with open(os.path.join(logging_run_dir(), \"argv\"), \"w\") as f:\ndiff --git a/tools/onnx/gen_diagnostics.py b/tools/onnx/gen_diagnostics.py\nindex 2aeb61a06318d7..4cf70289296050 100644\n--- a/tools/onnx/gen_diagnostics.py\n+++ b/tools/onnx/gen_diagnostics.py\n@@ -205,7 +205,7 @@ def gen_diagnostics(\n     out_cpp_dir: str,\n     out_docs_dir: str,\n ) -> None:\n-    with open(rules_path, \"r\") as f:\n+    with open(rules_path) as f:\n         rules = yaml.load(f, Loader=YamlLoader)\n \n     template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"templates\")\ndiff --git a/tools/onnx/update_default_opset_version.py b/tools/onnx/update_default_opset_version.py\nindex 6dc6ffbd2890f4..6463c6271b6ee6 100755\n--- a/tools/onnx/update_default_opset_version.py\n+++ b/tools/onnx/update_default_opset_version.py\n@@ -23,7 +23,7 @@\n def read_sub_write(path: str, prefix_pat: str, new_default: int) -> None:\n     with open(path, encoding=\"utf-8\") as f:\n         content_str = f.read()\n-    content_str = re.sub(prefix_pat, r\"\\g<1>{}\".format(new_default), content_str)\n+    content_str = re.sub(prefix_pat, fr\"\\g<1>{new_default}\", content_str)\n     with open(path, \"w\", encoding=\"utf-8\") as f:\n         f.write(content_str)\n     print(\"modified\", path)\ndiff --git a/tools/pyi/gen_pyi.py b/tools/pyi/gen_pyi.py\nindex 24ee8ad1bfe580..c74d737416870a 100644\n--- a/tools/pyi/gen_pyi.py\n+++ b/tools/pyi/gen_pyi.py\n@@ -191,15 +191,15 @@ def sig_for_ops(opname: str) -> List[str]:\n \n     name = opname[2:-2]\n     if name in binary_ops:\n-        return [\"def {}(self, other: Any) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self, other: Any) -> Tensor: ...\"]\n     elif name in comparison_ops:\n-        sig = \"def {}(self, other: Any) -> Tensor: ...\".format(opname)\n+        sig = f\"def {opname}(self, other: Any) -> Tensor: ...\"\n         if name in symmetric_comparison_ops:\n             # unsafe override https://github.com/python/mypy/issues/5704\n             sig += \"  # type: ignore[override]\"\n         return [sig]\n     elif name in unary_ops:\n-        return [\"def {}(self) -> Tensor: ...\".format(opname)]\n+        return [f\"def {opname}(self) -> Tensor: ...\"]\n     elif name in to_py_type_ops:\n         if name in {\"bool\", \"float\", \"complex\"}:\n             tname = name\n@@ -209,7 +209,7 @@ def sig_for_ops(opname: str) -> List[str]:\n             tname = \"int\"\n         if tname in {\"float\", \"int\", \"bool\", \"complex\"}:\n             tname = \"builtins.\" + tname\n-        return [\"def {}(self) -> {}: ...\".format(opname, tname)]\n+        return [f\"def {opname}(self) -> {tname}: ...\"]\n     else:\n         raise Exception(\"unknown op\", opname)\n \n@@ -1120,7 +1120,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     for name in simple_conversions:\n         unsorted_tensor_method_hints[name].append(\n-            \"def {}(self) -> Tensor: ...\".format(name)\n+            f\"def {name}(self) -> Tensor: ...\"\n         )\n \n     # pyi tensor methods don't currently include deprecated signatures for some reason\n@@ -1150,7 +1150,7 @@ def replace_special_case(hint: str) -> str:\n                 namedtuples[tuple_name] = tuple_def\n \n     for op in all_ops:\n-        name = \"__{}__\".format(op)\n+        name = f\"__{op}__\"\n         unsorted_tensor_method_hints[name] += sig_for_ops(name)\n \n     tensor_method_hints = []\n@@ -1164,7 +1164,7 @@ def replace_special_case(hint: str) -> str:\n     # Generate namedtuple definitions\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-    namedtuple_defs = [\"{}\\n\".format(defn) for defn in namedtuples.values()]\n+    namedtuple_defs = [f\"{defn}\\n\" for defn in namedtuples.values()]\n \n     # Generate type signatures for legacy classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1183,7 +1183,7 @@ def replace_special_case(hint: str) -> str:\n         \"ByteTensor\",\n         \"BoolTensor\",\n     ):\n-        legacy_class_hints.append(\"class {}(Tensor): ...\".format(c))\n+        legacy_class_hints.append(f\"class {c}(Tensor): ...\")\n \n     # Generate type signatures for dtype classes\n     # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -1191,7 +1191,7 @@ def replace_special_case(hint: str) -> str:\n     # TODO: don't explicitly list dtypes here; get it from canonical\n     # source\n     dtype_class_hints = [\n-        \"{}: dtype = ...\".format(n)\n+        f\"{n}: dtype = ...\"\n         for n in [\n             \"float32\",\n             \"float\",\n@@ -1232,7 +1232,7 @@ def replace_special_case(hint: str) -> str:\n     ]\n     all_symbols = sorted(list(namedtuples.keys()) + hinted_function_names)\n     all_directive = pformat(all_symbols, width=100, compact=True).split(\"\\n\")\n-    all_directive[0] = \"__all__ = {}\".format(all_directive[0])\n+    all_directive[0] = f\"__all__ = {all_directive[0]}\"\n \n     # Dispatch key hints\n     # ~~~~~~~~~~~~~~~~~~\ndiff --git a/tools/setup_helpers/cmake.py b/tools/setup_helpers/cmake.py\nindex 0bd6e5d4c2adc9..cf80bd3eb1e62c 100644\n--- a/tools/setup_helpers/cmake.py\n+++ b/tools/setup_helpers/cmake.py\n@@ -61,10 +61,10 @@ def _get_cmake_command() -> str:\n \n         _cmake_min_version = LooseVersion(\"3.13.0\")\n         if all(\n-            (\n+\n                 ver is None or ver < _cmake_min_version\n                 for ver in [cmake_version, cmake3_version]\n-            )\n+\n         ):\n             raise RuntimeError(\"no cmake or cmake3 with version >= 3.13.0 found\")\n \n@@ -108,7 +108,7 @@ def defines(args: List[str], **kwargs: CMakeValue) -> None:\n         \"Adds definitions to a cmake argument list.\"\n         for key, value in sorted(kwargs.items()):\n             if value is not None:\n-                args.append(\"-D{}={}\".format(key, value))\n+                args.append(f\"-D{key}={value}\")\n \n     def get_cmake_cache_variables(self) -> Dict[str, CMakeValue]:\n         r\"\"\"Gets values in CMakeCache.txt into a dictionary.\n@@ -173,7 +173,7 @@ def generate(\n                     toolset_dict[\"host\"] = \"x64\"\n             if toolset_dict:\n                 toolset_expr = \",\".join(\n-                    [\"{}={}\".format(k, v) for k, v in toolset_dict.items()]\n+                    [f\"{k}={v}\" for k, v in toolset_dict.items()]\n                 )\n                 args.append(\"-T\" + toolset_expr)\n \n@@ -322,10 +322,10 @@ def generate(\n         expected_wrapper = \"/usr/local/opt/ccache/libexec\"\n         if IS_DARWIN and os.path.exists(expected_wrapper):\n             if \"CMAKE_C_COMPILER\" not in build_options and \"CC\" not in os.environ:\n-                CMake.defines(args, CMAKE_C_COMPILER=\"{}/gcc\".format(expected_wrapper))\n+                CMake.defines(args, CMAKE_C_COMPILER=f\"{expected_wrapper}/gcc\")\n             if \"CMAKE_CXX_COMPILER\" not in build_options and \"CXX\" not in os.environ:\n                 CMake.defines(\n-                    args, CMAKE_CXX_COMPILER=\"{}/g++\".format(expected_wrapper)\n+                    args, CMAKE_CXX_COMPILER=f\"{expected_wrapper}/g++\"\n                 )\n \n         for env_var_name in my_env:\n@@ -336,10 +336,10 @@ def generate(\n                     my_env[env_var_name] = str(my_env[env_var_name].encode(\"utf-8\"))\n                 except UnicodeDecodeError as e:\n                     shex = \":\".join(\n-                        \"{:02x}\".format(ord(c)) for c in my_env[env_var_name]\n+                        f\"{ord(c):02x}\" for c in my_env[env_var_name]\n                     )\n                     print(\n-                        \"Invalid ENV[{}] = {}\".format(env_var_name, shex),\n+                        f\"Invalid ENV[{env_var_name}] = {shex}\",\n                         file=sys.stderr,\n                     )\n                     print(e, file=sys.stderr)\n@@ -396,7 +396,7 @@ def build(self, my_env: Dict[str, str]) -> None:\n             build_args += [\"--\"]\n             if IS_WINDOWS and not USE_NINJA:\n                 # We are likely using msbuild here\n-                build_args += [\"/p:CL_MPCount={}\".format(max_jobs)]\n+                build_args += [f\"/p:CL_MPCount={max_jobs}\"]\n             else:\n                 build_args += [\"-j\", max_jobs]\n         self.run(build_args, my_env)\ndiff --git a/tools/setup_helpers/cmake_utils.py b/tools/setup_helpers/cmake_utils.py\nindex dabd66a4e838bb..c15b6f7592c015 100644\n--- a/tools/setup_helpers/cmake_utils.py\n+++ b/tools/setup_helpers/cmake_utils.py\n@@ -72,7 +72,7 @@ def get_cmake_cache_variables_from_file(\n         )\n         if matched is None:  # Illegal line\n             raise ValueError(\n-                \"Unexpected line {} in {}: {}\".format(i, repr(cmake_cache_file), line)\n+                f\"Unexpected line {i} in {repr(cmake_cache_file)}: {line}\"\n             )\n         _, variable, type_, value = matched.groups()\n         if type_ is None:\ndiff --git a/tools/setup_helpers/generate_code.py b/tools/setup_helpers/generate_code.py\nindex c03fd87f25b6aa..afdd168d179fd6 100644\n--- a/tools/setup_helpers/generate_code.py\n+++ b/tools/setup_helpers/generate_code.py\n@@ -75,7 +75,7 @@ def generate_code(\n def get_selector_from_legacy_operator_selection_list(\n     selected_op_list_path: str,\n ) -> Any:\n-    with open(selected_op_list_path, \"r\") as f:\n+    with open(selected_op_list_path) as f:\n         # strip out the overload part\n         # It's only for legacy config - do NOT copy this code!\n         selected_op_list = {\ndiff --git a/tools/stats/import_test_stats.py b/tools/stats/import_test_stats.py\nindex b0719fc56d97b6..28d8ee0961bd9e 100644\n--- a/tools/stats/import_test_stats.py\n+++ b/tools/stats/import_test_stats.py\n@@ -46,7 +46,7 @@ def is_cached_file_valid() -> bool:\n \n     if os.path.exists(path) and is_cached_file_valid():\n         # Another test process already download the file, so don't re-do it\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             return cast(Dict[str, Any], json.load(f))\n \n     for _ in range(3):\ndiff --git a/tools/stats/upload_stats_lib.py b/tools/stats/upload_stats_lib.py\nindex c9223c528fe163..c7e90286d00a2b 100644\n--- a/tools/stats/upload_stats_lib.py\n+++ b/tools/stats/upload_stats_lib.py\n@@ -249,10 +249,10 @@ def value(self) -> Any:\n         value = os.environ.get(self.env_var)\n         if value is None and self.required:\n             raise ValueError(\n-                (\n+\n                     f\"Missing {self.name}. Please set the {self.env_var}\"\n                     \"environment variable to pass in this value.\"\n-                )\n+\n             )\n         if self.type_conversion_fn:\n             return self.type_conversion_fn(value)\ndiff --git a/tools/substitute.py b/tools/substitute.py\nindex c3b353bf740115..e9c05990c75f9a 100644\n--- a/tools/substitute.py\n+++ b/tools/substitute.py\n@@ -11,7 +11,7 @@\n     parser.add_argument(\"--replace\", action=\"append\", nargs=2)\n     options = parser.parse_args()\n \n-    with open(options.input_file, \"r\") as f:\n+    with open(options.input_file) as f:\n         contents = f.read()\n \n     output_file = os.path.join(options.install_dir, options.output_file)\ndiff --git a/tools/test/test_executorch_gen.py b/tools/test/test_executorch_gen.py\nindex b2a0f6768271bf..c9d6c79b85c1ea 100644\n--- a/tools/test/test_executorch_gen.py\n+++ b/tools/test/test_executorch_gen.py\n@@ -181,7 +181,7 @@ def test_translate_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\n@@ -268,7 +268,7 @@ def test_translate_kernel_native_yaml_writes_correct_data(self) -> None:\n                 use_aten_lib=False,\n                 out_file=out_file,\n             )\n-        with open(out_yaml_path, \"r\") as out_file:\n+        with open(out_yaml_path) as out_file:\n             es = yaml.load(out_file, Loader=LineLoader)\n         self.assertTrue(all(\"func\" in e for e in es))\n         self.assertTrue(all(e.get(\"variants\") == \"function\" for e in es))\ndiff --git a/tools/test/test_executorch_signatures.py b/tools/test/test_executorch_signatures.py\nindex c137f6982ec2b7..543926d4c31ef0 100644\n--- a/tools/test/test_executorch_signatures.py\n+++ b/tools/test/test_executorch_signatures.py\n@@ -21,7 +21,7 @@ def test_runtime_signature_contains_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=True)\n-            self.assertEquals(len(args), 3)\n+            self.assertEqual(len(args), 3)\n             self.assertTrue(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n@@ -30,7 +30,7 @@ def test_runtime_signature_does_not_contain_runtime_context(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             args = self.sig.arguments(include_context=False)\n-            self.assertEquals(len(args), 2)\n+            self.assertEqual(len(args), 2)\n             self.assertFalse(any(a.name == \"context\" for a in args))\n \n     def test_runtime_signature_declaration_correct(self) -> None:\n@@ -38,7 +38,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n             use_const_ref_for_mutable_tensors=False, use_ilistref_for_tensor_lists=False\n         ):\n             decl = self.sig.decl(include_context=True)\n-            self.assertEquals(\n+            self.assertEqual(\n                 decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\n@@ -48,7 +48,7 @@ def test_runtime_signature_declaration_correct(self) -> None:\n                 ),\n             )\n             no_context_decl = self.sig.decl(include_context=False)\n-            self.assertEquals(\n+            self.assertEqual(\n                 no_context_decl,\n                 (\n                     \"torch::executor::Tensor & foo_outf(\"\ndiff --git a/tools/test/test_vulkan_codegen.py b/tools/test/test_vulkan_codegen.py\nindex ae87c27e7aeb8e..196be229b348d2 100644\n--- a/tools/test/test_vulkan_codegen.py\n+++ b/tools/test/test_vulkan_codegen.py\n@@ -92,9 +92,9 @@ def test_missing_key_default_val(self) -> None:\n                     file_name_2 = os.path.join(tmp_dir, \"conv2d_pw_1x2.glsl\")\n                     self.assertTrue(os.path.exists(file_name_1))\n                     self.assertTrue(os.path.exists(file_name_2))\n-                    with open(file_name_1, \"r\") as f:\n+                    with open(file_name_1) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 1\" in contents)\n-                    with open(file_name_2, \"r\") as f:\n+                    with open(file_name_2) as f:\n                         contents = f.read()\n                         self.assertTrue(\"1 + 2\" in contents)\ndiff --git a/tools/testing/explicit_ci_jobs.py b/tools/testing/explicit_ci_jobs.py\nindex daff3cce8956ff..594e00d437f9f7 100755\n--- a/tools/testing/explicit_ci_jobs.py\n+++ b/tools/testing/explicit_ci_jobs.py\n@@ -127,7 +127,7 @@ def commit_ci(files: List[str], message: str) -> None:\n     args = parser.parse_args()\n \n     touched_files = [CONFIG_YML]\n-    with open(CONFIG_YML, \"r\") as f:\n+    with open(CONFIG_YML) as f:\n         config_yml = yaml.safe_load(f.read())\n \n     config_yml[\"workflows\"] = get_filtered_circleci_config(\ndiff --git a/tools/testing/test_selections.py b/tools/testing/test_selections.py\nindex 24fb7278d206f9..76f841b8902bdd 100644\n--- a/tools/testing/test_selections.py\n+++ b/tools/testing/test_selections.py\n@@ -163,7 +163,7 @@ def _get_previously_failing_tests() -> Set[str]:\n         )\n         return set()\n \n-    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH, \"r\") as f:\n+    with open(PYTEST_FAILED_TESTS_CACHE_FILE_PATH) as f:\n         last_failed_tests = json.load(f)\n \n     prioritized_tests = _parse_prev_failing_test_files(last_failed_tests)\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex 053ce0c0a89552..ebd24383e0b53f 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -156,14 +156,12 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-\n-                            \"      Note: While we usually use modules in the python standard library \"\n-                            f\"from the local environment, `{module_name}` has a lot of system \"\n-                            \"level access and therefore can pose a security risk. We heavily \"\n-                            f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n-                            \"is not possible, add it to the extern list by calling \"\n-                            f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-\n+                        \"      Note: While we usually use modules in the python standard library \"\n+                        f\"from the local environment, `{module_name}` has a lot of system \"\n+                        \"level access and therefore can pose a security risk. We heavily \"\n+                        f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n+                        \"is not possible, add it to the extern list by calling \"\n+                        f'PackageExporter.extern(\"`{module_name}`\")\\n'\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +171,8 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-\n-                    \"Set debug=True when invoking PackageExporter for a visualization of where \"\n-                    \"broken modules are coming from!\\n\"\n-\n+                \"Set debug=True when invoking PackageExporter for a visualization of where \"\n+                \"broken modules are coming from!\\n\"\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\n"
  },
  {
    "number": 105387,
    "title": "[BE] Enable ruff's UP rules and autoformat onnx/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "ffa7be21b4a2661f509abf32465072cb30b3ead5",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105387",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105387/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105387.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105387.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105387/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105387/comments",
    "labels": [
      "release notes: onnx",
      "open source"
    ],
    "_event_time": "2023-07-18T01:11:25.870114Z",
    "state": "closed",
    "patch": "From 6d4534eb9dd2390f30900a1e6247ef7a25f30dde Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:19 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat onnx/\n\n[ghstack-poisoned]\n---\n torch/onnx/_internal/diagnostics/infra/utils.py    | 2 +-\n torch/onnx/_internal/exporter.py                   | 4 +---\n torch/onnx/_internal/fx/onnxfunction_dispatcher.py | 6 +++---\n torch/onnx/_internal/fx/passes/type_promotion.py   | 2 +-\n torch/onnx/_internal/fx/registration.py            | 4 ++--\n torch/onnx/_internal/io_adapter.py                 | 6 +++---\n torch/onnx/_internal/jit_utils.py                  | 2 +-\n torch/onnx/symbolic_opset11.py                     | 2 +-\n torch/onnx/symbolic_opset17.py                     | 8 ++++----\n torch/onnx/verification.py                         | 4 ++--\n 10 files changed, 19 insertions(+), 21 deletions(-)\n\ndiff --git a/torch/onnx/_internal/diagnostics/infra/utils.py b/torch/onnx/_internal/diagnostics/infra/utils.py\nindex f287268df5727b..4648b477515025 100644\n--- a/torch/onnx/_internal/diagnostics/infra/utils.py\n+++ b/torch/onnx/_internal/diagnostics/infra/utils.py\n@@ -43,7 +43,7 @@ def python_call_stack(frames_to_skip: int = 0, frames_to_log: int = 16) -> _infr\n     return stack\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def _function_source_info(fn: Callable) -> Tuple[Sequence[str], int, Optional[str]]:\n     \"\"\"Returns the source lines, line number, and source file path for the given function.\n \ndiff --git a/torch/onnx/_internal/exporter.py b/torch/onnx/_internal/exporter.py\nindex aae7eb6f8e3804..33bf8c7afe44f5 100644\n--- a/torch/onnx/_internal/exporter.py\n+++ b/torch/onnx/_internal/exporter.py\n@@ -145,9 +145,7 @@ class ResolvedExportOptions(ExportOptions):\n     logging diagnostics, and generating the SARIF log.\"\"\"\n \n     @_beartype.beartype\n-    def __init__(\n-        self, options: Optional[Union[ExportOptions, \"ResolvedExportOptions\"]]\n-    ):\n+    def __init__(self, options: Optional[Union[ExportOptions, ResolvedExportOptions]]):\n         if options is None:\n             options = ExportOptions()\n         if isinstance(options, ResolvedExportOptions):\ndiff --git a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\nindex a312cf1aed3d80..2b489ca076b430 100644\n--- a/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n+++ b/torch/onnx/_internal/fx/onnxfunction_dispatcher.py\n@@ -105,7 +105,7 @@ def dispatch(\n         ],\n         onnx_kwargs: Dict[str, fx_type_utils.Argument],\n         diagnostic_context: diagnostics.DiagnosticContext,\n-    ) -> Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]:\n+    ) -> Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]:\n         \"\"\"Dispatches an ONNX function based on the given FX node, arguments, and keyword arguments.\n         Args:\n             node: The TorchFX node to dispatch the function for.\n@@ -405,7 +405,7 @@ def aten_new_full_dtype(self: TTensor, size: INT64, fill_value: TTensor, dtype:\n \n     \"\"\"\n \n-    def __init__(self, onnxfunction: \"onnxscript.OnnxFunction\"):\n+    def __init__(self, onnxfunction: onnxscript.OnnxFunction):\n         \"\"\"Initialize the OnnxSchemaChecker .\n \n         Args:\n@@ -579,7 +579,7 @@ def _record_matching_score(\n     @_beartype.beartype\n     def _separate_input_attributes_from_arguments(\n         self,\n-        param_schemas: Sequence[\"onnxscript.values.ParamSchema\"],\n+        param_schemas: Sequence[onnxscript.values.ParamSchema],\n         args: Sequence[\n             Optional[Union[fx_type_utils.TensorLike, str, int, float, bool, list]]\n         ],\ndiff --git a/torch/onnx/_internal/fx/passes/type_promotion.py b/torch/onnx/_internal/fx/passes/type_promotion.py\nindex e100afefe7814a..c8a10cc322fe90 100644\n--- a/torch/onnx/_internal/fx/passes/type_promotion.py\n+++ b/torch/onnx/_internal/fx/passes/type_promotion.py\n@@ -1217,7 +1217,7 @@ def add_rule(self, rule: TypePromotionRule) -> None:\n             ValueError: If the rule is invalid.\n         \"\"\"\n         if not rule.is_valid():\n-            raise ValueError(\"Invalid type promotion rule: {}\".format(rule))\n+            raise ValueError(f\"Invalid type promotion rule: {rule}\")\n         self._rule_table[f\"{rule.namespace}.{rule.op_name}\"] = rule\n \n     @_beartype.beartype\ndiff --git a/torch/onnx/_internal/fx/registration.py b/torch/onnx/_internal/fx/registration.py\nindex 135c9afe9cdd57..b7c8c3521e55f5 100644\n--- a/torch/onnx/_internal/fx/registration.py\n+++ b/torch/onnx/_internal/fx/registration.py\n@@ -29,7 +29,7 @@ class SymbolicFunction:\n \n     \"\"\"\n \n-    onnx_function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"]\n+    onnx_function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction]\n     op_full_name: str\n     is_custom: bool = False\n \n@@ -99,7 +99,7 @@ def _register(\n     @_beartype.beartype\n     def register_custom_op(\n         self,\n-        function: Union[\"onnxscript.OnnxFunction\", \"onnxscript.TracedOnnxFunction\"],\n+        function: Union[onnxscript.OnnxFunction, onnxscript.TracedOnnxFunction],\n         namespace: str,\n         op_name: str,\n         overload: Optional[str] = None,\ndiff --git a/torch/onnx/_internal/io_adapter.py b/torch/onnx/_internal/io_adapter.py\nindex 2654a1ade32ac4..1a80e179ac0b67 100644\n--- a/torch/onnx/_internal/io_adapter.py\n+++ b/torch/onnx/_internal/io_adapter.py\n@@ -60,7 +60,7 @@ def append_step(self, step: InputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, *model_args, **model_kwargs\n-    ) -> Sequence[Union[int, float, bool, str, \"torch.Tensor\", None]]:\n+    ) -> Sequence[Union[int, float, bool, str, torch.Tensor, None]]:\n         \"\"\"Converts the PyTorch model inputs to exported ONNX model inputs format.\n \n         Args:\n@@ -113,7 +113,7 @@ def append_step(self, step: OutputAdaptStep) -> None:\n     @_beartype.beartype\n     def apply(\n         self, model_outputs: Any\n-    ) -> Sequence[Union[\"torch.Tensor\", int, float, bool, str]]:\n+    ) -> Sequence[Union[torch.Tensor, int, float, bool, str]]:\n         \"\"\"Converts the PyTorch model outputs to exported ONNX model outputs format.\n \n         Args:\n@@ -228,7 +228,7 @@ def apply(\n class LiftParametersAndBuffersIntoArgsStep:\n     \"\"\"Append parameters and buffers to model's positional argument list.\"\"\"\n \n-    def __init__(self, inputs: Tuple[\"torch.Tensor\", ...]) -> None:\n+    def __init__(self, inputs: Tuple[torch.Tensor, ...]) -> None:\n         self.inputs = inputs\n \n     def apply(\ndiff --git a/torch/onnx/_internal/jit_utils.py b/torch/onnx/_internal/jit_utils.py\nindex 9052961fc7a646..c46a82c40dfec8 100644\n--- a/torch/onnx/_internal/jit_utils.py\n+++ b/torch/onnx/_internal/jit_utils.py\n@@ -40,7 +40,7 @@ class GraphContext:\n     block: _C.Block\n     opset: int\n     original_node: _C.Node\n-    params_dict: Dict[str, \"_C.IValue\"]\n+    params_dict: Dict[str, _C.IValue]\n     env: Dict[_C.Value, _C.Value]\n \n     # Relay methods from _C.Graph for compatibility with symbolic functions that expect\ndiff --git a/torch/onnx/symbolic_opset11.py b/torch/onnx/symbolic_opset11.py\nindex b432244c42aaa7..3bb63e0e8fa36b 100644\n--- a/torch/onnx/symbolic_opset11.py\n+++ b/torch/onnx/symbolic_opset11.py\n@@ -888,7 +888,7 @@ def _get_arange_dtype(dtype):\n         dtype = symbolic_helper._maybe_get_const(dtype, \"i\")\n         return dtype\n \n-    if len(args) == 2 and all((isinstance(val, int) for val in args)):\n+    if len(args) == 2 and all(isinstance(val, int) for val in args):\n         # aten::arange(Scalar start, Scalar end)\n         dtype = torch.int64\n         # Start index.\ndiff --git a/torch/onnx/symbolic_opset17.py b/torch/onnx/symbolic_opset17.py\nindex 92151a5e58d977..3fa03ab8e11915 100644\n--- a/torch/onnx/symbolic_opset17.py\n+++ b/torch/onnx/symbolic_opset17.py\n@@ -144,8 +144,8 @@ def stft(\n         # Center window around zeros if needed (required by ONNX's STFT)\n         if n_win < n_fft:\n             left, right = _compute_edge_sizes(n_fft, n_win)\n-            left_win = g.op(\"Constant\", value_t=torch.zeros((left)))\n-            right_win = g.op(\"Constant\", value_t=torch.zeros((right)))\n+            left_win = g.op(\"Constant\", value_t=torch.zeros(left))\n+            right_win = g.op(\"Constant\", value_t=torch.zeros(right))\n             window = g.op(\"Concat\", left_win, window, right_win, axis_i=0)\n \n     # Create window, if needed\n@@ -161,11 +161,11 @@ def stft(\n             # Center window, if needed\n             left, right = _compute_edge_sizes(n_fft, win_length)\n             torch_window = torch.hstack(\n-                (torch.zeros((left)), torch.ones((win_length)), torch.zeros((right)))\n+                (torch.zeros(left), torch.ones(win_length), torch.zeros(right))\n             )\n         else:\n             # Rectangle window\n-            torch_window = torch.ones((n_fft))\n+            torch_window = torch.ones(n_fft)\n         assert torch_window.shape[0] == n_fft\n         window = g.op(\"Constant\", value_t=torch_window)\n     window = g.op(\ndiff --git a/torch/onnx/verification.py b/torch/onnx/verification.py\nindex abfa4677eb21cb..27fe4e28e32cf6 100644\n--- a/torch/onnx/verification.py\n+++ b/torch/onnx/verification.py\n@@ -1310,7 +1310,7 @@ def essential_node_kinds(self) -> Set[str]:\n         }\n \n     @_beartype.beartype\n-    def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n+    def all_mismatch_leaf_graph_info(self) -> List[GraphInfo]:\n         \"\"\"Return a list of all leaf `GraphInfo` objects that have mismatch.\"\"\"\n         if not self.has_mismatch():\n             return []\n@@ -1333,7 +1333,7 @@ def all_mismatch_leaf_graph_info(self) -> List[\"GraphInfo\"]:\n         return results\n \n     @_beartype.beartype\n-    def find_partition(self, id: str) -> Optional[\"GraphInfo\"]:\n+    def find_partition(self, id: str) -> Optional[GraphInfo]:\n         \"\"\"Find the `GraphInfo` object with the given id.\"\"\"\n         if id == self.id:\n             return self\n"
  },
  {
    "number": 105386,
    "title": "[BE] Enable ruff's UP rules and autoformat optim/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "7ea27c04f3a766666ab8c148b50c3810764e0ff8",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105386",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105386/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105386.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105386.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105386/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105386/comments",
    "labels": [
      "open source",
      "release notes: optim"
    ],
    "_event_time": "2023-07-18T01:11:22.057401Z",
    "state": "closed",
    "patch": "From bcbc75d7fd83581bfe2937e42bf175a30bda155d Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:14 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat optim/\n\n[ghstack-poisoned]\n---\n test/distributions/test_constraints.py        |  14 +-\n test/distributions/test_distributions.py      | 238 +++++++++---------\n test/distributions/test_transforms.py         |  20 +-\n test/optim/test_optim.py                      |   4 +-\n torch/distributions/constraints.py            |  16 +-\n torch/distributions/independent.py            |   2 +-\n torch/distributions/kl.py                     |   6 +-\n .../lowrank_multivariate_normal.py            |   2 +-\n torch/distributions/mixture_same_family.py    |   8 +-\n .../distributions/transformed_distribution.py |   2 +-\n torch/distributions/transforms.py             |   6 +-\n torch/optim/adadelta.py                       |   8 +-\n torch/optim/adagrad.py                        |   8 +-\n torch/optim/adam.py                           |  10 +-\n torch/optim/adamax.py                         |  10 +-\n torch/optim/adamw.py                          |  10 +-\n torch/optim/asgd.py                           |   4 +-\n torch/optim/lr_scheduler.py                   |  18 +-\n torch/optim/nadam.py                          |  12 +-\n torch/optim/optimizer.py                      |  12 +-\n torch/optim/radam.py                          |  10 +-\n torch/optim/rmsprop.py                        |  10 +-\n torch/optim/rprop.py                          |   4 +-\n torch/optim/sgd.py                            |   6 +-\n torch/optim/sparse_adam.py                    |   8 +-\n torch/package/_importlib.py                   |   6 +-\n .../package/file_structure_representation.py  |   1 -\n torch/package/package_exporter.py             |  10 +-\n torch/package/package_importer.py             |   6 +-\n torch/profiler/_memory_profiler.py            |   4 +-\n torch/profiler/_pattern_matcher.py            |   2 +-\n torch/signal/windows/windows.py               |   1 -\n torch/sparse/semi_structured.py               |  16 +-\n 33 files changed, 246 insertions(+), 248 deletions(-)\n\ndiff --git a/test/distributions/test_constraints.py b/test/distributions/test_constraints.py\nindex b733cbc021e153..0753b246e37948 100644\n--- a/test/distributions/test_constraints.py\n+++ b/test/distributions/test_constraints.py\n@@ -83,7 +83,7 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         t = biject_to(constraint)\n     except NotImplementedError:\n         pytest.skip('`biject_to` not implemented.')\n-    assert t.bijective, \"biject_to({}) is not bijective\".format(constraint)\n+    assert t.bijective, f\"biject_to({constraint}) is not bijective\"\n     if constraint_fn is constraints.corr_cholesky:\n         # (D * (D-1)) / 2 (where D = 4) = 6 (size of last dim)\n         x = torch.randn(6, 6, dtype=torch.double)\n@@ -93,12 +93,12 @@ def test_biject_to(constraint_fn, args, is_cuda):\n         x = x.cuda()\n     y = t(x)\n     assert constraint.check(y).all(), '\\n'.join([\n-        \"Failed to biject_to({})\".format(constraint),\n-        \"x = {}\".format(x),\n-        \"biject_to(...)(x) = {}\".format(y),\n+        f\"Failed to biject_to({constraint})\",\n+        f\"x = {x}\",\n+        f\"biject_to(...)(x) = {y}\",\n     ])\n     x2 = t.inv(y)\n-    assert torch.allclose(x, x2), \"Error in biject_to({}) inverse\".format(constraint)\n+    assert torch.allclose(x, x2), f\"Error in biject_to({constraint}) inverse\"\n \n     j = t.log_abs_det_jacobian(x, y)\n     assert j.shape == x.shape[:x.dim() - t.domain.event_dim]\n@@ -119,10 +119,10 @@ def test_transform_to(constraint_fn, args, is_cuda):\n     if is_cuda:\n         x = x.cuda()\n     y = t(x)\n-    assert constraint.check(y).all(), \"Failed to transform_to({})\".format(constraint)\n+    assert constraint.check(y).all(), f\"Failed to transform_to({constraint})\"\n     x2 = t.inv(y)\n     y2 = t(x2)\n-    assert torch.allclose(y, y2), \"Error in transform_to({}) pseudoinverse\".format(constraint)\n+    assert torch.allclose(y, y2), f\"Error in transform_to({constraint}) pseudoinverse\"\n \n \n if __name__ == \"__main__\":\ndiff --git a/test/distributions/test_distributions.py b/test/distributions/test_distributions.py\nindex 69591d31c5ed20..2f4d256516c849 100644\n--- a/test/distributions/test_distributions.py\n+++ b/test/distributions/test_distributions.py\n@@ -862,7 +862,7 @@ def _check_sampler_sampler(self, torch_dist, ref_dist, message, multivariate=Fal\n         bins = samples.reshape((num_bins, samples_per_bin)).mean(axis=1)\n         stddev = samples_per_bin ** -0.5\n         threshold = stddev * scipy.special.erfinv(1 - 2 * failure_rate / num_bins)\n-        message = '{}.sample() is biased:\\n{}'.format(message, bins)\n+        message = f'{message}.sample() is biased:\\n{bins}'\n         for bias in bins:\n             self.assertLess(-threshold, bias, message)\n             self.assertLess(bias, threshold, message)\n@@ -971,7 +971,7 @@ def test_has_examples(self):\n             if isinstance(Dist, type) and issubclass(Dist, Distribution) \\\n                     and Dist is not Distribution and Dist is not ExponentialFamily:\n                 self.assertIn(Dist, distributions_with_examples,\n-                              \"Please add {} to the EXAMPLES list in test_distributions.py\".format(Dist.__name__))\n+                              f\"Please add {Dist.__name__} to the EXAMPLES list in test_distributions.py\")\n \n     def test_support_attributes(self):\n         for Dist, params in EXAMPLES:\n@@ -1120,7 +1120,7 @@ def test_geometric_sample(self):\n         for prob in [0.01, 0.18, 0.8]:\n             self._check_sampler_discrete(Geometric(prob),\n                                          scipy.stats.geom(p=prob, loc=-1),\n-                                         'Geometric(prob={})'.format(prob))\n+                                         f'Geometric(prob={prob})')\n \n     def test_binomial(self):\n         p = torch.arange(0.05, 1, 0.1).requires_grad_()\n@@ -1136,7 +1136,7 @@ def test_binomial_sample(self):\n             for count in [2, 10, 100, 500]:\n                 self._check_sampler_discrete(Binomial(total_count=count, probs=prob),\n                                              scipy.stats.binom(count, prob),\n-                                             'Binomial(total_count={}, probs={})'.format(count, prob))\n+                                             f'Binomial(total_count={count}, probs={prob})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_binomial_log_prob_and_entropy(self):\n@@ -1431,7 +1431,7 @@ def test_poisson_sample(self):\n         for rate in [0.1, 1.0, 5.0]:\n             self._check_sampler_discrete(Poisson(rate),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={})'.format(rate),\n+                                         f'Poisson(lambda={rate})',\n                                          failure_rate=1e-3)\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n@@ -1441,7 +1441,7 @@ def test_poisson_gpu_sample(self):\n         for rate in [0.12, 0.9, 4.0]:\n             self._check_sampler_discrete(Poisson(torch.tensor([rate]).cuda()),\n                                          scipy.stats.poisson(rate),\n-                                         'Poisson(lambda={}, cuda)'.format(rate),\n+                                         f'Poisson(lambda={rate}, cuda)',\n                                          failure_rate=1e-3)\n \n     def test_relaxed_bernoulli(self):\n@@ -1476,7 +1476,7 @@ def sample(self, *args, **kwargs):\n         for probs, temp in product([0.1, 0.2, 0.8], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(Rounded(RelaxedBernoulli(temp, probs)),\n                                          scipy.stats.bernoulli(probs),\n-                                         'Rounded(RelaxedBernoulli(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedBernoulli(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [0.001, 0.2, 0.999]:\n@@ -1534,7 +1534,7 @@ def pmf(self, samples):\n         for probs, temp in product([torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])], [0.1, 1.0, 10.0]):\n             self._check_sampler_discrete(ArgMax(RelaxedOneHotCategorical(temp, probs)),\n                                          ScipyCategorical(scipy.stats.multinomial(1, probs)),\n-                                         'Rounded(RelaxedOneHotCategorical(temp={}, probs={}))'.format(temp, probs),\n+                                         f'Rounded(RelaxedOneHotCategorical(temp={temp}, probs={probs}))',\n                                          failure_rate=1e-3)\n \n         for probs in [torch.tensor([0.1, 0.9]), torch.tensor([0.2, 0.2, 0.6])]:\n@@ -1588,7 +1588,7 @@ def test_vonmises_sample(self):\n             for concentration in [0.03, 0.3, 1.0, 10.0, 100.0]:\n                 self._check_sampler_sampler(VonMises(loc, concentration),\n                                             scipy.stats.vonmises(loc=loc, kappa=concentration),\n-                                            \"VonMises(loc={}, concentration={})\".format(loc, concentration),\n+                                            f\"VonMises(loc={loc}, concentration={concentration})\",\n                                             num_samples=int(1e5), circular=True)\n \n     def test_vonmises_logprob(self):\n@@ -1694,7 +1694,7 @@ def test_halfnormal_sample(self):\n         for std in [0.1, 1.0, 10.0]:\n             self._check_sampler_sampler(HalfNormal(std),\n                                         scipy.stats.halfnorm(scale=std),\n-                                        'HalfNormal(scale={})'.format(std))\n+                                        f'HalfNormal(scale={std})')\n \n     def test_lognormal(self):\n         mean = torch.randn(5, 5, requires_grad=True)\n@@ -1746,7 +1746,7 @@ def test_lognormal_sample(self):\n         for mean, std in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(LogNormal(mean, std),\n                                         scipy.stats.lognorm(scale=math.exp(mean), s=std),\n-                                        'LogNormal(loc={}, scale={})'.format(mean, std))\n+                                        f'LogNormal(loc={mean}, scale={std})')\n \n     def test_logisticnormal(self):\n         set_rng_seed(1)  # see Note [Randomized statistical tests]\n@@ -1814,7 +1814,7 @@ def test_logisticnormal_sample(self):\n             std_th = torch.tensor(np.sqrt(np.diag(cov)))\n             self._check_sampler_sampler(\n                 LogisticNormal(mean_th, std_th), ref_dist,\n-                'LogisticNormal(loc={}, scale={})'.format(mean_th, std_th),\n+                f'LogisticNormal(loc={mean_th}, scale={std_th})',\n                 multivariate=True)\n \n     def test_mixture_same_family_shape(self):\n@@ -1958,7 +1958,7 @@ def test_normal_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Normal(loc, scale),\n                                         scipy.stats.norm(loc=loc, scale=scale),\n-                                        'Normal(mean={}, std={})'.format(loc, scale))\n+                                        f'Normal(mean={loc}, std={scale})')\n \n     def test_lowrank_multivariate_normal_shape(self):\n         mean = torch.randn(5, 3, requires_grad=True)\n@@ -2191,15 +2191,15 @@ def test_multivariate_normal_sample(self):\n \n         self._check_sampler_sampler(MultivariateNormal(mean, cov),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, cov={})'.format(mean, cov),\n+                                    f'MultivariateNormal(loc={mean}, cov={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, precision_matrix=prec),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, atol={})'.format(mean, prec),\n+                                    f'MultivariateNormal(loc={mean}, atol={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(MultivariateNormal(mean, scale_tril=scale_tril),\n                                     scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy()),\n-                                    'MultivariateNormal(loc={}, scale_tril={})'.format(mean, scale_tril),\n+                                    f'MultivariateNormal(loc={mean}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_multivariate_normal_properties(self):\n@@ -2352,15 +2352,15 @@ def test_wishart_sample(self):\n \n         self._check_sampler_sampler(Wishart(df, cov),\n                                     ref_dist,\n-                                    'Wishart(df={}, covariance_matrix={})'.format(df, cov),\n+                                    f'Wishart(df={df}, covariance_matrix={cov})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, precision_matrix=prec),\n                                     ref_dist,\n-                                    'Wishart(df={}, precision_matrix={})'.format(df, prec),\n+                                    f'Wishart(df={df}, precision_matrix={prec})',\n                                     multivariate=True)\n         self._check_sampler_sampler(Wishart(df, scale_tril=scale_tril),\n                                     ref_dist,\n-                                    'Wishart(df={}, scale_tril={})'.format(df, scale_tril),\n+                                    f'Wishart(df={df}, scale_tril={scale_tril})',\n                                     multivariate=True)\n \n     def test_wishart_properties(self):\n@@ -2431,7 +2431,7 @@ def test_exponential_sample(self):\n         for rate in [1e-5, 1.0, 10.]:\n             self._check_sampler_sampler(Exponential(rate),\n                                         scipy.stats.expon(scale=1. / rate),\n-                                        'Exponential(rate={})'.format(rate))\n+                                        f'Exponential(rate={rate})')\n \n     def test_laplace(self):\n         loc = torch.randn(5, 5, requires_grad=True)\n@@ -2482,7 +2482,7 @@ def test_laplace_sample(self):\n         for loc, scale in product([-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Laplace(loc, scale),\n                                         scipy.stats.laplace(loc=loc, scale=scale),\n-                                        'Laplace(loc={}, scale={})'.format(loc, scale))\n+                                        f'Laplace(loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gamma_shape(self):\n@@ -2533,7 +2533,7 @@ def test_gamma_sample(self):\n         for alpha, beta in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gamma(alpha, beta),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(concentration={}, rate={})'.format(alpha, beta))\n+                                        f'Gamma(concentration={alpha}, rate={beta})')\n \n     @unittest.skipIf(not TEST_CUDA, \"CUDA not found\")\n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n@@ -2543,7 +2543,7 @@ def test_gamma_gpu_sample(self):\n             a, b = torch.tensor([alpha]).cuda(), torch.tensor([beta]).cuda()\n             self._check_sampler_sampler(Gamma(a, b),\n                                         scipy.stats.gamma(alpha, scale=1.0 / beta),\n-                                        'Gamma(alpha={}, beta={})'.format(alpha, beta),\n+                                        f'Gamma(alpha={alpha}, beta={beta})',\n                                         failure_rate=1e-4)\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -2575,7 +2575,7 @@ def test_pareto_sample(self):\n         for scale, alpha in product([0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Pareto(scale, alpha),\n                                         scipy.stats.pareto(alpha, scale=scale),\n-                                        'Pareto(scale={}, alpha={})'.format(scale, alpha))\n+                                        f'Pareto(scale={scale}, alpha={alpha})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_gumbel(self):\n@@ -2616,7 +2616,7 @@ def test_gumbel_sample(self):\n         for loc, scale in product([-5.0, -1.0, -0.1, 0.1, 1.0, 5.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Gumbel(loc, scale),\n                                         scipy.stats.gumbel_r(loc=loc, scale=scale),\n-                                        'Gumbel(loc={}, scale={})'.format(loc, scale))\n+                                        f'Gumbel(loc={loc}, scale={scale})')\n \n     def test_kumaraswamy_shape(self):\n         concentration1 = torch.randn(2, 3).abs().requires_grad_()\n@@ -2646,13 +2646,13 @@ def test_kumaraswamy_mean_variance(self):\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .mean\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .mean\")\n             expected = samples.var(0)\n             actual = m.variance\n             error = (expected - actual).abs()\n             max_error = max(error[error == error])\n             self.assertLess(max_error, 0.01,\n-                            \"Kumaraswamy example {}/{}, incorrect .variance\".format(i + 1, len(cases)))\n+                            f\"Kumaraswamy example {i + 1}/{len(cases)}, incorrect .variance\")\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_fishersnedecor(self):\n@@ -2683,7 +2683,7 @@ def test_fishersnedecor_sample(self):\n         for df1, df2 in product([0.1, 0.5, 1.0, 5.0, 10.0], [0.1, 0.5, 1.0, 5.0, 10.0]):\n             self._check_sampler_sampler(FisherSnedecor(df1, df2),\n                                         scipy.stats.f(df1, df2),\n-                                        'FisherSnedecor(loc={}, scale={})'.format(df1, df2))\n+                                        f'FisherSnedecor(loc={df1}, scale={df2})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_chi2_shape(self):\n@@ -2710,7 +2710,7 @@ def test_chi2_sample(self):\n         for df in [0.1, 1.0, 5.0]:\n             self._check_sampler_sampler(Chi2(df),\n                                         scipy.stats.chi2(df),\n-                                        'Chi2(df={})'.format(df))\n+                                        f'Chi2(df={df})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT(self):\n@@ -2740,7 +2740,7 @@ def test_studentT_sample(self):\n         for df, loc, scale in product([0.1, 1.0, 5.0, 10.0], [-1.0, 0.0, 1.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(StudentT(df=df, loc=loc, scale=scale),\n                                         scipy.stats.t(df=df, loc=loc, scale=scale),\n-                                        'StudentT(df={}, loc={}, scale={})'.format(df, loc, scale))\n+                                        f'StudentT(df={df}, loc={loc}, scale={scale})')\n \n     @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n     def test_studentT_log_prob(self):\n@@ -2793,7 +2793,7 @@ def test_dirichlet_sample(self):\n         alpha = torch.exp(torch.randn(3))\n         self._check_sampler_sampler(Dirichlet(alpha),\n                                     scipy.stats.dirichlet(alpha.numpy()),\n-                                    'Dirichlet(alpha={})'.format(list(alpha)),\n+                                    f'Dirichlet(alpha={list(alpha)})',\n                                     multivariate=True)\n \n     def test_dirichlet_mode(self):\n@@ -2837,11 +2837,11 @@ def test_beta_sample(self):\n         for con1, con0 in product([0.1, 1.0, 10.0], [0.1, 1.0, 10.0]):\n             self._check_sampler_sampler(Beta(con1, con0),\n                                         scipy.stats.beta(con1, con0),\n-                                        'Beta(alpha={}, beta={})'.format(con1, con0))\n+                                        f'Beta(alpha={con1}, beta={con0})')\n         # Check that small alphas do not cause NANs.\n         for Tensor in [torch.FloatTensor, torch.DoubleTensor]:\n             x = Beta(Tensor([1e-6]), Tensor([1e-6])).sample()[0]\n-            self.assertTrue(np.isfinite(x) and x > 0, 'Invalid Beta.sample(): {}'.format(x))\n+            self.assertTrue(np.isfinite(x) and x > 0, f'Invalid Beta.sample(): {x}')\n \n     def test_beta_underflow(self):\n         # For low values of (alpha, beta), the gamma samples can underflow\n@@ -2997,10 +2997,10 @@ def test_cdf_icdf_inverse(self):\n                     continue\n                 rel_error = torch.abs(actual - samples) / (1e-10 + torch.abs(samples))\n                 self.assertLess(rel_error.max(), 1e-4, msg='\\n'.join([\n-                    '{} example {}/{}, icdf(cdf(x)) != x'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf(x) = {}'.format(cdf),\n-                    'icdf(cdf(x)) = {}'.format(actual),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, icdf(cdf(x)) != x',\n+                    f'x = {samples}',\n+                    f'cdf(x) = {cdf}',\n+                    f'icdf(cdf(x)) = {actual}',\n                 ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3029,11 +3029,11 @@ def test_cdf_log_prob(self):\n                     continue\n                 cdfs_derivative = grad(cdfs.sum(), [samples])[0]  # this should not be wrapped in torch.abs()\n                 self.assertEqual(cdfs_derivative, pdfs, msg='\\n'.join([\n-                    '{} example {}/{}, d(cdf)/dx != pdf(x)'.format(Dist.__name__, i + 1, len(params)),\n-                    'x = {}'.format(samples),\n-                    'cdf = {}'.format(cdfs),\n-                    'pdf = {}'.format(pdfs),\n-                    'grad(cdf) = {}'.format(cdfs_derivative),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, d(cdf)/dx != pdf(x)',\n+                    f'x = {samples}',\n+                    f'cdf = {cdfs}',\n+                    f'pdf = {pdfs}',\n+                    f'grad(cdf) = {cdfs_derivative}',\n                 ]))\n \n     def test_valid_parameter_broadcasting(self):\n@@ -3144,13 +3144,13 @@ def test_valid_parameter_broadcasting(self):\n         for dist, expected_size in valid_examples:\n             actual_size = dist.sample().size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n             sample_shape = torch.Size((2,))\n             expected_size = sample_shape + expected_size\n             actual_size = dist.sample(sample_shape).size()\n             self.assertEqual(actual_size, expected_size,\n-                             msg='{} actual size: {} != expected size: {}'.format(dist, actual_size, expected_size))\n+                             msg=f'{dist} actual size: {actual_size} != expected size: {expected_size}')\n \n     def test_invalid_parameter_broadcasting(self):\n         # invalid broadcasting cases; should throw error\n@@ -3303,13 +3303,13 @@ def test_gamma(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.0005, '\\n'.join([\n-                'Bad gradient dx/alpha for x ~ Gamma({}, 1)'.format(alpha),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at alpha={}, x={}'.format(alpha, x[rel_error.argmax()]),\n+                f'Bad gradient dx/alpha for x ~ Gamma({alpha}, 1)',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at alpha={alpha}, x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3331,12 +3331,12 @@ def test_chi2(self):\n             expected_grad = -cdf_df / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx/ddf for x ~ Chi2({})'.format(df),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n+                f'Bad gradient dx/ddf for x ~ Chi2({df})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3361,13 +3361,13 @@ def test_dirichlet_on_diagonal(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.001, '\\n'.join([\n-                'Bad gradient dx[0]/dalpha[0] for Dirichlet([{}, {}, {}])'.format(a0, a1, a2),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x={}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx[0]/dalpha[0] for Dirichlet([{a0}, {a1}, {a2}])',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x={x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3391,13 +3391,13 @@ def test_beta_wrt_alpha(self):\n             expected_grad = -cdf_alpha / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon1 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon1 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]}',\n             ]))\n \n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n@@ -3421,13 +3421,13 @@ def test_beta_wrt_beta(self):\n             expected_grad = -cdf_beta / cdf_x\n             rel_error = np.abs(actual_grad - expected_grad) / (expected_grad + 1e-30)\n             self.assertLess(np.max(rel_error), 0.005, '\\n'.join([\n-                'Bad gradient dx/dcon0 for x ~ Beta({}, {})'.format(con1, con0),\n-                'x {}'.format(x),\n-                'expected {}'.format(expected_grad),\n-                'actual {}'.format(actual_grad),\n-                'rel error {}'.format(rel_error),\n-                'max error {}'.format(rel_error.max()),\n-                'at x = {!r}'.format(x[rel_error.argmax()]),\n+                f'Bad gradient dx/dcon0 for x ~ Beta({con1}, {con0})',\n+                f'x {x}',\n+                f'expected {expected_grad}',\n+                f'actual {actual_grad}',\n+                f'rel error {rel_error}',\n+                f'max error {rel_error.max()}',\n+                f'at x = {x[rel_error.argmax()]!r}',\n             ]))\n \n     def test_dirichlet_multivariate(self):\n@@ -3485,8 +3485,8 @@ def compute_v(x, alpha):\n             # expression in terms of log_prob rather than the less numerically stable log_prob.exp().\n             error = dlogp_da + (dlogp_dx * v).sum(-1) + div_v\n             self.assertLess(torch.abs(error).max(), 0.005, '\\n'.join([\n-                'Dirichlet([{}, {}, {}]) gradient violates continuity equation:'.format(a1, a2, a3),\n-                'error = {}'.format(error),\n+                f'Dirichlet([{a1}, {a2}, {a3}]) gradient violates continuity equation:',\n+                f'error = {error}',\n             ]))\n \n \n@@ -4147,9 +4147,9 @@ def test_kl_monte_carlo(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                'Expected ({} Monte Carlo samples): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                f'Expected ({denominator} Monte Carlo samples): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     # Multivariate normal has a separate Monte Carlo based test due to the requirement of random generation of\n@@ -4174,9 +4174,9 @@ def test_kl_multivariate_normal(self):\n                 if error[error == error].max() < self.precision:\n                     break\n             self.assertLess(error[error == error].max(), self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected ({} Monte Carlo sample): {}'.format(denominator, expected),\n-                'Actual (analytic): {}'.format(actual),\n+                f'Incorrect KL(MultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected ({denominator} Monte Carlo sample): {expected}',\n+                f'Actual (analytic): {actual}',\n             ]))\n \n     def test_kl_multivariate_normal_batched(self):\n@@ -4223,23 +4223,23 @@ def test_kl_lowrank_multivariate_normal(self):\n \n             error_lowrank_lowrank = torch.abs(actual_lowrank_lowrank - expected).max()\n             self.assertLess(error_lowrank_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_lowrank),\n+                f'Incorrect KL(LowRankMultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_lowrank}',\n             ]))\n \n             error_lowrank_full = torch.abs(actual_lowrank_full - expected).max()\n             self.assertLess(error_lowrank_full, self.precision, '\\n'.join([\n-                'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_lowrank_full),\n+                f'Incorrect KL(LowRankMultivariateNormal, MultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_lowrank_full}',\n             ]))\n \n             error_full_lowrank = torch.abs(actual_full_lowrank - expected).max()\n             self.assertLess(error_full_lowrank, self.precision, '\\n'.join([\n-                'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {}/{}'.format(i + 1, n),\n-                'Expected (from KL MultivariateNormal): {}'.format(expected),\n-                'Actual (analytic): {}'.format(actual_full_lowrank),\n+                f'Incorrect KL(MultivariateNormal, LowRankMultivariateNormal) instance {i + 1}/{n}',\n+                f'Expected (from KL MultivariateNormal): {expected}',\n+                f'Actual (analytic): {actual_full_lowrank}',\n             ]))\n \n     def test_kl_lowrank_multivariate_normal_batched(self):\n@@ -4261,16 +4261,16 @@ def test_kl_exponential_family(self):\n                 actual = kl_divergence(p, q)\n                 expected = _kl_expfamily_expfamily(p, q)\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    'Incorrect KL({}, {}).'.format(type(p).__name__, type(q).__name__),\n-                    'Expected (using Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'Incorrect KL({type(p).__name__}, {type(q).__name__}).',\n+                    f'Expected (using Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n     def test_kl_infinite(self):\n         for p, q in self.infinite_examples:\n             self.assertTrue((kl_divergence(p, q) == inf).all(),\n-                            'Incorrect KL({}, {})'.format(type(p).__name__, type(q).__name__))\n+                            f'Incorrect KL({type(p).__name__}, {type(q).__name__})')\n \n     def test_kl_edgecases(self):\n         self.assertEqual(kl_divergence(Bernoulli(0), Bernoulli(0)), 0)\n@@ -4287,9 +4287,9 @@ def test_kl_shape(self):\n                     continue\n                 expected_shape = dist.batch_shape if dist.batch_shape else torch.Size()\n                 self.assertEqual(kl.shape, expected_shape, msg='\\n'.join([\n-                    '{} example {}/{}'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected {}'.format(expected_shape),\n-                    'Actual {}'.format(kl.shape),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}',\n+                    f'Expected {expected_shape}',\n+                    f'Actual {kl.shape}',\n                 ]))\n \n     def test_kl_transformed(self):\n@@ -4316,10 +4316,10 @@ def test_entropy_monte_carlo(self):\n                 ignore = (expected == inf) | (expected == -inf)\n                 expected[ignore] = actual[ignore]\n                 self.assertEqual(actual, expected, atol=0.2, rtol=0, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (monte carlo) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max()),\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (monte carlo) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}',\n                 ]))\n \n     def test_entropy_exponential_family(self):\n@@ -4337,10 +4337,10 @@ def test_entropy_exponential_family(self):\n                 except NotImplementedError:\n                     continue\n                 self.assertEqual(actual, expected, msg='\\n'.join([\n-                    '{} example {}/{}, incorrect .entropy().'.format(Dist.__name__, i + 1, len(params)),\n-                    'Expected (Bregman Divergence) {}'.format(expected),\n-                    'Actual (analytic) {}'.format(actual),\n-                    'max error = {}'.format(torch.abs(actual - expected).max())\n+                    f'{Dist.__name__} example {i + 1}/{len(params)}, incorrect .entropy().',\n+                    f'Expected (Bregman Divergence) {expected}',\n+                    f'Actual (analytic) {actual}',\n+                    f'max error = {torch.abs(actual - expected).max()}'\n                 ]))\n \n \n@@ -4632,7 +4632,7 @@ def test_lazy_logits_initialization(self):\n             dist = Dist(**param)\n             # Create new instance to generate a valid sample\n             dist.log_prob(Dist(**param).sample())\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('probs', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -4649,7 +4649,7 @@ def test_lazy_probs_initialization(self):\n                 continue\n             dist = Dist(**param)\n             dist.sample()\n-            message = 'Failed for {} example 0/{}'.format(Dist.__name__, len(params))\n+            message = f'Failed for {Dist.__name__} example 0/{len(params)}'\n             self.assertNotIn('logits', dist.__dict__, msg=message)\n             try:\n                 dist.enumerate_support()\n@@ -5161,7 +5161,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_enumerate_support(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5185,7 +5185,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_mean(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5207,7 +5207,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_variance(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5231,7 +5231,7 @@ def f(*values):\n             expected[expected == float('inf')] = 0.\n             actual[actual == float('inf')] = 0.\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_entropy(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5255,7 +5255,7 @@ def f(*values):\n             expected = f(*values)\n             actual = traced_f(*values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n     def test_cdf(self):\n         for Dist, keys, values, sample in self._examples():\n@@ -5276,7 +5276,7 @@ def f(sample, *values):\n             expected = f(sample, *values)\n             actual = traced_f(sample, *values)\n             self.assertEqual(expected, actual,\n-                             msg='{}\\nExpected:\\n{}\\nActual:\\n{}'.format(Dist.__name__, expected, actual))\n+                             msg=f'{Dist.__name__}\\nExpected:\\n{expected}\\nActual:\\n{actual}')\n \n \n if __name__ == '__main__' and torch._C.has_lapack:\ndiff --git a/test/distributions/test_transforms.py b/test/distributions/test_transforms.py\nindex a4a025b83fd36e..6fd4cf818d6b83 100644\n--- a/test/distributions/test_transforms.py\n+++ b/test/distributions/test_transforms.py\n@@ -156,7 +156,7 @@ def generate_data(transform):\n         x /= x.norm(dim=-1, keepdim=True)\n         x.diagonal(dim1=-1).copy_(x.diagonal(dim1=-1).abs())\n         return x\n-    raise ValueError('Unsupported domain: {}'.format(domain))\n+    raise ValueError(f'Unsupported domain: {domain}')\n \n \n TRANSFORMS_CACHE_ACTIVE = get_transforms(cache_size=1)\n@@ -215,19 +215,19 @@ def test_forward_inverse(transform, test_cached):\n     if transform.bijective:\n         # verify function inverse\n         assert torch.allclose(x2, x, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t.inv(t(-)) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n+            f'{transform} t.inv(t(-)) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n         ])\n     else:\n         # verify weaker function pseudo-inverse\n         assert torch.allclose(y2, y, atol=1e-4, equal_nan=True), '\\n'.join([\n-            '{} t(t.inv(t(-))) error'.format(transform),\n-            'x = {}'.format(x),\n-            'y = t(x) = {}'.format(y),\n-            'x2 = t.inv(y) = {}'.format(x2),\n-            'y2 = t(x2) = {}'.format(y2),\n+            f'{transform} t(t.inv(t(-))) error',\n+            f'x = {x}',\n+            f'y = t(x) = {y}',\n+            f'x2 = t.inv(y) = {x2}',\n+            f'y2 = t(x2) = {y2}',\n         ])\n \n \ndiff --git a/test/optim/test_optim.py b/test/optim/test_optim.py\nindex 54307b2417eaf6..2f1f5536fc2fe1 100644\n--- a/test/optim/test_optim.py\n+++ b/test/optim/test_optim.py\n@@ -1701,8 +1701,8 @@ def test_fused_optimizer_does_not_step_if_foundinf(self):\n \n         num_tensors = 5\n         for functional_optim, amsgrad, no_grad_scale in itertools.product((adam.adam, adamw.adamw), (False, True), (False, True)):\n-            params, grads, exp_avgs, exp_avg_sqs = [\n-                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4)]\n+            params, grads, exp_avgs, exp_avg_sqs = (\n+                [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] for _ in range(4))\n             prev_params = [t.clone().detach() for t in params]\n             max_exp_avg_sqs = [torch.ones((1,), device=\"cuda\") for _ in range(num_tensors)] if amsgrad else []\n             state_steps = [torch.ones((), dtype=torch.float32, device=\"cuda\") for _ in range(num_tensors)]\ndiff --git a/torch/distributions/constraints.py b/torch/distributions/constraints.py\nindex a4e3c08461cde7..5f284959beb372 100644\n--- a/torch/distributions/constraints.py\n+++ b/torch/distributions/constraints.py\n@@ -258,7 +258,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -277,7 +277,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -296,7 +296,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -321,7 +321,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -338,7 +338,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={})'.format(self.lower_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound})'\n         return fmt_string\n \n \n@@ -355,7 +355,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(upper_bound={})'.format(self.upper_bound)\n+        fmt_string += f'(upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -373,7 +373,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \n@@ -391,7 +391,7 @@ def check(self, value):\n \n     def __repr__(self):\n         fmt_string = self.__class__.__name__[1:]\n-        fmt_string += '(lower_bound={}, upper_bound={})'.format(self.lower_bound, self.upper_bound)\n+        fmt_string += f'(lower_bound={self.lower_bound}, upper_bound={self.upper_bound})'\n         return fmt_string\n \n \ndiff --git a/torch/distributions/independent.py b/torch/distributions/independent.py\nindex 48442650ddcb77..44a01fd62f9130 100644\n--- a/torch/distributions/independent.py\n+++ b/torch/distributions/independent.py\n@@ -109,4 +109,4 @@ def enumerate_support(self, expand=True):\n         return self.base_dist.enumerate_support(expand=expand)\n \n     def __repr__(self):\n-        return self.__class__.__name__ + '({}, {})'.format(self.base_dist, self.reinterpreted_batch_ndims)\n+        return self.__class__.__name__ + f'({self.base_dist}, {self.reinterpreted_batch_ndims})'\ndiff --git a/torch/distributions/kl.py b/torch/distributions/kl.py\nindex 26d7b47d2f51a8..4eda85ef75b68a 100644\n--- a/torch/distributions/kl.py\n+++ b/torch/distributions/kl.py\n@@ -65,9 +65,9 @@ def kl_version2(p, q): ...\n         type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n     \"\"\"\n     if not isinstance(type_p, type) and issubclass(type_p, Distribution):\n-        raise TypeError('Expected type_p to be a Distribution subclass but got {}'.format(type_p))\n+        raise TypeError(f'Expected type_p to be a Distribution subclass but got {type_p}')\n     if not isinstance(type_q, type) and issubclass(type_q, Distribution):\n-        raise TypeError('Expected type_q to be a Distribution subclass but got {}'.format(type_q))\n+        raise TypeError(f'Expected type_q to be a Distribution subclass but got {type_q}')\n \n     def decorator(fun):\n         _KL_REGISTRY[type_p, type_q] = fun\n@@ -735,7 +735,7 @@ def _kl_uniform_beta(p, q):\n     common_term = p.high - p.low\n     t1 = torch.log(common_term)\n     t2 = (q.concentration1 - 1) * (_x_log_x(p.high) - _x_log_x(p.low) - common_term) / common_term\n-    t3 = (q.concentration0 - 1) * (_x_log_x((1 - p.high)) - _x_log_x((1 - p.low)) + common_term) / common_term\n+    t3 = (q.concentration0 - 1) * (_x_log_x(1 - p.high) - _x_log_x(1 - p.low) + common_term) / common_term\n     t4 = q.concentration1.lgamma() + q.concentration0.lgamma() - (q.concentration1 + q.concentration0).lgamma()\n     result = t3 + t4 - t1 - t2\n     result[(p.high > q.support.upper_bound) | (p.low < q.support.lower_bound)] = inf\ndiff --git a/torch/distributions/lowrank_multivariate_normal.py b/torch/distributions/lowrank_multivariate_normal.py\nindex f74ea47a7e53a4..5ca125a92dd006 100644\n--- a/torch/distributions/lowrank_multivariate_normal.py\n+++ b/torch/distributions/lowrank_multivariate_normal.py\n@@ -93,7 +93,7 @@ def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n             raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\"\n                              .format(event_shape[0]))\n         if cov_diag.shape[-1:] != event_shape:\n-            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n+            raise ValueError(f\"cov_diag must be a batch of vectors with shape {event_shape}\")\n \n         loc_ = loc.unsqueeze(-1)\n         cov_diag_ = cov_diag.unsqueeze(-1)\ndiff --git a/torch/distributions/mixture_same_family.py b/torch/distributions/mixture_same_family.py\nindex f12bef1da2c54d..a4d7bd6ff4610b 100644\n--- a/torch/distributions/mixture_same_family.py\n+++ b/torch/distributions/mixture_same_family.py\n@@ -71,17 +71,17 @@ def __init__(self,\n         cdbs = self._component_distribution.batch_shape[:-1]\n         for size1, size2 in zip(reversed(mdbs), reversed(cdbs)):\n             if size1 != 1 and size2 != 1 and size1 != size2:\n-                raise ValueError(\"`mixture_distribution.batch_shape` ({0}) is not \"\n+                raise ValueError(\"`mixture_distribution.batch_shape` ({}) is not \"\n                                  \"compatible with `component_distribution.\"\n-                                 \"batch_shape`({1})\".format(mdbs, cdbs))\n+                                 \"batch_shape`({})\".format(mdbs, cdbs))\n \n         # Check that the number of mixture component matches\n         km = self._mixture_distribution.logits.shape[-1]\n         kc = self._component_distribution.batch_shape[-1]\n         if km is not None and kc is not None and km != kc:\n-            raise ValueError(\"`mixture_distribution component` ({0}) does not\"\n+            raise ValueError(\"`mixture_distribution component` ({}) does not\"\n                              \" equal `component_distribution.batch_shape[-1]`\"\n-                             \" ({1})\".format(km, kc))\n+                             \" ({})\".format(km, kc))\n         self._num_component = km\n \n         event_shape = self._component_distribution.event_shape\ndiff --git a/torch/distributions/transformed_distribution.py b/torch/distributions/transformed_distribution.py\nindex d31064210d4ba7..cd7b5f088a99fe 100644\n--- a/torch/distributions/transformed_distribution.py\n+++ b/torch/distributions/transformed_distribution.py\n@@ -51,7 +51,7 @@ def __init__(self, base_distribution, transforms, validate_args=None):\n                 raise ValueError(\"transforms must be a Transform or a list of Transforms\")\n             self.transforms = transforms\n         else:\n-            raise ValueError(\"transforms must be a Transform or list, but was {}\".format(transforms))\n+            raise ValueError(f\"transforms must be a Transform or list, but was {transforms}\")\n \n         # Reshape base_distribution according to transforms.\n         base_shape = base_distribution.batch_shape + base_distribution.event_shape\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 06d21548384e3c..6745d1f6fbd51e 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -135,7 +135,7 @@ def with_cache(self, cache_size=1):\n             return self\n         if type(self).__init__ is Transform.__init__:\n             return type(self)(cache_size=cache_size)\n-        raise NotImplementedError(\"{}.with_cache is not implemented\".format(type(self)))\n+        raise NotImplementedError(f\"{type(self)}.with_cache is not implemented\")\n \n     def __eq__(self, other):\n         return self is other\n@@ -506,7 +506,7 @@ def forward_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.in_shape)\n         if shape[cut:] != self.in_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.in_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.in_shape}\")\n         return shape[:cut] + self.out_shape\n \n     def inverse_shape(self, shape):\n@@ -514,7 +514,7 @@ def inverse_shape(self, shape):\n             raise ValueError(\"Too few dimensions on input\")\n         cut = len(shape) - len(self.out_shape)\n         if shape[cut:] != self.out_shape:\n-            raise ValueError(\"Shape mismatch: expected {} but got {}\".format(shape[cut:], self.out_shape))\n+            raise ValueError(f\"Shape mismatch: expected {shape[cut:]} but got {self.out_shape}\")\n         return shape[:cut] + self.in_shape\n \n \ndiff --git a/torch/optim/adadelta.py b/torch/optim/adadelta.py\nindex d4cbd41883af65..a38337426313db 100644\n--- a/torch/optim/adadelta.py\n+++ b/torch/optim/adadelta.py\n@@ -22,13 +22,13 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= rho <= 1.0:\n-            raise ValueError(\"Invalid rho value: {}\".format(rho))\n+            raise ValueError(f\"Invalid rho value: {rho}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adagrad.py b/torch/optim/adagrad.py\nindex 5909818e5bfb6e..1a3e5120004f98 100644\n--- a/torch/optim/adagrad.py\n+++ b/torch/optim/adagrad.py\n@@ -23,11 +23,11 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= lr_decay:\n-            raise ValueError(\"Invalid lr_decay value: {}\".format(lr_decay))\n+            raise ValueError(f\"Invalid lr_decay value: {lr_decay}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= initial_accumulator_value:\n             raise ValueError(\n                 \"Invalid initial_accumulator_value value: {}\".format(\n@@ -35,7 +35,7 @@ def __init__(\n                 )\n             )\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adam.py b/torch/optim/adam.py\nindex 3c0d550f414b45..c7e4ed45a92156 100644\n--- a/torch/optim/adam.py\n+++ b/torch/optim/adam.py\n@@ -16,15 +16,15 @@ def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                  maximize: bool = False, capturable: bool = False,\n                  differentiable: bool = False, fused: Optional[bool] = None):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, amsgrad=amsgrad,\ndiff --git a/torch/optim/adamax.py b/torch/optim/adamax.py\nindex 9a5bf9131993ad..1ee927274558f1 100644\n--- a/torch/optim/adamax.py\n+++ b/torch/optim/adamax.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/adamw.py b/torch/optim/adamw.py\nindex da202d95c2a032..ff8dbef1d46e8a 100644\n--- a/torch/optim/adamw.py\n+++ b/torch/optim/adamw.py\n@@ -26,15 +26,15 @@ def __init__(\n         fused: Optional[bool] = None,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/asgd.py b/torch/optim/asgd.py\nindex 5e5bd759c1d540..e483e1c31fbc7c 100644\n--- a/torch/optim/asgd.py\n+++ b/torch/optim/asgd.py\n@@ -28,9 +28,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/lr_scheduler.py b/torch/optim/lr_scheduler.py\nindex b531f5149d1aff..d0f85a5daea0c8 100644\n--- a/torch/optim/lr_scheduler.py\n+++ b/torch/optim/lr_scheduler.py\n@@ -1366,11 +1366,11 @@ class CosineAnnealingWarmRestarts(LRScheduler):\n \n     def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n         if T_0 <= 0 or not isinstance(T_0, int):\n-            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n+            raise ValueError(f\"Expected positive integer T_0, but got {T_0}\")\n         if T_mult < 1 or not isinstance(T_mult, int):\n-            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n+            raise ValueError(f\"Expected integer T_mult >= 1, but got {T_mult}\")\n         if not isinstance(eta_min, (float, int)):\n-            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))\n+            raise ValueError(f\"Expected float or int eta_min, but got {eta_min} of type {type(eta_min)}\")\n         self.T_0 = T_0\n         self.T_i = T_0\n         self.T_mult = T_mult\n@@ -1425,7 +1425,7 @@ def step(self, epoch=None):\n                 self.T_i = self.T_i * self.T_mult\n         else:\n             if epoch < 0:\n-                raise ValueError(\"Expected non-negative epoch, but got {}\".format(epoch))\n+                raise ValueError(f\"Expected non-negative epoch, but got {epoch}\")\n             if epoch >= self.T_0:\n                 if self.T_mult == 1:\n                     self.T_cur = epoch % self.T_0\n@@ -1590,13 +1590,13 @@ def __init__(self,\n             raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n         elif total_steps is not None:\n             if total_steps <= 0 or not isinstance(total_steps, int):\n-                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n+                raise ValueError(f\"Expected positive integer total_steps, but got {total_steps}\")\n             self.total_steps = total_steps\n         else:\n             if epochs <= 0 or not isinstance(epochs, int):\n-                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n+                raise ValueError(f\"Expected positive integer epochs, but got {epochs}\")\n             if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n-                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n+                raise ValueError(f\"Expected positive integer steps_per_epoch, but got {steps_per_epoch}\")\n             self.total_steps = epochs * steps_per_epoch\n \n         if three_phase:\n@@ -1643,11 +1643,11 @@ def __init__(self,\n \n         # Validate pct_start\n         if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n-            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n+            raise ValueError(f\"Expected float between 0 and 1 pct_start, but got {pct_start}\")\n \n         # Validate anneal_strategy\n         if anneal_strategy not in ['cos', 'linear']:\n-            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n+            raise ValueError(f\"anneal_strategy must by one of 'cos' or 'linear', instead got {anneal_strategy}\")\n         elif anneal_strategy == 'cos':\n             self.anneal_func = self._annealing_cos\n         elif anneal_strategy == 'linear':\ndiff --git a/torch/optim/nadam.py b/torch/optim/nadam.py\nindex 23fa563f044d0d..aeb3fc8b77dd2c 100644\n--- a/torch/optim/nadam.py\n+++ b/torch/optim/nadam.py\n@@ -11,17 +11,17 @@ def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                  weight_decay=0, momentum_decay=4e-3, *, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= momentum_decay:\n-            raise ValueError(\"Invalid momentum_decay value: {}\".format(momentum_decay))\n+            raise ValueError(f\"Invalid momentum_decay value: {momentum_decay}\")\n         defaults = dict(lr=lr, betas=betas, eps=eps,\n                         weight_decay=weight_decay, momentum_decay=momentum_decay,\n                         foreach=foreach, differentiable=differentiable)\ndiff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py\nindex 34d27bdaca6058..2356a073f3719d 100644\n--- a/torch/optim/optimizer.py\n+++ b/torch/optim/optimizer.py\n@@ -246,10 +246,10 @@ def __repr__(self):\n         format_string = self.__class__.__name__ + ' ('\n         for i, group in enumerate(self.param_groups):\n             format_string += '\\n'\n-            format_string += 'Parameter Group {0}\\n'.format(i)\n+            format_string += f'Parameter Group {i}\\n'\n             for key in sorted(group.keys()):\n                 if key != 'params':\n-                    format_string += '    {0}: {1}\\n'.format(key, group[key])\n+                    format_string += f'    {key}: {group[key]}\\n'\n         format_string += ')'\n         return format_string\n \n@@ -304,7 +304,7 @@ def profile_hook_step(func):\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             self, *_ = args\n-            profile_name = \"Optimizer.step#{}.step\".format(self.__class__.__name__)\n+            profile_name = f\"Optimizer.step#{self.__class__.__name__}.step\"\n             with torch.autograd.profiler.record_function(profile_name):\n                 # call optimizer step pre hooks\n                 for pre_hook in chain(_global_optimizer_pre_hooks.values(), self._optimizer_step_pre_hooks.values()):\n@@ -337,7 +337,7 @@ def _group_tensors_by_device_and_dtype(tensorlistlist, with_indices=False):\n             return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)\n \n     def _patch_step_function(self):\n-        self._zero_grad_profile_name = \"Optimizer.zero_grad#{}.zero_grad\".format(self.__class__.__name__)\n+        self._zero_grad_profile_name = f\"Optimizer.zero_grad#{self.__class__.__name__}.zero_grad\"\n         hooked = getattr(self.__class__.step, \"hooked\", None)\n         if not hooked:\n             self.__class__.step = self.profile_hook_step(self.__class__.step)  # type: ignore[method-assign]\n@@ -468,8 +468,8 @@ def load_state_dict(self, state_dict):\n                              \"that doesn't match the size of optimizer's group\")\n \n         # Update the state\n-        id_map = dict(zip(chain.from_iterable((g['params'] for g in saved_groups)),\n-                      chain.from_iterable((g['params'] for g in groups))))\n+        id_map = dict(zip(chain.from_iterable(g['params'] for g in saved_groups),\n+                      chain.from_iterable(g['params'] for g in groups)))\n \n         def cast(param, value, param_id=None, param_groups=None, key=None):\n             r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\ndiff --git a/torch/optim/radam.py b/torch/optim/radam.py\nindex 3078db48cfd2fc..120620ab949cc1 100644\n--- a/torch/optim/radam.py\n+++ b/torch/optim/radam.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         defaults = dict(\n             lr=lr,\n             betas=betas,\ndiff --git a/torch/optim/rmsprop.py b/torch/optim/rmsprop.py\nindex 88acf98a1bcbda..cec27d95506840 100644\n--- a/torch/optim/rmsprop.py\n+++ b/torch/optim/rmsprop.py\n@@ -22,15 +22,15 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 <= eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= momentum:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if not 0.0 <= weight_decay:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n         if not 0.0 <= alpha:\n-            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n+            raise ValueError(f\"Invalid alpha value: {alpha}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/rprop.py b/torch/optim/rprop.py\nindex a0812f5fbc903f..93e7241010500a 100644\n--- a/torch/optim/rprop.py\n+++ b/torch/optim/rprop.py\n@@ -20,9 +20,9 @@ def __init__(\n         differentiable: bool = False,\n     ):\n         if not 0.0 <= lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < etas[0] < 1.0 < etas[1]:\n-            raise ValueError(\"Invalid eta values: {}, {}\".format(etas[0], etas[1]))\n+            raise ValueError(f\"Invalid eta values: {etas[0]}, {etas[1]}\")\n \n         defaults = dict(\n             lr=lr,\ndiff --git a/torch/optim/sgd.py b/torch/optim/sgd.py\nindex c34761d5e48555..d22fb2a697fd41 100644\n--- a/torch/optim/sgd.py\n+++ b/torch/optim/sgd.py\n@@ -11,11 +11,11 @@ def __init__(self, params, lr=required, momentum=0, dampening=0,\n                  weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None,\n                  differentiable: bool = False):\n         if lr is not required and lr < 0.0:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if momentum < 0.0:\n-            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n+            raise ValueError(f\"Invalid momentum value: {momentum}\")\n         if weight_decay < 0.0:\n-            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n+            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n \n         defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                         weight_decay=weight_decay, nesterov=nesterov,\ndiff --git a/torch/optim/sparse_adam.py b/torch/optim/sparse_adam.py\nindex 383b6866e822af..c68441cb389c04 100644\n--- a/torch/optim/sparse_adam.py\n+++ b/torch/optim/sparse_adam.py\n@@ -7,13 +7,13 @@\n class SparseAdam(Optimizer):\n     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, maximize: bool = False):\n         if not 0.0 < lr:\n-            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n+            raise ValueError(f\"Invalid learning rate: {lr}\")\n         if not 0.0 < eps:\n-            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n+            raise ValueError(f\"Invalid epsilon value: {eps}\")\n         if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n+            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n         if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n+            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n \n         params = list(params)\n \ndiff --git a/torch/package/_importlib.py b/torch/package/_importlib.py\nindex 327c79c67ef9d7..011567b89f5d6c 100644\n--- a/torch/package/_importlib.py\n+++ b/torch/package/_importlib.py\n@@ -31,13 +31,13 @@ def _resolve_name(name, package, level):\n     if len(bits) < level:\n         raise ValueError(\"attempted relative import beyond top-level package\")\n     base = bits[0]\n-    return \"{}.{}\".format(base, name) if name else base\n+    return f\"{base}.{name}\" if name else base\n \n \n def _sanity_check(name, package, level):\n     \"\"\"Verify arguments are \"sane\".\"\"\"\n     if not isinstance(name, str):\n-        raise TypeError(\"module name must be str, not {}\".format(type(name)))\n+        raise TypeError(f\"module name must be str, not {type(name)}\")\n     if level < 0:\n         raise ValueError(\"level must be >= 0\")\n     if level > 0:\n@@ -90,6 +90,6 @@ def _normalize_path(path):\n     \"\"\"\n     parent, file_name = os.path.split(path)\n     if parent:\n-        raise ValueError(\"{!r} must be only a file name\".format(path))\n+        raise ValueError(f\"{path!r} must be only a file name\")\n     else:\n         return file_name\ndiff --git a/torch/package/file_structure_representation.py b/torch/package/file_structure_representation.py\nindex 6ea69173ed3f69..cc5f055c1a20ef 100644\n--- a/torch/package/file_structure_representation.py\n+++ b/torch/package/file_structure_representation.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Dict, List\n \n from .glob_group import GlobGroup, GlobPattern\ndiff --git a/torch/package/package_exporter.py b/torch/package/package_exporter.py\nindex f9478b66327605..053ce0c0a89552 100644\n--- a/torch/package/package_exporter.py\n+++ b/torch/package/package_exporter.py\n@@ -79,7 +79,7 @@ class PackagingErrorReason(Enum):\n     \"\"\"\n \n     def __repr__(self):\n-        return \"<%s.%s>\" % (self.__class__.__name__, self.name)\n+        return f\"<{self.__class__.__name__}.{self.name}>\"\n \n     IS_EXTENSION_MODULE = (\n         \"Module is a C extension module. torch.package supports Python modules only.\"\n@@ -156,14 +156,14 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n                     message.write(f\"      Context: {error_context}\\n\")\n                 if module_name in _DISALLOWED_MODULES:\n                     message.write(\n-                        (\n+\n                             \"      Note: While we usually use modules in the python standard library \"\n                             f\"from the local environment, `{module_name}` has a lot of system \"\n                             \"level access and therefore can pose a security risk. We heavily \"\n                             f\"recommend removing `{module_name}` from your packaged code. However, if that \"\n                             \"is not possible, add it to the extern list by calling \"\n                             f'PackageExporter.extern(\"`{module_name}`\")\\n'\n-                        )\n+\n                     )\n                 if debug:\n                     module_path = dependency_graph.first_path(module_name)\n@@ -173,10 +173,10 @@ def __init__(self, dependency_graph: DiGraph, debug=False):\n         if not debug:\n             message.write(\"\\n\")\n             message.write(\n-                (\n+\n                     \"Set debug=True when invoking PackageExporter for a visualization of where \"\n                     \"broken modules are coming from!\\n\"\n-                )\n+\n             )\n         # Save the dependency graph so that tooling can get at it.\n         self.dependency_graph = dependency_graph\ndiff --git a/torch/package/package_importer.py b/torch/package/package_importer.py\nindex 8369e79e783ad7..2d313c8f14eb45 100644\n--- a/torch/package/package_importer.py\n+++ b/torch/package/package_importer.py\n@@ -539,7 +539,7 @@ def _handle_fromlist(self, module, fromlist, *, recursive=False):\n                     if not recursive and hasattr(module, \"__all__\"):\n                         self._handle_fromlist(module, module.__all__, recursive=True)\n                 elif not hasattr(module, x):\n-                    from_name = \"{}.{}\".format(module_name, x)\n+                    from_name = f\"{module_name}.{x}\"\n                     try:\n                         self._gcd_import(from_name)\n                     except ModuleNotFoundError as exc:\n@@ -587,13 +587,13 @@ def _get_package(self, package):\n         \"\"\"\n         if hasattr(package, \"__spec__\"):\n             if package.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package.__spec__.name))\n+                raise TypeError(f\"{package.__spec__.name!r} is not a package\")\n             else:\n                 return package\n         else:\n             module = self.import_module(package)\n             if module.__spec__.submodule_search_locations is None:\n-                raise TypeError(\"{!r} is not a package\".format(package))\n+                raise TypeError(f\"{package!r} is not a package\")\n             else:\n                 return module\n \ndiff --git a/torch/profiler/_memory_profiler.py b/torch/profiler/_memory_profiler.py\nindex 7ade85a85caa11..fbbcd4d67b7889 100644\n--- a/torch/profiler/_memory_profiler.py\n+++ b/torch/profiler/_memory_profiler.py\n@@ -738,11 +738,11 @@ def _category_snapshot(self) -> Dict[TensorAndID, Optional[Category]]:\n \n         for node in self._data_flow_graph.flow_nodes:\n             all_tensor_versions.update(((k, v) for k, (_, v) in node.inputs.items()))\n-            all_tensor_versions.update(((key, 0) for key in node.intermediates))\n+            all_tensor_versions.update((key, 0) for key in node.intermediates)\n             all_tensor_versions.update(node.outputs.items())\n \n         for i in self._categories._values.values():\n-            all_tensor_versions.update(((key, 0) for key in i._by_id_keyset))\n+            all_tensor_versions.update((key, 0) for key in i._by_id_keyset)\n \n         return {\n             (key, version): self._categories.get(key, version)\ndiff --git a/torch/profiler/_pattern_matcher.py b/torch/profiler/_pattern_matcher.py\nindex ae95faf0d2bae7..1d85d193ecf894 100644\n--- a/torch/profiler/_pattern_matcher.py\n+++ b/torch/profiler/_pattern_matcher.py\n@@ -642,7 +642,7 @@ def report_all_anti_patterns(prof,\n         json_report_path = os.path.join(json_report_dir,\n                                         \"torchtidy_report.json\")\n         if os.path.exists(json_report_path):\n-            with open(json_report_path, \"r\") as f:\n+            with open(json_report_path) as f:\n                 exisiting_report = json.load(f)\n                 exisiting_report.update(report_dict)\n                 report_dict = exisiting_report\ndiff --git a/torch/signal/windows/windows.py b/torch/signal/windows/windows.py\nindex 1ddfff96228927..d1b8e2529bb97e 100644\n--- a/torch/signal/windows/windows.py\n+++ b/torch/signal/windows/windows.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from typing import Optional, Iterable\n \n import torch\ndiff --git a/torch/sparse/semi_structured.py b/torch/sparse/semi_structured.py\nindex 0e4c217a50aafb..d1e4321d66f36b 100644\n--- a/torch/sparse/semi_structured.py\n+++ b/torch/sparse/semi_structured.py\n@@ -136,28 +136,28 @@ def __init__(\n             # check device\n             if not original_tensor.is_cuda:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.device= {original_tensor.device} is not supported! \"\n                         \"Only CUDA tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dim\n             if original_tensor.dim() != 2:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dim = {original_tensor.dim()} is not supported! \"\n                         \"Only 2d tensors are currently supported.\"\n-                    )\n+\n                 )\n \n             # check dtype\n             if original_tensor.dtype not in _DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG:\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.dtype {original_tensor.dtype} is not a supported dtype! \"\n                         \"dtype must be one of: {_DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG}\"\n-                    )\n+\n                 )\n \n             # check shape\n@@ -167,10 +167,10 @@ def __init__(\n             if m < min_rows or m % min_rows or n < min_cols or n % min_cols:\n                 # TODO in the future we can add in padding to support dimensions that aren't perfect multiples\n                 raise RuntimeError(\n-                    (\n+\n                         f\"Error original_tensor.shape {original_tensor.shape} is not supported! \"\n                         \"Both dimensions must be larger or equal than and a multiple of ({min_rows}, {min_cols})\"\n-                    )\n+\n                 )\n \n             # This code calculates the size of the compressed tensor.\n"
  },
  {
    "number": 105385,
    "title": "[BE] Enable ruff's UP rules and autoformat testing/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "dc019b0723de74565684d7440363f14ef2b663f9",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105385",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105385/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105385.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105385.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105385/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105385/comments",
    "labels": [
      "open source",
      "release notes: distributed (rpc)"
    ],
    "_event_time": "2023-07-18T01:11:16.869204Z",
    "state": "closed",
    "patch": "From b6286d07db5f7c7cbba02642b2e5aa7c2b37abdd Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:10 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat testing/\n\n[ghstack-poisoned]\n---\n torch/testing/_comparison.py                  | 10 +--\n .../_internal/check_kernel_launches.py        |  2 +-\n .../_internal/codegen/random_topo_test.py     | 16 ++---\n torch/testing/_internal/common_cuda.py        |  2 +-\n torch/testing/_internal/common_device_type.py | 50 +++++++--------\n torch/testing/_internal/common_distributed.py |  4 +-\n torch/testing/_internal/common_fsdp.py        |  2 +-\n .../_internal/common_methods_invocations.py   | 18 +++---\n torch/testing/_internal/common_modules.py     | 64 +++++++++----------\n torch/testing/_internal/common_nn.py          | 12 ++--\n torch/testing/_internal/common_pruning.py     |  1 -\n .../testing/_internal/common_quantization.py  |  5 +-\n torch/testing/_internal/common_utils.py       | 34 +++++-----\n torch/testing/_internal/dist_utils.py         |  4 +-\n .../_internal/distributed/distributed_test.py | 14 ++--\n .../distributed/nn/api/remote_module_test.py  | 20 +++---\n .../distributed/rpc/dist_autograd_test.py     | 22 +++----\n .../reinforcement_learning_rpc_test.py        |  2 +-\n .../distributed/rpc/faulty_agent_rpc_test.py  |  8 +--\n .../rpc/faulty_rpc_agent_test_fixture.py      |  2 +-\n .../_internal/distributed/rpc/jit/rpc_test.py |  2 +-\n .../distributed/rpc/jit/rpc_test_faulty.py    |  8 +--\n .../_internal/distributed/rpc/rpc_test.py     |  4 +-\n .../rpc/tensorpipe_rpc_agent_test_fixture.py  |  2 +-\n .../_internal/jit_metaprogramming_utils.py    | 18 +++---\n torch/testing/_internal/jit_utils.py          | 12 ++--\n torch/testing/_internal/opinfo/core.py        |  9 +--\n .../_internal/opinfo/definitions/linalg.py    |  2 +-\n .../_internal/opinfo/definitions/sparse.py    |  2 +-\n 29 files changed, 172 insertions(+), 179 deletions(-)\n\ndiff --git a/torch/testing/_comparison.py b/torch/testing/_comparison.py\nindex 1ccc7447ed7486..4204a6c0e69441 100644\n--- a/torch/testing/_comparison.py\n+++ b/torch/testing/_comparison.py\n@@ -465,9 +465,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[bool, bool]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_bool(bool_like, id=id) for bool_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_bool(self, bool_like: Any, *, id: Tuple[Any, ...]) -> bool:\n@@ -559,9 +559,9 @@ def _process_inputs(\n         self, actual: Any, expected: Any, *, id: Tuple[Any, ...]\n     ) -> Tuple[Union[int, float, complex], Union[int, float, complex]]:\n         self._check_inputs_isinstance(actual, expected, cls=self._supported_types)\n-        actual, expected = [\n+        actual, expected = (\n             self._to_number(number_like, id=id) for number_like in (actual, expected)\n-        ]\n+        )\n         return actual, expected\n \n     def _to_number(\n@@ -675,7 +675,7 @@ def _process_inputs(\n         if not allow_subclasses and type(actual) is not type(expected):\n             self._inputs_not_supported()\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\ndiff --git a/torch/testing/_internal/check_kernel_launches.py b/torch/testing/_internal/check_kernel_launches.py\nindex 667e3412ceaf2c..131ea461ce544a 100644\n--- a/torch/testing/_internal/check_kernel_launches.py\n+++ b/torch/testing/_internal/check_kernel_launches.py\n@@ -111,7 +111,7 @@ def check_file(filename):\n         return 0\n     if should_exclude_file(filename):\n         return 0\n-    with open(filename, \"r\") as fo:\n+    with open(filename) as fo:\n         contents = fo.read()\n         unsafeCount = check_code_for_cuda_kernel_launches(contents, filename)\n     return unsafeCount\ndiff --git a/torch/testing/_internal/codegen/random_topo_test.py b/torch/testing/_internal/codegen/random_topo_test.py\nindex fdb13d4ef139c0..b94f8f60a301d3 100644\n--- a/torch/testing/_internal/codegen/random_topo_test.py\n+++ b/torch/testing/_internal/codegen/random_topo_test.py\n@@ -96,7 +96,7 @@ def get_root(x, dependency_map):\n         out_tensor = None\n \n         if DEBUG_PRINT:\n-            print(\"iteration {0}, num_sets{1}, candidates {2}, tensor_list {3}, lh_index {4}, op_index {5}\".format(\n+            print(\"iteration {}, num_sets{}, candidates {}, tensor_list {}, lh_index {}, op_index {}\".format(\n                 num_operations, num_sets, candidate, len(tensor_list), lh_index, op_index))\n         if num_operations >= 0:\n             num_operations -= 1\n@@ -125,7 +125,7 @@ def get_root(x, dependency_map):\n                     #  right = tensor_list[lh_index]\n                     out_tensor = binary_operations[op_index - u_op_size](left, right)\n                 if DEBUG_PRINT:\n-                    print(\"binary, op_2_index {0}, rh_index ?{1}\".format(op_2_index, rh_index))\n+                    print(f\"binary, op_2_index {op_2_index}, rh_index ?{rh_index}\")\n         else:\n             # binary operation, we just randomly pick two candidates.\n             # this is not the most efficient way to close dependency, as we could have\n@@ -136,7 +136,7 @@ def get_root(x, dependency_map):\n             # [if rh_index: create binary operator output tensor]\n             rh_index = candidate[cand_index]\n             if DEBUG_PRINT:\n-                print(\"binary rh_index ?{0}\".format(rh_index))\n+                print(f\"binary rh_index ?{rh_index}\")\n \n         # update candidate should happen before we remove rh_index\n         candidate[index] = len(tensor_list)\n@@ -185,7 +185,7 @@ def get_root(x, dependency_map):\n             ret_list.append(tensor_list[ind])\n \n     if DEBUG_PRINT:\n-        print(\"ended with tensor_list: {0}\".format(len(tensor_list)))\n+        print(f\"ended with tensor_list: {len(tensor_list)}\")\n \n     return tuple(ret_list)\n \n@@ -248,7 +248,7 @@ def prepareInputTensorsToRandomTopoTest(seed,\n \n \n def reproString(current_seed, args):\n-    repro_str = \"python {0}\".format(__file__)\n+    repro_str = f\"python {__file__}\"\n     if args.cuda_fuser:\n         repro_str += \" --cuda-fuser\"\n     if args.legacy_fuser:\n@@ -259,8 +259,8 @@ def reproString(current_seed, args):\n         repro_str += \" --fp16\"\n     if args.cpu:\n         repro_str += \" --cpu\"\n-    repro_str += \" --max-num-tensor {0} --max-tensor-dim {1} --max-tensor-size {2}\"\\\n-        \" --depth-factor {3} --seed {4} --repro-run\".format(\n+    repro_str += \" --max-num-tensor {} --max-tensor-dim {} --max-tensor-size {}\"\\\n+        \" --depth-factor {} --seed {} --repro-run\".format(\n             args.max_num_tensor, args.max_tensor_dim, args.max_tensor_size,\n             args.depth_factor, current_seed)\n     return repro_str\n@@ -390,7 +390,7 @@ def parse_args():\n         if len(failing_repros) == 0:\n             print(\"test passed\")\n         else:\n-            print(\"{0} out of {1} tests failed;\".format(\n+            print(\"{} out of {} tests failed;\".format(\n                   len(failing_repros), args.iterations))\n             print(\"To repro failing tests, run\\n\")\n             for repro in failing_repros:\ndiff --git a/torch/testing/_internal/common_cuda.py b/torch/testing/_internal/common_cuda.py\nindex c380dd5e6d7250..f427f7b62b9f77 100644\n--- a/torch/testing/_internal/common_cuda.py\n+++ b/torch/testing/_internal/common_cuda.py\n@@ -48,7 +48,7 @@ def initialize_cuda_context_rng():\n     if not __cuda_ctx_rng_initialized:\n         # initialize cuda context and rng for memory tests\n         for i in range(torch.cuda.device_count()):\n-            torch.randn(1, device=\"cuda:{}\".format(i))\n+            torch.randn(1, device=f\"cuda:{i}\")\n         __cuda_ctx_rng_initialized = True\n \n \ndiff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py\nindex d9c362c332479d..891c878cc5f059 100644\n--- a/torch/testing/_internal/common_device_type.py\n+++ b/torch/testing/_internal/common_device_type.py\n@@ -276,9 +276,9 @@ def _dtype_test_suffix(dtypes):\n     if isinstance(dtypes, (list, tuple)):\n         if len(dtypes) == 0:\n             return ''\n-        return '_' + '_'.join((dtype_name(d) for d in dtypes))\n+        return '_' + '_'.join(dtype_name(d) for d in dtypes)\n     elif dtypes:\n-        return '_{}'.format(dtype_name(dtypes))\n+        return f'_{dtype_name(dtypes)}'\n     else:\n         return ''\n \n@@ -286,7 +286,7 @@ def _dtype_test_suffix(dtypes):\n def _update_param_kwargs(param_kwargs, name, value):\n     \"\"\" Adds a kwarg with the specified name and value to the param_kwargs dict. \"\"\"\n     # Make name plural (e.g. devices / dtypes) if the value is composite.\n-    plural_name = '{}s'.format(name)\n+    plural_name = f'{name}s'\n \n     # Clear out old entries of the arg if any.\n     if name in param_kwargs:\n@@ -432,7 +432,7 @@ def instantiated_test(self, param_kwargs=param_kwargs):\n \n                 return result\n \n-            assert not hasattr(cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(cls, name), f\"Redefinition of test {name}\"\n             setattr(cls, name, instantiated_test)\n \n         def default_parametrize_fn(test, generic_cls, device_cls):\n@@ -467,7 +467,7 @@ def dtype_parametrize_fn(test, generic_cls, device_cls, dtypes=dtypes):\n             dtype_kwarg = None\n             if 'dtype' in param_kwargs or 'dtypes' in param_kwargs:\n                 dtype_kwarg = param_kwargs['dtypes'] if 'dtypes' in param_kwargs else param_kwargs['dtype']\n-            test_name = '{}{}{}{}'.format(name, test_suffix, device_suffix, _dtype_test_suffix(dtype_kwarg))\n+            test_name = f'{name}{test_suffix}{device_suffix}{_dtype_test_suffix(dtype_kwarg)}'\n \n             instantiate_test_helper(cls=cls, name=test_name, test=test, param_kwargs=param_kwargs,\n                                     decorator_fn=decorator_fn)\n@@ -523,7 +523,7 @@ def setUpClass(cls):\n         cls.cudnn_version = None if cls.no_cudnn else torch.backends.cudnn.version()\n \n         # Acquires the current device as the primary (test) device\n-        cls.primary_device = 'cuda:{0}'.format(torch.cuda.current_device())\n+        cls.primary_device = f'cuda:{torch.cuda.current_device()}'\n \n # See Note [Lazy Tensor tests in device agnostic testing]\n lazy_ts_backend_init = False\n@@ -589,7 +589,7 @@ def setUpClass(cls):\n         cls.device_mod = getattr(torch, cls.device_type, None)\n         assert cls.device_mod is not None, f'''torch has no module of `{cls.device_type}`, you should register\n                                             a module by `torch._register_device_module`.'''\n-        cls.primary_device = '{device_type}:{id}'.format(device_type=cls.device_type, id=cls.device_mod.current_device())\n+        cls.primary_device = f'{cls.device_type}:{cls.device_mod.current_device()}'\n \n # Adds available device-type-specific test base classes\n def get_device_type_test_bases():\n@@ -744,7 +744,7 @@ def split_if_not_empty(x: str):\n                 else:\n                     device_type_test_class.instantiate_test(name, copy.deepcopy(test))\n             else:  # Ports non-test member\n-                assert name not in device_type_test_class.__dict__, \"Redefinition of directly defined member {0}\".format(name)\n+                assert name not in device_type_test_class.__dict__, f\"Redefinition of directly defined member {name}\"\n                 nontest = getattr(generic_test_class, name)\n                 setattr(device_type_test_class, name, nontest)\n \n@@ -913,7 +913,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for op {1}!\".format(test_name, op.name))\n+                    print(f\"Failed to instantiate {test_name} for op {op.name}!\")\n                     raise ex\n         if op is check_exhausted_iterator:\n             raise ValueError('An empty op_list was passed to @ops. '\n@@ -1034,7 +1034,7 @@ def dep_fn(self, *args, **kwargs):\n             size_bytes = size(self, *args, **kwargs) if callable(size) else size\n             _device = device if device is not None else self.get_primary_device()\n             if not _has_sufficient_memory(_device, size_bytes):\n-                raise unittest.SkipTest('Insufficient {} memory'.format(_device))\n+                raise unittest.SkipTest(f'Insufficient {_device} memory')\n \n             return fn(self, *args, **kwargs)\n         return dep_fn\n@@ -1072,7 +1072,7 @@ def __call__(self, fn):\n         @wraps(fn)\n         def only_fn(slf, *args, **kwargs):\n             if self.device_type != slf.device_type:\n-                reason = \"Only runs on {0}\".format(self.device_type)\n+                reason = f\"Only runs on {self.device_type}\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, *args, **kwargs)\n@@ -1090,13 +1090,13 @@ def __init__(self, num_required_devices):\n         self.num_required_devices = num_required_devices\n \n     def __call__(self, fn):\n-        assert not hasattr(fn, 'num_required_devices'), \"deviceCountAtLeast redefinition for {0}\".format(fn.__name__)\n+        assert not hasattr(fn, 'num_required_devices'), f\"deviceCountAtLeast redefinition for {fn.__name__}\"\n         fn.num_required_devices = self.num_required_devices\n \n         @wraps(fn)\n         def multi_fn(slf, devices, *args, **kwargs):\n             if len(devices) < self.num_required_devices:\n-                reason = \"fewer than {0} devices detected\".format(self.num_required_devices)\n+                reason = f\"fewer than {self.num_required_devices} devices detected\"\n                 raise unittest.SkipTest(reason)\n \n             return fn(slf, devices, *args, **kwargs)\n@@ -1108,7 +1108,7 @@ def onlyNativeDeviceTypes(fn):\n     @wraps(fn)\n     def only_fn(self, *args, **kwargs):\n         if self.device_type not in NATIVE_DEVICES:\n-            reason = \"onlyNativeDeviceTypes: doesn't run on {0}\".format(self.device_type)\n+            reason = f\"onlyNativeDeviceTypes: doesn't run on {self.device_type}\"\n             raise unittest.SkipTest(reason)\n \n         return fn(self, *args, **kwargs)\n@@ -1137,7 +1137,7 @@ class precisionOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"precisionOverride not given a dtype : precision dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"precisionOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"precisionOverride given unknown dtype {dtype}\"\n \n         self.d = d\n \n@@ -1168,7 +1168,7 @@ class toleranceOverride:\n     def __init__(self, d):\n         assert isinstance(d, dict), \"toleranceOverride not given a dtype : tol dict!\"\n         for dtype, prec in d.items():\n-            assert isinstance(dtype, torch.dtype), \"toleranceOverride given unknown dtype {0}\".format(dtype)\n+            assert isinstance(dtype, torch.dtype), f\"toleranceOverride given unknown dtype {dtype}\"\n             assert isinstance(prec, tol), \"toleranceOverride not given a dtype : tol dict!\"\n \n         self.d = d\n@@ -1195,17 +1195,17 @@ def __init__(self, *args, device_type=\"all\"):\n                 assert isinstance(arg, (list, tuple)), \\\n                     \"When one dtype variant is a tuple or list, \" \\\n                     \"all dtype variants must be. \" \\\n-                    \"Received non-list non-tuple dtype {0}\".format(str(arg))\n-                assert all(isinstance(dtype, torch.dtype) for dtype in arg), \"Unknown dtype in {0}\".format(str(arg))\n+                    \"Received non-list non-tuple dtype {}\".format(str(arg))\n+                assert all(isinstance(dtype, torch.dtype) for dtype in arg), f\"Unknown dtype in {str(arg)}\"\n         else:\n-            assert all(isinstance(arg, torch.dtype) for arg in args), \"Unknown dtype in {0}\".format(str(args))\n+            assert all(isinstance(arg, torch.dtype) for arg in args), f\"Unknown dtype in {str(args)}\"\n \n         self.args = args\n         self.device_type = device_type\n \n     def __call__(self, fn):\n         d = getattr(fn, 'dtypes', {})\n-        assert self.device_type not in d, \"dtypes redefinition for {0}\".format(self.device_type)\n+        assert self.device_type not in d, f\"dtypes redefinition for {self.device_type}\"\n         d[self.device_type] = self.args\n         fn.dtypes = d\n         return fn\n@@ -1244,7 +1244,7 @@ def onlyPRIVATEUSE1(fn):\n     device_type = torch._C._get_privateuse1_backend_name()\n     device_mod = getattr(torch, device_type, None)\n     if device_mod is None:\n-        reason = \"Skip as torch has no module of {0}\".format(device_type)\n+        reason = f\"Skip as torch has no module of {device_type}\"\n         return unittest.skip(reason)(fn)\n     return onlyOn(device_type)(fn)\n \n@@ -1358,7 +1358,7 @@ def wrap_fn(self, *args, **kwargs):\n                     raise unittest.SkipTest(reason)\n                 rocm_version_tuple = _get_torch_rocm_version()\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\n@@ -1375,7 +1375,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version in (versions or []):\n-                reason = \"test skipped for CUDA version {0}\".format(version)\n+                reason = f\"test skipped for CUDA version {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1391,7 +1391,7 @@ def wrap_fn(self, *args, **kwargs):\n             if version == (0, 0):  # cpu or rocm\n                 return fn(self, *args, **kwargs)\n             if version < versions:\n-                reason = \"test skipped for CUDA versions < {0}\".format(version)\n+                reason = f\"test skipped for CUDA versions < {version}\"\n                 raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n \n@@ -1409,7 +1409,7 @@ def wrap_fn(self, *args, **kwargs):\n                     reason = \"cuDNN not available\"\n                     raise unittest.SkipTest(reason)\n                 if self.cudnn_version is None or self.cudnn_version < version:\n-                    reason = \"cuDNN version {0} is available but {1} required\".format(self.cudnn_version, version)\n+                    reason = f\"cuDNN version {self.cudnn_version} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n \n             return fn(self, *args, **kwargs)\ndiff --git a/torch/testing/_internal/common_distributed.py b/torch/testing/_internal/common_distributed.py\nindex 8981aa78d06a09..d1cf02749b79ce 100644\n--- a/torch/testing/_internal/common_distributed.py\n+++ b/torch/testing/_internal/common_distributed.py\n@@ -770,7 +770,7 @@ def _check_no_test_errors(self, elapsed_time) -> None:\n         for i, p in enumerate(self.processes):\n             if p.exitcode is None:\n                 raise RuntimeError(\n-                    \"Process {} timed out after {} seconds\".format(i, elapsed_time)\n+                    f\"Process {i} timed out after {elapsed_time} seconds\"\n                 )\n             self.assertNotEqual(self.TEST_ERROR_EXIT_CODE, p.exitcode)\n \n@@ -1102,7 +1102,7 @@ def _check_return_codes(cls, failed_ranks, timeout, fn):\n                     \"Caught exception: \\n%s exiting thread %s\", msg, rank\n                 )\n                 error_msg += (\n-                    \"Thread {} exited with exception:\\n{}\\n\".format(rank, msg)\n+                    f\"Thread {rank} exited with exception:\\n{msg}\\n\"\n                 )\n             elif isinstance(exc, SystemExit):\n                 if type(exc.code) == int and skip_code < 0:\ndiff --git a/torch/testing/_internal/common_fsdp.py b/torch/testing/_internal/common_fsdp.py\nindex 589372eaa8e216..20a35930a7f5f4 100644\n--- a/torch/testing/_internal/common_fsdp.py\n+++ b/torch/testing/_internal/common_fsdp.py\n@@ -881,7 +881,7 @@ def process_group(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     def _check_cpu_offload(self, fsdp_model, cpu_offload):\n         self.assertEqual(cpu_offload, fsdp_model.cpu_offload)\ndiff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py\nindex 3b9d3269853a05..a9f58c76d90f2d 100644\n--- a/torch/testing/_internal/common_methods_invocations.py\n+++ b/torch/testing/_internal/common_methods_invocations.py\n@@ -851,7 +851,7 @@ def error_inputs_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"normal expects std >= 0.0, but found std {}\".format(invalid_std),\n+        error_regex=fr\"normal expects std >= 0.0, but found std {invalid_std}\",\n     )\n \n def sample_inputs_cauchy(op, device, dtype, requires_grad, **kwargs):\n@@ -871,7 +871,7 @@ def error_inputs_cauchy(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_scale,)),\n         error_type=RuntimeError,\n-        error_regex=r\"cauchy_ expects sigma > 0.0, but found sigma={}\".format(invalid_scale),\n+        error_regex=fr\"cauchy_ expects sigma > 0.0, but found sigma={invalid_scale}\",\n     )\n \n \n@@ -893,7 +893,7 @@ def error_inputs_exponential(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(invalid_rate,)),\n         error_type=RuntimeError,\n-        error_regex=r\"exponential_ expects lambda > 0.0, but found lambda={}\".format(invalid_rate),\n+        error_regex=fr\"exponential_ expects lambda > 0.0, but found lambda={invalid_rate}\",\n     )\n \n \n@@ -915,7 +915,7 @@ def error_inputs_geometric(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(neg_prob,)),\n         error_type=RuntimeError,\n-        error_regex=r\"geometric_ expects p to be in \\(0, 1\\), but got p={}\".format(neg_prob),\n+        error_regex=fr\"geometric_ expects p to be in \\(0, 1\\), but got p={neg_prob}\",\n     )\n \n \n@@ -937,7 +937,7 @@ def error_inputs_log_normal(op, device, **kwargs):\n     yield ErrorInput(\n         SampleInput(t, args=(0, invalid_std)),\n         error_type=RuntimeError,\n-        error_regex=r\"log_normal_ expects std > 0.0, but found std={}\".format(invalid_std),\n+        error_regex=fr\"log_normal_ expects std > 0.0, but found std={invalid_std}\",\n     )\n \n \n@@ -1889,9 +1889,9 @@ def sample_inputs_logcumsumexp(self, device, dtype, requires_grad, **kwargs):\n             yield SampleInput(t, dim)\n \n def sample_inputs_trace(self, device, dtype, requires_grad, **kwargs):\n-    yield SampleInput((make_tensor((S, S), dtype=dtype, device=device,\n+    yield SampleInput(make_tensor((S, S), dtype=dtype, device=device,\n                                    low=None, high=None,\n-                                   requires_grad=requires_grad)))\n+                                   requires_grad=requires_grad))\n \n \n def error_inputs_trace(op, device):\n@@ -4020,7 +4020,7 @@ def error_inputs_group_norm(opinfo, device, **kwargs):\n \n     # check that input has minimum number of dimensions\n     err_msg1 = \"Expected at least 2 dimensions for input tensor but received\"\n-    s1 = SampleInput(make_arg((1)), args=(1,))\n+    s1 = SampleInput(make_arg(1), args=(1,))\n     yield ErrorInput(s1, error_regex=err_msg1)\n \n     # check that the channels dimension is compatible with number of groups\n@@ -6950,7 +6950,7 @@ def make_bool_mask(shape):\n \n         if mask_t.sum() == 0:\n             def random_index(shape):\n-                return tuple((random.randrange(0, max_idx) for max_idx in shape))\n+                return tuple(random.randrange(0, max_idx) for max_idx in shape)\n \n             mask_t[random_index(mask_t.shape)] = True\n             return mask_t\ndiff --git a/torch/testing/_internal/common_modules.py b/torch/testing/_internal/common_modules.py\nindex 2119678a33f5ef..c3ac11454ab410 100644\n--- a/torch/testing/_internal/common_modules.py\n+++ b/torch/testing/_internal/common_modules.py\n@@ -123,7 +123,7 @@ def test_wrapper(*args, **kwargs):\n                     yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                 except Exception as ex:\n                     # Provides an error message for debugging before rethrowing the exception\n-                    print(\"Failed to instantiate {0} for module {1}!\".format(test_name, module_info.name))\n+                    print(f\"Failed to instantiate {test_name} for module {module_info.name}!\")\n                     raise ex\n \n \n@@ -252,7 +252,7 @@ def bilinear_reference_fn(m, p, x1, x2, bias=True):\n                     desc='no_bias',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n         ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n-                    forward_input=FunctionInput(make_input((2)), make_input((3))),\n+                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                     desc='no_batch_dim',\n                     reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n     ]\n@@ -312,9 +312,9 @@ def module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_\n     for desc, constructor_kwargs in cases:\n         module_inputs.append(\n             ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n-                        forward_input=FunctionInput(make_input((3)),\n-                                                    make_target((3)),\n-                                                    make_input((1)).abs()),\n+                        forward_input=FunctionInput(make_input(3),\n+                                                    make_target(3),\n+                                                    make_input(1).abs()),\n                         desc=desc,\n                         reference_fn=no_batch_dim_reference_fn)\n         )\n@@ -454,7 +454,7 @@ def generate_regression_criterion_inputs(make_input):\n             constructor_input=FunctionInput(reduction=reduction),\n             forward_input=FunctionInput(make_input((4, )), make_input(4,)),\n             reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True),\n-            desc='no_batch_dim_{}'.format(reduction)\n+            desc=f'no_batch_dim_{reduction}'\n         ) for reduction in ['none', 'mean', 'sum']]\n \n \n@@ -752,7 +752,7 @@ def module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, tra\n     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n     conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n     kernel_size, C_in, C_out = 3, 4, 5\n-    input_no_batch_shape = (C_in,) + tuple((i + 3 for i in range(N)))\n+    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n     input_batch_shape = (2,) + input_no_batch_shape\n     return [\n         ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n@@ -878,7 +878,7 @@ def module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad,\n         ModuleInput(constructor_input=FunctionInput(),\n                     forward_input=FunctionInput(make_input((3, 2, 5)))),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(0.5),\n@@ -897,10 +897,10 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n \n     return [\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(()))),\n+                    forward_input=FunctionInput(make_input(())),\n                     desc='scalar'),\n         ModuleInput(constructor_input=FunctionInput(),\n-                    forward_input=FunctionInput((make_input(4))),\n+                    forward_input=FunctionInput(make_input(4)),\n                     reference_fn=no_batch_dim_reference_fn,\n                     desc='no_batch_dim'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -908,7 +908,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='1d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -916,7 +916,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='2d_multiparam'),\n         ModuleInput(constructor_input=FunctionInput(),\n@@ -924,7 +924,7 @@ def module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, trai\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d'),\n         ModuleInput(constructor_input=FunctionInput(3),\n-                    forward_input=FunctionInput((make_input((2, 3, 4, 5, 6)))),\n+                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                     reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                     desc='3d_multiparam')]\n \n@@ -1216,11 +1216,11 @@ def module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3, 6, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 6, 5)))),\n+            forward_input=FunctionInput(make_input((4, 6, 5))),\n             desc='1d_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput(3, 12, 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 12)))),\n+            forward_input=FunctionInput(make_input((4, 12))),\n             desc='1d_affine_GN'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 6, 1e-3),\n@@ -1334,13 +1334,13 @@ def module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_g\n             constructor_input=(\n                 FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape)))),\n+            forward_input=FunctionInput(make_input(input_batch_shape))),\n         ModuleInput(\n             constructor_input=(\n                 FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                 FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n             ),\n-            forward_input=FunctionInput(make_input((input_batch_shape))),\n+            forward_input=FunctionInput(make_input(input_batch_shape)),\n             desc='tracking_stats'),\n         ModuleInput(\n             constructor_input=(\n@@ -1365,15 +1365,15 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3),\n-            forward_input=FunctionInput(make_input(((128, 5, 5)))),\n+            forward_input=FunctionInput(make_input((128, 5, 5))),\n             desc='1d_elementwise_affine_large_batch'),\n         ModuleInput(\n             constructor_input=FunctionInput([5], 1e-3, False),\n-            forward_input=FunctionInput(make_input(((4, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5))),\n             desc='1d_no_elementwise_affine'),\n         ModuleInput(\n             constructor_input=FunctionInput([2, 2, 5], 1e-3),\n@@ -1396,11 +1396,11 @@ def module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, require\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(3,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7))),\n             desc='1d'),\n         ModuleInput(\n             constructor_input=FunctionInput(2,),\n-            forward_input=FunctionInput(make_input(((1, 5, 7, 7)))),\n+            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n             desc='2d_uneven_pad'),\n         ModuleInput(\n             constructor_input=FunctionInput(1, 1., 0.5, 2.),\n@@ -1415,7 +1415,7 @@ def module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, t\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(1.5, 2),\n-            forward_input=FunctionInput(make_input(((1, 3, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 7))),\n             desc='norm'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, 2, 3),\n@@ -1449,7 +1449,7 @@ def module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(4),\n-            forward_input=FunctionInput(make_input(((2, 10, 4)))),\n+            forward_input=FunctionInput(make_input((2, 10, 4))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput(4, 4),\n@@ -1468,7 +1468,7 @@ def module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n-            forward_input=FunctionInput(make_input(((3, 7, 7)))),\n+            forward_input=FunctionInput(make_input((3, 7, 7))),\n             desc='3d_input'),\n         ModuleInput(\n             constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n@@ -1486,7 +1486,7 @@ def module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad,\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2)),\n-            forward_input=FunctionInput(make_input(((2, 3, 5, 5, 5))))),\n+            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n         ModuleInput(\n             constructor_input=FunctionInput(2, (2, 2, 2)),\n             forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n@@ -1511,7 +1511,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n@@ -1521,11 +1521,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((1, 3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((3, 5, 7)))),\n+            forward_input=FunctionInput(make_input((3, 5, 7))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\n@@ -1545,7 +1545,7 @@ def make_random_samples():\n     return [\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio'),\n         ModuleInput(\n             constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n@@ -1559,11 +1559,11 @@ def make_random_samples():\n             constructor_input=FunctionInput(\n                 2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n             ),\n-            forward_input=FunctionInput(make_input(((2, 4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n             desc='ratio_return_indices'),\n         ModuleInput(\n             constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n-            forward_input=FunctionInput(make_input(((4, 5, 5, 5)))),\n+            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n             reference_fn=no_batch_dim_reference_fn,\n             desc='ratio_no_batch_dim'),\n         ModuleInput(\ndiff --git a/torch/testing/_internal/common_nn.py b/torch/testing/_internal/common_nn.py\nindex 6bc41ab20f3fcd..85f9e35ac0a6bb 100644\n--- a/torch/testing/_internal/common_nn.py\n+++ b/torch/testing/_internal/common_nn.py\n@@ -2540,7 +2540,7 @@ def unsqueeze_inp(inp):\n         output_size = (2, 3) + tuple(p + 1 for p in padding)  # simplified from `(4 + 2 * p - 3) // 2 + 1`\n         new_module_tests.append(\n             dict(\n-                module_name='Conv{}d'.format(d),\n+                module_name=f'Conv{d}d',\n                 constructor_args=(2, 3, 3, 2, padding, 1, 1, True, padding_mode),\n                 cpp_constructor_args='''torch::nn::Conv{}dOptions(2, 3, 3)\n                                         .stride(2)\n@@ -2552,7 +2552,7 @@ def unsqueeze_inp(inp):\n                 input_size=input_size,\n                 output_size=output_size,\n                 cudnn=True,\n-                desc='{}_stride2_pad2'.format(padding_mode),\n+                desc=f'{padding_mode}_stride2_pad2',\n                 with_tf32=True,\n                 tf32_precision=0.05\n             ),\n@@ -3906,7 +3906,7 @@ def flatten(xs):\n reductions = ['none', 'mean', 'sum']\n for name, reduction in product(regression_criterion_no_batch, reductions):\n     regression_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_size=(3, ),\n         target_size=(3, ),\n@@ -3959,7 +3959,7 @@ def flatten(xs):\n for (name, input_fn, target_fn), reduction in product(classification_criterion_no_batch,\n                                                       reductions):\n     classification_test_info = dict(\n-        fullname=\"{}_no_batch_dim_{}\".format(name, reduction),\n+        fullname=f\"{name}_no_batch_dim_{reduction}\",\n         constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction),\n         input_fn=lambda f=input_fn: f(),\n         target_fn=lambda f=target_fn: f(),\n@@ -4152,7 +4152,7 @@ def _get_arg(self, name, unpack):\n                 self._arg_cache[name] = self._extra_kwargs[fn_name]()\n             else:\n                 assert size_name in self._extra_kwargs, \\\n-                    \"Missing `{}`, `{}` or `{}` for {}\".format(name, size_name, fn_name, self.get_name())\n+                    f\"Missing `{name}`, `{size_name}` or `{fn_name}` for {self.get_name()}\"\n \n                 def map_tensor_sizes(sizes):\n                     if isinstance(sizes, list):\n@@ -4281,7 +4281,7 @@ def test_cuda(self, test_case):\n         type_map = {torch.double: torch.float}\n         cpu_input_tuple = cpu_input if isinstance(cpu_input, tuple) else (cpu_input,)\n \n-        is_any_input_complex = any((isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple))\n+        is_any_input_complex = any(isinstance(t, torch.Tensor) and t.dtype.is_complex for t in cpu_input_tuple)\n \n         gpu_input_tuple = to_gpu(cpu_input_tuple, type_map=type_map)\n \ndiff --git a/torch/testing/_internal/common_pruning.py b/torch/testing/_internal/common_pruning.py\nindex 32732818a25b02..b6cbd92105f3f4 100644\n--- a/torch/testing/_internal/common_pruning.py\n+++ b/torch/testing/_internal/common_pruning.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Owner(s): [\"module: unknown\"]\n \n from torch.ao.pruning import BaseSparsifier\ndiff --git a/torch/testing/_internal/common_quantization.py b/torch/testing/_internal/common_quantization.py\nindex 95686be4511264..dcc575c942bc84 100644\n--- a/torch/testing/_internal/common_quantization.py\n+++ b/torch/testing/_internal/common_quantization.py\n@@ -791,8 +791,7 @@ def _get_underlying_op_type(\n                     (exp_type_end_b is act_type_end_b)\n                 self.assertTrue(\n                     types_match,\n-                    'Type mismatch at %s: expected %s, got %s' %\n-                    (k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n+                    'Type mismatch at {}: expected {}, got {}'.format(k, (exp_type_start_a, exp_type_end_a, exp_type_start_b, exp_type_end_b),\n                         (act_type_start_a, act_type_end_a, act_type_start_b, act_type_end_b))\n                 )\n \n@@ -1601,7 +1600,7 @@ def __init__(self):\n         super().__init__()\n         self.quant = torch.ao.quantization.QuantStub()\n         self.fc1 = torch.nn.Linear(5, 8).to(dtype=torch.float)\n-        self.layer_norm = torch.nn.LayerNorm((8))\n+        self.layer_norm = torch.nn.LayerNorm(8)\n         self.group_norm = torch.nn.GroupNorm(2, 8)\n         self.instance_norm1d = torch.nn.InstanceNorm1d(8)\n         self.instance_norm2d = torch.nn.InstanceNorm2d(8)\ndiff --git a/torch/testing/_internal/common_utils.py b/torch/testing/_internal/common_utils.py\nindex 3ba2331e7cd40e..c96d13bd46ac53 100644\n--- a/torch/testing/_internal/common_utils.py\n+++ b/torch/testing/_internal/common_utils.py\n@@ -203,7 +203,7 @@ def repro_env_var_prefix() -> str:\n \n def maybe_load_json(filename):\n     if os.path.isfile(filename):\n-        with open(filename, 'r') as fp:\n+        with open(filename) as fp:\n             return json.load(fp)\n     log.warning(\"Attempted to load json file '%s' but it does not exist.\", filename)\n     return {}\n@@ -355,12 +355,12 @@ def instantiate_test_helper(cls, name, test, param_kwargs):\n             def instantiated_test(self, param_kwargs=param_kwargs):\n                 test(self, **param_kwargs)\n \n-            assert not hasattr(generic_cls, name), \"Redefinition of test {0}\".format(name)\n+            assert not hasattr(generic_cls, name), f\"Redefinition of test {name}\"\n             setattr(generic_cls, name, instantiated_test)\n \n         for (test, test_suffix, param_kwargs, decorator_fn) in class_attr.parametrize_fn(\n                 class_attr, generic_cls=generic_cls, device_cls=None):\n-            full_name = '{}_{}'.format(test.__name__, test_suffix)\n+            full_name = f'{test.__name__}_{test_suffix}'\n \n             # Apply decorators based on full param kwargs.\n             for decorator in decorator_fn(param_kwargs):\n@@ -830,7 +830,7 @@ def run_tests(argv=UNITTEST_ARGS):\n     # import test files.\n     if SLOW_TESTS_FILE:\n         if os.path.exists(SLOW_TESTS_FILE):\n-            with open(SLOW_TESTS_FILE, 'r') as fp:\n+            with open(SLOW_TESTS_FILE) as fp:\n                 global slow_tests_dict\n                 slow_tests_dict = json.load(fp)\n                 # use env vars so pytest-xdist subprocesses can still access them\n@@ -839,7 +839,7 @@ def run_tests(argv=UNITTEST_ARGS):\n             warnings.warn(f'slow test file provided but not found: {SLOW_TESTS_FILE}')\n     if DISABLED_TESTS_FILE:\n         if os.path.exists(DISABLED_TESTS_FILE):\n-            with open(DISABLED_TESTS_FILE, 'r') as fp:\n+            with open(DISABLED_TESTS_FILE) as fp:\n                 global disabled_tests_dict\n                 disabled_tests_dict = json.load(fp)\n                 os.environ['DISABLED_TESTS_FILE'] = DISABLED_TESTS_FILE\n@@ -905,7 +905,7 @@ def run_tests(argv=UNITTEST_ARGS):\n         test_batches = chunk_list(get_test_names(test_cases), RUN_PARALLEL)\n         processes = []\n         for i in range(RUN_PARALLEL):\n-            command = [sys.executable] + argv + ['--log-suffix=-shard-{}'.format(i + 1)] + test_batches[i]\n+            command = [sys.executable] + argv + [f'--log-suffix=-shard-{i + 1}'] + test_batches[i]\n             processes.append(subprocess.Popen(command, universal_newlines=True))\n         failed = False\n         for p in processes:\n@@ -1294,7 +1294,7 @@ def wrap_fn(self, *args, **kwargs):\n                 rocm_version = rocm_version.split(\"-\")[0]    # ignore git sha\n                 rocm_version_tuple = tuple(int(x) for x in rocm_version.split(\".\"))\n                 if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):\n-                    reason = \"ROCm {0} is available but {1} required\".format(rocm_version_tuple, version)\n+                    reason = f\"ROCm {rocm_version_tuple} is available but {version} required\"\n                     raise unittest.SkipTest(reason)\n             return fn(self, *args, **kwargs)\n         return wrap_fn\n@@ -1672,7 +1672,7 @@ def is_iterable_of_tensors(iterable, include_empty=False):\n     return True\n \n \n-class CudaNonDefaultStream():\n+class CudaNonDefaultStream:\n     def __enter__(self):\n         # Before starting CUDA test save currently active streams on all\n         # CUDA devices and set new non default streams to all CUDA devices\n@@ -1698,7 +1698,7 @@ def __exit__(self, exec_type, exec_value, traceback):\n                                      device_type=self.beforeStreams[d].device_type)\n         torch._C._cuda_setDevice(beforeDevice)\n \n-class CudaMemoryLeakCheck():\n+class CudaMemoryLeakCheck:\n     def __init__(self, testcase, name=None):\n         self.name = testcase.id() if name is None else name\n         self.testcase = testcase\n@@ -2104,7 +2104,7 @@ def __init__(self, actual, expected, *, rtol_override=0.0, atol_override=0.0, **\n     def _process_inputs(self, actual, expected, *, id, allow_subclasses):\n         self._check_inputs_isinstance(actual, expected, cls=(torch.Tensor, np.ndarray))\n \n-        actual, expected = [self._to_tensor(input) for input in (actual, expected)]\n+        actual, expected = (self._to_tensor(input) for input in (actual, expected))\n         for tensor in (actual, expected):\n             self._check_supported(tensor, id=id)\n         return actual, expected\n@@ -2201,7 +2201,7 @@ def set_warn_always_context(new_val: bool):\n         torch.set_warn_always(old_val)\n \n \n-class NoTest():\n+class NoTest:\n     # causes pytest to not recognize this class as a test\n     __test__ = False\n \n@@ -3408,12 +3408,12 @@ def remove_prefix(text, prefix):\n         subname_output = \"\"\n         if subname:\n             expected_file += \"-\" + subname\n-            subname_output = \" ({})\".format(subname)\n+            subname_output = f\" ({subname})\"\n         expected_file += \".expect\"\n         expected = None\n \n         def accept_output(update_type):\n-            print(\"Accepting {} for {}{}:\\n\\n{}\".format(update_type, munged_id, subname_output, s))\n+            print(f\"Accepting {update_type} for {munged_id}{subname_output}:\\n\\n{s}\")\n             with open(expected_file, 'w') as f:\n                 # Adjust for producer_version, leave s unmodified\n                 s_tag = re.sub(r'(producer_version): \"[0-9.]*\"',\n@@ -3423,7 +3423,7 @@ def accept_output(update_type):\n         try:\n             with open(expected_file) as f:\n                 expected = f.read()\n-        except IOError as e:\n+        except OSError as e:\n             if e.errno != errno.ENOENT:\n                 raise\n             elif expecttest.ACCEPT:\n@@ -3442,7 +3442,7 @@ def accept_output(update_type):\n         # Adjust for producer_version\n         expected = expected.replace(\n             'producer_version: \"CURRENT_VERSION\"',\n-            'producer_version: \"{}\"'.format(torch.onnx.producer_version)\n+            f'producer_version: \"{torch.onnx.producer_version}\"'\n         )\n         if expecttest.ACCEPT:\n             if expected != s:\n@@ -3600,7 +3600,7 @@ def download_file(url, binary=True):\n             f.write(data)\n         return path\n     except error.URLError as e:\n-        msg = \"could not download test file '{}'\".format(url)\n+        msg = f\"could not download test file '{url}'\"\n         warnings.warn(msg, RuntimeWarning)\n         raise unittest.SkipTest(msg) from e\n \n@@ -4359,7 +4359,7 @@ def wrap_fn(*args, **kwargs):\n     return wrap_fn\n \n \n-@functools.lru_cache()\n+@functools.lru_cache\n def get_cycles_per_ms() -> float:\n     \"\"\"Measure and return approximate number of cycles per millisecond for torch.cuda._sleep\n     \"\"\"\ndiff --git a/torch/testing/_internal/dist_utils.py b/torch/testing/_internal/dist_utils.py\nindex 94aafe31773261..daee88e4580cd5 100644\n--- a/torch/testing/_internal/dist_utils.py\n+++ b/torch/testing/_internal/dist_utils.py\n@@ -101,7 +101,7 @@ def wait_until_node_failure(rank: int, expected_error_regex: str = \".*\") -> str:\n     \"\"\"\n     while True:\n         try:\n-            rpc.rpc_sync(\"worker{}\".format(rank), noop, args=())\n+            rpc.rpc_sync(f\"worker{rank}\", noop, args=())\n             time.sleep(0.1)\n         except Exception as e:\n             if re.search(pattern=expected_error_regex, string=str(e)):\n@@ -187,7 +187,7 @@ def initialize_pg(init_method, rank: int, world_size: int) -> None:\n \n \n def worker_name(rank: int) -> str:\n-    return \"worker{}\".format(rank)\n+    return f\"worker{rank}\"\n \n \n def get_function_event(function_events, partial_event_name):\ndiff --git a/torch/testing/_internal/distributed/distributed_test.py b/torch/testing/_internal/distributed/distributed_test.py\nindex 19a0c3f50d1ae4..5fdc796310d44d 100644\n--- a/torch/testing/_internal/distributed/distributed_test.py\n+++ b/torch/testing/_internal/distributed/distributed_test.py\n@@ -517,7 +517,7 @@ def sync(cls, wait_for=None, timeout=10):\n             arrived = 0\n             with _lock():\n                 for f_name in os.listdir(barrier_dir):\n-                    with open(os.path.join(barrier_dir, f_name), \"r\") as f:\n+                    with open(os.path.join(barrier_dir, f_name)) as f:\n                         data = f.read()\n                         if int(data) >= cls.barrier_id:\n                             arrived += 1\n@@ -552,7 +552,7 @@ def tearDown(self):\n \n     @property\n     def init_method(self):\n-        return \"{}{file_name}\".format(FILE_SCHEMA, file_name=self.file_name)\n+        return f\"{FILE_SCHEMA}{self.file_name}\"\n \n     @classmethod\n     def _run(cls, rank, test_name, file_name, pipe):\n@@ -654,7 +654,7 @@ def test_dump_DDP_relevant_env_vars(self):\n                 lines = out.getvalue().splitlines()\n \n             def format_line(var):\n-                return \"env:%s=%s\" % (\n+                return \"env:{}={}\".format(\n                     var,\n                     os.environ[var] if var in os.environ else \"N/A\",\n                 )\n@@ -692,7 +692,7 @@ def test_get_rank(self):\n \n             all_ranks = set()\n             for f_name in os.listdir(test_dir):\n-                with open(os.path.join(test_dir, f_name), \"r\") as f:\n+                with open(os.path.join(test_dir, f_name)) as f:\n                     all_ranks.add(int(f.read()))\n             self.assertEqual(len(all_ranks), num_processes)\n \n@@ -9640,7 +9640,7 @@ def backward(ctx, grad_output):\n \n             class MyModel(nn.Module):\n                 def __init__(self, device):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.error = True\n                     self.fc1 = nn.Linear(10, 10).cuda(device)\n \n@@ -9683,12 +9683,12 @@ def forward(self, inp):\n         def test_ddp_has_finalized(self):\n \n             @dataclass\n-            class MyClass():\n+            class MyClass:\n                 obj: torch.Tensor\n \n             class MyModel(nn.Module):\n                 def __init__(self, rank):\n-                    super(MyModel, self).__init__()\n+                    super().__init__()\n                     self.rank = rank\n                     self.fc1 = nn.Linear(1024, 1024).cuda(rank)\n                     self.fc2 = nn.Linear(1024, 2 * 1024).cuda(rank)\ndiff --git a/torch/testing/_internal/distributed/nn/api/remote_module_test.py b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\nindex f955c0fc1ace1a..4d9f1d9b53ddc4 100644\n--- a/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n+++ b/torch/testing/_internal/distributed/nn/api/remote_module_test.py\n@@ -131,7 +131,7 @@ def test_bad_module(self):\n         if self.rank != 0:\n             return\n         dst_worker_name = dist_utils.worker_name((self.rank + 1) % self.world_size)\n-        remote_device = \"{}/cpu\".format(dst_worker_name)\n+        remote_device = f\"{dst_worker_name}/cpu\"\n         args = (1,)\n         kwargs = dict(first_kwarg=2)\n \n@@ -575,7 +575,7 @@ def test_valid_device(self):\n         dst_worker_name = dist_utils.worker_name(dst_rank)\n \n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -585,7 +585,7 @@ def test_valid_device(self):\n \n         # Test rank works as well.\n         for remote_module in self._create_remote_module_iter(\n-            \"rank:{}/cuda:0\".format(dst_rank), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"rank:{dst_rank}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             device = rpc.rpc_sync(\n                 dst_worker_name, remote_device, (remote_module.module_rref,)\n@@ -607,7 +607,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/foo\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/foo\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -618,7 +618,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:100\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:100\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -627,7 +627,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cpu2\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cpu2\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -636,7 +636,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -648,7 +648,7 @@ def test_invalid_devices(self):\n             [\n                 m.forward()\n                 for m in self._create_remote_module_iter(\n-                    \"{}/cuda:0/cuda:1\".format(dst_worker_name),\n+                    f\"{dst_worker_name}/cuda:0/cuda:1\",\n                     modes=[ModuleCreationMode.MODULE_CTOR],\n                 )\n             ]\n@@ -692,7 +692,7 @@ def test_input_moved_to_cuda_device(self):\n \n         # Only test Python nn.Module, because script module methods don't support taking kwargs.\n         for remote_module in self._create_remote_module_iter(\n-            \"{}/cuda:0\".format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR]\n+            f\"{dst_worker_name}/cuda:0\", modes=[ModuleCreationMode.MODULE_CTOR]\n         ):\n             ret_fut = remote_module.forward_async(*args, **kwargs)\n             ret = ret_fut.wait()\n@@ -716,7 +716,7 @@ def test_input_moved_to_cuda_device_script(self):\n \n         scripted_remote_module = next(\n             self._create_remote_module_iter(\n-                \"{}/cuda:0\".format(dst_worker_name),\n+                f\"{dst_worker_name}/cuda:0\",\n                 modes=[ModuleCreationMode.MODULE_CTOR_WITH_INTERFACE],\n             )\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\nindex 8e8c353460e6ff..b08b51c31d9f7d 100644\n--- a/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n+++ b/torch/testing/_internal/distributed/rpc/dist_autograd_test.py\n@@ -226,7 +226,7 @@ def _exec_func_with_dst(self, dst, exec_mode, method, *args):\n             fut = rpc.rpc_async(worker_name(dst), method, args=(args))\n             return fut.wait()\n         else:\n-            raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+            raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n     def _exec_func(self, exec_mode, method, *args):\n         return self._exec_func_with_dst(\n@@ -288,7 +288,7 @@ def _test_graph(self, fn, exec_mode, sparse):\n                     worker_name(dst_rank), fn, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -355,7 +355,7 @@ def _test_graph_for_py_nested_call(self, exec_mode, sparse):\n                     args=(t1, t2, dst_rank, self.world_size, 1),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             # Barrier to ensure all RPCs are done.\n             dist.barrier()\n@@ -449,7 +449,7 @@ def _test_graph_for_py_nested_call_itself(self, exec_mode, sparse):\n                     ),\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name((self.rank + 1) % self.world_size),\n@@ -505,7 +505,7 @@ def _test_no_graph_with_tensors_not_require_grad(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.add, args=(t1, t2)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             rpc.rpc_sync(\n                 worker_name(dst_rank), _set_rpc_done, args=(context_id, 1)\n@@ -548,7 +548,7 @@ def _test_rpc_complex_args(self, exec_mode, sparse):\n                     worker_name(dst_rank), torch.stack, args=(tensors,)\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             self.assertEqual(torch.stack(tensors), ret)\n \n@@ -1292,7 +1292,7 @@ def test_autograd_context(self):\n         for context_id in context_ids:\n             with self.assertRaisesRegex(\n                 RuntimeError,\n-                \"Could not find autograd context with id: {}\".format(context_id),\n+                f\"Could not find autograd context with id: {context_id}\",\n             ):\n                 dist_autograd._retrieve_context(context_id)\n \n@@ -1357,7 +1357,7 @@ def _test_grad_only_on_return_value(self, exec_mode):\n                     worker_name(dst_rank), ret_requires_grad\n                 ).to_here()\n             else:\n-                raise ValueError(\"Unrecognized ExecMode {}\".format(exec_mode))\n+                raise ValueError(f\"Unrecognized ExecMode {exec_mode}\")\n \n             dist_autograd.backward(context_id, [ret.sum()])\n \n@@ -1748,7 +1748,7 @@ def test_backward_without_context(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             res = rpc.rpc_sync(\n                 worker_name(self._next_rank()), torch.add, args=(t1, t2)\n@@ -2031,7 +2031,7 @@ def test_clean_context_during_backward(self):\n         context_id = 100  # dummy context_id\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"Could not find autograd context with id: {}\".format(context_id),\n+            f\"Could not find autograd context with id: {context_id}\",\n         ):\n             dist_autograd.backward(context_id, [t1.sum()])\n \n@@ -2234,7 +2234,7 @@ def test_multiple_backward_with_errors(self):\n         t2 = torch.rand((3, 3), requires_grad=True)\n         with dist_autograd.context() as context_id:\n             loss = rpc.rpc_sync(\n-                'worker{}'.format(self._next_rank()),\n+                f'worker{self._next_rank()}',\n                 DistAutogradTest._python_udf_with_backward_error,\n                 args=(t1, t2)).sum()\n \ndiff --git a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\nindex 9f8b71911a07cc..98db73d7401845 100644\n--- a/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py\n@@ -225,7 +225,7 @@ def run_agent(agent, n_steps):\n         last_reward = agent.finish_episode()\n \n         if agent.running_reward > agent.reward_threshold:\n-            print(\"Solved! Running reward is now {}!\".format(agent.running_reward))\n+            print(f\"Solved! Running reward is now {agent.running_reward}!\")\n             break\n \n \ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\nindex d050a2138b7922..b7683064dcfd13 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py\n@@ -70,7 +70,7 @@ def _test_remote_message_dropped_pickle(self, dst=None):\n         if self.rank != 0:\n             return\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -100,7 +100,7 @@ def _test_remote_message_dropped_timeout(self, func, args, dst=None):\n \n         # test the case where rpc.remote() message creation is completely dropped.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # Since we fail python_remote_call messages synchronously, the future\n         # corresponding to this remote call will be marked with an error when\n         # this function returns.\n@@ -143,7 +143,7 @@ def _test_remote_message_delay_timeout(self, func, args, dst=None):\n         # Test the case where remote message is eventually processed on the owner,\n         # but the future on the creator times out before the response comes back.\n         dst_rank = dst if dst is not None else (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, func, args=args, timeout=0.001)\n         # Future corresponding to the remote creation should time out.\n@@ -233,7 +233,7 @@ def test_rref_to_here_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\nindex b08e897ec464af..af73fef4794b06 100644\n--- a/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py\n@@ -54,7 +54,7 @@ def get_shutdown_error_regex(self):\n             \"Connection reset by peer\",\n             \"Connection closed by peer\"\n         ]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\nindex fce0b5e8802567..0bb45ddeadb186 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test.py\n@@ -310,7 +310,7 @@ def future_return_to_python(\n             dst_rank: int, inputs: Tuple[Tensor, Tensor]\n         ) -> Future[Tensor]:\n             return rpc.rpc_async(\n-                \"worker{}\".format(dst_rank), two_args_two_kwargs, inputs\n+                f\"worker{dst_rank}\", two_args_two_kwargs, inputs\n             )\n \n         fut_res = future_return_to_python(dst_rank, inputs)\ndiff --git a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\nindex 96ede7231a9722..2e4eea3a365176 100644\n--- a/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n+++ b/torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py\n@@ -157,7 +157,7 @@ def test_remote_timeout_to_here_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -173,7 +173,7 @@ def test_rref_to_here_timeout_in_jit(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -188,7 +188,7 @@ def test_rref_timeout_pickle_in_jit(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\n@@ -205,7 +205,7 @@ def test_rref_timeout_pickle_script_func(self):\n         if self.rank != 0:\n             return\n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         rref = rpc.remote(\n             dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1))\n         )\ndiff --git a/torch/testing/_internal/distributed/rpc/rpc_test.py b/torch/testing/_internal/distributed/rpc/rpc_test.py\nindex 2d350d06cc6794..47b13a837a0355 100644\n--- a/torch/testing/_internal/distributed/rpc/rpc_test.py\n+++ b/torch/testing/_internal/distributed/rpc/rpc_test.py\n@@ -3153,7 +3153,7 @@ def test_rref_str(self):\n         rref1 = RRef(self.rank)\n         id_class = \"GloballyUniqueId\"\n         self.assertEqual(\n-            \"OwnerRRef({}(created_on={}, local_id=0))\".format(id_class, self.rank), rref1.__str__()\n+            f\"OwnerRRef({id_class}(created_on={self.rank}, local_id=0))\", rref1.__str__()\n         )\n \n         dst_rank = (self.rank + 1) % self.world_size\n@@ -4296,7 +4296,7 @@ def test_rref_timeout(self):\n             return\n \n         dst_rank = (self.rank + 1) % self.world_size\n-        dst_worker = \"worker{}\".format(dst_rank)\n+        dst_worker = f\"worker{dst_rank}\"\n         # 10 ms timeout\n         rref = rpc.remote(dst_worker, my_sleep_func, args=(2, ), timeout=0.01)\n         # Future corresponding to the remote creation should time out.\ndiff --git a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\nindex 0f5cb0a4987a8f..191017caad139e 100644\n--- a/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n+++ b/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py\n@@ -26,7 +26,7 @@ def get_shutdown_error_regex(self):\n         # FIXME Once we consolidate the error messages returned by the\n         # TensorPipe agent put some more specific regex here.\n         error_regexes = [\".*\"]\n-        return \"|\".join([\"({})\".format(error_str) for error_str in error_regexes])\n+        return \"|\".join([f\"({error_str})\" for error_str in error_regexes])\n \n     def get_timeout_error_regex(self):\n         return \"RPC ran for more than\"\ndiff --git a/torch/testing/_internal/jit_metaprogramming_utils.py b/torch/testing/_internal/jit_metaprogramming_utils.py\nindex 72b7e477c76a49..88137fd1029a18 100644\n--- a/torch/testing/_internal/jit_metaprogramming_utils.py\n+++ b/torch/testing/_internal/jit_metaprogramming_utils.py\n@@ -338,11 +338,11 @@ def get_call(method_name, func_type, args, kwargs):\n     argument_str += kwargs_str\n \n     if func_type == 'functional' or func_type == 'function':\n-        call = 'torch.{}({})'.format(method_name, argument_str)\n+        call = f'torch.{method_name}({argument_str})'\n     elif func_type == 'method':\n-        call = '{}.{}({})'.format(self_arg, method_name, argument_str)\n+        call = f'{self_arg}.{method_name}({argument_str})'\n     elif func_type == 'nn_functional':\n-        call = 'torch.nn.functional.{}({})'.format(method_name, argument_str)\n+        call = f'torch.nn.functional.{method_name}({argument_str})'\n     else:\n         raise TypeError('Unsupported function type')\n \n@@ -361,17 +361,17 @@ def get_script_args(args):\n     actuals: List[str] = []\n     for arg in args:\n         if isinstance(arg, torch.Tensor):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name)\n             actuals.append(name)\n             tensors.append(arg)\n         elif is_iterable_of_tensors(arg):\n-            name = 'i{}'.format(len(formals))\n+            name = f'i{len(formals)}'\n             formals.append(name + ': List[torch.Tensor]')\n             actuals.append(name)\n             tensors.append(list(arg))\n         elif isinstance(arg, str):\n-            actuals.append(\"'{}'\".format(arg))\n+            actuals.append(f\"'{arg}'\")\n         else:\n             actuals.append(str(get_constant(arg)))\n     return (formals, tensors, actuals)\n@@ -399,7 +399,7 @@ def script_fn(*args, **kwargs):\n         return output\n     return script_fn\n \n-class SplitInputs():\n+class SplitInputs:\n     all_tensors: List[Any]\n     tensor_args: List[Any]\n     nontensor_args: List[Any]\n@@ -584,7 +584,7 @@ def script_module(*args, **kwargs):\n \n         method_args = ', '.join(['self'] + actuals)\n         call_args_str = ', '.join(actuals)\n-        call = \"self.submodule({})\".format(call_args_str)\n+        call = f\"self.submodule({call_args_str})\"\n         script = script_method_template.format(method_args, call)\n \n         submodule_constants = []\n@@ -640,7 +640,7 @@ def get_nn_mod_test_name(**kwargs):\n         test_name = get_nn_module_name_from_kwargs(**kwargs)\n         if 'desc' in kwargs:\n             test_name = \"{}_{}\".format(test_name, kwargs['desc'])\n-    return 'test_nn_{}'.format(test_name)\n+    return f'test_nn_{test_name}'\n \n def get_nn_module_class_from_kwargs(**kwargs):\n     name = get_nn_module_name_from_kwargs(**kwargs)\ndiff --git a/torch/testing/_internal/jit_utils.py b/torch/testing/_internal/jit_utils.py\nindex b72dd5dc1285a6..2f6675234d3e7c 100644\n--- a/torch/testing/_internal/jit_utils.py\n+++ b/torch/testing/_internal/jit_utils.py\n@@ -176,12 +176,12 @@ def get_nodes_and_parents_recursively(block, kind, acc):\n \n         fusion_groups : Dict[torch._C.Block, List[torch._C.Node]] = defaultdict(list)\n         get_nodes_and_parents_recursively(graph, FUSION_GROUP, fusion_groups)\n-        self.assertTrue(len(fusion_groups) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_groups) == 1, f'got {graph}')\n         (graph, fusion_nodes) = list(fusion_groups.items())[0]\n         # the block contains one FUSION_GROUP and the rest of nodes are `allowed_nodes`\n-        self.assertTrue(len(fusion_nodes) == 1, 'got {}'.format(graph))\n+        self.assertTrue(len(fusion_nodes) == 1, f'got {graph}')\n         self.assertTrue(all(node.kind() in allowed_nodes for node in graph.nodes()),\n-                        'got {}'.format(graph))\n+                        f'got {graph}')\n \n     def _isHookExceptionOk(self, e):\n         se = str(e)\n@@ -294,7 +294,7 @@ def assertGraphContains(self, graph, kind, consider_subgraphs=False):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             self.assertTrue(count > 0)\n             return\n \n@@ -321,7 +321,7 @@ def perform_assert(graph, kind, actual, expected, consider_subgraphs):\n \n         if consider_subgraphs:\n             strgraph = str(graph)\n-            count = strgraph.count(kind) - strgraph.count('with {}'.format(kind))\n+            count = strgraph.count(kind) - strgraph.count(f'with {kind}')\n             perform_assert(graph, kind, count, num_kind_nodes,\n                            consider_subgraphs)\n             return\n@@ -768,7 +768,7 @@ def _get_py3_code(code, fn_name):\n         fn = getattr(module, fn_name)\n         return fn\n \n-class TensorExprTestOptions():\n+class TensorExprTestOptions:\n     def __init__(self):\n         self.old_profiling_executor = torch._C._jit_set_profiling_executor(True)\n         self.old_profiling_mode = torch._C._get_graph_executor_optimize(True)\ndiff --git a/torch/testing/_internal/opinfo/core.py b/torch/testing/_internal/opinfo/core.py\nindex 8f86cbd06af61a..5e7e1dcd5a49ea 100644\n--- a/torch/testing/_internal/opinfo/core.py\n+++ b/torch/testing/_internal/opinfo/core.py\n@@ -859,9 +859,7 @@ class OpInfo:\n     def __post_init__(self):\n         self._original_opinfo_args = asdict(self).copy()\n \n-        assert self.dtypes is not None, \"OpInfo for {0} has no dtypes!\".format(\n-            self.name\n-        )\n+        assert self.dtypes is not None, \"OpInfo for {} has no dtypes!\".format(self.name)\n \n         dtypes_args = (self.dtypes, self.dtypesIfCUDA, self.dtypesIfROCM)\n \n@@ -874,10 +872,7 @@ def __post_init__(self):\n \n         # Attribute to verify dynamic_dtypes are used.\n         self.dynamic_dtypes = any(\n-            (\n-                isinstance(dtypes, utils._dynamic_dispatch_dtypes)\n-                for dtypes in dtypes_args\n-            )\n+            isinstance(dtypes, utils._dynamic_dispatch_dtypes) for dtypes in dtypes_args\n         )\n \n         if self.dynamic_dtypes:\ndiff --git a/torch/testing/_internal/opinfo/definitions/linalg.py b/torch/testing/_internal/opinfo/definitions/linalg.py\nindex ca84eca5d3d027..a8c29dbf09309d 100644\n--- a/torch/testing/_internal/opinfo/definitions/linalg.py\n+++ b/torch/testing/_internal/opinfo/definitions/linalg.py\n@@ -1007,7 +1007,7 @@ def sample_inputs_linalg_solve(\n         nrhs = [(1,), (3,)]\n \n     for n, batch, rhs in product(ns, batches, nrhs):\n-        yield SampleInput(make_a(*batch, n, n), args=(make_b((batch + (n,) + rhs)),))\n+        yield SampleInput(make_a(*batch, n, n), args=(make_b(batch + (n,) + rhs),))\n \n \n def sample_inputs_linalg_solve_triangular(\ndiff --git a/torch/testing/_internal/opinfo/definitions/sparse.py b/torch/testing/_internal/opinfo/definitions/sparse.py\nindex 6baff3b2f86fea..570b2c546f099a 100644\n--- a/torch/testing/_internal/opinfo/definitions/sparse.py\n+++ b/torch/testing/_internal/opinfo/definitions/sparse.py\n@@ -331,7 +331,7 @@ def _validate_sample_input_sparse_reduction_sum(sample, check_validate=False):\n     }:\n         if (isinstance(dim, int) and (t_inp.dim() != 2 or keepdim)) or (\n             isinstance(dim, (list, tuple))\n-            and (((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim))\n+            and ((t_inp.dim() != 2 and len(dim) != t_inp.dim()) or keepdim)\n         ):\n             if layout in {torch.sparse_bsr, torch.sparse_bsc}:\n                 return ErrorInput(\n"
  },
  {
    "number": 105384,
    "title": "[BE] Enable ruff's UP rules and autoformat utils/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "ffd5e8c57e5d9dce101efcbc02c1761c3601f7b3",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105384",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105384/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105384.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105384.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105384/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105384/comments",
    "labels": [
      "release notes: dataloader",
      "open source"
    ],
    "_event_time": "2023-07-18T01:11:11.593505Z",
    "state": "closed",
    "patch": "From b426aafa41d6ca38bfec8c460efb7e75d47e94cb Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:11:05 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat utils/\n\n[ghstack-poisoned]\n---\n torch/utils/_freeze.py                        |  2 +-\n torch/utils/benchmark/examples/end_to_end.py  |  5 ++--\n .../utils/benchmark/examples/op_benchmark.py  |  4 +--\n .../benchmark/examples/sparse/op_benchmark.py |  4 +--\n .../examples/spectral_ops_fuzz_test.py        |  2 +-\n torch/utils/benchmark/utils/common.py         |  4 +--\n torch/utils/benchmark/utils/cpp_jit.py        |  6 ++--\n .../utils/valgrind_wrapper/timer_interface.py | 12 ++++----\n torch/utils/bottleneck/__main__.py            |  6 ++--\n torch/utils/bundled_inputs.py                 |  8 +++---\n torch/utils/checkpoint.py                     |  2 +-\n torch/utils/collect_env.py                    | 26 ++++++++---------\n torch/utils/cpp_extension.py                  |  8 +++---\n torch/utils/data/_utils/pin_memory.py         |  2 +-\n torch/utils/data/_utils/worker.py             |  8 +++---\n torch/utils/data/dataloader.py                |  6 ++--\n torch/utils/data/datapipes/_decorator.py      |  2 +-\n torch/utils/data/datapipes/_typing.py         |  8 +++---\n .../data/datapipes/dataframe/dataframes.py    | 18 ++++++------\n torch/utils/data/datapipes/datapipe.py        | 10 +++----\n torch/utils/data/datapipes/gen_pyi.py         |  2 +-\n torch/utils/data/datapipes/iter/callable.py   |  2 +-\n .../data/datapipes/iter/combinatorics.py      |  4 +--\n torch/utils/data/datapipes/iter/combining.py  |  6 ++--\n torch/utils/data/datapipes/iter/filelister.py |  2 +-\n torch/utils/data/datapipes/iter/fileopener.py |  4 +--\n torch/utils/data/datapipes/iter/grouping.py   |  2 +-\n .../data/datapipes/iter/routeddecoder.py      |  2 +-\n torch/utils/data/datapipes/iter/sharding.py   |  2 +-\n torch/utils/data/datapipes/map/combining.py   |  2 +-\n torch/utils/data/datapipes/map/grouping.py    |  2 +-\n torch/utils/data/datapipes/utils/common.py    |  2 +-\n torch/utils/data/datapipes/utils/decoder.py   |  6 ++--\n torch/utils/data/graph.py                     |  2 +-\n torch/utils/dlpack.py                         |  2 +-\n torch/utils/hipify/cuda_to_hip_mappings.py    |  2 +-\n torch/utils/hipify/hipify_python.py           | 28 +++++++++----------\n torch/utils/jit/log_extract.py                |  2 +-\n torch/utils/mobile_optimizer.py               |  4 +--\n torch/utils/tensorboard/_caffe2_graph.py      |  4 +--\n torch/utils/tensorboard/_embedding.py         |  2 +-\n torch/utils/tensorboard/_pytorch_graph.py     |  6 ++--\n torch/utils/tensorboard/writer.py             |  2 +-\n torch/utils/throughput_benchmark.py           | 10 +++----\n torch/utils/viz/_cycles.py                    | 12 ++++----\n torch/utils/weak.py                           |  5 ++--\n 46 files changed, 131 insertions(+), 131 deletions(-)\n\ndiff --git a/torch/utils/_freeze.py b/torch/utils/_freeze.py\nindex 5245ac011e19ac..6590ff4b769e42 100644\n--- a/torch/utils/_freeze.py\n+++ b/torch/utils/_freeze.py\n@@ -237,7 +237,7 @@ def compile_file(self, path: Path, top_package_path: Path):\n         module_mangled_name = \"__\".join(module_qualname)\n         c_name = \"M_\" + module_mangled_name\n \n-        with open(path, \"r\") as src_file:\n+        with open(path) as src_file:\n             co = self.compile_string(src_file.read())\n \n         bytecode = marshal.dumps(co)\ndiff --git a/torch/utils/benchmark/examples/end_to_end.py b/torch/utils/benchmark/examples/end_to_end.py\nindex 5e0f42712d7c7a..a6d05a91c94253 100644\n--- a/torch/utils/benchmark/examples/end_to_end.py\n+++ b/torch/utils/benchmark/examples/end_to_end.py\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"End-to-end example to test a PR for regressions:\n \n $ python -m examples.end_to_end --pr 39850\n@@ -111,7 +110,7 @@ def parse_args():\n \n def construct_stmt_and_label(pr, params):\n     if pr == \"39850\":\n-        k0, k1, k2, dim = [params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"]]\n+        k0, k1, k2, dim = (params[i] for i in [\"k0\", \"k1\", \"k2\", \"dim\"])\n         state = np.random.RandomState(params[\"random_value\"])\n         topk_dim = state.randint(low=0, high=dim)\n         dim_size = [k0, k1, k2][topk_dim]\n@@ -291,7 +290,7 @@ def construct_table(results, device_str, test_variance):\n     )\n \n     _, result_log_file = tempfile.mkstemp(suffix=\".log\")\n-    with open(result_log_file, \"wt\") as f:\n+    with open(result_log_file, \"w\") as f:\n         f.write(f\"{device_str}\\n\\n{column_labels}\\n\")\n         print(f\"\\n{column_labels}\\n[First twenty omitted (these tend to be noisy) ]\")\n         for key, (r_ref, r_pr), rel_diff in results:\ndiff --git a/torch/utils/benchmark/examples/op_benchmark.py b/torch/utils/benchmark/examples/op_benchmark.py\nindex 65b69d84b41f44..b7536b9ec26bb8 100644\n--- a/torch/utils/benchmark/examples/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/op_benchmark.py\n@@ -37,13 +37,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/sparse/op_benchmark.py b/torch/utils/benchmark/examples/sparse/op_benchmark.py\nindex f9ee17d5617e08..d7e97d33cc1101 100644\n--- a/torch/utils/benchmark/examples/sparse/op_benchmark.py\n+++ b/torch/utils/benchmark/examples/sparse/op_benchmark.py\n@@ -32,13 +32,13 @@ def run(n, stmt, fuzzer_cls):\n         assert_dicts_equal(float_params, int_params)\n         assert_dicts_equal(float_tensor_params[\"x\"], int_tensor_params[\"x\"])\n \n-        float_measurement, int_measurement = [\n+        float_measurement, int_measurement = (\n             Timer(\n                 stmt,\n                 globals=tensors,\n             ).blocked_autorange(min_run_time=_MEASURE_TIME)\n             for tensors in (float_tensors, int_tensors)\n-        ]\n+        )\n \n         descriptions = []\n         for name in float_tensors:\ndiff --git a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\nindex d8284ee4187c49..c70395573adb2c 100644\n--- a/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n+++ b/torch/utils/benchmark/examples/spectral_ops_fuzz_test.py\n@@ -27,7 +27,7 @@ def run_benchmark(name: str, function: object, dtype: torch.dtype, seed: int, de\n     results = []\n     for tensors, tensor_params, params in spectral_fuzzer.take(samples):\n         shape = [params['k0'], params['k1'], params['k2']][:params['ndim']]\n-        str_shape = ' x '.join([\"{:<4}\".format(s) for s in shape])\n+        str_shape = ' x '.join([f\"{s:<4}\" for s in shape])\n         sub_label = f\"{str_shape} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n         for dim in _dim_options(params['ndim']):\n             for nthreads in (1, 4, 16) if not cuda else (1,):\ndiff --git a/torch/utils/benchmark/utils/common.py b/torch/utils/benchmark/utils/common.py\nindex a8bbef3bfbeb4f..c1636ddb78a2bf 100644\n--- a/torch/utils/benchmark/utils/common.py\n+++ b/torch/utils/benchmark/utils/common.py\n@@ -325,7 +325,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n                 if not os.path.exists(owner_file):\n                     continue\n \n-                with open(owner_file, \"rt\") as f:\n+                with open(owner_file) as f:\n                     owner_pid = int(f.read())\n \n                 if owner_pid == os.getpid():\n@@ -349,7 +349,7 @@ def _make_temp_dir(prefix: Optional[str] = None, gc_dev_shm: bool = False) -> st\n     os.makedirs(path, exist_ok=False)\n \n     if use_dev_shm:\n-        with open(os.path.join(path, \"owner.pid\"), \"wt\") as f:\n+        with open(os.path.join(path, \"owner.pid\"), \"w\") as f:\n             f.write(str(os.getpid()))\n \n     return path\ndiff --git a/torch/utils/benchmark/utils/cpp_jit.py b/torch/utils/benchmark/utils/cpp_jit.py\nindex 65b8c70ee43e6c..a09f1a00aace6f 100644\n--- a/torch/utils/benchmark/utils/cpp_jit.py\n+++ b/torch/utils/benchmark/utils/cpp_jit.py\n@@ -137,7 +137,7 @@ def _compile_template(\n         os.makedirs(build_dir, exist_ok=True)\n \n         src_path = os.path.join(build_dir, \"timer_src.cpp\")\n-        with open(src_path, \"wt\") as f:\n+        with open(src_path, \"w\") as f:\n             f.write(src)\n \n     # `cpp_extension` has its own locking scheme, so we don't need our lock.\n@@ -154,7 +154,7 @@ def _compile_template(\n \n def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> TimeitModuleType:\n     template_path: str = os.path.join(SOURCE_ROOT, \"timeit_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     module = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)\n@@ -164,7 +164,7 @@ def compile_timeit_template(*, stmt: str, setup: str, global_setup: str) -> Time\n \n def compile_callgrind_template(*, stmt: str, setup: str, global_setup: str) -> str:\n     template_path: str = os.path.join(SOURCE_ROOT, \"valgrind_wrapper\", \"timer_callgrind_template.cpp\")\n-    with open(template_path, \"rt\") as f:\n+    with open(template_path) as f:\n         src: str = f.read()\n \n     target = _compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)\ndiff --git a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\nindex 71753bd59548ae..11ce6d90fc47f3 100644\n--- a/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n+++ b/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\n@@ -28,7 +28,9 @@\n     CompletedProcessType = subprocess.CompletedProcess\n \n \n-FunctionCount = NamedTuple(\"FunctionCount\", [(\"count\", int), (\"function\", str)])\n+class FunctionCount(NamedTuple):\n+    count: int\n+    function: str\n \n \n @dataclasses.dataclass(repr=False, eq=False, frozen=True)\n@@ -598,7 +600,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     stderr=subprocess.STDOUT,\n                     **kwargs,\n                 )\n-                with open(stdout_stderr_log, \"rt\") as f:\n+                with open(stdout_stderr_log) as f:\n                     return invocation, f.read()\n             finally:\n                 f_stdout_stderr.close()\n@@ -612,7 +614,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n                     )\n \n                 script_file = os.path.join(working_dir, \"timer_callgrind.py\")\n-                with open(script_file, \"wt\") as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(self._construct_script(\n                         task_spec,\n                         globals=GlobalsBridge(globals, data_dir),\n@@ -652,7 +654,7 @@ def run(args: List[str], **kwargs: Any) -> Tuple[CompletedProcessType, str]:\n             if valgrind_invocation.returncode:\n                 error_report = \"\"\n                 if os.path.exists(error_log):\n-                    with open(error_log, \"rt\") as f:\n+                    with open(error_log) as f:\n                         error_report = f.read()\n                 if not error_report:\n                     error_report = \"Unknown error.\\n\" + valgrind_invocation_output\n@@ -724,7 +726,7 @@ def read_results(i: int) -> Tuple[FunctionCounts, FunctionCounts, Optional[str]]\n                 fpath = f\"{callgrind_out}.{i + 1}\"  # Callgrind one-indexes files.\n                 callgrind_out_contents: Optional[str] = None\n                 if retain_out_file:\n-                    with open(fpath, \"rt\") as f:\n+                    with open(fpath) as f:\n                         callgrind_out_contents = f.read()\n \n                 return (\ndiff --git a/torch/utils/bottleneck/__main__.py b/torch/utils/bottleneck/__main__.py\nindex 86c1af04baa0e6..f7fd209e1438fa 100644\n--- a/torch/utils/bottleneck/__main__.py\n+++ b/torch/utils/bottleneck/__main__.py\n@@ -16,7 +16,7 @@ def redirect_argv(new_argv):\n \n def compiled_with_cuda(sysinfo):\n     if sysinfo.cuda_compiled_version:\n-        return 'compiled w/ CUDA {}'.format(sysinfo.cuda_compiled_version)\n+        return f'compiled w/ CUDA {sysinfo.cuda_compiled_version}'\n     return 'not compiled w/ CUDA'\n \n \n@@ -59,7 +59,7 @@ def run_env_analysis():\n         'debug_str': debug_str,\n         'pytorch_version': info.torch_version,\n         'cuda_compiled': compiled_with_cuda(info),\n-        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'py_version': f'{sys.version_info[0]}.{sys.version_info[1]}',\n         'cuda_runtime': cuda_avail,\n         'pip_version': pip_version,\n         'pip_list_output': pip_list_output,\n@@ -138,7 +138,7 @@ def print_autograd_prof_summary(prof, mode, sortby='cpu_time', topk=15):\n \n     result = {\n         'mode': mode,\n-        'description': 'top {} events sorted by {}'.format(topk, sortby),\n+        'description': f'top {topk} events sorted by {sortby}',\n         'output': torch.autograd.profiler_util._build_table(topk_events),\n         'cuda_warning': cuda_warning\n     }\ndiff --git a/torch/utils/bundled_inputs.py b/torch/utils/bundled_inputs.py\nindex 4ae39733ff2e4b..ad34e15e6bfa17 100644\n--- a/torch/utils/bundled_inputs.py\n+++ b/torch/utils/bundled_inputs.py\n@@ -261,11 +261,11 @@ def augment_many_model_functions_with_bundled_inputs(\n \n \n         if input_list is not None and not isinstance(input_list, Sequence):\n-            raise TypeError(\"Error inputs for function {0} is not a Sequence\".format(function_name))\n+            raise TypeError(f\"Error inputs for function {function_name} is not a Sequence\")\n \n         function_arg_types = [arg.type for arg in function.schema.arguments[1:]]  # type: ignore[attr-defined]\n         deflated_inputs_type: ListType = ListType(TupleType(function_arg_types))\n-        model._c._register_attribute(\"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs_type, [])\n+        model._c._register_attribute(f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs_type, [])\n \n         if hasattr(model, \"_generate_bundled_inputs_for_\" + function_name):\n             if input_list is not None:\n@@ -290,7 +290,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             for inp_idx, args in enumerate(input_list):\n                 if not isinstance(args, Tuple) and not isinstance(args, List):  # type: ignore[arg-type]\n                     raise TypeError(\n-                        \"Error bundled input for function {0} idx: {1} is not a Tuple or a List\".format(function_name, inp_idx)\n+                        f\"Error bundled input for function {function_name} idx: {inp_idx} is not a Tuple or a List\"\n                     )\n                 deflated_args = []\n                 parts.append(\"(\")\n@@ -314,7 +314,7 @@ def augment_many_model_functions_with_bundled_inputs(\n             # Back-channel return this expr for debugging.\n             if _receive_inflate_expr is not None:\n                 _receive_inflate_expr.append(expr)\n-            setattr(model, \"_bundled_inputs_deflated_{name}\".format(name=function_name), deflated_inputs)\n+            setattr(model, f\"_bundled_inputs_deflated_{function_name}\", deflated_inputs)\n             definition = textwrap.dedent(\"\"\"\n                 def _generate_bundled_inputs_for_{name}(self):\n                     deflated = self._bundled_inputs_deflated_{name}\ndiff --git a/torch/utils/checkpoint.py b/torch/utils/checkpoint.py\nindex 8c023c705df58f..4da281d32ac3bd 100644\n--- a/torch/utils/checkpoint.py\n+++ b/torch/utils/checkpoint.py\n@@ -66,7 +66,7 @@ def _get_device_module(device=\"cuda\"):\n     return device_module\n \n \n-class DefaultDeviceType(object):\n+class DefaultDeviceType:\n     r\"\"\"\n     A class that manages the default device type for checkpointing.\n     If no non-CPU tensors are present, the default device type will\ndiff --git a/torch/utils/collect_env.py b/torch/utils/collect_env.py\nindex de03564a2b75d1..2266d64c1944d5 100644\n--- a/torch/utils/collect_env.py\n+++ b/torch/utils/collect_env.py\n@@ -91,7 +91,7 @@ def run_and_return_first_line(run_lambda, command):\n \n def get_conda_packages(run_lambda):\n     conda = os.environ.get('CONDA_EXE', 'conda')\n-    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n+    out = run_and_read_all(run_lambda, f\"{conda} list\")\n     if out is None:\n         return out\n \n@@ -157,7 +157,7 @@ def get_cudnn_version(run_lambda):\n         system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n         cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n         where_cmd = os.path.join(system_root, 'System32', 'where')\n-        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n+        cudnn_cmd = f'{where_cmd} /R \"{cuda_path}\\\\bin\" cudnn*.dll'\n     elif get_platform() == 'darwin':\n         # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n         # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n@@ -185,7 +185,7 @@ def get_cudnn_version(run_lambda):\n     if len(files) == 1:\n         return files[0]\n     result = '\\n'.join(files)\n-    return 'Probably one of the following:\\n{}'.format(result)\n+    return f'Probably one of the following:\\n{result}'\n \n \n def get_nvidia_smi():\n@@ -199,7 +199,7 @@ def get_nvidia_smi():\n         smis = [new_path, legacy_path]\n         for candidate_smi in smis:\n             if os.path.exists(candidate_smi):\n-                smi = '\"{}\"'.format(candidate_smi)\n+                smi = f'\"{candidate_smi}\"'\n                 break\n     return smi\n \n@@ -317,7 +317,7 @@ def get_windows_version(run_lambda):\n     system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n     wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n     findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n-    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n+    return run_and_read_all(run_lambda, f'{wmic_cmd} os get Caption | {findstr_cmd} /v Caption')\n \n \n def get_lsb_version(run_lambda):\n@@ -340,20 +340,20 @@ def get_os(run_lambda):\n         version = get_mac_version(run_lambda)\n         if version is None:\n             return None\n-        return 'macOS {} ({})'.format(version, machine())\n+        return f'macOS {version} ({machine()})'\n \n     if platform == 'linux':\n         # Ubuntu/Debian based\n         desc = get_lsb_version(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n         # Try reading /etc/*-release\n         desc = check_release_file(run_lambda)\n         if desc is not None:\n-            return '{} ({})'.format(desc, machine())\n+            return f'{desc} ({machine()})'\n \n-        return '{} ({})'.format(platform, machine())\n+        return f'{platform} ({machine()})'\n \n     # Unknown platform\n     return platform\n@@ -450,7 +450,7 @@ def get_version_or_na(cfg, prefix):\n     return SystemEnv(\n         torch_version=version_str,\n         is_debug_build=debug_mode_str,\n-        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n+        python_version=f'{sys_version} ({sys.maxsize.bit_length() + 1}-bit runtime)',\n         python_platform=get_python_platform(),\n         is_cuda_available=cuda_available_str,\n         cuda_compiled_version=cuda_version_str,\n@@ -537,7 +537,7 @@ def replace_if_empty(text, replacement='No relevant packages'):\n     def maybe_start_on_next_line(string):\n         # If `string` is multiline, prepend a \\n to it.\n         if string is not None and len(string.split('\\n')) > 1:\n-            return '\\n{}\\n'.format(string)\n+            return f'\\n{string}\\n'\n         return string\n \n     mutable_dict = envinfo._asdict()\n@@ -575,7 +575,7 @@ def maybe_start_on_next_line(string):\n     # If they were previously None, they'll show up as ie '[conda] Could not collect'\n     if mutable_dict['pip_packages']:\n         mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n-                                               '[{}] '.format(envinfo.pip_version))\n+                                               f'[{envinfo.pip_version}] ')\n     if mutable_dict['conda_packages']:\n         mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n                                                  '[conda] ')\n@@ -599,7 +599,7 @@ def main():\n             latest = max(dumps, key=os.path.getctime)\n             ctime = os.path.getctime(latest)\n             creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n-            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n+            msg = f\"\\n*** Detected a minidump at {latest} created on {creation_time}, \" + \\\n                   \"if this is related to your bug please include it when you file a report ***\"\n             print(msg, file=sys.stderr)\n \ndiff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py\nindex ee3c61c9978e96..e847f4e30915b9 100644\n--- a/torch/utils/cpp_extension.py\n+++ b/torch/utils/cpp_extension.py\n@@ -150,10 +150,10 @@ def _join_rocm_home(*paths) -> str:\n     only once we need to get any ROCm-specific path.\n     '''\n     if ROCM_HOME is None:\n-        raise EnvironmentError('ROCM_HOME environment variable is not set. '\n+        raise OSError('ROCM_HOME environment variable is not set. '\n                                'Please set it to your ROCm install root.')\n     elif IS_WINDOWS:\n-        raise EnvironmentError('Building PyTorch extensions using '\n+        raise OSError('Building PyTorch extensions using '\n                                'ROCm and Windows is not supported.')\n     return os.path.join(ROCM_HOME, *paths)\n \n@@ -264,7 +264,7 @@ def _maybe_write(filename, new_content):\n     if it already had the right content (to avoid triggering recompile).\n     '''\n     if os.path.exists(filename):\n-        with open(filename, 'r') as f:\n+        with open(filename) as f:\n             content = f.read()\n \n         if content == new_content:\n@@ -2247,7 +2247,7 @@ def _join_cuda_home(*paths) -> str:\n     only once we need to get any CUDA-specific path.\n     '''\n     if CUDA_HOME is None:\n-        raise EnvironmentError('CUDA_HOME environment variable is not set. '\n+        raise OSError('CUDA_HOME environment variable is not set. '\n                                'Please set it to your CUDA install root.')\n     return os.path.join(CUDA_HOME, *paths)\n \ndiff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py\nindex 074b89b624b9d3..cdd53c2d9ea2b1 100644\n--- a/torch/utils/data/_utils/pin_memory.py\n+++ b/torch/utils/data/_utils/pin_memory.py\n@@ -37,7 +37,7 @@ def do_one_step():\n                 data = pin_memory(data, device)\n             except Exception:\n                 data = ExceptionWrapper(\n-                    where=\"in pin memory thread for device {}\".format(device_id))\n+                    where=f\"in pin memory thread for device {device_id}\")\n             r = (idx, data)\n         while not done_event.is_set():\n             try:\ndiff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py\nindex b4fc8e0748f0f1..0d43f63a6a2f20 100644\n--- a/torch/utils/data/_utils/worker.py\n+++ b/torch/utils/data/_utils/worker.py\n@@ -76,13 +76,13 @@ def __init__(self, **kwargs):\n \n     def __setattr__(self, key, val):\n         if self.__initialized:\n-            raise RuntimeError(\"Cannot assign attributes to {} objects\".format(self.__class__.__name__))\n+            raise RuntimeError(f\"Cannot assign attributes to {self.__class__.__name__} objects\")\n         return super().__setattr__(key, val)\n \n     def __repr__(self):\n         items = []\n         for k in self.__keys:\n-            items.append('{}={}'.format(k, getattr(self, k)))\n+            items.append(f'{k}={getattr(self, k)}')\n         return '{}({})'.format(self.__class__.__name__, ', '.join(items))\n \n \n@@ -252,7 +252,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n             fetcher = _DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)\n         except Exception:\n             init_exception = ExceptionWrapper(\n-                where=\"in DataLoader worker process {}\".format(worker_id))\n+                where=f\"in DataLoader worker process {worker_id}\")\n \n         # When using Iterable mode, some worker can exit earlier than others due\n         # to the IterableDataset behaving differently for different workers.\n@@ -318,7 +318,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n                         # `ExceptionWrapper` does the correct thing.\n                         # See NOTE [ Python Traceback Reference Cycle Problem ]\n                         data = ExceptionWrapper(\n-                            where=\"in DataLoader worker process {}\".format(worker_id))\n+                            where=f\"in DataLoader worker process {worker_id}\")\n             data_queue.put((idx, data))\n             del data, idx, index, r  # save memory\n     except KeyboardInterrupt:\ndiff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py\nindex ec86f778023ba6..1c33592f02f146 100644\n--- a/torch/utils/data/dataloader.py\n+++ b/torch/utils/data/dataloader.py\n@@ -604,7 +604,7 @@ def __init__(self, loader: DataLoader) -> None:\n         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\n         self._persistent_workers = loader.persistent_workers\n         self._num_yielded = 0\n-        self._profile_name = \"enumerate(DataLoader)#{}.__next__\".format(self.__class__.__name__)\n+        self._profile_name = f\"enumerate(DataLoader)#{self.__class__.__name__}.__next__\"\n \n     def __iter__(self) -> '_BaseDataLoaderIter':\n         return self\n@@ -1145,7 +1145,7 @@ def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n                     self._mark_worker_as_unavailable(worker_id)\n             if len(failed_workers) > 0:\n                 pids_str = ', '.join(str(w.pid) for w in failed_workers)\n-                raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\n+                raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n             if isinstance(e, queue.Empty):\n                 return (False, None)\n             import tempfile\n@@ -1281,7 +1281,7 @@ def _get_data(self):\n             if success:\n                 return data\n             else:\n-                raise RuntimeError('DataLoader timed out after {} seconds'.format(self._timeout))\n+                raise RuntimeError(f'DataLoader timed out after {self._timeout} seconds')\n         elif self._pin_memory:\n             while self._pin_memory_thread.is_alive():\n                 success, data = self._try_get_data()\ndiff --git a/torch/utils/data/datapipes/_decorator.py b/torch/utils/data/datapipes/_decorator.py\nindex e4cc9e4e59365d..96b7e00e076f02 100644\n--- a/torch/utils/data/datapipes/_decorator.py\n+++ b/torch/utils/data/datapipes/_decorator.py\n@@ -80,7 +80,7 @@ def __init__(self, arg: Union[Type[IterDataPipe], Callable[[], bool]]) -> None:\n         elif isinstance(arg, Callable):  # type:ignore[arg-type]\n             self.deterministic_fn = arg  # type: ignore[assignment, misc]\n         else:\n-            raise TypeError(\"{} can not be decorated by non_deterministic\".format(arg))\n+            raise TypeError(f\"{arg} can not be decorated by non_deterministic\")\n \n     def __call__(self, *args, **kwargs):\n         global _determinism\ndiff --git a/torch/utils/data/datapipes/_typing.py b/torch/utils/data/datapipes/_typing.py\nindex 6377a2ec940860..68049ba30d9018 100644\n--- a/torch/utils/data/datapipes/_typing.py\n+++ b/torch/utils/data/datapipes/_typing.py\n@@ -234,7 +234,7 @@ def issubtype(self, other):\n             return issubtype(self.param, other.param)\n         if isinstance(other, type):\n             return issubtype(self.param, other)\n-        raise TypeError(\"Expected '_DataPipeType' or 'type', but found {}\".format(type(other)))\n+        raise TypeError(f\"Expected '_DataPipeType' or 'type', but found {type(other)}\")\n \n     def issubtype_of_instance(self, other):\n         return issubinstance(other, self.param)\n@@ -279,13 +279,13 @@ def __init__(self, name, bases, namespace, **kwargs):\n     @_tp_cache\n     def _getitem_(self, params):\n         if params is None:\n-            raise TypeError('{}[t]: t can not be None'.format(self.__name__))\n+            raise TypeError(f'{self.__name__}[t]: t can not be None')\n         if isinstance(params, str):\n             params = ForwardRef(params)\n         if not isinstance(params, tuple):\n             params = (params, )\n \n-        msg = \"{}[t]: t must be a type\".format(self.__name__)\n+        msg = f\"{self.__name__}[t]: t must be a type\"\n         params = tuple(_type_check(p, msg) for p in params)\n \n         if isinstance(self.type.param, _GenericAlias):\n@@ -303,7 +303,7 @@ def _getitem_(self, params):\n                                        '__type_class__': True})\n \n         if len(params) > 1:\n-            raise TypeError('Too many parameters for {} actual {}, expected 1'.format(self, len(params)))\n+            raise TypeError(f'Too many parameters for {self} actual {len(params)}, expected 1')\n \n         t = _DataPipeType(params[0])\n \ndiff --git a/torch/utils/data/datapipes/dataframe/dataframes.py b/torch/utils/data/datapipes/dataframe/dataframes.py\nindex 06029e07851685..72d93cde66c3cb 100644\n--- a/torch/utils/data/datapipes/dataframe/dataframes.py\n+++ b/torch/utils/data/datapipes/dataframe/dataframes.py\n@@ -36,7 +36,7 @@ def disable_capture():\n     CaptureControl.disabled = True\n \n \n-class CaptureControl():\n+class CaptureControl:\n     disabled = False\n \n \n@@ -184,7 +184,7 @@ def execute(self):\n         return value\n \n \n-class CaptureLikeMock():\n+class CaptureLikeMock:\n     def __init__(self, name):\n         import unittest.mock as mock\n         # TODO(VitalyFedyunin): Do not use provate function here, copy own implementation instead.\n@@ -232,7 +232,7 @@ class CaptureVariableAssign(CaptureF):\n     def __str__(self):\n         variable = self.kwargs['variable']\n         value = self.kwargs['value']\n-        return \"{variable} = {value}\".format(variable=variable, value=value)\n+        return f\"{variable} = {value}\"\n \n     def execute(self):\n         self.kwargs['variable'].calculated_value = self.kwargs['value'].execute()\n@@ -272,7 +272,7 @@ def __init__(self, left, key, ctx):\n         self.key = key\n \n     def __str__(self):\n-        return \"%s[%s]\" % (self.left, get_val(self.key))\n+        return f\"{self.left}[{get_val(self.key)}]\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -287,7 +287,7 @@ def __init__(self, left, key, value, ctx):\n         self.value = value\n \n     def __str__(self):\n-        return \"%s[%s] = %s\" % (self.left, get_val(self.key), self.value)\n+        return f\"{self.left}[{get_val(self.key)}] = {self.value}\"\n \n     def execute(self):\n         left = self.left.execute()\n@@ -302,7 +302,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s + %s\" % (self.left, self.right)\n+        return f\"{self.left} + {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) + get_val(self.right)\n@@ -315,7 +315,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s * %s\" % (self.left, self.right)\n+        return f\"{self.left} * {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) * get_val(self.right)\n@@ -328,7 +328,7 @@ def __init__(self, left, right, ctx):\n         self.right = right\n \n     def __str__(self):\n-        return \"%s - %s\" % (self.left, self.right)\n+        return f\"{self.left} - {self.right}\"\n \n     def execute(self):\n         return get_val(self.left) - get_val(self.right)\n@@ -341,7 +341,7 @@ def __init__(self, src, name, ctx):\n         self.name = name\n \n     def __str__(self):\n-        return \"%s.%s\" % (self.src, self.name)\n+        return f\"{self.src}.{self.name}\"\n \n     def execute(self):\n         val = get_val(self.src)\ndiff --git a/torch/utils/data/datapipes/datapipe.py b/torch/utils/data/datapipes/datapipe.py\nindex 445400ecb59c32..1017b52af0fbce 100644\n--- a/torch/utils/data/datapipes/datapipe.py\n+++ b/torch/utils/data/datapipes/datapipe.py\n@@ -126,7 +126,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -135,7 +135,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register, enable_df_api_tracing=False):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, enable_df_api_tracing, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -265,7 +265,7 @@ def __getattr__(self, attribute_name):\n             functools.update_wrapper(wrapper=function, wrapped=f, assigned=(\"__doc__\",))\n             return function\n         else:\n-            raise AttributeError(\"'{0}' object has no attribute '{1}\".format(self.__class__.__name__, attribute_name))\n+            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attribute_name}\")\n \n     @classmethod\n     def register_function(cls, function_name, function):\n@@ -274,7 +274,7 @@ def register_function(cls, function_name, function):\n     @classmethod\n     def register_datapipe_as_function(cls, function_name, cls_to_register):\n         if function_name in cls.functions:\n-            raise Exception(\"Unable to add DataPipe function name {} as it is already taken\".format(function_name))\n+            raise Exception(f\"Unable to add DataPipe function name {function_name} as it is already taken\")\n \n         def class_function(cls, source_dp, *args, **kwargs):\n             result_pipe = cls(source_dp, *args, **kwargs)\n@@ -363,7 +363,7 @@ def __len__(self):\n             return len(self._datapipe)\n         except Exception as e:\n             raise TypeError(\n-                \"{} instance doesn't have valid length\".format(type(self).__name__)\n+                f\"{type(self).__name__} instance doesn't have valid length\"\n             ) from e\n \n \ndiff --git a/torch/utils/data/datapipes/gen_pyi.py b/torch/utils/data/datapipes/gen_pyi.py\nindex 1b77fbfecf0290..ed3e75bc5da12e 100644\n--- a/torch/utils/data/datapipes/gen_pyi.py\n+++ b/torch/utils/data/datapipes/gen_pyi.py\n@@ -19,7 +19,7 @@ def gen_from_template(dir: str, template_name: str, output_name: str, replacemen\n     template_path = os.path.join(dir, template_name)\n     output_path = os.path.join(dir, output_name)\n \n-    with open(template_path, \"r\") as f:\n+    with open(template_path) as f:\n         content = f.read()\n     for placeholder, lines, indentation in replacements:\n         with open(output_path, \"w\") as f:\ndiff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py\nindex 4e3dce4b82d1dd..9916b094e408d1 100644\n--- a/torch/utils/data/datapipes/iter/callable.py\n+++ b/torch/utils/data/datapipes/iter/callable.py\n@@ -126,7 +126,7 @@ def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n         raise TypeError(\n-            \"{} instance doesn't have valid length\".format(type(self).__name__)\n+            f\"{type(self).__name__} instance doesn't have valid length\"\n         )\n \n \ndiff --git a/torch/utils/data/datapipes/iter/combinatorics.py b/torch/utils/data/datapipes/iter/combinatorics.py\nindex 30b569e329b654..4d2973bbc5a2e9 100644\n--- a/torch/utils/data/datapipes/iter/combinatorics.py\n+++ b/torch/utils/data/datapipes/iter/combinatorics.py\n@@ -48,7 +48,7 @@ def __len__(self) -> int:\n         # Dataset has been tested as `Sized`\n         if isinstance(self.sampler, Sized):\n             return len(self.sampler)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('shuffle')\n@@ -137,7 +137,7 @@ def __iter__(self) -> Iterator[T_co]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self._buffer = []\ndiff --git a/torch/utils/data/datapipes/iter/combining.py b/torch/utils/data/datapipes/iter/combining.py\nindex 7c76e986b230d4..4fe05ea717cf16 100644\n--- a/torch/utils/data/datapipes/iter/combining.py\n+++ b/torch/utils/data/datapipes/iter/combining.py\n@@ -56,7 +56,7 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return sum(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('fork')\n@@ -567,7 +567,7 @@ def __len__(self):\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes) * len(self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n     def reset(self) -> None:\n         self.buffer = []\n@@ -627,4 +627,4 @@ def __len__(self) -> int:\n         if all(isinstance(dp, Sized) for dp in self.datapipes):\n             return min(len(dp) for dp in self.datapipes)\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/filelister.py b/torch/utils/data/datapipes/iter/filelister.py\nindex b2ecd71b5ce9c9..22e2cd432d6a3a 100644\n--- a/torch/utils/data/datapipes/iter/filelister.py\n+++ b/torch/utils/data/datapipes/iter/filelister.py\n@@ -61,5 +61,5 @@ def __iter__(self) -> Iterator[str] :\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/fileopener.py b/torch/utils/data/datapipes/iter/fileopener.py\nindex 03d5761a9f164c..50737d9b587b25 100644\n--- a/torch/utils/data/datapipes/iter/fileopener.py\n+++ b/torch/utils/data/datapipes/iter/fileopener.py\n@@ -51,7 +51,7 @@ def __init__(\n         self.encoding: Optional[str] = encoding\n \n         if self.mode not in ('b', 't', 'rb', 'rt', 'r'):\n-            raise ValueError(\"Invalid mode {}\".format(mode))\n+            raise ValueError(f\"Invalid mode {mode}\")\n         # TODO: enforce typing for each instance based on mode, otherwise\n         #       `argument_validation` with this DataPipe may be potentially broken\n \n@@ -68,5 +68,5 @@ def __iter__(self):\n \n     def __len__(self):\n         if self.length == -1:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n         return self.length\ndiff --git a/torch/utils/data/datapipes/iter/grouping.py b/torch/utils/data/datapipes/iter/grouping.py\nindex c83bd2748b78fb..b26847d7319740 100644\n--- a/torch/utils/data/datapipes/iter/grouping.py\n+++ b/torch/utils/data/datapipes/iter/grouping.py\n@@ -83,7 +83,7 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\n \n \n @functional_datapipe('unbatch')\ndiff --git a/torch/utils/data/datapipes/iter/routeddecoder.py b/torch/utils/data/datapipes/iter/routeddecoder.py\nindex 8bfbe1442180ab..5e68ae133e05ab 100644\n--- a/torch/utils/data/datapipes/iter/routeddecoder.py\n+++ b/torch/utils/data/datapipes/iter/routeddecoder.py\n@@ -62,4 +62,4 @@ def __iter__(self) -> Iterator[Tuple[str, Any]]:\n     def __len__(self) -> int:\n         if isinstance(self.datapipe, Sized):\n             return len(self.datapipe)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/iter/sharding.py b/torch/utils/data/datapipes/iter/sharding.py\nindex 730caeaf7d4da3..1f4a3a291bd11f 100644\n--- a/torch/utils/data/datapipes/iter/sharding.py\n+++ b/torch/utils/data/datapipes/iter/sharding.py\n@@ -80,4 +80,4 @@ def __len__(self):\n         if isinstance(self.source_datapipe, Sized):\n             return len(self.source_datapipe) // self.num_of_instances +\\\n                 (1 if (self.instance_id < len(self.source_datapipe) % self.num_of_instances) else 0)\n-        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+        raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/map/combining.py b/torch/utils/data/datapipes/map/combining.py\nindex 85146f8345cbdc..4a4a785eff78e5 100644\n--- a/torch/utils/data/datapipes/map/combining.py\n+++ b/torch/utils/data/datapipes/map/combining.py\n@@ -47,7 +47,7 @@ def __getitem__(self, index) -> T_co:  # type: ignore[type-var]\n                 return dp[index - offset]\n             else:\n                 offset += len(dp)\n-        raise IndexError(\"Index {} is out of range.\".format(index))\n+        raise IndexError(f\"Index {index} is out of range.\")\n \n     def __len__(self) -> int:\n         return sum(len(dp) for dp in self.datapipes)\ndiff --git a/torch/utils/data/datapipes/map/grouping.py b/torch/utils/data/datapipes/map/grouping.py\nindex da3cf5688a1bb0..65b30d8eba1f40 100644\n--- a/torch/utils/data/datapipes/map/grouping.py\n+++ b/torch/utils/data/datapipes/map/grouping.py\n@@ -64,4 +64,4 @@ def __len__(self) -> int:\n             else:\n                 return (len(self.datapipe) + self.batch_size - 1) // self.batch_size\n         else:\n-            raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n+            raise TypeError(f\"{type(self).__name__} instance doesn't have valid length\")\ndiff --git a/torch/utils/data/datapipes/utils/common.py b/torch/utils/data/datapipes/utils/common.py\nindex e39d67ee6c81b9..99ae0cb4cbd024 100644\n--- a/torch/utils/data/datapipes/utils/common.py\n+++ b/torch/utils/data/datapipes/utils/common.py\n@@ -305,7 +305,7 @@ def __init__(self, file_obj, parent_stream=None, name=None):\n         self.closed = False\n         if parent_stream is not None:\n             if not isinstance(parent_stream, StreamWrapper):\n-                raise RuntimeError('Parent stream should be StreamWrapper, {} was given'.format(type(parent_stream)))\n+                raise RuntimeError(f'Parent stream should be StreamWrapper, {type(parent_stream)} was given')\n             parent_stream.child_counter += 1\n             self.parent_stream = parent_stream\n         if StreamWrapper.debug_unclosed_streams:\ndiff --git a/torch/utils/data/datapipes/utils/decoder.py b/torch/utils/data/datapipes/utils/decoder.py\nindex 4da810c3276684..8a7cb71b619de4 100644\n--- a/torch/utils/data/datapipes/utils/decoder.py\n+++ b/torch/utils/data/datapipes/utils/decoder.py\n@@ -137,7 +137,7 @@ class ImageHandler:\n     - pilrgba: pil None rgba\n     \"\"\"\n     def __init__(self, imagespec):\n-        assert imagespec in list(imagespecs.keys()), \"unknown image specification: {}\".format(imagespec)\n+        assert imagespec in list(imagespecs.keys()), f\"unknown image specification: {imagespec}\"\n         self.imagespec = imagespec.lower()\n \n     def __call__(self, extension, data):\n@@ -167,14 +167,14 @@ def __call__(self, extension, data):\n                 return img\n             elif atype == \"numpy\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n                 if etype == \"uint8\":\n                     return result\n                 else:\n                     return result.astype(\"f\") / 255.0\n             elif atype == \"torch\":\n                 result = np.asarray(img)\n-                assert result.dtype == np.uint8, \"numpy image array should be type uint8, but got {}\".format(result.dtype)\n+                assert result.dtype == np.uint8, f\"numpy image array should be type uint8, but got {result.dtype}\"\n \n                 if etype == \"uint8\":\n                     result = np.array(result.transpose(2, 0, 1))\ndiff --git a/torch/utils/data/graph.py b/torch/utils/data/graph.py\nindex 2769e326c03e3b..7fc95d58fa2198 100644\n--- a/torch/utils/data/graph.py\n+++ b/torch/utils/data/graph.py\n@@ -130,7 +130,7 @@ def traverse(datapipe: DataPipe, only_datapipe: Optional[bool] = None) -> DataPi\n # Add cache here to prevent infinite recursion on DataPipe\n def _traverse_helper(datapipe: DataPipe, only_datapipe: bool, cache: Set[int]) -> DataPipeGraph:\n     if not isinstance(datapipe, (IterDataPipe, MapDataPipe)):\n-        raise RuntimeError(\"Expected `IterDataPipe` or `MapDataPipe`, but {} is found\".format(type(datapipe)))\n+        raise RuntimeError(f\"Expected `IterDataPipe` or `MapDataPipe`, but {type(datapipe)} is found\")\n \n     dp_id = id(datapipe)\n     if dp_id in cache:\ndiff --git a/torch/utils/dlpack.py b/torch/utils/dlpack.py\nindex f903de94eb67b2..a987bca6dcd51b 100644\n--- a/torch/utils/dlpack.py\n+++ b/torch/utils/dlpack.py\n@@ -102,7 +102,7 @@ def from_dlpack(ext_tensor: Any) -> 'torch.Tensor':\n         # device is either CUDA or ROCm, we need to pass the current\n         # stream\n         if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM):\n-            stream = torch.cuda.current_stream('cuda:{}'.format(device[1]))\n+            stream = torch.cuda.current_stream(f'cuda:{device[1]}')\n             # cuda_stream is the pointer to the stream and it is a public\n             # attribute, but it is not documented\n             # The array API specify that the default legacy stream must be passed\ndiff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py\nindex 3b583dbf790109..163f3649c41279 100644\n--- a/torch/utils/hipify/cuda_to_hip_mappings.py\n+++ b/torch/utils/hipify/cuda_to_hip_mappings.py\n@@ -46,7 +46,7 @@\n     RE_MINOR = re.compile(r\"#define\\s+ROCM_VERSION_MINOR\\s+(\\d+)\")\n     RE_PATCH = re.compile(r\"#define\\s+ROCM_VERSION_PATCH\\s+(\\d+)\")\n     major, minor, patch = 0, 0, 0\n-    for line in open(rocm_version_h, \"r\"):\n+    for line in open(rocm_version_h):\n         match = RE_MAJOR.search(line)\n         if match:\n             major = int(match.group(1))\ndiff --git a/torch/utils/hipify/hipify_python.py b/torch/utils/hipify/hipify_python.py\nindex 34a066750e1cdc..fa800659595bd7 100755\n--- a/torch/utils/hipify/hipify_python.py\n+++ b/torch/utils/hipify/hipify_python.py\n@@ -219,13 +219,13 @@ def compute_stats(stats):\n     unsupported_calls = {cuda_call for (cuda_call, _filepath) in stats[\"unsupported_calls\"]}\n \n     # Print the number of unsupported calls\n-    print(\"Total number of unsupported CUDA function calls: {0:d}\".format(len(unsupported_calls)))\n+    print(f\"Total number of unsupported CUDA function calls: {len(unsupported_calls):d}\")\n \n     # Print the list of unsupported calls\n     print(\", \".join(unsupported_calls))\n \n     # Print the number of kernel launches\n-    print(\"\\nTotal number of replaced kernel launches: {0:d}\".format(len(stats[\"kernel_launches\"])))\n+    print(\"\\nTotal number of replaced kernel launches: {:d}\".format(len(stats[\"kernel_launches\"])))\n \n \n def add_dim3(kernel_string, cuda_kernel):\n@@ -254,8 +254,8 @@ def add_dim3(kernel_string, cuda_kernel):\n     first_arg_clean = kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace(\"\\n\", \"\").strip(\" \")\n     second_arg_clean = kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace(\"\\n\", \"\").strip(\" \")\n \n-    first_arg_dim3 = \"dim3({})\".format(first_arg_clean)\n-    second_arg_dim3 = \"dim3({})\".format(second_arg_clean)\n+    first_arg_dim3 = f\"dim3({first_arg_clean})\"\n+    second_arg_dim3 = f\"dim3({second_arg_clean})\"\n \n     first_arg_raw_dim3 = first_arg_raw.replace(first_arg_clean, first_arg_dim3)\n     second_arg_raw_dim3 = second_arg_raw.replace(second_arg_clean, second_arg_dim3)\n@@ -269,7 +269,7 @@ def add_dim3(kernel_string, cuda_kernel):\n def processKernelLaunches(string, stats):\n     \"\"\" Replace the CUDA style Kernel launches with the HIP style kernel launches.\"\"\"\n     # Concat the namespace with the kernel names. (Find cleaner way of doing this later).\n-    string = RE_KERNEL_LAUNCH.sub(lambda inp: \"{0}{1}::\".format(inp.group(1), inp.group(2)), string)\n+    string = RE_KERNEL_LAUNCH.sub(lambda inp: f\"{inp.group(1)}{inp.group(2)}::\", string)\n \n     def grab_method_and_template(in_kernel):\n         # The positions for relevant kernel components.\n@@ -482,7 +482,7 @@ def replace_math_functions(input_string):\n     \"\"\"\n     output_string = input_string\n     for func in MATH_TRANSPILATIONS:\n-        output_string = output_string.replace(r'{}('.format(func), '{}('.format(MATH_TRANSPILATIONS[func]))\n+        output_string = output_string.replace(fr'{func}(', f'{MATH_TRANSPILATIONS[func]}(')\n \n     return output_string\n \n@@ -531,7 +531,7 @@ def replace_extern_shared(input_string):\n     \"\"\"\n     output_string = input_string\n     output_string = RE_EXTERN_SHARED.sub(\n-        lambda inp: \"HIP_DYNAMIC_SHARED({0} {1}, {2})\".format(\n+        lambda inp: \"HIP_DYNAMIC_SHARED({} {}, {})\".format(\n             inp.group(1) or \"\", inp.group(2), inp.group(3)), output_string)\n \n     return output_string\n@@ -657,7 +657,7 @@ def is_caffe2_gpu_file(rel_filepath):\n \n \n # Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508\n-class Trie():\n+class Trie:\n     \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n     The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n \n@@ -750,7 +750,7 @@ def pattern(self):\n             CAFFE2_TRIE.add(src)\n             CAFFE2_MAP[src] = dst\n RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.pattern())\n-RE_PYTORCH_PREPROCESSOR = re.compile(r'(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))\n+RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\\W)({PYTORCH_TRIE.pattern()})(?=\\W)')\n \n RE_QUOTE_HEADER = re.compile(r'#include \"([^\"]+)\"')\n RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')\n@@ -789,7 +789,7 @@ def preprocessor(\n \n     rel_filepath = os.path.relpath(filepath, output_directory)\n \n-    with open(fin_path, 'r', encoding='utf-8') as fin:\n+    with open(fin_path, encoding='utf-8') as fin:\n         if fin.readline() == HIPIFY_C_BREADCRUMB:\n             hipify_result.hipified_path = None\n             hipify_result.status = \"[ignored, input is hipified output]\"\n@@ -929,7 +929,7 @@ def repl(m):\n \n     do_write = True\n     if os.path.exists(fout_path):\n-        with open(fout_path, 'r', encoding='utf-8') as fout_old:\n+        with open(fout_path, encoding='utf-8') as fout_old:\n             do_write = fout_old.read() != output_source\n     if do_write:\n         try:\n@@ -956,7 +956,7 @@ def file_specific_replacement(filepath, search_string, replace_string, strict=Fa\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if strict:\n-            contents = re.sub(r'\\b({0})\\b'.format(re.escape(search_string)), lambda x: replace_string, contents)\n+            contents = re.sub(fr'\\b({re.escape(search_string)})\\b', lambda x: replace_string, contents)\n         else:\n             contents = contents.replace(search_string, replace_string)\n         f.seek(0)\n@@ -968,8 +968,8 @@ def file_add_header(filepath, header):\n     with openf(filepath, \"r+\") as f:\n         contents = f.read()\n         if header[0] != \"<\" and header[-1] != \">\":\n-            header = '\"{0}\"'.format(header)\n-        contents = ('#include {0} \\n'.format(header)) + contents\n+            header = f'\"{header}\"'\n+        contents = (f'#include {header} \\n') + contents\n         f.seek(0)\n         f.write(contents)\n         f.truncate()\ndiff --git a/torch/utils/jit/log_extract.py b/torch/utils/jit/log_extract.py\nindex d9d0e442c1dbf6..2e89a769eff0c8 100644\n--- a/torch/utils/jit/log_extract.py\n+++ b/torch/utils/jit/log_extract.py\n@@ -11,7 +11,7 @@ def extract_ir(filename: str) -> List[str]:\n     pfx = None\n     current = \"\"\n     graphs = []\n-    with open(filename, \"r\") as f:\n+    with open(filename) as f:\n         split_strs = f.read().split(BEGIN)\n         for i, split_str in enumerate(split_strs):\n             if i == 0:\ndiff --git a/torch/utils/mobile_optimizer.py b/torch/utils/mobile_optimizer.py\nindex ec200423e10c5b..66d57a2372baf9 100644\n--- a/torch/utils/mobile_optimizer.py\n+++ b/torch/utils/mobile_optimizer.py\n@@ -31,7 +31,7 @@ def optimize_for_mobile(\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     if optimization_blocklist is None:\n         optimization_blocklist = set()\n@@ -86,7 +86,7 @@ def generate_mobile_module_lints(script_module: torch.jit.ScriptModule):\n     \"\"\"\n     if not isinstance(script_module, torch.jit.ScriptModule):\n         raise TypeError(\n-            'Got {}, but ScriptModule is expected.'.format(type(script_module)))\n+            f'Got {type(script_module)}, but ScriptModule is expected.')\n \n     lint_list = []\n \ndiff --git a/torch/utils/tensorboard/_caffe2_graph.py b/torch/utils/tensorboard/_caffe2_graph.py\nindex 8bba2aeffddef2..2aa162af7ad5c3 100644\n--- a/torch/utils/tensorboard/_caffe2_graph.py\n+++ b/torch/utils/tensorboard/_caffe2_graph.py\n@@ -232,7 +232,7 @@ def _add_gradient_scope(shapes, blob_name_tracker, ops):\n \n     def f(name):\n         if \"_grad\" in name:\n-            return \"GRADIENTS/{}\".format(name)\n+            return f\"GRADIENTS/{name}\"\n         else:\n             return name\n \n@@ -317,7 +317,7 @@ def _tf_device(device_option):\n     ):\n         return \"/cpu:*\"\n     if device_option.device_type == caffe2_pb2.CUDA:\n-        return \"/gpu:{}\".format(device_option.device_id)\n+        return f\"/gpu:{device_option.device_id}\"\n     raise Exception(\"Unhandled device\", device_option)\n \n \ndiff --git a/torch/utils/tensorboard/_embedding.py b/torch/utils/tensorboard/_embedding.py\nindex f172e092608337..afbe68191aa98f 100644\n--- a/torch/utils/tensorboard/_embedding.py\n+++ b/torch/utils/tensorboard/_embedding.py\n@@ -62,7 +62,7 @@ def make_sprite(label_img, save_path):\n \n def get_embedding_info(metadata, label_img, subdir, global_step, tag):\n     info = EmbeddingInfo()\n-    info.tensor_name = \"{}:{}\".format(tag, str(global_step).zfill(5))\n+    info.tensor_name = f\"{tag}:{str(global_step).zfill(5)}\"\n     info.tensor_path = _gfile_join(subdir, \"tensors.tsv\")\n     if metadata is not None:\n         info.metadata_path = _gfile_join(subdir, \"metadata.tsv\")\ndiff --git a/torch/utils/tensorboard/_pytorch_graph.py b/torch/utils/tensorboard/_pytorch_graph.py\nindex f03812b603e1c0..280b503c515c0b 100644\n--- a/torch/utils/tensorboard/_pytorch_graph.py\n+++ b/torch/utils/tensorboard/_pytorch_graph.py\n@@ -275,7 +275,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n                     parent_scope, attr_scope, attr_name\n                 )\n             else:\n-                attr_to_scope[attr_key] = \"__module.{}\".format(attr_name)\n+                attr_to_scope[attr_key] = f\"__module.{attr_name}\"\n             # We don't need classtype nodes; scope will provide this information\n             if node.output().type().kind() != CLASSTYPE_KIND:\n                 node_py = NodePyOP(node)\n@@ -286,7 +286,7 @@ def parse(graph, trace, args=None, omit_useless_nodes=True):\n \n     for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n         node_pyio = NodePyIO(node, \"output\")\n-        node_pyio.debugName = \"output.{}\".format(i + 1)\n+        node_pyio.debugName = f\"output.{i + 1}\"\n         node_pyio.inputs = [node.debugName()]\n         nodes_py.append(node_pyio)\n \n@@ -302,7 +302,7 @@ def parse_traced_name(module):\n     for name, module in trace.named_modules(prefix=\"__module\"):\n         mod_name = parse_traced_name(module)\n         attr_name = name.split(\".\")[-1]\n-        alias_to_name[name] = \"{}[{}]\".format(mod_name, attr_name)\n+        alias_to_name[name] = f\"{mod_name}[{attr_name}]\"\n \n     for node in nodes_py.nodes_op:\n         module_aliases = node.scopeName.split(\"/\")\ndiff --git a/torch/utils/tensorboard/writer.py b/torch/utils/tensorboard/writer.py\nindex 2fa34d05e727d9..b592707df2c1e1 100644\n--- a/torch/utils/tensorboard/writer.py\n+++ b/torch/utils/tensorboard/writer.py\n@@ -953,7 +953,7 @@ def add_embedding(\n \n         # Maybe we should encode the tag so slashes don't trip us up?\n         # I don't think this will mess us up, but better safe than sorry.\n-        subdir = \"%s/%s\" % (str(global_step).zfill(5), self._encode(tag))\n+        subdir = f\"{str(global_step).zfill(5)}/{self._encode(tag)}\"\n         save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n \n         fs = tf.io.gfile\ndiff --git a/torch/utils/throughput_benchmark.py b/torch/utils/throughput_benchmark.py\nindex 8b2fd1a76ca8a4..2dc3ce8543a9b6 100644\n--- a/torch/utils/throughput_benchmark.py\n+++ b/torch/utils/throughput_benchmark.py\n@@ -18,10 +18,10 @@ def format_time(time_us=None, time_ms=None, time_s=None):\n             raise AssertionError(\"Shouldn't reach here :)\")\n \n     if time_us >= US_IN_SECOND:\n-        return '{:.3f}s'.format(time_us / US_IN_SECOND)\n+        return f'{time_us / US_IN_SECOND:.3f}s'\n     if time_us >= US_IN_MS:\n-        return '{:.3f}ms'.format(time_us / US_IN_MS)\n-    return '{:.3f}us'.format(time_us)\n+        return f'{time_us / US_IN_MS:.3f}ms'\n+    return f'{time_us:.3f}us'\n \n \n class ExecutionStats:\n@@ -52,8 +52,8 @@ def total_time_seconds(self):\n     def __str__(self):\n         return '\\n'.join([\n             \"Average latency per example: \" + format_time(time_ms=self.latency_avg_ms),\n-            \"Total number of iterations: {}\".format(self.num_iters),\n-            \"Total number of iterations per second (across all threads): {:.2f}\".format(self.iters_per_second),\n+            f\"Total number of iterations: {self.num_iters}\",\n+            f\"Total number of iterations per second (across all threads): {self.iters_per_second:.2f}\",\n             \"Total time: \" + format_time(time_s=self.total_time_seconds)\n         ])\n \ndiff --git a/torch/utils/viz/_cycles.py b/torch/utils/viz/_cycles.py\nindex a64d5e9c35830a..13a425cd1b8285 100644\n--- a/torch/utils/viz/_cycles.py\n+++ b/torch/utils/viz/_cycles.py\n@@ -220,29 +220,29 @@ def format_sequence(obj):\n     if isinstance(obj, BASE_TYPES):\n         return repr(obj)\n     if type(obj).__name__ == 'function':\n-        return \"function\\n{}\".format(obj.__name__)\n+        return f\"function\\n{obj.__name__}\"\n     elif isinstance(obj, types.MethodType):\n         try:\n             func_name = obj.__func__.__qualname__\n         except AttributeError:\n             func_name = \"<anonymous>\"\n-        return \"instancemethod\\n{}\".format(func_name)\n+        return f\"instancemethod\\n{func_name}\"\n     elif isinstance(obj, list):\n         return f\"[{format_sequence(obj)}]\"\n     elif isinstance(obj, tuple):\n         return f\"({format_sequence(obj)})\"\n     elif isinstance(obj, dict):\n-        return \"dict[{}]\".format(len(obj))\n+        return f\"dict[{len(obj)}]\"\n     elif isinstance(obj, types.ModuleType):\n-        return \"module\\n{}\".format(obj.__name__)\n+        return f\"module\\n{obj.__name__}\"\n     elif isinstance(obj, type):\n-        return \"type\\n{}\".format(obj.__name__)\n+        return f\"type\\n{obj.__name__}\"\n     elif isinstance(obj, weakref.ref):\n         referent = obj()\n         if referent is None:\n             return \"weakref (dead referent)\"\n         else:\n-            return \"weakref to id 0x{:x}\".format(id(referent))\n+            return f\"weakref to id 0x{id(referent):x}\"\n     elif isinstance(obj, types.FrameType):\n         filename = obj.f_code.co_filename\n         if len(filename) > FRAME_FILENAME_LIMIT:\ndiff --git a/torch/utils/weak.py b/torch/utils/weak.py\nindex 2a7d597c4f2a06..bcd3025bc68e3a 100644\n--- a/torch/utils/weak.py\n+++ b/torch/utils/weak.py\n@@ -4,7 +4,6 @@\n from weakref import ref\n from _weakrefset import _IterationGuard  # type: ignore[attr-defined]\n from collections.abc import MutableMapping, Mapping\n-from typing import Dict\n from torch import Tensor\n import collections.abc as _collections_abc\n \n@@ -83,7 +82,7 @@ def __eq__(self, other):\n \n # This is directly adapted from cpython/Lib/weakref.py\n class WeakIdKeyDictionary(MutableMapping):\n-    data: Dict[WeakIdRef, object]\n+    data: dict[WeakIdRef, object]\n \n     def __init__(self, dict=None):\n         self.data = {}\n@@ -144,7 +143,7 @@ def __len__(self):\n         return len(self.data) - len(self._pending_removals)\n \n     def __repr__(self):\n-        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n+        return f\"<{self.__class__.__name__} at {id(self):#x}>\"\n \n     def __setitem__(self, key, value):\n         self.data[WeakIdRef(key, self._remove)] = value  # CHANGED\n"
  },
  {
    "number": 105383,
    "title": "[BE] Enable ruff's UP rules and autoformat torchgen/",
    "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
    "merge_commit_sha": "9290e13369d84ea622cd3b219c0ae1dc77bbe21b",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105383",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105383/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105383.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105383.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105383/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105383/comments",
    "labels": [
      "open source",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T01:11:07.378238Z",
    "state": "closed",
    "patch": "From 507c8e477f8ef1fe8772254db30cfff5384430ce Mon Sep 17 00:00:00 2001\nFrom: Justin Chu <justinchu@microsoft.com>\nDate: Tue, 18 Jul 2023 01:10:06 +0000\nSubject: [PATCH] [BE] Enable ruff's UP rules and autoformat torchgen/\n\n[ghstack-poisoned]\n---\n torchgen/api/python.py               | 16 ++++++++--------\n torchgen/code_template.py            |  2 +-\n torchgen/executorch/parse.py         |  2 +-\n torchgen/gen.py                      |  4 ++--\n torchgen/gen_backend_stubs.py        |  6 +++---\n torchgen/gen_executorch.py           |  6 +++---\n torchgen/gen_lazy_tensor.py          |  6 +++---\n torchgen/model.py                    |  2 +-\n torchgen/selective_build/operator.py |  2 +-\n torchgen/selective_build/selector.py |  4 ++--\n torchgen/utils.py                    | 10 +++++-----\n 11 files changed, 30 insertions(+), 30 deletions(-)\n\ndiff --git a/torchgen/api/python.py b/torchgen/api/python.py\nindex b4da5d1113dce6..96aa43be1060b5 100644\n--- a/torchgen/api/python.py\n+++ b/torchgen/api/python.py\n@@ -315,7 +315,7 @@ def from_outputs(\n                 outputs=outputs,\n             )\n         elif size > 1:\n-            if any((not a.type.is_tensor_like() for a in outputs)):\n+            if any(not a.type.is_tensor_like() for a in outputs):\n                 raise RuntimeError(f\"Unsupported output type: {outputs}\")\n             return PythonOutArgument(\n                 name=\"out\",\n@@ -882,10 +882,10 @@ def topt_default_init(name: str) -> Optional[str]:\n \n \n def namedtuple_fieldnames(returns: Tuple[Return, ...]) -> List[str]:\n-    if len(returns) <= 1 or all((r.name is None for r in returns)):\n+    if len(returns) <= 1 or all(r.name is None for r in returns):\n         return []\n     else:\n-        if any((r.name is None for r in returns)):\n+        if any(r.name is None for r in returns):\n             # When building on Windows, `PyStructSequence_UnnamedField` could not be\n             # resolved by the linker for some reason, which cause error in building:\n             #\n@@ -1163,7 +1163,7 @@ def dispatch_lambda_return_str(f: NativeFunction) -> str:\n     # mutable reference to temporary.  Maybe we could assign it to a\n     # variable itself.)\n     returns_without_annotation = tuple(\n-        (Return(r.name, r.type, None) for r in f.func.returns)\n+        Return(r.name, r.type, None) for r in f.func.returns\n     )\n     return_str = cpp.returns_type(returns_without_annotation, symint=True).cpp_type()\n     if return_str not in SUPPORTED_RETURN_TYPES:\n@@ -1195,7 +1195,7 @@ def cpp_dispatch_exprs(\n     exprs: Tuple[str, ...] = tuple()\n     if not isinstance(python_signature, PythonSignatureDeprecated):\n         # By default the exprs are consistent with the C++ signature.\n-        exprs = tuple((a.name for a in cpp_args))\n+        exprs = tuple(a.name for a in cpp_args)\n     else:\n         # For deprecated python signature we may need fill in some constants.\n         exprs = tuple(\n@@ -1426,7 +1426,7 @@ def dispatch_lambda_exprs(\n                     f\"{f.func}: unrecognized type '{str(a.type)}' for tensor options field '{a.name}'\"\n                 )\n         if not all(\n-            (a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys())\n+            a in tensor_options_args_names for a in TENSOR_OPTIONS_FIELDS.keys()\n         ):\n             raise RuntimeError(\n                 f\"{f.func}: incomplete tensor options args: {tensor_options_args_names}\"\n@@ -1454,7 +1454,7 @@ def dispatch_lambda_exprs(\n                 raise RuntimeError(\n                     f\"{f.func}: dtype in tensor_options_args without output arg\"\n                 )\n-            if not all((a in tensor_options_args_names for a in (\"layout\", \"device\"))):\n+            if not all(a in tensor_options_args_names for a in (\"layout\", \"device\")):\n                 raise RuntimeError(\n                     f\"{f.func}: incomplete tensor options for output check\"\n                 )\n@@ -1473,6 +1473,6 @@ def dispatch_lambda_exprs(\n             )\n \n     return DispatchLambdaArgumentExprs(\n-        exprs=tuple((lambda_args_exprs[a.name] for a in lambda_args)),\n+        exprs=tuple(lambda_args_exprs[a.name] for a in lambda_args),\n         inits=inits,\n     )\ndiff --git a/torchgen/code_template.py b/torchgen/code_template.py\nindex 9f877771afe9be..b932a94ecc9192 100644\n--- a/torchgen/code_template.py\n+++ b/torchgen/code_template.py\n@@ -20,7 +20,7 @@ class CodeTemplate:\n \n     @staticmethod\n     def from_file(filename: str) -> \"CodeTemplate\":\n-        with open(filename, \"r\") as f:\n+        with open(filename) as f:\n             return CodeTemplate(f.read(), filename)\n \n     def __init__(self, pattern: str, filename: str = \"\") -> None:\ndiff --git a/torchgen/executorch/parse.py b/torchgen/executorch/parse.py\nindex f6f30b4554aafb..89b4b93558a6a2 100644\n--- a/torchgen/executorch/parse.py\n+++ b/torchgen/executorch/parse.py\n@@ -124,7 +124,7 @@ def parse_et_yaml(\n     \"\"\"Parse native_functions.yaml into NativeFunctions and an Operator Indexed Dict\n     of fields to persist from native_functions.yaml to functions.yaml\n     \"\"\"\n-    with open(path, \"r\") as f:\n+    with open(path) as f:\n         es = yaml.load(f, Loader=LineLoader)\n \n     et_kernel = extract_kernel_fields(es)\ndiff --git a/torchgen/gen.py b/torchgen/gen.py\nindex dcdd0945dff019..9766c8af5bc0f5 100644\n--- a/torchgen/gen.py\n+++ b/torchgen/gen.py\n@@ -212,7 +212,7 @@ def parse_tags_yaml_struct(es: object, path: str = \"<stdin>\") -> Set[str]:\n def parse_tags_yaml(path: str) -> Set[str]:\n     global _GLOBAL_PARSE_TAGS_YAML_CACHE\n     if path not in _GLOBAL_PARSE_TAGS_YAML_CACHE:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n             _GLOBAL_PARSE_TAGS_YAML_CACHE[path] = parse_tags_yaml_struct(es, path=path)\n \n@@ -233,7 +233,7 @@ def parse_native_yaml(\n \n         # if a loaded yaml is provided, use that instead of reading from path\n         if loaded_yaml is None:\n-            with open(path, \"r\") as f:\n+            with open(path) as f:\n                 es = yaml.load(f, Loader=LineLoader)\n         else:\n             es = loaded_yaml\ndiff --git a/torchgen/gen_backend_stubs.py b/torchgen/gen_backend_stubs.py\nindex 7322daa5dc7602..ff23aa9be39713 100644\n--- a/torchgen/gen_backend_stubs.py\n+++ b/torchgen/gen_backend_stubs.py\n@@ -47,7 +47,7 @@ def parse_backend_yaml(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -253,9 +253,9 @@ def error_on_missing_kernels(\n     full_codegen: Optional[List[OperatorName]] = None,\n ) -> None:\n     try:\n-        with open(kernel_defn_file_path, \"r\") as f:\n+        with open(kernel_defn_file_path) as f:\n             backend_defns = f.read()\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified impl_path file: {kernel_defn_file_path}\"\n         ) from e\ndiff --git a/torchgen/gen_executorch.py b/torchgen/gen_executorch.py\nindex bfd42a7985e49d..6f5df46944f0f6 100644\n--- a/torchgen/gen_executorch.py\n+++ b/torchgen/gen_executorch.py\n@@ -575,7 +575,7 @@ def translate_native_yaml(\n         None\n     \"\"\"\n     if use_aten_lib:\n-        with open(aten_yaml_path, \"r\") as aten_yaml:\n+        with open(aten_yaml_path) as aten_yaml:\n             out_file.writelines(aten_yaml.readlines())\n         return\n \n@@ -604,7 +604,7 @@ def translate_native_yaml(\n         or os.stat(native_yaml_path).st_size == 0\n     ):\n         return\n-    with open(native_yaml_path, \"r\") as native_yaml:\n+    with open(native_yaml_path) as native_yaml:\n         native_es = yaml.load(native_yaml, Loader=LineLoader)\n         if not native_es:\n             return\n@@ -641,7 +641,7 @@ def parse_yaml(\n     Union[Dict[DispatchKey, Dict[OperatorName, BackendMetadata]], ETKernelIndex],\n ]:\n     if path and os.path.exists(path) and os.stat(path).st_size > 0:\n-        with open(path, \"r\") as f:\n+        with open(path) as f:\n             es = yaml.load(f, Loader=LineLoader)\n \n         # Check for kernel index structure\ndiff --git a/torchgen/gen_lazy_tensor.py b/torchgen/gen_lazy_tensor.py\nindex f995bdb2619838..3e4e4b0414277c 100644\n--- a/torchgen/gen_lazy_tensor.py\n+++ b/torchgen/gen_lazy_tensor.py\n@@ -115,7 +115,7 @@ def parse_native_functions_keys(\n         )\n     }\n \n-    with open(backend_yaml_path, \"r\") as f:\n+    with open(backend_yaml_path) as f:\n         yaml_values = yaml.load(f, Loader=YamlLoader)\n     assert isinstance(yaml_values, dict)\n \n@@ -134,10 +134,10 @@ def validate_shape_inference_header(\n     shape_inference_hdr: str, expected_shape_infr_decls: List[str]\n ) -> None:\n     try:\n-        with open(shape_inference_hdr, \"r\") as f:\n+        with open(shape_inference_hdr) as f:\n             shape_infr_decls = f.read()\n             shape_infr_decl_lines = set(shape_infr_decls.split(\"\\n\"))\n-    except IOError as e:\n+    except OSError as e:\n         raise AssertionError(\n             f\"Unable to read from the specified shape_inference_hdr file: {shape_inference_hdr}\"\n         ) from e\ndiff --git a/torchgen/model.py b/torchgen/model.py\nindex 151fb02bb2c908..0b44732455ea2e 100644\n--- a/torchgen/model.py\n+++ b/torchgen/model.py\n@@ -40,7 +40,7 @@ class Location:\n     line: int\n \n     def __str__(self) -> str:\n-        return \"{}:{}\".format(self.file, self.line)\n+        return f\"{self.file}:{self.line}\"\n \n \n # Valid values of the 'variants' field in native_functions.yaml\ndiff --git a/torchgen/selective_build/operator.py b/torchgen/selective_build/operator.py\nindex 52fdcb74fca84b..d7f5c56f63a60d 100644\n--- a/torchgen/selective_build/operator.py\n+++ b/torchgen/selective_build/operator.py\n@@ -83,7 +83,7 @@ def from_yaml_dict(\n         if \"debug_info\" in op_info:\n             di_list = op_info[\"debug_info\"]\n             assert isinstance(di_list, list)\n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         return SelectiveBuildOperator(\n             name=op_name,\ndiff --git a/torchgen/selective_build/selector.py b/torchgen/selective_build/selector.py\nindex 1d4a00e5968950..4fdc513534444d 100644\n--- a/torchgen/selective_build/selector.py\n+++ b/torchgen/selective_build/selector.py\n@@ -93,7 +93,7 @@ def from_yaml_dict(data: Dict[str, object]) -> \"SelectiveBuilder\":\n             di_list = data[\"debug_info\"]\n             assert isinstance(di_list, list)\n \n-            debug_info = tuple((str(x) for x in di_list))\n+            debug_info = tuple(str(x) for x in di_list)\n \n         operators = {}\n         operators_dict = data.get(\"operators\", {})\n@@ -141,7 +141,7 @@ def from_yaml_str(config_contents: str) -> \"SelectiveBuilder\":\n \n     @staticmethod\n     def from_yaml_path(config_path: str) -> \"SelectiveBuilder\":\n-        with open(config_path, \"r\") as f:\n+        with open(config_path) as f:\n             contents = yaml.safe_load(f)\n             return SelectiveBuilder.from_yaml_dict(contents)\n \ndiff --git a/torchgen/utils.py b/torchgen/utils.py\nindex dd187c737c93ed..0729645ef10b92 100644\n--- a/torchgen/utils.py\n+++ b/torchgen/utils.py\n@@ -105,7 +105,7 @@ def context(msg_fn: Callable[[], str]) -> Iterator[None]:\n # for getting mypy to do exhaustiveness checking\n # TODO: put this somewhere else, maybe\n def assert_never(x: NoReturn) -> NoReturn:\n-    raise AssertionError(\"Unhandled type: {}\".format(type(x).__name__))\n+    raise AssertionError(f\"Unhandled type: {type(x).__name__}\")\n \n \n @functools.lru_cache(maxsize=None)\n@@ -137,9 +137,9 @@ def __init__(self, install_dir: str, template_dir: str, dry_run: bool) -> None:\n     def _write_if_changed(self, filename: str, contents: str) -> None:\n         old_contents: Optional[str]\n         try:\n-            with open(filename, \"r\") as f:\n+            with open(filename) as f:\n                 old_contents = f.read()\n-        except IOError:\n+        except OSError:\n             old_contents = None\n         if contents != old_contents:\n             # Create output directory if it doesn't exist\n@@ -157,7 +157,7 @@ def substitute_with_template(\n             # TODO: Update the comment reference to the correct location\n             if \"generated_comment\" not in env:\n                 comment = \"@\" + \"generated by torchgen/gen.py\"\n-                comment += \" from {}\".format(os.path.basename(template_path))\n+                comment += f\" from {os.path.basename(template_path)}\"\n                 env[\"generated_comment\"] = comment\n             template = _read_template(template_path)\n             return template.substitute(env)\n@@ -172,7 +172,7 @@ def write_with_template(\n         template_fn: str,\n         env_callable: Callable[[], Union[str, Dict[str, Any]]],\n     ) -> None:\n-        filename = \"{}/{}\".format(self.install_dir, filename)\n+        filename = f\"{self.install_dir}/{filename}\"\n         assert filename not in self.filenames, \"duplicate file write {filename}\"\n         self.filenames.add(filename)\n         if not self.dry_run:\n"
  },
  {
    "number": 105380,
    "title": "[Pytorch][Vulkan] Templatize BinaryOps",
    "body": "Summary:\nUse templates to generate the kernels for add, sub, mul, div and their variants (tensor/scalar, in-place/not in-place).\n\nRename Arithmetic.cpp to BinaryOp.cpp\n\nTest Plan:\nhttps://www.internalfb.com/phabricator/paste/view/P785131030\n\n```\n buck run --target-platforms ovr_config//platform/macos:arm64-fbsource //xplat/caffe2:pt_vulkan_api_test_binAppleMac\\#macosx-arm64 -c pt.vulkan_full_precision=1\n\n...\n\nxplat/caffe2/aten/src/ATen/test/vulkan_api_test.cpp:6377: Skipped\nQueryPool is not available\n[  SKIPPED ] VulkanAPITest.querypool_flushed_shader_log (0 ms)\n[----------] 307 tests from VulkanAPITest (5427 ms total)\n\n[----------] Global test environment tear-down\n[==========] 307 tests from 1 test suite ran. (5427 ms total)\n[  PASSED  ] 306 tests.\n[  SKIPPED ] 1 test, listed below:\n[  SKIPPED ] VulkanAPITest.querypool_flushed_shader_log\n\n  YOU HAVE 5 DISABLED TESTS\n```\n\nDifferential Revision: D47307169\n\n",
    "merge_commit_sha": "a530a7e260d99111817db69a13036e75ffcf07f2",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105380",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105380/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105380.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105380.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105380/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105380/comments",
    "labels": [
      "Merged",
      "ciflow/periodic",
      "fb-exported",
      "ciflow/trunk",
      "release notes: vulkan",
      "module: vulkan"
    ],
    "_event_time": "2023-07-18T00:58:55.981825Z",
    "state": "closed",
    "patch": "From a8c0a816b241802b663a76daf0f2243e00efa12b Mon Sep 17 00:00:00 2001\nFrom: Lucy Qiu <lfq@meta.com>\nDate: Mon, 17 Jul 2023 17:54:43 -0700\nSubject: [PATCH] [Pytorch][Vulkan] Templatize BinaryOps\n\nSummary:\nUse templates to generate the kernels for add, sub, mul, div and their variants (tensor/scalar, in-place/not in-place).\n\nRename Arithmetic.cpp to BinaryOp.cpp\n\nTest Plan:\nhttps://www.internalfb.com/phabricator/paste/view/P785131030\n\n```\n buck run --target-platforms ovr_config//platform/macos:arm64-fbsource //xplat/caffe2:pt_vulkan_api_test_binAppleMac\\#macosx-arm64 -c pt.vulkan_full_precision=1\n\n...\n\nxplat/caffe2/aten/src/ATen/test/vulkan_api_test.cpp:6377: Skipped\nQueryPool is not available\n[  SKIPPED ] VulkanAPITest.querypool_flushed_shader_log (0 ms)\n[----------] 307 tests from VulkanAPITest (5427 ms total)\n\n[----------] Global test environment tear-down\n[==========] 307 tests from 1 test suite ran. (5427 ms total)\n[  PASSED  ] 306 tests.\n[  SKIPPED ] 1 test, listed below:\n[  SKIPPED ] VulkanAPITest.querypool_flushed_shader_log\n\n  YOU HAVE 5 DISABLED TESTS\n```\n\nDifferential Revision: D47307169\n\nfbshipit-source-id: 36406608fe8d1409eb25a419f2f934d74ec99dd0\n---\n aten/src/ATen/native/vulkan/glsl/add_.glsl    | 32 --------\n .../ATen/native/vulkan/glsl/add_scalar.glsl   | 27 -------\n .../ATen/native/vulkan/glsl/add_scalar_.glsl  | 26 -------\n aten/src/ATen/native/vulkan/glsl/div_.glsl    | 40 ----------\n aten/src/ATen/native/vulkan/glsl/mul_.glsl    | 32 --------\n .../ATen/native/vulkan/glsl/mul_scalar.glsl   | 27 -------\n .../ATen/native/vulkan/glsl/mul_scalar_.glsl  | 26 -------\n aten/src/ATen/native/vulkan/glsl/sub_.glsl    | 32 --------\n .../glsl/templates/binary_op_params.yaml      | 47 ++++++++++++\n .../glsl/templates/binary_op_scalar.glslt     | 30 ++++++++\n .../templates/binary_op_scalar_inplace.glslt  | 29 ++++++++\n ...inary_ops.glslt => binary_op_tensor.glslt} |  0\n .../templates/binary_op_tensor_inplace.glslt  | 58 +++++++++++++++\n .../glsl/templates/binary_ops_params.yaml     | 15 ----\n .../ops/{Arithmetic.cpp => BinaryOp.cpp}      | 74 ++++++++++---------\n 15 files changed, 205 insertions(+), 290 deletions(-)\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/add_.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/add_scalar.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/add_scalar_.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/div_.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/mul_.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/mul_scalar.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/mul_scalar_.glsl\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/sub_.glsl\n create mode 100644 aten/src/ATen/native/vulkan/glsl/templates/binary_op_params.yaml\n create mode 100644 aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar.glslt\n create mode 100644 aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar_inplace.glslt\n rename aten/src/ATen/native/vulkan/glsl/templates/{binary_ops.glslt => binary_op_tensor.glslt} (100%)\n create mode 100644 aten/src/ATen/native/vulkan/glsl/templates/binary_op_tensor_inplace.glslt\n delete mode 100644 aten/src/ATen/native/vulkan/glsl/templates/binary_ops_params.yaml\n rename aten/src/ATen/native/vulkan/ops/{Arithmetic.cpp => BinaryOp.cpp} (93%)\n\ndiff --git a/aten/src/ATen/native/vulkan/glsl/add_.glsl b/aten/src/ATen/native/vulkan/glsl/add_.glsl\ndeleted file mode 100644\nindex 1fe72bb7a878a8..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/add_.glsl\n+++ /dev/null\n@@ -1,32 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D   uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION          sampler3D uInput;\n-layout(set = 0, binding = 2)         uniform PRECISION restrict Block {\n-  ivec4 size;\n-  ivec4 isize;\n-  float alpha;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size.xyz))) {\n-    const ivec3 input_pos = pos % uBlock.isize.xyz;\n-    const vec4 v = uBlock.isize.w == 1\n-                     ? texelFetch(uInput, input_pos, 0).xxxx\n-                     : texelFetch(uInput, input_pos, 0);\n-    imageStore(\n-        uOutput,\n-        pos,\n-        imageLoad(uOutput, pos) + uBlock.alpha * v);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/add_scalar.glsl b/aten/src/ATen/native/vulkan/glsl/add_scalar.glsl\ndeleted file mode 100644\nindex 90c33d81c99e1b..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/add_scalar.glsl\n+++ /dev/null\n@@ -1,27 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict writeonly image3D   uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION                    sampler3D uInput;\n-layout(set = 0, binding = 2)         uniform PRECISION restrict           Block {\n-  ivec3 size;\n-  float other;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size))) {\n-    imageStore(\n-        uOutput,\n-        pos,\n-        texelFetch(uInput, pos, 0) + uBlock.other);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/add_scalar_.glsl b/aten/src/ATen/native/vulkan/glsl/add_scalar_.glsl\ndeleted file mode 100644\nindex 9db3e709cb1993..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/add_scalar_.glsl\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION restrict Block {\n-  ivec3 size;\n-  float other;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size))) {\n-    imageStore(\n-        uOutput,\n-        pos,\n-        imageLoad(uOutput, pos) + uBlock.other);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/div_.glsl b/aten/src/ATen/native/vulkan/glsl/div_.glsl\ndeleted file mode 100644\nindex a1c7a1359a5d0c..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/div_.glsl\n+++ /dev/null\n@@ -1,40 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D   uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION          sampler3D uInput;\n-layout(set = 0, binding = 2)         uniform PRECISION restrict Block {\n-  ivec4 size;\n-  ivec4 isize;\n-  float alpha;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size.xyz))) {\n-    const ivec3 input_pos = pos % uBlock.isize.xyz;\n-    vec4 v = uBlock.isize.w == 1\n-                ? texelFetch(uInput, input_pos, 0).xxxx\n-                : texelFetch(uInput, input_pos, 0);\n-\n-    const int c_index = (pos.z % ((uBlock.size.w + 3) / 4)) * 4;\n-    if (uBlock.isize.w != 1 && c_index + 3 >= uBlock.size.w) {\n-      ivec4 c_ind = ivec4(c_index) + ivec4(0, 1, 2, 3);\n-      vec4 mask = vec4(lessThan(c_ind, ivec4(uBlock.size.w)));\n-      v = v * mask + vec4(1, 1, 1, 1) - mask;\n-    }\n-\n-    imageStore(\n-        uOutput,\n-        pos,\n-        imageLoad(uOutput, pos) / v);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/mul_.glsl b/aten/src/ATen/native/vulkan/glsl/mul_.glsl\ndeleted file mode 100644\nindex 6487c6c52760d1..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/mul_.glsl\n+++ /dev/null\n@@ -1,32 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D   uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION          sampler3D uInput;\n-layout(set = 0, binding = 2)         uniform PRECISION restrict Block {\n-  ivec4 size;\n-  ivec4 isize;\n-  float alpha;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size.xyz))) {\n-    const ivec3 input_pos = pos % uBlock.isize.xyz;\n-    const vec4 v = uBlock.isize.w == 1\n-                     ? texelFetch(uInput, input_pos, 0).xxxx\n-                     : texelFetch(uInput, input_pos, 0);\n-    imageStore(\n-        uOutput,\n-        pos,\n-        imageLoad(uOutput, pos) * v);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/mul_scalar.glsl b/aten/src/ATen/native/vulkan/glsl/mul_scalar.glsl\ndeleted file mode 100644\nindex 925f5d633992c8..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/mul_scalar.glsl\n+++ /dev/null\n@@ -1,27 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict writeonly image3D   uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION                    sampler3D uInput;\n-layout(set = 0, binding = 2)         uniform PRECISION restrict           Block {\n-  ivec3 size;\n-  float other;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size))) {\n-    imageStore(\n-        uOutput,\n-        pos,\n-        texelFetch(uInput, pos, 0) * uBlock.other);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/mul_scalar_.glsl b/aten/src/ATen/native/vulkan/glsl/mul_scalar_.glsl\ndeleted file mode 100644\nindex 2f80b2b1e4ac71..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/mul_scalar_.glsl\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION restrict Block {\n-  ivec3 size;\n-  float other;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size))) {\n-    imageStore(\n-        uOutput,\n-        pos,\n-        imageLoad(uOutput, pos) * uBlock.other);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/sub_.glsl b/aten/src/ATen/native/vulkan/glsl/sub_.glsl\ndeleted file mode 100644\nindex a68e6f9dc2286b..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/sub_.glsl\n+++ /dev/null\n@@ -1,32 +0,0 @@\n-#version 450 core\n-#define PRECISION $precision\n-#define FORMAT    $format\n-\n-layout(std430) buffer;\n-\n-/* Qualifiers: layout - storage - precision - memory */\n-\n-layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D   uOutput;\n-layout(set = 0, binding = 1)         uniform PRECISION          sampler3D uInput;\n-layout(set = 0, binding = 2)         uniform PRECISION restrict Block {\n-  ivec4 size;\n-  ivec4 isize;\n-  float alpha;\n-} uBlock;\n-\n-layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n-\n-void main() {\n-  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n-\n-  if (all(lessThan(pos, uBlock.size.xyz))) {\n-    const ivec3 input_pos = pos % uBlock.isize.xyz;\n-    const vec4 v = uBlock.isize.w == 1\n-                     ? texelFetch(uInput, input_pos, 0).xxxx\n-                     : texelFetch(uInput, input_pos, 0);\n-    imageStore(\n-        uOutput,\n-        pos,\n-        imageLoad(uOutput, pos) - uBlock.alpha * v);\n-  }\n-}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/templates/binary_op_params.yaml b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_params.yaml\nnew file mode 100644\nindex 00000000000000..1ccd60e46c6abc\n--- /dev/null\n+++ b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_params.yaml\n@@ -0,0 +1,47 @@\n+binary_op_scalar:\n+  parameter_names_with_default_values:\n+      NAME: add_scalar\n+      OPERATOR: X + Y\n+  parameter_values:\n+      - NAME: mul_scalar\n+        OPERATOR: X * Y\n+\n+binary_op_scalar_inplace:\n+  parameter_names_with_default_values:\n+      NAME: add_scalar_\n+      OPERATOR: X + Y\n+  parameter_values:\n+      - NAME: mul_scalar_\n+        OPERATOR: X * Y\n+\n+binary_op_tensor:\n+  parameter_names_with_default_values:\n+      NAME: add\n+      IS_DIV: 0\n+      OPERATOR: X + A * Y\n+  parameter_values:\n+      - NAME: sub\n+        IS_DIV: 0\n+        OPERATOR: X - A * Y\n+      - NAME: mul\n+        IS_DIV: 0\n+        OPERATOR: X * Y\n+      - NAME: div\n+        IS_DIV: 1\n+        OPERATOR: X / Y\n+\n+binary_op_tensor_inplace:\n+  parameter_names_with_default_values:\n+      NAME: add_\n+      IS_DIV: 0\n+      OPERATOR: X + A * Y\n+  parameter_values:\n+      - NAME: sub_\n+        IS_DIV: 0\n+        OPERATOR: X - A * Y\n+      - NAME: mul_\n+        IS_DIV: 0\n+        OPERATOR: X * Y\n+      - NAME: div_\n+        IS_DIV: 1\n+        OPERATOR: X / Y\ndiff --git a/aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar.glslt b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar.glslt\nnew file mode 100644\nindex 00000000000000..4f1baec9c0c422\n--- /dev/null\n+++ b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar.glslt\n@@ -0,0 +1,30 @@\n+/*\n+ * OPERATOR = $OPERATOR\n+ */\n+\n+#define OP(X, Y) $OPERATOR\n+\n+layout(std430) buffer;\n+\n+/* Qualifiers: layout - storage - precision - memory */\n+\n+layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict writeonly image3D uOutput;\n+layout(set = 0, binding = 1) uniform PRECISION sampler3D uInput;\n+layout(set = 0, binding = 2) uniform PRECISION restrict Block {\n+  // output texture size (x=width, y=height, z=depth, w=unused)\n+  ivec4 extents;\n+  float other;\n+}\n+uBlock;\n+\n+layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n+\n+void main() {\n+  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n+\n+  if (any(greaterThanEqual(pos, uBlock.extents.xyz))) {\n+    return;\n+  }\n+\n+  imageStore(uOutput, pos, OP(texelFetch(uInput, pos, 0), uBlock.other));\n+}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar_inplace.glslt b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar_inplace.glslt\nnew file mode 100644\nindex 00000000000000..02cfc82a0a473f\n--- /dev/null\n+++ b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_scalar_inplace.glslt\n@@ -0,0 +1,29 @@\n+/*\n+ * OPERATOR = $OPERATOR\n+ */\n+\n+#define OP(X, Y) $OPERATOR\n+\n+layout(std430) buffer;\n+\n+/* Qualifiers: layout - storage - precision - memory */\n+\n+layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D uOutput;\n+layout(set = 0, binding = 1) uniform PRECISION restrict Block {\n+  // output texture size (x=width, y=height, z=depth, w=unused)\n+  ivec4 extents;\n+  float other;\n+}\n+uBlock;\n+\n+layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n+\n+void main() {\n+  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n+\n+  if (any(greaterThanEqual(pos, uBlock.extents.xyz))) {\n+    return;\n+  }\n+\n+  imageStore(uOutput, pos, OP(imageLoad(uOutput, pos), uBlock.other));\n+}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/templates/binary_ops.glslt b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_tensor.glslt\nsimilarity index 100%\nrename from aten/src/ATen/native/vulkan/glsl/templates/binary_ops.glslt\nrename to aten/src/ATen/native/vulkan/glsl/templates/binary_op_tensor.glslt\ndiff --git a/aten/src/ATen/native/vulkan/glsl/templates/binary_op_tensor_inplace.glslt b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_tensor_inplace.glslt\nnew file mode 100644\nindex 00000000000000..e15665e7e5990f\n--- /dev/null\n+++ b/aten/src/ATen/native/vulkan/glsl/templates/binary_op_tensor_inplace.glslt\n@@ -0,0 +1,58 @@\n+/*\n+ * OPERATOR = $OPERATOR\n+ */\n+\n+#define OP(X, Y, A) $OPERATOR\n+#define IS_DIV $IS_DIV\n+\n+#include \"texel_access.h\"\n+\n+layout(std430) buffer;\n+\n+/* Qualifiers: layout - storage - precision - memory */\n+\n+layout(set = 0, binding = 0, FORMAT) uniform PRECISION restrict image3D uInput;\n+layout(set = 0, binding = 1) uniform PRECISION sampler3D uOther;\n+layout(set = 0, binding = 2) uniform PRECISION restrict Block {\n+  // input tensor size (x=width,y=height,z=channel,w=batch)\n+  ivec4 input_sizes;\n+  // other tensor size (x=width,y=height,z=channel,w=batch)\n+  ivec4 other_sizes;\n+  float alpha;\n+}\n+uBlock;\n+\n+layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;\n+\n+void main() {\n+  const ivec3 pos = ivec3(gl_GlobalInvocationID);\n+\n+  ivec3 output_extents;\n+  output_extents.xy = uBlock.input_sizes.xy;\n+  output_extents.z =\n+      uBlock.input_sizes.w * int(ceil(uBlock.input_sizes.z / 4.0));\n+  if (any(greaterThanEqual(pos, output_extents.xyz))) {\n+    return;\n+  }\n+\n+  ivec3 other_pos =\n+      map_output_pos_to_input_pos(pos, uBlock.input_sizes, uBlock.other_sizes);\n+\n+  vec4 vOther =\n+      load_texel(other_pos, uBlock.input_sizes, uBlock.other_sizes, uOther);\n+\n+// Zero padding is added to the channels dimension when tensors are stored as\n+// image textures. This will cause a divide-by-zero when performing division.\n+// For division, apply an extra step of detecting which elements are zero\n+// padding to avoid division by zero.\n+#if IS_DIV == 1\n+  const int c_index = (pos.z % ((uBlock.input_sizes.z + 3) / 4)) * 4;\n+  if (uBlock.other_sizes.z != 1 && c_index + 3 >= uBlock.input_sizes.z) {\n+    ivec4 c_ind = ivec4(c_index) + ivec4(0, 1, 2, 3);\n+    vec4 mask = vec4(lessThan(c_ind, ivec4(uBlock.input_sizes.z)));\n+    vOther = vOther * mask + vec4(1, 1, 1, 1) - mask;\n+  }\n+#endif\n+\n+  imageStore(uInput, pos, OP(imageLoad(uInput, pos), vOther, uBlock.alpha));\n+}\ndiff --git a/aten/src/ATen/native/vulkan/glsl/templates/binary_ops_params.yaml b/aten/src/ATen/native/vulkan/glsl/templates/binary_ops_params.yaml\ndeleted file mode 100644\nindex aa3046e352e103..00000000000000\n--- a/aten/src/ATen/native/vulkan/glsl/templates/binary_ops_params.yaml\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-binary_ops:\n-  parameter_names_with_default_values:\n-      NAME: add\n-      IS_DIV: 0\n-      OPERATOR: X + A * Y\n-  parameter_values:\n-      - NAME: sub\n-        IS_DIV: 0\n-        OPERATOR: X - A * Y\n-      - NAME: mul\n-        IS_DIV: 0\n-        OPERATOR: X * Y\n-      - NAME: div\n-        IS_DIV: 1\n-        OPERATOR: X / Y\ndiff --git a/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp b/aten/src/ATen/native/vulkan/ops/BinaryOp.cpp\nsimilarity index 93%\nrename from aten/src/ATen/native/vulkan/ops/Arithmetic.cpp\nrename to aten/src/ATen/native/vulkan/ops/BinaryOp.cpp\nindex 66a646b639e4f6..f45131caf87a7c 100644\n--- a/aten/src/ATen/native/vulkan/ops/Arithmetic.cpp\n+++ b/aten/src/ATen/native/vulkan/ops/BinaryOp.cpp\n@@ -77,7 +77,7 @@ std::vector<int64_t> broadcast_size(const Tensor& t1, const Tensor& t2) {\n } // namespace\n using namespace api::utils;\n \n-Tensor arithmetic_scalar(\n+Tensor binary_op_scalar(\n     const Tensor& self_arg,\n     const Scalar& other,\n     const c10::optional<Scalar>& alpha_arg,\n@@ -97,9 +97,11 @@ Tensor arithmetic_scalar(\n                                     : other.to<float>();\n   const struct Block final {\n     uvec3 extents;\n+    int fill0;\n     float other;\n   } block{\n       v_self.extents(),\n+      0,\n       other_val,\n   };\n \n@@ -129,7 +131,7 @@ Tensor arithmetic_scalar(\n   return convert(v_output);\n }\n \n-Tensor& arithmetic_scalar_(\n+Tensor& binary_op_scalar_(\n     Tensor& self_arg,\n     const Scalar& other,\n     const c10::optional<Scalar>& alpha_arg,\n@@ -146,9 +148,11 @@ Tensor& arithmetic_scalar_(\n                                     : other.to<float>();\n   const struct Block final {\n     uvec3 extents;\n+    int fill0;\n     float other;\n   } block{\n       v_self.extents(),\n+      0,\n       other_val,\n   };\n \n@@ -177,7 +181,7 @@ Tensor& arithmetic_scalar_(\n   return self_arg;\n }\n \n-Tensor arithmetic_tensor(\n+Tensor binary_op_tensor(\n     const Tensor& self_arg,\n     const Tensor& other_arg,\n     const c10::optional<Scalar>& alpha_arg,\n@@ -249,7 +253,7 @@ Tensor arithmetic_tensor(\n   return convert(v_output);\n }\n \n-Tensor quantized_arithmetic_tensor(\n+Tensor quantized_binary_op_tensor(\n     const Tensor& self_arg,\n     const Tensor& other_arg,\n     const double scale,\n@@ -337,7 +341,7 @@ Tensor quantized_arithmetic_tensor(\n   return convert_quantized(v_output);\n }\n \n-Tensor& arithmetic_tensor_(\n+Tensor& binary_op_tensor_(\n     Tensor& self_arg,\n     const Tensor& other_arg,\n     const c10::optional<Scalar>& alpha_arg,\n@@ -367,16 +371,20 @@ Tensor& arithmetic_tensor_(\n \n   const double alpha = alpha_arg ? alpha_arg->to<double>() : 1.0;\n   const struct Block final {\n-    uvec3 extents;\n-    uint32_t channelSize;\n-    uvec3 inputExtents;\n-    uint32_t channelBatchSizeOther;\n+    uvec4 input_tensor_size;\n+    uvec4 other_tensor_size;\n     float alpha;\n   } block{\n-      v_self.extents(),\n-      get_dim<Dim4D::Channel>(v_self),\n-      v_other.extents(),\n-      get_dim<Dim4D::Channel>(other) * get_dim<Dim4D::Batch>(other),\n+      {get_dim<Dim4D::Width>(v_self),\n+       get_dim<Dim4D::Height>(v_self),\n+       get_dim<Dim4D::Channel>(v_self),\n+       get_dim<Dim4D::Batch>(v_self)},\n+\n+      {get_dim<Dim4D::Width>(v_other),\n+       get_dim<Dim4D::Height>(v_other),\n+       get_dim<Dim4D::Channel>(v_other),\n+       get_dim<Dim4D::Batch>(v_other)},\n+      // alpha\n       safe_downcast<float>(alpha),\n   };\n \n@@ -410,12 +418,12 @@ Tensor add_scalar(\n     const Tensor& self_arg,\n     const Scalar& other,\n     const Scalar& alpha) {\n-  return arithmetic_scalar(\n+  return binary_op_scalar(\n       self_arg, other, c10::optional<Scalar>(alpha), VK_KERNEL(add_scalar));\n }\n \n Tensor& add_scalar_(Tensor& self, const Scalar& other, const Scalar& alpha) {\n-  return arithmetic_scalar_(\n+  return binary_op_scalar_(\n       self, other, c10::optional<Scalar>(alpha), VK_KERNEL(add_scalar_));\n }\n \n@@ -424,7 +432,7 @@ Tensor quantized_add(\n     const Tensor& other_arg,\n     const double scale,\n     const int64_t zero_point) {\n-  return quantized_arithmetic_tensor(\n+  return quantized_binary_op_tensor(\n       self_arg, other_arg, scale, zero_point, VK_KERNEL(quantized_add));\n }\n \n@@ -433,7 +441,7 @@ Tensor quantized_sub(\n     const Tensor& other_arg,\n     const double scale,\n     const int64_t zero_point) {\n-  return quantized_arithmetic_tensor(\n+  return quantized_binary_op_tensor(\n       self_arg, other_arg, scale, zero_point, VK_KERNEL(quantized_sub));\n }\n \n@@ -442,7 +450,7 @@ Tensor quantized_mul(\n     const Tensor& other_arg,\n     const double scale,\n     const int64_t zero_point) {\n-  return quantized_arithmetic_tensor(\n+  return quantized_binary_op_tensor(\n       self_arg, other_arg, scale, zero_point, VK_KERNEL(quantized_mul));\n }\n \n@@ -451,7 +459,7 @@ Tensor quantized_div(\n     const Tensor& other_arg,\n     const double scale,\n     const int64_t zero_point) {\n-  return quantized_arithmetic_tensor(\n+  return quantized_binary_op_tensor(\n       self_arg, other_arg, scale, zero_point, VK_KERNEL(quantized_div));\n }\n \n@@ -459,7 +467,7 @@ Tensor add_tensor(\n     const Tensor& self_arg,\n     const Tensor& other_arg,\n     const Scalar& alpha) {\n-  return arithmetic_tensor(\n+  return binary_op_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(add));\n }\n \n@@ -467,7 +475,7 @@ Tensor& add_tensor_(\n     Tensor& self,\n     const Tensor& other_arg,\n     const Scalar& alpha) {\n-  return arithmetic_tensor_(\n+  return binary_op_tensor_(\n       self, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(add_));\n }\n \n@@ -475,7 +483,7 @@ Tensor sub_scalar(\n     const Tensor& self_arg,\n     const Scalar& other,\n     const Scalar& alpha) {\n-  return arithmetic_scalar(\n+  return binary_op_scalar(\n       self_arg,\n       other,\n       c10::optional<Scalar>(-1 * alpha.to<float>()),\n@@ -483,7 +491,7 @@ Tensor sub_scalar(\n }\n \n Tensor& sub_scalar_(Tensor& self, const Scalar& other, const Scalar& alpha) {\n-  return arithmetic_scalar_(\n+  return binary_op_scalar_(\n       self,\n       other,\n       c10::optional<Scalar>(-1 * alpha.to<float>()),\n@@ -494,7 +502,7 @@ Tensor sub_tensor(\n     const Tensor& self_arg,\n     const Tensor& other_arg,\n     const Scalar& alpha) {\n-  return arithmetic_tensor(\n+  return binary_op_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(sub));\n }\n \n@@ -502,32 +510,32 @@ Tensor& sub_tensor_(\n     Tensor& self,\n     const Tensor& other_arg,\n     const Scalar& alpha) {\n-  return arithmetic_tensor_(\n+  return binary_op_tensor_(\n       self, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(sub_));\n }\n \n Tensor mul_scalar(const Tensor& self_arg, const Scalar& other) {\n-  return arithmetic_scalar(\n+  return binary_op_scalar(\n       self_arg, other, c10::optional<Scalar>(), VK_KERNEL(mul_scalar));\n }\n \n Tensor& mul_scalar_(Tensor& self, const Scalar& other) {\n-  return arithmetic_scalar_(\n+  return binary_op_scalar_(\n       self, other, c10::optional<Scalar>(), VK_KERNEL(mul_scalar_));\n }\n \n Tensor mul_tensor(const Tensor& self_arg, const Tensor& other_arg) {\n-  return arithmetic_tensor(\n+  return binary_op_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(), VK_KERNEL(mul));\n }\n \n Tensor& mul_tensor_(Tensor& self, const Tensor& other_arg) {\n-  return arithmetic_tensor_(\n+  return binary_op_tensor_(\n       self, other_arg, c10::optional<Scalar>(), VK_KERNEL(mul_));\n }\n \n Tensor div_scalar(const Tensor& self_arg, const Scalar& other) {\n-  return arithmetic_scalar(\n+  return binary_op_scalar(\n       self_arg,\n       1.0 / other.to<float>(),\n       c10::optional<Scalar>(),\n@@ -535,7 +543,7 @@ Tensor div_scalar(const Tensor& self_arg, const Scalar& other) {\n }\n \n Tensor& div_scalar_(Tensor& self, const Scalar& other) {\n-  return arithmetic_scalar_(\n+  return binary_op_scalar_(\n       self,\n       1.0 / other.to<float>(),\n       c10::optional<Scalar>(),\n@@ -543,12 +551,12 @@ Tensor& div_scalar_(Tensor& self, const Scalar& other) {\n }\n \n Tensor div_tensor(const Tensor& self_arg, const Tensor& other_arg) {\n-  return arithmetic_tensor(\n+  return binary_op_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(), VK_KERNEL(div));\n }\n \n Tensor& div_tensor_(Tensor& self, const Tensor& other_arg) {\n-  return arithmetic_tensor_(\n+  return binary_op_tensor_(\n       self, other_arg, c10::optional<Scalar>(), VK_KERNEL(div_));\n }\n \n"
  },
  {
    "number": 105378,
    "title": "[7/n][FSDP] make use_dtensor=True work with offload_to_cpu=True for load_state_dict",
    "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @zhaojuanmao @mrshenli @rohan-varma @awgu",
    "merge_commit_sha": "1dd0e914f5138479ac31bfafdbbaa64256365471",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105378",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105378/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105378.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105378.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105378/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105378/comments",
    "labels": [
      "module: fsdp",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-18T00:11:12.304145Z",
    "state": "open",
    "patch": "From dac7f0b68f3151b23fe7c2e08408974aab7d3081 Mon Sep 17 00:00:00 2001\nFrom: Iris <wz337@cornell.edu>\nDate: Tue, 18 Jul 2023 00:09:31 +0000\nSubject: [PATCH] Finish TODO: make use_dtensor=True work with\n offload_to_cpu=True.\n\n---\n .../distributed/fsdp/test_fsdp_dtensor_state_dict.py |  8 ++++----\n torch/distributed/fsdp/_state_dict_utils.py          | 12 +++---------\n 2 files changed, 7 insertions(+), 13 deletions(-)\n\ndiff --git a/test/distributed/fsdp/test_fsdp_dtensor_state_dict.py b/test/distributed/fsdp/test_fsdp_dtensor_state_dict.py\nindex e85197c3823736..971ac065032a59 100644\n--- a/test/distributed/fsdp/test_fsdp_dtensor_state_dict.py\n+++ b/test/distributed/fsdp/test_fsdp_dtensor_state_dict.py\n@@ -134,8 +134,8 @@ def test_dtensor_sharded_model_state_dict(self, offload_to_cpu):\n \n     @with_comms\n     @skip_if_lt_x_gpu(2)\n-    @parametrize(\"map_location\", [\"cpu\", \"cuda\"])\n-    def test_dtensor_sharded_model_load_state_dict(self, map_location):\n+    @parametrize(\"offload_to_cpu\", [True, False])\n+    def test_dtensor_sharded_model_load_state_dict(self, offload_to_cpu):\n         model = FSDP(TestDummyModel().cuda())\n         optim = torch.optim.Adam(model.parameters(), lr=0.1)\n \n@@ -144,7 +144,7 @@ def test_dtensor_sharded_model_load_state_dict(self, map_location):\n             StateDictType.SHARDED_STATE_DICT,\n             state_dict_config=ShardedStateDictConfig(\n                 use_dtensor=True,\n-                offload_to_cpu=False,\n+                offload_to_cpu=offload_to_cpu,\n             ),\n         )\n \n@@ -160,7 +160,7 @@ def test_dtensor_sharded_model_load_state_dict(self, map_location):\n         # Load ref_state_dict back.\n         checkpoint.seek(0)\n         # Test both parameters in state_dict are loaded to CPU and GPU.\n-        load_ref_state_dict = torch.load(checkpoint, map_location=map_location)\n+        load_ref_state_dict = torch.load(checkpoint)\n         model.load_state_dict(load_ref_state_dict)\n         new_state_dict = model.state_dict()\n \ndiff --git a/torch/distributed/fsdp/_state_dict_utils.py b/torch/distributed/fsdp/_state_dict_utils.py\nindex 39e60400a98263..8c228004f0b611 100644\n--- a/torch/distributed/fsdp/_state_dict_utils.py\n+++ b/torch/distributed/fsdp/_state_dict_utils.py\n@@ -16,7 +16,7 @@\n     Shard,\n     ShardedTensor,\n )\n-from torch.distributed._tensor import DTensor, Replicate, Shard as DShard\n+from torch.distributed._tensor import DTensor, Replicate\n \n from torch.distributed.distributed_c10d import _get_pg_default_device\n from torch.distributed.fsdp._common_utils import (\n@@ -642,14 +642,8 @@ def _sharded_pre_load_state_dict_hook(\n             tensor = tensor.narrow(0, 0, param_numel).reshape(param.size())\n             state_dict[fqn_from_global_root] = tensor\n         else:\n-            # TODO: make use_dtensor=True work with offload_to_cpu=True.\n-            local_tensor = param.to_local()\n-            if param.device != local_tensor.device:\n-                # construct from a cpu local tensor with cuda device mesh\n-                # will automatically convert the dist tensor to cuda\n-                param = DTensor.from_local(\n-                    local_tensor, fsdp_state._device_mesh, [DShard(0)]\n-                )\n+            if param.device != fsdp_state._device_mesh.device_type:\n+                param = param.to(fsdp_state._device_mesh.device_type)\n \n             param = param.redistribute(\n                 device_mesh=param.device_mesh, placements=[Replicate()]\n"
  },
  {
    "number": 105377,
    "title": "[quant][pt2e] rename _quantize_pt2e to quantize_pt2e",
    "body": "Summary: att\n\nTest Plan: CIs\n\nReviewed By: andrewor14\n\nDifferential Revision: D47234357\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "1fabf30e2c71c928a5c43969c00f9068e4e93ff0",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105377",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105377/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105377.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105377.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105377/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105377/comments",
    "labels": [
      "Merged",
      "fb-exported",
      "ciflow/trunk",
      "release notes: quantization",
      "module: inductor"
    ],
    "_event_time": "2023-07-17T23:58:57.295768Z",
    "state": "closed",
    "patch": "From 882fc9d61cd21707898ff940f85195f30a0829b9 Mon Sep 17 00:00:00 2001\nFrom: Jerry Zhang <jerryzh@meta.com>\nDate: Mon, 17 Jul 2023 21:07:40 -0700\nSubject: [PATCH] [quant][pt2e] rename _quantize_pt2e to quantize_pt2e\n (#105377)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105377\n\natt\n\nTest Plan: CIs\n\nReviewed By: andrewor14\n\nDifferential Revision: D47234357\n\nfbshipit-source-id: adfa93bdd2bb85e6f2d2780c82ec62f01364fb1d\n---\n test/inductor/test_inductor_freezing.py        |  2 +-\n test/quantization/pt2e/test_quantize_pt2e.py   |  2 +-\n .../quantization/pt2e/test_quantize_pt2e_fx.py | 18 +++++++++---------\n .../pt2e/test_x86inductor_quantizer.py         |  2 +-\n .../{_quantize_pt2e.py => quantize_pt2e.py}    |  8 +++++++-\n 5 files changed, 19 insertions(+), 13 deletions(-)\n rename torch/ao/quantization/{_quantize_pt2e.py => quantize_pt2e.py} (96%)\n\ndiff --git a/test/inductor/test_inductor_freezing.py b/test/inductor/test_inductor_freezing.py\nindex 1764883efdaa56..610aa2ea9a24e5 100644\n--- a/test/inductor/test_inductor_freezing.py\n+++ b/test/inductor/test_inductor_freezing.py\n@@ -17,8 +17,8 @@\n from torch._inductor import config\n from torch._inductor.compile_fx import compile_fx\n from torch._inductor.utils import override_lowering, run_and_get_code\n-from torch.ao.quantization._quantize_pt2e import convert_pt2e, prepare_pt2e_quantizer\n from torch.ao.quantization.pt2e.quantizer import X86InductorQuantizer\n+from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e_quantizer\n from torch.testing import FileCheck\n from torch.testing._internal.common_quantization import (\n     skipIfNoDynamoSupport,\ndiff --git a/test/quantization/pt2e/test_quantize_pt2e.py b/test/quantization/pt2e/test_quantize_pt2e.py\nindex f3eaf2916ea341..df66771989f451 100644\n--- a/test/quantization/pt2e/test_quantize_pt2e.py\n+++ b/test/quantization/pt2e/test_quantize_pt2e.py\n@@ -33,7 +33,7 @@\n from torch.ao.quantization.pt2e.quantizer.qnnpack_quantizer import (\n     get_symmetric_quantization_config,\n )\n-from torch.ao.quantization._quantize_pt2e import (\n+from torch.ao.quantization.quantize_pt2e import (\n     _convert_to_reference_decomposed_fx,\n     convert_pt2e,\n     prepare_pt2e_quantizer,\ndiff --git a/test/quantization/pt2e/test_quantize_pt2e_fx.py b/test/quantization/pt2e/test_quantize_pt2e_fx.py\nindex 4ddf108ab607a4..4cf25194c26a72 100644\n--- a/test/quantization/pt2e/test_quantize_pt2e_fx.py\n+++ b/test/quantization/pt2e/test_quantize_pt2e_fx.py\n@@ -13,9 +13,9 @@\n     QConfigMapping,\n     default_per_channel_symmetric_qnnpack_qconfig,\n )\n-from torch.ao.quantization._quantize_pt2e import (\n+from torch.ao.quantization.quantize_pt2e import (\n     convert_pt2e,\n-    prepare_pt2e,\n+    _prepare_pt2e_deprecated,\n )\n from torch.ao.quantization.backend_config import get_qnnpack_backend_config\n from torch.ao.quantization.backend_config._qnnpack_pt2e import (\n@@ -77,7 +77,7 @@ def forward(self, x):\n                 QConfigMapping().set_global(qconfig).set_module_name(\"conv2\", None)\n             )\n             backend_config = get_qnnpack_pt2e_backend_config()\n-            m = prepare_pt2e(m, qconfig_mapping, example_inputs, backend_config)\n+            m = _prepare_pt2e_deprecated(m, qconfig_mapping, example_inputs, backend_config)\n             m(*example_inputs)\n             m = convert_pt2e(m)\n             m(*example_inputs)\n@@ -131,7 +131,7 @@ def forward(self, x):\n             qconfig = get_default_qconfig(\"qnnpack\")\n             qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Conv2d, qconfig)\n             backend_config = get_qnnpack_pt2e_backend_config()\n-            m = prepare_pt2e(m, qconfig_mapping, example_inputs, backend_config)\n+            m = _prepare_pt2e_deprecated(m, qconfig_mapping, example_inputs, backend_config)\n             m(*example_inputs)\n             m = convert_pt2e(m)\n             m(*example_inputs)\n@@ -192,7 +192,7 @@ def forward(self, x):\n             qconfig = get_default_qconfig(\"qnnpack\")\n             qconfig_mapping = QConfigMapping().set_global(qconfig)\n             backend_config = get_qnnpack_pt2e_backend_config()\n-            m = prepare_pt2e(m, qconfig_mapping, example_inputs, backend_config)\n+            m = _prepare_pt2e_deprecated(m, qconfig_mapping, example_inputs, backend_config)\n \n             # 1. Check graph nodes:\n             # - args[0] of t should be the weight observer\n@@ -254,7 +254,7 @@ def forward(self, x):\n             qconfig = get_default_qconfig(\"qnnpack\")\n             qconfig_mapping = QConfigMapping().set_global(qconfig)\n             backend_config = get_qnnpack_pt2e_backend_config()\n-            m = prepare_pt2e(m, qconfig_mapping, example_inputs, backend_config)\n+            m = _prepare_pt2e_deprecated(m, qconfig_mapping, example_inputs, backend_config)\n             # make sure it runs\n             m(*example_inputs)\n \n@@ -356,7 +356,7 @@ def forward(self, x):\n                     qconfig = get_default_qconfig(\"x86\")\n                     qconfig_mapping = QConfigMapping().set_global(qconfig)\n                     backend_config = get_x86_inductor_pt2e_backend_config()\n-                    prepare_module = prepare_pt2e(\n+                    prepare_module = _prepare_pt2e_deprecated(\n                         export_module, qconfig_mapping, example_inputs, backend_config\n                     )\n                     prepare_module(*example_inputs)\n@@ -482,7 +482,7 @@ def forward(self, x):\n                     qconfig = get_default_qconfig(\"x86\")\n                     qconfig_mapping = QConfigMapping().set_global(qconfig)\n                     backend_config_inductor = get_x86_inductor_pt2e_backend_config()\n-                    prepared_model = prepare_pt2e(\n+                    prepared_model = _prepare_pt2e_deprecated(\n                         exported_model,\n                         qconfig_mapping,\n                         example_inputs,\n@@ -535,7 +535,7 @@ def test_resnet18(self):\n             qconfig_mapping = QConfigMapping().set_global(qconfig)\n             before_fusion_result = m(*example_inputs)\n \n-            m = prepare_pt2e(m, qconfig_mapping, example_inputs, backend_config)\n+            m = _prepare_pt2e_deprecated(m, qconfig_mapping, example_inputs, backend_config)\n \n             # checking that we inserted observers correctly for maxpool operator (input and\n             # output share observer instance)\ndiff --git a/test/quantization/pt2e/test_x86inductor_quantizer.py b/test/quantization/pt2e/test_x86inductor_quantizer.py\nindex 5030559db0fc75..6d8e9b0cbd1dcc 100644\n--- a/test/quantization/pt2e/test_x86inductor_quantizer.py\n+++ b/test/quantization/pt2e/test_x86inductor_quantizer.py\n@@ -6,7 +6,7 @@\n from torch.ao.quantization.pt2e.quantizer import (\n     X86InductorQuantizer,\n )\n-from torch.ao.quantization._quantize_pt2e import (\n+from torch.ao.quantization.quantize_pt2e import (\n     convert_pt2e,\n     prepare_pt2e_quantizer,\n )\ndiff --git a/torch/ao/quantization/_quantize_pt2e.py b/torch/ao/quantization/quantize_pt2e.py\nsimilarity index 96%\nrename from torch/ao/quantization/_quantize_pt2e.py\nrename to torch/ao/quantization/quantize_pt2e.py\nindex 5f6b7b11750826..d78904ebffae99 100644\n--- a/torch/ao/quantization/_quantize_pt2e.py\n+++ b/torch/ao/quantization/quantize_pt2e.py\n@@ -45,7 +45,13 @@\n \n from typing import Any, Tuple\n \n-def prepare_pt2e(\n+__all__ = [\n+    \"prepare_pt2e_quantizer\",\n+    \"prepare_qat_pt2e_quantizer\",\n+    \"convert_pt2e\",\n+]\n+\n+def _prepare_pt2e_deprecated(\n     model: GraphModule,\n     qconfig_mapping: QConfigMapping,\n     example_inputs: Tuple[Any, ...],\n"
  },
  {
    "number": 105376,
    "title": "Restructure torch.compile docs",
    "body": "Current torch.compile docs have become a bit of a mess with the docs expanded in the left nav. This PR moves them under the torch.compiler menu item in the left nav. A bunch of rewrites were made in collaboration with @msaroufim to address formatting issues, latest updates that moved some of the APIs to the public torch.compiler namespace were addressed as well. The documentation is broken down in three categories that address three main audiences: PyTorch users, Pytorch Developers and PyTorch backend vendors. While, the user-facing documentation was significantly rewritten, dev docs and vendor docs kept mostly untouched. This can be addressed in the follow up PRs. \n\ncc @carljparker @albanD",
    "merge_commit_sha": "f99fb78567231ac64fb9d99779f836f774468f8d",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105376",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105376/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105376.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105376.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105376/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105376/comments",
    "labels": [
      "module: docs",
      "skip-pr-sanity-checks",
      "topic: not user facing"
    ],
    "_event_time": "2023-07-17T23:50:49.531180Z",
    "state": "open",
    "patch": "From eef4b85338b2d254724c8f9f4b5f85f838c4e8ff Mon Sep 17 00:00:00 2001\nFrom: Svetlana Karslioglu <svekars@fb.com>\nDate: Mon, 17 Jul 2023 16:45:12 -0700\nSubject: [PATCH 1/3] Restructure torch.compile docs\n\n---\n docs/source/compile/faq.rst                   | 384 --------------\n docs/source/compile/fine_grained_apis.rst     | 118 -----\n docs/source/compile/get-started.rst           | 165 ------\n docs/source/compile/index.rst                 |  77 ---\n docs/source/compile/inductor_profiling.rst    | 167 ------\n docs/source/compile/technical-overview.rst    |  41 --\n .../compile/torchfunc-and-torchcompile.rst    |  78 ---\n docs/source/index.rst                         |  27 +-\n docs/source/torch.compiler.rst                | 116 +++++\n .../{compiler.rst => torch.compiler_api.rst}  |  12 +-\n ....compiler_best_practices_for_backends.rst} |   0\n ...rst => torch.compiler_cudagraph_trees.rst} |   0\n ...rst => torch.compiler_custom_backends.rst} |   0\n ...p-dive.rst => torch.compiler_deepdive.rst} |  47 +-\n ....rst => torch.compiler_dynamic_shapes.rst} |   0\n ...sor.rst => torch.compiler_fake_tensor.rst} |   0\n docs/source/torch.compiler_faq.rst            | 491 ++++++++++++++++++\n .../source/torch.compiler_fine_grain_apis.rst | 100 ++++\n docs/source/torch.compiler_get_started.rst    | 151 ++++++\n ...rst => torch.compiler_guards_overview.rst} |   0\n .../torch.compiler_inductor_profiling.rst     | 177 +++++++\n docs/source/{ir.rst => torch.compiler_ir.rst} |   0\n ...odule.rst => torch.compiler_nn_module.rst} |   0\n ... torch.compiler_performance_dashboard.rst} |   0\n ...orch.compiler_profiling_torch_compile.rst} |   0\n ...rst => torch.compiler_transformations.rst} |   0\n ...rst => torch.compiler_troubleshooting.rst} | 141 ++---\n 27 files changed, 1141 insertions(+), 1151 deletions(-)\n delete mode 100644 docs/source/compile/faq.rst\n delete mode 100644 docs/source/compile/fine_grained_apis.rst\n delete mode 100644 docs/source/compile/get-started.rst\n delete mode 100644 docs/source/compile/index.rst\n delete mode 100644 docs/source/compile/inductor_profiling.rst\n delete mode 100644 docs/source/compile/technical-overview.rst\n delete mode 100644 docs/source/compile/torchfunc-and-torchcompile.rst\n create mode 100644 docs/source/torch.compiler.rst\n rename docs/source/{compiler.rst => torch.compiler_api.rst} (62%)\n rename docs/source/{compile/best-practices-for-backends.rst => torch.compiler_best_practices_for_backends.rst} (100%)\n rename docs/source/{compile/cudagraph_trees.rst => torch.compiler_cudagraph_trees.rst} (100%)\n rename docs/source/{compile/custom-backends.rst => torch.compiler_custom_backends.rst} (100%)\n rename docs/source/{compile/deep-dive.rst => torch.compiler_deepdive.rst} (71%)\n rename docs/source/{compile/dynamic-shapes.rst => torch.compiler_dynamic_shapes.rst} (100%)\n rename docs/source/{compile/fake-tensor.rst => torch.compiler_fake_tensor.rst} (100%)\n create mode 100644 docs/source/torch.compiler_faq.rst\n create mode 100644 docs/source/torch.compiler_fine_grain_apis.rst\n create mode 100644 docs/source/torch.compiler_get_started.rst\n rename docs/source/{compile/guards-overview.rst => torch.compiler_guards_overview.rst} (100%)\n create mode 100644 docs/source/torch.compiler_inductor_profiling.rst\n rename docs/source/{ir.rst => torch.compiler_ir.rst} (100%)\n rename docs/source/{compile/nn-module.rst => torch.compiler_nn_module.rst} (100%)\n rename docs/source/{compile/performance-dashboard.rst => torch.compiler_performance_dashboard.rst} (100%)\n rename docs/source/{compile/profiling_torch_compile.rst => torch.compiler_profiling_torch_compile.rst} (100%)\n rename docs/source/{compile/transformations.rst => torch.compiler_transformations.rst} (100%)\n rename docs/source/{compile/troubleshooting.rst => torch.compiler_troubleshooting.rst} (83%)\n\ndiff --git a/docs/source/compile/faq.rst b/docs/source/compile/faq.rst\ndeleted file mode 100644\nindex e2fc6129561e09..00000000000000\n--- a/docs/source/compile/faq.rst\n+++ /dev/null\n@@ -1,384 +0,0 @@\n-Frequently Asked Questions\n-==========================\n-**Author**: `Mark Saroufim <https://github.com/msaroufim>`_\n-\n-At a high level, the PyTorch 2.0 stack consists of a graph capture from\n-Python code using dynamo and a backend compiler. In this example the\n-backend compiler consists of backward graph tracing using AOTAutograd\n-and graph lowering using TorchInductor. There are of course many more\n-compilers available `here <https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#existing-backend>`__\n-but for this document we will focus on inductor as a motivating example.\n-\n-Torchdynamo supports training, using AotAutograd to capture backwards:\n-\n-   1. the ``.forward()`` graph and ``optimizer.step()`` is captured by torchdynamo\u2019s python evalframe frontend\n-   2. for each segment of ``.forward()`` that torchdynamo captures, it uses AotAutograd to generate a backward graph segment\n-   3. each pair of forward, backward graph are (optionally) min-cut partitioned to save the minimal state between forward/backward\n-   4. the forward, backward pairs are wrapped in autograd.function modules 5. usercode calling\\ ``.backward()`` still triggers eager\u2019s autograd engine, which runs each \u2018compiled backward\u2019 graph as if it were one op, also running any non-compiled eager ops\u2019 .backward() functions\n-\n-Do you support Distributed code?\n---------------------------------\n-\n-DDP has been tested and works, support for other distributed training\n-libraries is under discussion.\n-\n-The main reason why Distributed code is challenging with dynamo is\n-because AOTAutograd unrolls both the forward and backward pass and\n-provides 2 graphs for backends to optimize. This is a problem for\n-distributed code because we\u2019d like to ideally overlap communication\n-operations with computations. Eager pytorch accomplishes this in\n-different ways for DDP/FSDP- using autograd hooks, module hooks, and\n-modifications/mutations of module states. In a naive application of\n-dynamo, hooks that should run directly after an operation during\n-backwards may be delayed until after the entire compiled region of\n-backwards ops, due to how AOTAutograd compiled functions interact with\n-dispatcher hooks.\n-\n-The basic strategy for optimizing DDP with Dynamo is outlined in\n-`distributed.py <https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/optimizations/distributed.py>`__\n-where the main idea will be to graph break on `DDP bucket\n-boundaries <https://pytorch.org/docs/stable/notes/ddp.html#internal-design>`__.\n-\n-When each node in DDP needs to synchronize its weights with the other\n-nodes it organizes its gradients and parameters into buckets which\n-reduces communication times and allows a node to broadcast a fraction of\n-its gradients to other waiting nodes.\n-\n-Graph breaks in distributed code mean you can expect dynamo and its\n-backends to optimize the compute overhead of a distributed program but\n-not its communication overhead. Graph-breaks may interfere with\n-compilation speedups, if the reduced graph-size robs the compiler of\n-fusion opportunities. However, there are diminishing returns with\n-increasing graph size since most of the current compute optimizations\n-are local fusions. So in practice this approach may be sufficient.\n-\n-Do I still need to export whole graphs?\n----------------------------------------\n-\n-For the vast majority of models you probably don\u2019t and you can use\n-``torch._dynamo()`` optimize as is but there are a few situations where\n-full graphs are necessary and you can can ensure a full graph by simply\n-running ``torch.dynamo(..., nopython=True)`` \\* Large scale training\n-runs, think $250K+ that require pipeline parallelism and other advanced\n-sharding strategies \\* Inference optimizers like\n-`TensorRT <https://github.com/pytorch/TensorRT>`__ or\n-`AITemplate <https://github.com/facebookincubator/AITemplate>`__ that rely\n-on fusing much more aggressively than training optimizers \\* Mobile training or\n-inference.\n-\n-Future work will include tracing communication operations into graphs,\n-coordinating these operations with compute optimizations, and optimizing\n-the communication operations.\n-\n-Why is my code crashing?\n-------------------------\n-\n-If your code ran just fine without dynamo and started to crash with it\n-enabled then the most important first step is figuring out which part of\n-the stack your failure occurred in so try running things in the below\n-order and only try the next step if the previous step succeeded.\n-\n-1. ``torch.compile(..., backend=\"eager\")`` which only runs torchdynamo forward graph\n-   capture and then runs the captured graph with PyTorch. If this fails\n-   then there\u2019s an issue with TorchDynamo.\n-\n-2. ``torch.compile(..., backend=\"aot_eager\")``\n-   which runs torchdynamo to capture a forward graph, and then AOTAutograd\n-   to trace the backward graph without any additional backend compiler\n-   steps. PyTorch eager will then be used to run the forward and backward\n-   graphs. If this fails then there\u2019s an issue with AOTAutograd.\n-\n-3. ``torch.compile(..., backend=\"inductor\")`` which runs torchdynamo to capture a\n-   forward graph, and then AOTAutograd to trace the backward graph with the\n-   TorchInductor compiler. If this fails then there\u2019s an issue with TorchInductor\n-\n-TorchDynamo Errors\n-~~~~~~~~~~~~~~~~~~\n-\n-If the error that is generated occurs with the ``\"eager\"`` backend, then\n-torchdynamo is the most likely source of the error.\n-\n-To debug these issues we recommend setting\n-``torch._dynamo.config.verbose=True`` to get a full stack trace to both\n-the error in torchdynamo and the user code. In addition to this flag,\n-you can also set the ``log_level`` of torchdynamo through\n-``torch._dynamo.config.log_level``. The available levels are the\n-following: - ``logging.DEBUG``: Print every instruction that is\n-encountered in addition to all below log levels - ``logging.INFO``:\n-Print each function that is compiled (original and modified bytecode)\n-and the graph that is captured in addition to all below log levels -\n-``logging.WARNING`` (default): Print graph breaks in addition to all\n-below log levels - ``logging.ERROR``: Print errors only\n-\n-If a model is sufficiently large, the logs can become overwhelming. If\n-an error occurs deep within a model\u2019s python code, it can be useful to\n-execute only the frame in which the error occurs to enable easier\n-debugging. There are 2 tools available to enable this:\n-\n-* ``env TORCHDYNAMO_DEBUG_FUNCTION=<desired_function_name>`` will only run TorchDynamo on functions with that name.\n-\n-* ``env torch._dynamo.config.replay_record_enabled = True``) which dumps an execution record when an error is encountered. This record can then be replayed to run only the frame where an error occurred.\n-\n-TorchInductor Errors\n---------------------\n-\n-With TorchInductor as the chosen backend, AOTAutograd is used to\n-generate the backward graph from the forward graph captured by\n-torchdynamo. It\u2019s important to note that errors can occur during this\n-tracing and also while TorchInductor lowers the forward and backward\n-graphs to GPU code or C++.\n-\n-A model can often consist of hundreds or thousands of FX nodes, so\n-narrowing the exact nodes where this problem occurred can be very\n-difficult which is why we highly recommend you use our minifier to\n-create tiny reproducible examples of failures you\u2019re seeing. We can\n-minify errors that occur either at the AOTAutograd layer or Inductor\n-layer which you should try in the following order.\n-\n-1. ``env TORCHDYNAMO_REPRO_AFTER=\"aot\" python your_model.py``\n-2.  ``env TORCHDYNAMO_REPRO_AFTER=\"dynamo\" python your_model.py``\n-\n-Minifying your error is the quickest path to getting it fixed.\n-\n-The minifier will actually create a ``repro.py`` for you at the location\n-set by ``env TORCHDYNAMO_REPRO_DIR`` so make you have right access to\n-that directory. You can then run ``python repro.py`` and confirm that\n-you are getting the same error.\n-\n-.. note::\n-   For other compilers such as nvfuser, the process is similar but\n-   instead you would leverage ``env TORCHDYNAMO_REPRO_AFTER=\"dynamo\" python your_model.py``.\n-\n-Why is compilation slow?\n-------------------------\n-\n-Dynamo Compilation\n-~~~~~~~~~~~~~~~~~~\n-\n-TorchDynamo has a builtin stats function for collecting and displaying\n-the time spent in each compilation phase. These stats can be accessed by\n-calling ``torch._dynamo.utils.compile_times()`` after executing\n-``torch._dynamo``. By default, this returns a string representation of\n-the compile times spent in each TorchDynamo function by name.\n-\n-Inductor Compilation\n-~~~~~~~~~~~~~~~~~~~~\n-\n-TorchInductor has a builtin stats and trace function for displaying time\n-spent in each compilation phase, output code, output graph visualization\n-and IR dump. ``env TORCH_COMPILE_DEBUG=1 python repro.py``. This is a\n-debugging tool designed to make it easier to debug/understand the\n-internals of TorchInductor with an output that will look something like\n-`this <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\n-\n-Each file in that debug trace can be enabled/disabled via\n-``torch._inductor.config.trace.*``. The profile and the diagram are both\n-disabled by default since they are expensive to generate. See the\n-`example debug directory\n-output <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\n-for more examples.\n-\n-Excessive Recompilation\n-~~~~~~~~~~~~~~~~~~~~~~~\n-\n-When TorchDynamo compiles a function (or part of one), it makes certain\n-assumptions about locals and globals in order to allow compiler\n-optimizations, and expresses these assumptions as guards that check\n-particular values at runtime. If any of these guards fail, Dynamo will\n-recompile that function (or part) up to\n-``torch._dynamo.config.cache_size_limit`` times. If your program is\n-hitting the cache limit, you will first need to determine which guard is\n-failing and what part of your program is triggering it.\n-\n-The `recompilation profiler <#recompilation-profiler>`__ automates the\n-process of setting TorchDynamo\u2019s cache limit to 1 and running your\n-program under an observation-only \u2018compiler\u2019 that records the causes of\n-any guard failures. You should be sure to run your program for at least\n-as long (as many iterations) as you were running when you ran into\n-trouble, and the profiler will accumulate statistics over this duration.\n-\n-.. code-block:: python\n-\n-   from torch._dynamo.utils import CompileProfiler\n-\n-   def my_model():\n-       ...\n-\n-   with CompileProfiler() as prof:\n-       profiler_model = torch.compile(my_model, backend=prof)\n-       profiler_model()\n-       print(prof.report())\n-\n-Many of the reasons for graph breaks and excessive recompilation will be\n-fixed with upcoming support for `tracing dynamic tensor\n-shapes <https://docs.google.com/document/d/1QJB-GOnbv-9PygGlOMXwiO9K6vVNm8sNg_olixJ9koc/edit?usp=sharing>`__,\n-more careful choices for guards and better tuned heuristics.\n-\n-Why are you recompiling in production?\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n-\n-In some cases, you may not want unexpected compiles after a program has\n-warmed up. For example, if you are serving production traffic in a\n-latency critical application. For this, TorchDynamo provides an\n-alternate mode where prior compiled graphs are used, but no new ones are\n-generated:\n-\n-.. code-block:: python\n-\n-   frozen_toy_example = dynamo.run(toy_example)\n-   frozen_toy_example(torch.randn(10), torch.randn(10))\n-\n-How are you speeding up my code?\n---------------------------------\n-\n-There are 3 major ways to accelerate PyTorch code:\n-\n-1. Kernel fusion via vertical fusions which fuse sequential operations to avoid\n-   excessive read/writes. For example, fuse 2 subsequent cosines means you\n-   can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:\n-   the simplest example being batching where a single matrix is multiplied\n-   with a batch of examples but the more general scenario is a grouped GEMM\n-   where a group of matrix multiplications are scheduled together\n-\n-2. Out of order execution: A general optimization for compilers, by looking ahead\n-   at the exact data dependencies within a graph we can decide on the most\n-   opportune time to execute a node and which buffers can be reused\n-\n-3. Automatic work placement: Similar of the out of order execution point,\n-   but by matching nodes of a graph to resources like physical hardware or\n-   memory we can design an appropriate schedule\n-\n-The above are general principles for accelerating PyTorch code but\n-different backends will each make different tradeoffs on what to\n-optimize. For example Inductor first takes care of fusing whatever it\n-can and only then generates `Triton <https://openai.com/blog/triton/>`__\n-kernels. It can also\n-\n-Triton in addition offers speedups because of automatic memory\n-coalescing, memory management and scheduling within each Streaming\n-Multiprocessor and has been designed to handle tiled computations.\n-\n-However, regardless of the backend you use it\u2019s best to use a benchmark\n-and see approach so try out the PyTorch profiler, visually inspect the\n-generated kernels and try to see what\u2019s going on for yourself.\n-\n-Why am I not seeing speedups?\n------------------------------\n-\n-Graph Breaks\n-~~~~~~~~~~~~\n-\n-The main reason you won\u2019t see the speedups you\u2019d like to by using dynamo\n-is excessive graph breaks. So what\u2019s a graph break?\n-\n-Given a program like:\n-\n-.. code-block:: python\n-\n-   def some_fun(x):\n-       ...\n-\n-   torch.compile(some_fun)(x)\n-   ...\n-\n-Torchdynamo will attempt to compile all of the torch/tensor operations\n-within ``some_fun()`` into a single FX graph, but it may fail to capture\n-everything into one graph.\n-\n-Some graph break reasons are insurmountable to TorchDynamo like calling\n-into a C extension other than torch is invisible to torchdynamo, and\n-could do arbitrary things without TorchDynamo being able to introduce\n-necessary guards to ensure that the compiled program would be safe to reuse.\n-\n-   To maximize performance, it\u2019s important to have as few graph breaks\n-   as possible.\n-\n-Identifying the cause of a graph break\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n-\n-To identify all graph breaks in a program and the associated reasons for\n-the breaks, ``torch._dynamo.explain`` can be used. This tool runs\n-TorchDynamo on the supplied function and aggregates the graph breaks\n-that are encountered. Here is an example usage:\n-\n-.. code-block:: python\n-\n-   import torch\n-   import torch._dynamo as dynamo\n-   def toy_example(a, b):\n-       x = a / (torch.abs(a) + 1)\n-       print(\"woo\")\n-       if b.sum() < 0:\n-           b = b * -1\n-       return x * b\n-   explanation, out_guards, graphs, ops_per_graph = dynamo.explain(toy_example, torch.randn(10), torch.randn(10))\n-   print(explanation)\n-   \"\"\"\n-   Dynamo produced 3 graphs, with 2 graph break and 6 ops.\n-    Break reasons:\n-   1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n-      File \"t2.py\", line 16, in toy_example\n-       print(\"woo\")\n-\n-   2. generic_jump\n-      File \"t2.py\", line 17, in toy_example\n-       if b.sum() < 0:\n-    \"\"\"\n-\n-To throw an error on the first graph break encountered you can use\n-disable python fallback by using ``nopython=True``, this should be\n-familiar if you\u2019ve worked with export based compilers.\n-\n-.. code-block:: python\n-\n-   def toy_example(a, b):\n-      ...\n-\n-   torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n-\n-Why didn\u2019t my code recompile when I changed it?\n------------------------------------------------\n-\n-If you went ahead and enabled dynamic shapes via\n-``env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py`` then your code\n-won\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes\n-which avoids recompilations in the case when shapes vary by less than a\n-factor of 2. This is especially useful in scenarios like varying image\n-sizes in CV or variable sequence length in NLP. In inference scenarios\n-it\u2019s often not possible to know what a batch size will be beforehand\n-because you take what you can get from different client apps.\n-\n-In general, TorchDynamo tries very hard not to recompile things\n-unnecessarily so if for example torchdynamo finds 3 graphs and your\n-change only modified one graph then only that graph will recompile. So\n-another tip to avoid potentially slow compilation times is to warmup a\n-model by compiling it once after which subsequent compilations will be\n-much faster. Cold start compile times is still a metric we track\n-visibly.\n-\n-Why am I getting incorrect results?\n------------------------------------\n-\n-Accuracy issues can also be minified if you set the environment variable\n-``TORCHDYNAMO_REPRO_LEVEL=4``, it operates with a similar git bisect\n-model and a full repro might be something like\n-``TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4`` the reason\n-we need this is downstream compilers will codegen code whether it\u2019s\n-Triton code or the C++ backend, the numerics from those downstream\n-compilers can be different in subtle ways yet have dramatic impact on\n-your training stability. So the accuracy debugger is very useful for us\n-to detect bugs in our codegen or with a backend compiler.\n-\n-If you'd like to ensure that random number generation is the same across both torch\n-and triton then you can enable ``torch._inductor.config.fallback_random = True``\n-\n-Why am I getting OOMs?\n-----------------------\n-\n-Dynamo is still an alpha product so there\u2019s a few sources of OOMs and if\n-you\u2019re seeing an OOM try disabling the following configurations in this\n-order and then open an issue on GitHub so we can solve the root problem\n-1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled\n-them by default: ``env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py`` 2.\n-CUDA graphs with Triton are enabled by default in inductor but removing\n-them may alleviate some OOM issues: ``torch._inductor.config.triton.cudagraphs = False``.\ndiff --git a/docs/source/compile/fine_grained_apis.rst b/docs/source/compile/fine_grained_apis.rst\ndeleted file mode 100644\nindex 32845ed9febb1b..00000000000000\n--- a/docs/source/compile/fine_grained_apis.rst\n+++ /dev/null\n@@ -1,118 +0,0 @@\n-TorchDynamo APIs to control fine-grained tracing\n-================================================\n-\n-`torch.compile` performs TorchDynamo tracing on all the user model. However, it is possible that a small part of the model code is not amenable to PT2 compilation stack. And you want to just disable PT2 on that particular portion, while running compilation on the rest of the model. This doc shares the existing APIs that give you such control and the relevant usecases.\n-\n-Existing APIs\n-\n-* torch._dynamo.disable\n-* torch._dynamo.disallow_in_graph\n-* torch._dynamo.graph_break\n-* torch._dynamo.allow_in_graph\n-\n-\n-## Section 1 - Summary Table\n-\n-.. csv-table:: TorchDynamo APIs to control fine-grained tracing\n-   :header: \"API\", \"Description\", \"When to use?\"\n-   :widths: auto\n-\n-   \"torch._dynamo.disable\", \"Disables Dynamo on the decorated function as well as recursively invoked functions.\", \"Excellent for unblocking a user, if a small portion of the model is not PT2-friendly.\"\n-   \"torch._dynamo.disallow_in_graph\", \"Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.\\n\\nThis is suitable for ops, while _dynamo.disable is suitable for decorating functions.\", \"Excellent for both debugging and unblocking if a custom op (like torch.ops.fbgemm.*) is causing issues with the PT2 compile.\"\n-   \"torch._dynamo.allow_in_graph\", \"The annotated callable goes as-is in the TorchDynamo graph, i.e., a black-box for TorchDynamo Dynamo.\\n\\nNote that AOT Autograd will trace through it, so the allow_in_graph is only a Dynamo level concept.\", \"Useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks, autograd.Function. However, each usage of allow_in_graph **must be carefully screened** (no graph breaks, no closures).\"\n-   \"torch._dynamo.graph_break\", \"Adds a graph break. The code before and after the graph break goes through TorchDynamo.\", \"**Rarely useful for deployment** - If you think you need this, most probably you need either `disable` or `disallow_in_graph`.\"\n-\n-\n-\n-## Section 2 - torch._dynamo.disable\n-\n-**tl;dr** - Disables PT2 stack on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\n-\n-**Explanation** - TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function `fn` calls functions `a_fn` and `b_fn`. And `a_fn` calls `aa_fn` and `ab_fn`. In the eager world (no torch.compile), these function frames run as-is. With torch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color).\n-\n-.. figure:: ../_static/img/fine_grained_apis/api_diagram.png\n-   :alt: Callstack diagram of differnet apis.\n-\n-**Usecase** - Now suppose function `a_fn` is causing troubles with `torch.compile`. And this is a non-critical portion of the model. You can use `_dynamo.disable` on function `a_fn`. As shown above, TorchDynamo will stop looking at frames originating from `a_fn` call (white color indicates original Python behavior).\n-\n-**Usage**\n-\n-You can decorate the offending function with `@torch._dynamo.disable`\n-\n-You can also use the non-decorator syntax if you don\u2019t want to change the source code (however avoid this style if possible. Here, you have to take care that all users of the original function are now using the patched version).\n-\n-## Section 3 - torch._dynamo.disallow_in_graph\n-\n-**tl;dr** - Disallows an operator (not the function) to be present in the TorchDynamo extracted graph. Note that this is suitable for operators (and not general functions as in the case of `_dynamo.disable`).\n-\n-**Usecase** - Suppose you compile your model with PT2. TorchDynamo is able to extract a graph, but then you see the downstream compiler failing (like the meta kernel is missing, or some autograd dispatch key is set incorrectly etc) for a particular operator. Then you can mark that operator as `disallow_in_graph`, and TorchDynamo will cause a graph break and run that operator on eager.\n-\n-The catch is that you will have to find the corresponding Dynamo level operator here (and not the aten level operator). See more in the Limitations section of the doc.\n-\n-**Warning** - This is a global flag. So be cautious, if you are comparing different backend compilers. You might have to call `allow_in_graph` for the disallowed op when switching to the other compiler.\n-\n-\n-## Section 4 - torch._dynamo.disallow_in_graph\n-\n-**Usecase** - This is useful when the relevant function frame has some known hard-to-support TorchDynamo feature (like hooks and autograd.Function) and you are confident that downstream PT2 components like AOTAutograd can safely trace through the decorated function. When a function is decorated with `allow_in_graph`, TorchDynamo treats it as a black-box and puts it as-is in the generated graph.\n-\n-\n-**Warning - `allow_in_graph`** skips TorchDynamo completely on the decorated function, skipping all TorchDynamo safety checks (graph breaks, handling closures etc). Therefore, one has to be very careful with `allow_in_graph`. Today downstream components like AOT Autograd rely on TorchDynamo to take care of complex Python features, but `allow_in_graph` bypasses TorchDynamo. If not careful, this could lead to soundness and really hard-to-debug issues.\n-\n-\n-## Section 5 - Limitations\n-\n-\n-All the existing APIs are applied at the TorchDynamo level. Therefore, these APIs have visibility to only what TorchDynamo sees. This can lead to confusing scenarios.\n-\n-For example, `_dynamo.disallow_in_graph` will not work for aten operators because they are visible to AOT Autograd (example - `torch._dynamo.disallow_in_graph(torch.ops.aten.add)` will not work in the above example).\n-\n-\n-## Section 6 - FAQ\n-\n-**FAQ - How do I graph break on a function?**\n-\n-Graph break on a function is not enough to describe what you really want. So, you will have to be more specific about your usecase. From experience, these are the interesting usecases\n-\n-\n-\n-* Do you want to disable PT2 stack on this function frame and the recursively invoked frames? Use `_dynamo.disable`.\n-* Do you want a particular operator (like fbgemm custom ops) to be eager?  Use `_dynamo.disallow_in_graph`\n-\n-Following are rare scenarios\n-\n-\n-\n-* Rare scenario - you want to disable dynamo on the function frame but enable it back on the recursively invoked frames. Use `_dynamo.disable(recursive=False)`\n-* Even rare scenario - you want to prevent inlining of a function frame. Use `_dynamo.graph_break` at the beginning of the function you want to prevent inlining.\n-\n-**FAQ - Difference between disable and disallow-in-graph**\n-\n-Disallow-in-graph works at the level of operators, or more specifically, the operators that you see in the TorchDynamo extracted graphs.\n-\n-Disable works at the function frame level and decides if TorchDynamo should look into the function frame or not.\n-\n-**FAQ - Difference between disable and now-deprecated skip -** You most likely need `_dynamo.disable`. But in an unlikely scenario, you might need even finer control. Suppose you want to disable the tracing on just the function `a_fn`, but want to continue the tracing back in `aa_fn` and `ab_fn`. This is shown below\n-\n-\n-.. figure:: ../_static/img/fine_grained_apis/call_stack_diagram.png\n-   :alt: diagram of torch.compile + disable(a_fn, recursive=False)\n-\n-\n-In this case, you can use `torch._dynamo.disable(recursive=False)`. Earlier, this functionality was provided by `_dynamo.skip.` This is now supported by the `recursive` flag inside disable.\n-\n-\n-# Wishlist\n-\n-We will evaluate if the following options make sense\n-\n-\n-* Disable at a file/directory in addition to function frames (cc ?)\n-* Disable all the ops inside a namespace like torch.ops.fbgemm.* (cc )\n-* What is skipfiles_inline_module_allowlist - Can we refactor this? (cc )\n-\n-Others\n-\n-\n-* Logging - Breadcrumbs to signal/warn the user about usage of disable. If users forgot to remove the _dynamo.disable from the code, this warning can be super helpful.\n-* Namespace discussion - Should we move to torch.compile instead of torch._dynamo\n\\ No newline at end of file\ndiff --git a/docs/source/compile/get-started.rst b/docs/source/compile/get-started.rst\ndeleted file mode 100644\nindex 0e075c227412b2..00000000000000\n--- a/docs/source/compile/get-started.rst\n+++ /dev/null\n@@ -1,165 +0,0 @@\n-Getting Started\n-===============\n-\n-Let\u2019s start with a simple example. Note that you are likely to see more\n-significant speedups the newer your GPU is.\n-\n-The below is a tutorial for inference, for a training specific tutorial, make sure to checkout `example on training <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__\n-\n-.. code:: python\n-\n-   import torch\n-   def fn(x, y):\n-       a = torch.cos(x).cuda()\n-       b = torch.sin(y).cuda()\n-       return a + b\n-   new_fn = torch.compile(fn, backend=\"inductor\")\n-   input_tensor = torch.randn(10000).to(device=\"cuda:0\")\n-   a = new_fn(input_tensor, input_tensor)\n-\n-This example will not actually run faster. Its purpose is to demonstrate\n-the ``torch.cos()`` and ``torch.sin()`` features which are\n-examples of pointwise ops as in they operate element by element on a\n-vector. A more famous pointwise op you might want to use would\n-be something like ``torch.relu()``. Pointwise ops in eager mode are\n-suboptimal because each one would need to read a tensor from\n-memory, make some changes, and then write back those changes. The single\n-most important optimization that inductor does is fusion. So back to our\n-example we can turn 2 reads and 2 writes into 1 read and 1 write which\n-is crucial especially for newer GPUs where the bottleneck is memory\n-bandwidth (how quickly you can send data to a GPU) rather than compute\n-(how quickly your GPU can crunch floating point operations).\n-\n-Another major optimization that inductor makes available is automatic\n-support for CUDA graphs.\n-CUDA graphs help eliminate the overhead from launching individual\n-kernels from a Python program which is especially relevant for newer GPUs.\n-\n-TorchDynamo supports many different backends but inductor specifically works\n-by generating `Triton <https://github.com/openai/triton>`__ kernels. Suppose our example above\n-was called ``trig.py`` we can inspect the code generated triton kernels by\n-running ``TORCH_COMPILE_DEBUG=1 python trig.py`` with the actual generated kernel being\n-\n-.. code-block:: python\n-\n-   @pointwise(size_hints=[16384], filename=__file__, meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n-   @triton.jit\n-   def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n-       xnumel = 10000\n-       xoffset = tl.program_id(0) * XBLOCK\n-       xindex = xoffset + tl.arange(0, XBLOCK)[:]\n-       xmask = xindex < xnumel\n-       x0 = xindex\n-       tmp0 = tl.load(in_ptr0 + (x0), xmask)\n-       tmp1 = tl.cos(tmp0)\n-       tmp2 = tl.sin(tmp0)\n-       tmp3 = tmp1 + tmp2\n-       tl.store(out_ptr0 + (x0), tmp3, xmask)\n-\n-And you can verify that fusing the ``cos`` and ``sin`` did actually occur\n-because the ``cos`` and ``sin`` operations occur within a single Triton kernel\n-and the temporary variables are held in registers with very fast access.\n-\n-You can read up a lot more on Triton\u2019s performance\n-`here <https://openai.com/blog/triton/>`__ but the key is it\u2019s in Python\n-so you can easily understand it even if you have not written all that\n-many CUDA kernels.\n-\n-Next, let\u2019s try a real model like resnet50 from the PyTorch\n-hub.\n-\n-.. code-block:: python\n-\n-   import torch\n-   model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n-   opt_model = torch.compile(model, backend=\"inductor\")\n-   model(torch.randn(1,3,64,64))\n-\n-And that is not the only available backend, you can run in a REPL\n-``torch._dynamo.list_backends()`` to see all the available backends. Try out the\n-``cudagraphs`` or ``nvfuser`` next as inspiration.\n-\n-Let\u2019s do something a bit more interesting now, our community frequently\n-uses pretrained models from\n-`transformers <https://github.com/huggingface/transformers>`__ or\n-`TIMM <https://github.com/rwightman/pytorch-image-models>`__ and one of\n-our design goals is for Dynamo and inductor to work out of the box with\n-any model that people would like to author.\n-\n-So we will directly download a pretrained model from the\n-HuggingFace hub and optimize it:\n-\n-.. code-block:: python\n-\n-   import torch\n-   from transformers import BertTokenizer, BertModel\n-   # Copy pasted from here https://huggingface.co/bert-base-uncased\n-   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n-   model = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\n-   model = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\n-   text = \"Replace me by any text you'd like.\"\n-   encoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\n-   output = model(**encoded_input)\n-\n-If you remove the ``to(device=\"cuda:0\")`` from the model and\n-``encoded_input``, then Triton will generate C++ kernels that will be\n-optimized for running on your CPU. You can inspect both Triton or C++\n-kernels for BERT, they\u2019re obviously more complex than the trigonometry\n-example we had above but you can similarly skim it and understand if you\n-understand PyTorch.\n-\n-Similarly let\u2019s try out a TIMM example\n-\n-.. code-block:: python\n-\n-   import timm\n-   import torch._dynamo as dynamo\n-   import torch\n-   model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\n-   opt_model = torch.compile(model, backend=\"inductor\")\n-   opt_model(torch.randn(64,3,7,7))\n-\n-Our goal with Dynamo and inductor is to build the highest coverage ML compiler\n-which should work with any model you throw at it.\n-\n-Existing Backends\n-~~~~~~~~~~~~~~~~~\n-\n-TorchDynamo has a growing list of backends, which can be found in the\n-`backends <https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/backends/>`__ folder\n-or ``torch._dynamo.list_backends()`` each of which with its optional dependencies.\n-\n-Some of the most commonly used backends include:\n-\n-**Training & inference backends**:\n-  * ``torch.compile(m, backend=\"inductor\")`` - Uses ``TorchInductor`` backend. `Read more <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__\n-  * ``torch.compile(m, backend=\"aot_ts_nvfuser\")`` - nvFuser with AotAutograd/TorchScript. `Read more <https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593>`__\n-  * ``torch.compile(m, backend=\"nvprims_nvfuser\")`` - nvFuser with PrimTorch. `Read more <https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593>`__\n-  * ``torch.compile(m, backend=\"cudagraphs\")`` - cudagraphs with AotAutograd. `Read more <https://github.com/pytorch/torchdynamo/pull/757>`__\n-\n-**Inference-only backends**:\n-  * ``torch.compile(m, backend=\"onnxrt\")`` - Uses ONNXRT for inference on CPU/GPU. `Read more <https://onnxruntime.ai/>`__\n-  * ``torch.compile(m, backend=\"tensorrt\")`` - Uses ONNXRT to run TensorRT for inference optimizations. `Read more <https://github.com/onnx/onnx-tensorrt>`__\n-  * ``torch.compile(m, backend=\"ipex\")`` - Uses IPEX for inference on CPU. `Read more <https://github.com/intel/intel-extension-for-pytorch>`__\n-  * ``torch.compile(m, backend=\"tvm\")`` - Uses Apache TVM for inference optimizations. `Read more <https://tvm.apache.org/>`__\n-\n-Why do you need another way of optimizing PyTorch code?\n--------------------------------------------------------\n-\n-While a number of other code optimization tools exist in the PyTorch\n-ecosystem, each of them has its own flow.\n-Here is a few examples of existing methods and their limitations:\n-\n--  ``torch.jit.trace()`` is silently wrong if it cannot trace, for example:\n-   during control flow\n--  ``torch.jit.script()`` requires modifications to user or library code\n-   by adding type annotations and removing non PyTorch code\n--  ``torch.fx.symbolic_trace()`` either traces correctly or gives a hard\n-   error but it\u2019s limited to traceable code so still can\u2019t handle\n-   control flow\n--  ``torch._dynamo`` works out of the box and produces partial graphs.\n-   It still has the option of producing a single graph with\n-   ``nopython=True`` which are needed for `some\n-   situations <./documentation/FAQ.md#do-i-still-need-to-export-whole-graphs>`__\n-   but allows a smoother transition where partial graphs can be\n-   optimized without code modification\ndiff --git a/docs/source/compile/index.rst b/docs/source/compile/index.rst\ndeleted file mode 100644\nindex 0f18f5edf2ac78..00000000000000\n--- a/docs/source/compile/index.rst\n+++ /dev/null\n@@ -1,77 +0,0 @@\n-.. currentmodule:: torch\n-\n-torch.compile\n-====================\n-\n-:func:`~torch.compile` was introduced in `PyTorch 2.0 <https://pytorch.org/get-started/pytorch-2.0/>`__\n-\n-Our default and supported backend is `inductor` with benchmarks `showing 30% to 2x speedups and 10% memory compression <https://github.com/pytorch/pytorch/issues/93794>`__\n-on real world models for both training and inference with a single line of code.\n-\n-.. note::\n-    The :func:`~torch.compile` API is experimental and subject to change.\n-\n-The simplest possible interesting program is the below which we go over in a lot more detail in `getting started <https://pytorch.org/docs/main/compile/get-started.html>`__\n-showing how to use :func:`~torch.compile` to speed up inference on a variety of real world models from both TIMM and HuggingFace which we\n-co-announced `here <https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/>`__\n-\n-.. code:: python\n-\n-   import torch\n-   def fn(x):\n-       x = torch.cos(x).cuda()\n-       x = torch.sin(x).cuda()\n-       return x\n-   compiled_fn = torch.compile(fn(torch.randn(10).cuda()))\n-\n-If you happen to be running your model on an Ampere GPU, it's crucial to enable tensor cores. We will actually warn you to set\n-``torch.set_float32_matmul_precision('high')``\n-\n-:func:`~torch.compile` works over :class:`~torch.nn.Module` as well as functions so you can pass in your entire training loop.\n-\n-The above example was for inference but you can follow this tutorial for an `example on training <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__\n-\n-\n-Optimizations\n--------------\n-\n-Optimizations can be passed in :func:`~torch.compile` with either a backend mode parameter or as passes. To understand what are the available options you can run\n-``torch._inductor.list_options`` and ``torch._inductor.list_mode_options()``\n-\n-The default backend is `inductor` which will likely be the most reliable and performant option for most users and library maintainers,\n-other backends are there for power users who don't mind more experimental community support.\n-\n-There is some nuance involved in benchmarking ``torch.compile`` so we've provided a utility to make this simpler with :func:`~torch.utils.benchmark.utils.compile.bench_all`\n-\n-You can get the full list of community backends by running :func:`~torch._dynamo.list_backends`\n-\n-Troubleshooting\n----------------\n-\n-If you experience issues with models failing to compile, running out of memory, recompiling too often, not giving accurate results, you might find the right tool to solve your problem in one of our guides.\n-\n-.. WARNING::\n-    A few features are still in development and not likely to work for most users. Please do not use these features\n-    in production code and if you're a library maintainer please do not expose these options to your users.\n-    Dynamic shapes ``dynamic=true`` and max autotune ``mode=\"max-autotune\"`` which can be passed in to :func:`~torch.compile`.\n-    Distributed training has some quirks which you can follow in the troubleshooting guide below. Model export is not ready yet.\n-\n-.. toctree::\n-   :maxdepth: 1\n-\n-   troubleshooting\n-   faq\n-\n-Learn more\n-----------\n-\n-To learn more about the internals of the PyTorch 2.0 stack, check out the\n-following references:\n-\n-.. toctree::\n-   :maxdepth: 1\n-\n-   get-started\n-   technical-overview\n-   nn-module\n-   transformations\ndiff --git a/docs/source/compile/inductor_profiling.rst b/docs/source/compile/inductor_profiling.rst\ndeleted file mode 100644\nindex ab837580e7333f..00000000000000\n--- a/docs/source/compile/inductor_profiling.rst\n+++ /dev/null\n@@ -1,167 +0,0 @@\n-.. _torchinductor-gpu-profiling:\n-\n-TorchInductor GPU Profiling\n-===========================\n-\n-This document lists some useful commands and workflows that can help\n-people dive into a model\u2019s perf in TorchInductor. When a model is not\n-running as fast as expected, we may want to dive into the model and\n-check individual kernels. Usually those kernels taking the majority of\n-GPU time are the most interesting ones. Once we decide the kernels, we\n-may also want to run individual kernels directly and inspect its perf.\n-We have tools to cover everything mentioned above.\n-\n-## Relevant Environment Variables\n-\n--  TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\n-\n-   -  By default, inductor names a triton kernel as \u2018triton\\_\u2019. When\n-      this envvar is enabled, inductor generates a more meaningful\n-      kernel name in the trace, like ``triton_poi_fused_cat_155`` which\n-      contains the kernel category (poi for pointwise) and original aten\n-      ops. This config is disabled by default to improve the chance of\n-      compilation cache hit.\n-\n--  TORCHINDUCTOR_BENCHMARK_KERNEL\n-\n-   -  Enabling this will make inductor codegen harness to benchmark\n-      individual triton kernels.\n-\n--  TORCHINDUCTOR_MAX_AUTOTUNE\n-\n-   -  Inductor autotuner will benchmark more triton.Configs and pick the\n-      one with the best perf. This will increase compilation time with\n-      the hope to improve perf.\n-\n-## Breakdown Model GPU Time\n-\n-Below are the steps to breakdown execution time of a model into\n-individual kernels. We take mixnet_l as an example.\n-\n-1. Run the benchmark script for the model:\n-\n-TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1\n-python -u benchmarks/dynamo/timm_models.py \u2013backend inductor \u2013amp\n-\u2013performance \u2013dashboard \u2013only mixnet_l \u2013disable-cudagraphs \u2013training\n-\n-NOTE: the tool relies on kernel name to decide its category. Enabling\n-TORCHINDUCTOR_UNIQUE_KERNEL_NAMES is crucial for that.\n-\n-2. In the output log, look for lines like: **Compiled module path:\n-   /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n-\n-We have one line for each compiled module. If there are no extra graph\n-breaks, we would see 2 such lines in the log, one for the forward graph\n-and one for the backward graph.\n-\n-For our example command, we get the following compiled module for the\n-forward and backward graphs respectively:\n-\n--  https://gist.github.com/shunting314/c2a4d8a28b00fcb5586d0e9d9bf77f9f\n--  https://gist.github.com/shunting314/48efc83b12ec3ead950052e4a0220b10\n-\n-3. Now we can dive into the perf for each individual compiled module.\n-   Let\u2019s pick the one for the forward graph for illustration purposes.\n-   I\u2019ll name it fwd.py for convenience. Run it directly with \u2018-p\u2019\n-   argument:\n-\n-**> python fwd.py -p**\n-\n-Here is the full output log I get:\n-https://gist.github.com/shunting314/8243734a38b5733ea78479209c0ae893\n-\n-There are quite a lot of interesting things to note in the output\n-\n-A.\n-\n-We write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.\n-\n-**Chrome trace for the profile is written to\n-/tmp/compiled_module_profile.json**\n-\n-Loading the trace into chrome ( visit chrome://tracing in the chrome\n-browser and load the file as the UI suggested ) will show UI as follows:\n-\n-.. image:: ../_static/img/inductor_profiling/trace.png\n-\n-One can zoom in and out to check the profile.\n-\n-B.\n-\n-We report the percent of GPU time regarding to the wall time by log line like:\n-\n-**Percent of time when GPU is busy: 102.88%**\n-\n-Sometimes we see a value larger than 100%. The reason is we use the\n-kernel execution time with profiling enabled while using wall time with\n-profiling disabled. Profiling may distort the kernel execution time a\n-bit. But overall it should not be a big deal.\n-\n-If we run model like densenet121 with a small batch size, we would see\n-low percent of time when GPU is busy:\n-\n-::\n-\n-   (Forward graph) Percent of time when GPU is busy: 32.69%\n-\n-This means the model has a lot of CPU overhead. This is consistent with\n-the fact that enabling cudagraphs improve densenet121\u2019s perf a lot.\n-\n-C.\n-\n-We can break down the GPU time to different categories of kernels. In the mixnet_l example, we see\n-\n--  pointwise kernel takes 28.58%\n--  reduction kernel takes 13.85%\n--  persistent reduction kernel takes 3.89%\n--  the rest are cutlass/cudnn kernels for mm/conv which takes 56.57%\n-\n-This information can be found in the summary line (i.e. the last line)\n-of the report for each kernel category.\n-\n-D.\n-\n-We call also zoom into a certain category of kernels, e.g. let\u2019s check reduction kernels:\n-\n-.. image:: ../_static/img/inductor_profiling/kernel_breakdown.png\n-\n-We can see an ordered table of execution time for each individual\n-reduction kernel. We also see how many times a kernel is executed. This\n-is helpful for a few reasons\n-\n--  if a kernel only takes a tiny amount of time (say 0.1%), improving it\n-   will at most bring 0.1% overall gain. It is not worth spending a lot\n-   of effort on it.\n--  if a kernel takes 2% of time, improving it by 2x will bring in 1%\n-   overall gain which should be nice\n-\n-## Benchmark Individual Triton Kernel\n-\n-Let\u2019s say we want to take a closer look at\n-triton_red_fused\\__native_batch_norm_legit_functional_16 which is the\n-most expensive reduction kernel and takes 2.19% of overall wall time for\n-the forward graph.\n-\n-We can lookup the kernel name in the fwd.py, and find comment like\n-\n-**# kernel path:\n-/tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py**\n-\n-.. image:: ../_static/img/inductor_profiling/inductor_code.png\n-\n-I\u2019ll rename it k.py for convenience. Here is a paste for this file:\n-https://gist.github.com/shunting314/96a0afef9dce53d6357bf1633094f358\n-\n-k.py is a standalone python module containing the kernel code and its\n-benchmark.\n-\n-Run k.py directly will report it\u2019s execution time and bandwidth:\n-\n-.. image:: ../_static/img/inductor_profiling/terminal_printout.png\n-\n-We can check if max-autotune helps this kernel, by running:\n-\n-**TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py**\n-\n-We may also temporarily add more reduction heuristics and run the script\n-again to check how that helps with the kernel.\ndiff --git a/docs/source/compile/technical-overview.rst b/docs/source/compile/technical-overview.rst\ndeleted file mode 100644\nindex b8dcd6dbb2e9a2..00000000000000\n--- a/docs/source/compile/technical-overview.rst\n+++ /dev/null\n@@ -1,41 +0,0 @@\n-Technical Overview\n-====================\n-\n-**TorchDynamo** is a Python-level JIT compiler designed to make unmodified\n-PyTorch programs faster. TorchDynamo hooks into the frame evaluation API\n-in CPython (`PEP 523 <https://peps.python.org/pep-0523/>`__) to\n-dynamically modify Python bytecode right before it is executed. It\n-rewrites Python bytecode in order to extract sequences of PyTorch\n-operations into an `FX Graph <https://pytorch.org/docs/stable/fx.html>`__\n-which is then just-in-time compiled with a customizable backend.\n-It creates this FX Graph through bytecode analysis and is designed to\n-mix Python execution with compiled backends to get the best of both\n-worlds \u2014 usability and performance.\n-\n-TorchDynamo makes it easy to experiment with different compiler\n-backends to make PyTorch code faster with a single line decorator\n-``torch._dynamo.optimize()`` which is wrapped for convenience by ``torch.compile()``\n-\n-.. image:: ../_static/img/dynamo/TorchDynamo.png\n-\n-`TorchInductor` is one of the backends\n-supported by `TorchDynamo Graph <https://pytorch.org/docs/stable/fx.html>`__\n-into `Triton <https://github.com/openai/triton>`__ for GPUs or\n-`C++/OpenMP <https://www.openmp.org/>`__ for CPUs. We have a\n-`training performance dashboard <https://github.com/pytorch/torchdynamo/issues/681#issuecomment-1233828468>`__\n-that provides performance comparison for different training backends. You can read\n-more in the `TorchInductor post on PyTorch\n-dev-discuss <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__.\n-\n-.. seealso::\n-\n-   * `TorchDynamo deep-dive video <https://www.youtube.com/watch?v=egZB5Uxki0I>`__\n-   * `dev-discuss topics <https://dev-discuss.pytorch.org/search?q=TorchDynamo%20order%3Alatest>`__\n-\n-.. toctree::\n-   :maxdepth: 1\n-\n-   guards-overview\n-   best-practices-for-backends\n-   custom-backends\n-   deep-dive\ndiff --git a/docs/source/compile/torchfunc-and-torchcompile.rst b/docs/source/compile/torchfunc-and-torchcompile.rst\ndeleted file mode 100644\nindex 53e8fc795b3c20..00000000000000\n--- a/docs/source/compile/torchfunc-and-torchcompile.rst\n+++ /dev/null\n@@ -1,78 +0,0 @@\n-torch.func interaction with torch.compile\n-==============================================\n-\n-So you want to use a `torch.func` (\"functorch\") transform (like `vmap`, `grad`, `jacrev`, etc) with `torch.compile`. Here's a guide to what works today, what doesn't, and how to work around it.\n-\n-Applying a `torch.func` transform to a `torch.compile`'d function\n------------------------------------------------------------------\n-\n-This doesn't work and is being tracked by `https://github.com/pytorch/pytorch/issues/100320`.\n-\n-.. code:: python\n-\n-    import torch\n-\n-    @torch.compile\n-    def f(x):\n-        return torch.sin(x)\n-\n-    def g(x):\n-        return torch.grad(f)(x)\n-\n-    x = torch.randn(2, 3)\n-    g(x)\n-\n-As a workaround, please put the `torch.compile` outside of the `torch.func` transform:\n-\n-.. code:: python\n-\n-    import torch\n-\n-    def f(x):\n-        return torch.sin(x)\n-\n-    @torch.compile\n-    def g(x):\n-        return torch.vmap(f)(x)\n-\n-    x = torch.randn(2, 3)\n-    g(x)\n-\n-Doesn't work (PT 2.0): calling a `torch.func` transform inside of a `torch.compile`'ed function\n-------------------------------------------------------------------------------------------------\n-\n-.. code:: python\n-\n-    import torch\n-\n-    @torch.compile\n-    def f(x):\n-        return torch.vmap(torch.sum)(x)\n-\n-    x = torch.randn(2, 3)\n-    f(x)\n-\n-This doesn't work yet. Please see the workaround (the next section).\n-\n-Workaround: use `torch._dynamo.allow_in_graph`\n-----------------------------------------------\n-\n-`allow_in_graph` is an escape hatch. If your code does not work with `torch.compile`, which introspects Python bytecode, but you believe it will work via a symbolic tracing approach (like `jax.jit`), then use `allow_in_graph`.\n-\n-By using `allow_in_graph` to annotate a function, you promise PyTorch a couple of things that we are unable to completely verify:\n-- Your function is pure. That is, all outputs only depend on the inputs and do not depend on any captured Tensors.\n-- Your function is functional. That is, it does not mutate any state. This may be relaxed; we actually support functions that appear to be functional from the outside: they may have in-place PyTorch operations, but may not mutate global state or inputs to the function.\n-- Your function does not raise data-dependent errors.\n-\n-.. code:: python\n-\n-    import torch\n-\n-    @torch.compile\n-    def f(x):\n-        return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n-\n-    x = torch.randn(2, 3)\n-    f(x)\n-\n-A common pitfall is using `allow_in_graph` to annotate a function that invokes an `nn.Module`. This is because the outputs now depend on the parameters of the `nn.Module`. To actually get this to work, use `torch.func.functional_call` to extract the module state.\ndiff --git a/docs/source/index.rst b/docs/source/index.rst\nindex df662065833e6b..2289c06a2c93d3 100644\n--- a/docs/source/index.rst\n+++ b/docs/source/index.rst\n@@ -42,31 +42,6 @@ Features described in this documentation are classified by release status:\n \n    notes/*\n \n-.. toctree::\n-   :glob:\n-   :maxdepth: 1\n-   :caption: torch.compile\n-\n-   compile/index\n-   compile/get-started\n-   compile/troubleshooting\n-   compile/faq\n-   compile/technical-overview\n-   compile/guards-overview\n-   compile/custom-backends\n-   compile/fine_grained_apis\n-   compile/profiling_torch_compile\n-   compile/inductor_profiling\n-   compile/deep-dive\n-   compile/cudagraph_trees\n-   compile/performance-dashboard\n-   compile/torchfunc-and-torchcompile\n-   ir\n-   compile/dynamic-shapes\n-   compile/fake-tensor\n-   logging\n-   compile/transformations\n-\n .. toctree::\n    :maxdepth: 1\n    :caption: Language Bindings\n@@ -102,7 +77,7 @@ Features described in this documentation are classified by release status:\n    torch.distributed.tensor.parallel <distributed.tensor.parallel>\n    torch.distributed.checkpoint <distributed.checkpoint>\n    torch.distributions <distributions>\n-   torch.compiler <compiler>\n+   torch.compiler <torch.compiler>\n    torch.fft <fft>\n    torch.func <func>\n    futures\ndiff --git a/docs/source/torch.compiler.rst b/docs/source/torch.compiler.rst\nnew file mode 100644\nindex 00000000000000..02167ca5fd7a8e\n--- /dev/null\n+++ b/docs/source/torch.compiler.rst\n@@ -0,0 +1,116 @@\n+.. _torch.compiler_overview:\n+\n+torch.compiler\n+==============\n+\n+``torch.compiler`` is a namespace through which some of the internal compiler\n+methods are surfaced for user consumption. The main function and the feature in\n+this namespace is ``torch.compile``.\n+\n+``torch.compile`` is a PyTorch function introduced in PyTorch 2.x that aims to\n+solve the problem of accurate graph capturing in PyTorch and ultimately enable\n+software engineers to run their PyTorch programs faster. ``torch.compile`` is\n+written in PyThon and it marks the transition of PyTorch from C++ to PyThon.\n+\n+``torch.compile`` leverages the following underlying technologies:\n+\n+* TorchDynamo (``torch._dynamo``) is an internal API that uses a CPython\n+  feature called the Frame Evaluation API to safely capture PyTorch graphs.\n+  Methods that are available externally for PyTorch users are surfaced\n+  through the ``torch.compiler`` namespace.\n+\n+* TorchInductor is the default ``torch.compile`` deep learning compiler\n+  that generates fast code for multiple accelerators and backends. You\n+  need to use a backend compiler to make speedups through ``torch.compile``\n+  possible. For NVIDIA and AMD GPUs, it leverages OpenAI Triton as the key\n+  building block.\n+\n+* AOTAutograd captures not only the user-level code, but also backpropagation,\n+  which results in capturing the backwards pass \"ahead-of-time\". This enables\n+  acceleration of both forwards and backwards pass using TorchInductor.\n+\n+.. note:: In some cases, the terms ``torch.compile``, TorchDynamo, ``torch.compiler``\n+  might be used interchangeably in this documentation.\n+\n+As mentioned above, to run your workflows faster, ``torch.compile`` through\n+TorchDynamo requires a backend that converts the captured graphs into fast\n+machine code. Different backends can result in various optimization gains.\n+The default backend is called TorchInductor, also known as *inductor*,\n+TorchDynamo has a growing list of backends developed by our partners,\n+which can be found in the `backends <https://github.com/pytorch/pytorch/tree/main/torch/_dynamo/backends>`_\n+folder or by running  ``torch.compile.list_backends()`` each of which with its\n+optional dependencies.\n+\n+Some of the most commonly used backends include:\n+\n+.. list-table:: Training & inference backends\n+   :widths: 50 50\n+   :header-rows: 1\n+\n+   * - Backend\n+     - Description\n+   * - ``torch.compile(m, backend=\"inductor\")``\n+     - Uses the TorchInductor backend. `Read more <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__\n+   * - ``torch.compile(m, backend=\"aot_ts_nvfuser\")``\n+     - nvFuser with AotAutograd/TorchScript. `Read more <https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593>`__\n+   * - ``torch.compile(m, backend=\"nvprims_nvfuser\")``\n+     - `Read more <https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593>`__\n+   * - ``torch.compile(m, backend=\"cudagraphs\")``\n+     - CUDA graphs with AotAutograd. `Read more <https://github.com/pytorch/torchdynamo/pull/757>`__\n+\n+.. list-table:: Inference-only backends\n+   :widths: 50 50\n+   :header-rows: 1\n+\n+   * - Backend\n+     - Description\n+   * - ``torch.compile(m, backend=\"onnxrt\")``\n+     - Uses ONNXRT for inference on CPU/GPU. `Read more <https://onnxruntime.ai/>`__\n+   * - ``torch.compile(m, backend=\"tensorrt\")``\n+     - Uses ONNXRT to run TensorRT for inference optimizations. `Read more <https://github.com/onnx/onnx-tensorrt>`__\n+   * - ``torch.compile(m, backend=\"ipex\")``\n+     - Uses IPEX for inference on CPU. `Read more <https://github.com/intel/intel-extension-for-pytorch>`__\n+   * - ``torch.compile(m, backend=\"tvm\")``\n+     - Uses Apache TVM for inference optimizations. `Read more <https://tvm.apache.org/>`__\n+\n+Read More\n+~~~~~~~~~\n+\n+.. toctree::\n+   :caption: Getting Started for PyTorch Users\n+   :maxdepth: 2\n+\n+   torch.compiler_get_started\n+   torch.compiler_api\n+   torch.compiler_performance_dashboard\n+   torch.compiler_fine_grain_apis\n+   torch.compiler_inductor_profiling\n+   torch.compiler_profiling_torch_compile\n+   torch.compiler_faq\n+   torch.compiler_troubleshooting\n+\n+..\n+  _If you want to contribute a developer-level topic\n+   that provides in-depth overview of a torch._dynamo feature,\n+   add in the below toc.\n+\n+.. toctree::\n+   :caption: Deep Dive for PyTorch Developers\n+   :maxdepth: 2\n+\n+   torch.compiler_deepdive\n+   torch.compiler_guards_overview\n+   torch.compiler_dynamic_shapes\n+   torch.compiler_nn_module\n+   torch.compiler.best_practices_for_backends\n+   torch.compiler_cudagraph_trees\n+   torch.compiler_fake_tensor\n+\n+.. toctree::\n+   :caption: HowTo for PyTorch Backend Vendors\n+   :maxdepth: 2\n+\n+   torch.compiler_backend_vendors\n+   torch.compiler_custom_backends\n+   torch.compiler_transformations\n+   torch.compiler_ir\ndiff --git a/docs/source/compiler.rst b/docs/source/torch.compiler_api.rst\nsimilarity index 62%\nrename from docs/source/compiler.rst\nrename to docs/source/torch.compiler_api.rst\nindex 31ed37171e19f4..169922c5b59d4e 100644\n--- a/docs/source/compiler.rst\n+++ b/docs/source/torch.compiler_api.rst\n@@ -1,12 +1,14 @@\n-torch.compiler\n-========================\n-\n .. currentmodule:: torch.compiler\n \n .. automodule:: torch.compiler\n \n+.. _torch.compiler_api:\n+\n torch.compiler API reference\n-------------------------------\n+============================\n+\n+For a quick overview of ``torch.compiler``, see :ref:`torch.compiler_overview`.\n+\n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n@@ -16,4 +18,4 @@ torch.compiler API reference\n      allow_in_graph\n      assume_constant_result\n      list_backends\n-     disable\n\\ No newline at end of file\n+     disable\ndiff --git a/docs/source/compile/best-practices-for-backends.rst b/docs/source/torch.compiler_best_practices_for_backends.rst\nsimilarity index 100%\nrename from docs/source/compile/best-practices-for-backends.rst\nrename to docs/source/torch.compiler_best_practices_for_backends.rst\ndiff --git a/docs/source/compile/cudagraph_trees.rst b/docs/source/torch.compiler_cudagraph_trees.rst\nsimilarity index 100%\nrename from docs/source/compile/cudagraph_trees.rst\nrename to docs/source/torch.compiler_cudagraph_trees.rst\ndiff --git a/docs/source/compile/custom-backends.rst b/docs/source/torch.compiler_custom_backends.rst\nsimilarity index 100%\nrename from docs/source/compile/custom-backends.rst\nrename to docs/source/torch.compiler_custom_backends.rst\ndiff --git a/docs/source/compile/deep-dive.rst b/docs/source/torch.compiler_deepdive.rst\nsimilarity index 71%\nrename from docs/source/compile/deep-dive.rst\nrename to docs/source/torch.compiler_deepdive.rst\nindex 4942030ddcd881..32eb7b7ef49a01 100644\n--- a/docs/source/compile/deep-dive.rst\n+++ b/docs/source/torch.compiler_deepdive.rst\n@@ -1,7 +1,50 @@\n-TorchDynamo Deeper Dive\n-=======================\n+TorchDynamo Deep Dive\n+=====================\n+\n+Before you read this section, read :ref:`torch.compiler_overview`.`\n+\n+**TorchDynamo** is a Python-level Just-In-Time (JIT) compiler designed to make\n+unmodified PyTorch programs faster. TorchDynamo hooks into the frame evaluation\n+API in CPython (`PEP 523 <https://peps.python.org/pep-0523/>`__) to\n+dynamically modify Python bytecode right before it is executed. It\n+rewrites Python bytecode to extract sequences of PyTorch\n+operations into an `FX Graph <https://pytorch.org/docs/stable/fx.html>`__\n+which is then compiled with a customizable backend.\n+It creates this FX Graph through bytecode analysis and is designed to\n+mix Python execution with compiled backends to get the best of both\n+worlds \u2014 usability and performance.\n+\n+TorchDynamo makes it easy to experiment with different compiler\n+backends to make PyTorch code faster with a single line decorator\n+``torch._dynamo.optimize()`` which is wrapped for convenience by ``torch.compile()``\n+\n+The following diagram demonstrates how PyTorch works with ``torch.compile``\n+and without it:\n+\n+.. image:: _static/img/dynamo/TorchDynamo.png\n+\n+`TorchInductor` is one of the backends\n+supported by `TorchDynamo Graph <https://pytorch.org/docs/stable/fx.html>`__\n+into `Triton <https://github.com/openai/triton>`__ for GPUs or\n+`C++/OpenMP <https://www.openmp.org/>`__ for CPUs. We have a\n+`training performance dashboard <https://github.com/pytorch/torchdynamo/issues/681#issuecomment-1233828468>`__\n+that provides performance comparison for different training backends. You can read\n+more in the `TorchInductor post on PyTorch\n+dev-discuss <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__.\n+\n+For an in-depth overview, read the sections below, watch the deep-dive video,\n+and check out the dev-discuss topics.\n+\n+   * `TorchDynamo deep-dive video <https://www.youtube.com/watch?v=egZB5Uxki0I>`__\n+   * `dev-discuss topics <https://dev-discuss.pytorch.org/search?q=TorchDynamo%20order%3Alatest>`__\n+\n+TorchDynamo Internals\n+~~~~~~~~~~~~~~~~~~~~~\n **Author**: `Jason Ansel <https://github.com/jansel>`_\n \n+This section will go over some of the TorchDynamo internals and will\n+demonstrate how TorchDynamo works under the hood.\n+\n What is a guard?\n ----------------\n \ndiff --git a/docs/source/compile/dynamic-shapes.rst b/docs/source/torch.compiler_dynamic_shapes.rst\nsimilarity index 100%\nrename from docs/source/compile/dynamic-shapes.rst\nrename to docs/source/torch.compiler_dynamic_shapes.rst\ndiff --git a/docs/source/compile/fake-tensor.rst b/docs/source/torch.compiler_fake_tensor.rst\nsimilarity index 100%\nrename from docs/source/compile/fake-tensor.rst\nrename to docs/source/torch.compiler_fake_tensor.rst\ndiff --git a/docs/source/torch.compiler_faq.rst b/docs/source/torch.compiler_faq.rst\nnew file mode 100644\nindex 00000000000000..9190ec9292acf6\n--- /dev/null\n+++ b/docs/source/torch.compiler_faq.rst\n@@ -0,0 +1,491 @@\n+Frequently Asked Questions\n+==========================\n+**Author**: `Mark Saroufim <https://github.com/msaroufim>`_\n+\n+Does ``torch.compile`` support training?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+``torch.compile`` supports training, using AOTAutograd to capture backwards:\n+\n+1. The ``.forward()`` graph and ``optimizer.step()`` is captured by\n+   TorchDynamo\u2019s python ``evalframe`` frontend.\n+2. For each segment of ``.forward()`` that torchdynamo captures, it uses\n+   AOTAutograd to generate a backward graph segment.\n+3. Each pair of forward and backward graph are (optionally) min-cut\n+   partitioned to save the minimal state between forward and backward.\n+4. The forward and backward pairs are wrapped in ``autograd.function`` modules.\n+5. Usercode calling\\ ``.backward()`` still triggers eager\u2019s autograd engine,\n+   which runs each *compiled backward* graph as if it were one op, also running\n+   any non-compiled eager ops\u2019 ``.backward()`` functions.\n+\n+Do you support Distributed code?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+``torch.compile`` supports ``DistributedDataParallel`` (DDP).\n+Support for other distributed training libraries is being considered.\n+\n+The main reason why Distributed code is challenging with dynamo is\n+because AOTAutograd unrolls both the forward and backward pass and\n+provides 2 graphs for backends to optimize. This is a problem for\n+distributed code because we\u2019d like to ideally overlap communication\n+operations with computations. Eager pytorch accomplishes this in\n+different ways for DDP/FSDP- using autograd hooks, module hooks, and\n+modifications/mutations of module states. In a naive application of\n+dynamo, hooks that should run directly after an operation during\n+backwards may be delayed until after the entire compiled region of\n+backwards ops, due to how AOTAutograd compiled functions interact with\n+dispatcher hooks.\n+\n+The basic strategy for optimizing DDP with Dynamo is outlined in\n+`distributed.py <https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/optimizations/distributed.py>`__\n+where the main idea will be to graph break on `DDP bucket\n+boundaries <https://pytorch.org/docs/stable/notes/ddp.html#internal-design>`__.\n+\n+When each node in DDP needs to synchronize its weights with the other\n+nodes it organizes its gradients and parameters into buckets which\n+reduces communication times and allows a node to broadcast a fraction of\n+its gradients to other waiting nodes.\n+\n+Graph breaks in distributed code mean you can expect dynamo and its\n+backends to optimize the compute overhead of a distributed program but\n+not its communication overhead. Graph-breaks may interfere with\n+compilation speedups, if the reduced graph-size robs the compiler of\n+fusion opportunities. However, there are diminishing returns with\n+increasing graph size since most of the current compute optimizations\n+are local fusions. So in practice this approach may be sufficient.\n+\n+Do I still need to export whole graphs?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+For the vast majority of models you probably don\u2019t and you can use\n+``torch.compile()`` as is but there are a few situations where\n+full graphs are necessary and you can can ensure a full graph by simply\n+running ``torch.compile(..., nopython=True)``. These situations include\n+* Large scale training runs, such as $250K+ that require pipeline parallelism\n+  and other advanced sharding strategies\n+* Inference optimizers like `TensorRT <https://github.com/pytorch/TensorRT>`__\n+  or `AITemplate <https://github.com/facebookincubator/AITemplate>`__ that\n+  rely on fusing much more aggressively than training optimizers\n+* Mobile training or inference.\n+\n+Future work will include tracing communication operations into graphs,\n+coordinating these operations with compute optimizations, and optimizing\n+the communication operations.\n+\n+Why is my code crashing?\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+If your code ran just fine without ``torch.compile`` and started to\n+crash with it is enabled, then the most important first step is figuring\n+out which part of the stack your failure occurred. To troubleshoot that,\n+follow the steps below and only try the next step if the previous one\n+succeeded.\n+\n+1. ``torch.compile(..., backend=\"eager\")`` which only runs TorchDynamo\n+   forward graph capture and then runs the captured graph with PyTorch.\n+   If this fails then there\u2019s an issue with TorchDynamo.\n+\n+2. ``torch.compile(..., backend=\"aot_eager\")``\n+   which runs TorchDynamo to capture a forward graph, and then AOTAutograd\n+   to trace the backward graph without any additional backend compiler\n+   steps. PyTorch eager will then be used to run the forward and backward\n+   graphs. If this fails then there\u2019s an issue with AOTAutograd.\n+\n+3. ``torch.compile(..., backend=\"inductor\")`` which runs TorchDynamo to capture a\n+   forward graph, and then AOTAutograd to trace the backward graph with the\n+   TorchInductor compiler. If this fails then there\u2019s an issue with TorchInductor\n+\n+Why is compilation slow?\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+* **Dynamo Compilation**\u2013 TorchDynamo has a builtin stats function for\n+  collecting and displaying the time spent in each compilation phase.\n+  These stats can be accessed by calling ``torch._dynamo.utils.compile_times()``\n+  after executing ``torch._dynamo``. By default, this returns a string\n+  representation of the compile times spent in each TorchDynamo function by name.\n+\n+* **Inductor Compilation**\u2013 TorchInductor has a builtin stats and trace function\n+  for displaying time spent in each compilation phase, output code, output\n+  graph visualization and IR dump. ``env TORCH_COMPILE_DEBUG=1 python repro.py``.\n+  This is a debugging tool designed to make it easier to debug/understand the\n+  internals of TorchInductor with an output that will look something like\n+  `this <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\n+  Each file in that debug trace can be enabled/disabled via\n+  ``torch._inductor.config.trace.*``. The profile and the diagram are both\n+  disabled by default since they are expensive to generate. See the\n+  `example debug directory\n+  output <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\n+  for more examples.\n+\n+* **Excessive Recompilation**\n+  When TorchDynamo compiles a function (or part of one), it makes certain\n+  assumptions about locals and globals in order to allow compiler\n+  optimizations, and expresses these assumptions as guards that check\n+  particular values at runtime. If any of these guards fail, Dynamo will\n+  recompile that function (or part) up to\n+  ``torch._dynamo.config.cache_size_limit`` times. If your program is\n+  hitting the cache limit, you will first need to determine which guard is\n+  failing and what part of your program is triggering it. The\n+  `recompilation profiler <#recompilation-profiler>`__ automates the\n+  process of setting TorchDynamo\u2019s cache limit to 1 and running your\n+  program under an observation-only \u2018compiler\u2019 that records the causes of\n+  any guard failures. You should be sure to run your program for at least\n+  as long (as many iterations) as you were running when you ran into\n+  trouble, and the profiler will accumulate statistics over this duration.\n+\n+.. code-block:: python\n+\n+   from torch._dynamo.utils import CompileProfiler\n+\n+   def my_model():\n+       ...\n+\n+   with CompileProfiler() as prof:\n+       profiler_model = torch.compile(my_model, backend=prof)\n+       profiler_model()\n+       print(prof.report())\n+\n+Why are you recompiling in production?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+In some cases, you may not want unexpected compiles after a program has\n+warmed up. For example, if you are serving production traffic in a\n+latency critical application. For this, TorchDynamo provides an\n+alternate mode where prior compiled graphs are used, but no new ones are\n+generated:\n+\n+.. code-block:: python\n+\n+   frozen_toy_example = dynamo.run(toy_example)\n+   frozen_toy_example(torch.randn(10), torch.randn(10))\n+\n+How are you speeding up my code?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+There are 3 major ways to accelerate PyTorch code:\n+\n+1. Kernel fusion via vertical fusions which fuse sequential operations to avoid\n+   excessive read/writes. For example, fuse 2 subsequent cosines means you\n+   can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:\n+   the simplest example being batching where a single matrix is multiplied\n+   with a batch of examples but the more general scenario is a grouped GEMM\n+   where a group of matrix multiplications are scheduled together\n+\n+2. Out of order execution: A general optimization for compilers, by looking ahead\n+   at the exact data dependencies within a graph we can decide on the most\n+   opportune time to execute a node and which buffers can be reused\n+\n+3. Automatic work placement: Similar of the out of order execution point,\n+   but by matching nodes of a graph to resources like physical hardware or\n+   memory we can design an appropriate schedule\n+\n+The above are general principles for accelerating PyTorch code but\n+different backends will each make different tradeoffs on what to\n+optimize. For example Inductor first takes care of fusing whatever it\n+can and only then generates `Triton <https://openai.com/blog/triton/>`__\n+kernels. It can also\n+\n+Triton in addition offers speedups because of automatic memory\n+coalescing, memory management and scheduling within each Streaming\n+Multiprocessor and has been designed to handle tiled computations.\n+\n+However, regardless of the backend you use it\u2019s best to use a benchmark\n+and see approach so try out the PyTorch profiler, visually inspect the\n+generated kernels and try to see what\u2019s going on for yourself.\n+\n+Why am I not seeing speedups?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Graph Breaks\n+------------\n+\n+The main reason you won\u2019t see the speedups you\u2019d like to by using dynamo\n+is excessive graph breaks. So what\u2019s a graph break?\n+\n+Given a program like:\n+\n+.. code-block:: python\n+\n+   def some_fun(x):\n+       ...\n+\n+   torch.compile(some_fun)(x)\n+   ...\n+\n+Torchdynamo will attempt to compile all of the torch/tensor operations\n+within ``some_fun()`` into a single FX graph, but it may fail to capture\n+everything into one graph.\n+\n+Some graph break reasons are insurmountable to TorchDynamo like calling\n+into a C extension other than PyTorch is invisible to TorchDynamo, and\n+could do arbitrary things without TorchDynamo being able to introduce\n+necessary guards to ensure that the compiled program would be safe to reuse.\n+\n+   To maximize performance, it\u2019s important to have as few graph breaks\n+   as possible.\n+\n+Identifying the cause of a graph break\n+--------------------------------------\n+\n+To identify all graph breaks in a program and the associated reasons for\n+the breaks, ``torch._dynamo.explain`` can be used. This tool runs\n+TorchDynamo on the supplied function and aggregates the graph breaks\n+that are encountered. Here is an example usage:\n+\n+.. code-block:: python\n+\n+   import torch\n+   import torch._dynamo as dynamo\n+   def toy_example(a, b):\n+       x = a / (torch.abs(a) + 1)\n+       print(\"woo\")\n+       if b.sum() < 0:\n+           b = b * -1\n+       return x * b\n+   explanation, out_guards, graphs, ops_per_graph = dynamo.explain(toy_example, torch.randn(10), torch.randn(10))\n+   print(explanation)\n+   \"\"\"\n+   Dynamo produced 3 graphs, with 2 graph break and 6 ops.\n+    Break reasons:\n+   1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n+      File \"t2.py\", line 16, in toy_example\n+       print(\"woo\")\n+\n+   2. generic_jump\n+      File \"t2.py\", line 17, in toy_example\n+       if b.sum() < 0:\n+    \"\"\"\n+\n+To throw an error on the first graph break encountered you can use\n+disable python fallback by using ``nopython=True``, this should be\n+familiar if you\u2019ve worked with export based compilers.\n+\n+.. code-block:: python\n+\n+   def toy_example(a, b):\n+      ...\n+\n+   torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n+\n+Why didn\u2019t my code recompile when I changed it?\n+-----------------------------------------------\n+\n+If you enabled dynamic shapes by setting\n+``env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py`` then your code\n+won\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes\n+which avoids recompilations in the case when shapes vary by less than a\n+factor of 2. This is especially useful in scenarios like varying image\n+sizes in CV or variable sequence length in NLP. In inference scenarios\n+it\u2019s often not possible to know what a batch size will be beforehand\n+because you take what you can get from different client apps.\n+\n+In general, TorchDynamo tries very hard not to recompile things\n+unnecessarily so if for example TorchDynamo finds 3 graphs and your\n+change only modified one graph then only that graph will recompile. So\n+another tip to avoid potentially slow compilation times is to warmup a\n+model by compiling it once after which subsequent compilations will be\n+much faster. Cold start compile times is still a metric we track\n+visibly.\n+\n+Why am I getting incorrect results?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Accuracy issues can also be minified if you set the environment variable\n+``TORCHDYNAMO_REPRO_LEVEL=4``, it operates with a similar git bisect\n+model and a full repro might be something like\n+``TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4`` the reason\n+we need this is downstream compilers will codegen code whether it\u2019s\n+Triton code or the C++ backend, the numerics from those downstream\n+compilers can be different in subtle ways yet have dramatic impact on\n+your training stability. So the accuracy debugger is very useful for us\n+to detect bugs in our codegen or with a backend compiler.\n+\n+If you'd like to ensure that random number generation is the same across both torch\n+and triton then you can enable ``torch._inductor.config.fallback_random = True``\n+\n+Why am I getting OOMs?\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+Dynamo is still an alpha product so there\u2019s a few sources of OOMs and if\n+you\u2019re seeing an OOM try disabling the following configurations in this\n+order and then open an issue on GitHub so we can solve the root problem\n+1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled\n+them by default: ``env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py`` 2.\n+CUDA graphs with Triton are enabled by default in inductor but removing\n+them may alleviate some OOM issues: ``torch._inductor.config.triton.cudagraphs = False``.\n+\n+``torch.func`` does not work with ``torch.compile``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Applying a ``torch.func`` transform to a function that uses ``torch.compile``\n+does not work:\n+\n+.. code-block:: python\n+\n+    import torch\n+\n+    @torch.compile\n+    def f(x):\n+        return torch.sin(x)\n+\n+    def g(x):\n+        return torch.grad(f)(x)\n+\n+    x = torch.randn(2, 3)\n+    g(x)\n+\n+As a workaround, use ``torch.compile`` outside of the ``torch.func`` function:\n+\n+.. code-block:: python\n+\n+    import torch\n+\n+    def f(x):\n+        return torch.sin(x)\n+\n+    @torch.compile\n+    def g(x):\n+        return torch.vmap(f)(x)\n+\n+    x = torch.randn(2, 3)\n+    g(x)\n+\n+Applying a ``torch.func`` transform to a function handled with ``torch.compile``\n+--------------------------------------------------------------------------------\n+\n+For example, you have the following code:\n+\n+.. code-block:: python\n+\n+    import torch\n+\n+    @torch.compile\n+    def f(x):\n+        return torch.sin(x)\n+\n+    def g(x):\n+        return torch.grad(f)(x)\n+\n+    x = torch.randn(2, 3)\n+    g(x)\n+\n+This code will not work. There is an `issue <https://github.com/pytorch/pytorch/issues/100320>`__\n+that you can track for this.\n+As a workaround, please put the ``torch.compile`` outside of ``torch.func`` transform:\n+\n+.. code-block:: python\n+\n+    import torch\n+\n+    def f(x):\n+        return torch.sin(x)\n+\n+    @torch.compile\n+    def g(x):\n+        return torch.vmap(f)(x)\n+\n+    x = torch.randn(2, 3)\n+    g(x)\n+\n+Calling ``torch.func`` transform inside of a function handled with ``torch.compile``\n+------------------------------------------------------------------------------------\n+\n+.. code-block:: python\n+\n+    import torch\n+\n+    @torch.compile\n+    def f(x):\n+        return torch.vmap(torch.sum)(x)\n+\n+    x = torch.randn(2, 3)\n+    f(x)\n+\n+This doesn't work yet. As a workaround, use ``torch._dynamo.allow_in_graph``\n+\n+``allow_in_graph`` is an escape hatch. If your code does not work with\n+``torch.compile``, which introspects Python bytecode, but you believe it\n+will work via a symbolic tracing approach (like ``jax.jit``), then use\n+``allow_in_graph``.\n+\n+By using ``allow_in_graph`` to annotate a function, you must make sure\n+your code meets the following requirements:\n+\n+- All outputs in your function only depend on the inputs and\n+  do not depend on any captured Tensors.\n+- Your function is functional. That is, it does not mutate any state. This may\n+  be relaxed; we actually support functions that appear to be functional from\n+  the outside: they may have in-place PyTorch operations, but may not mutate\n+  global state or inputs to the function.\n+- Your function does not raise data-dependent errors.\n+\n+.. code-block:: python\n+\n+    import torch\n+\n+    @torch.compile\n+    def f(x):\n+        return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n+\n+    x = torch.randn(2, 3)\n+    f(x)\n+\n+A common pitfall is using ``allow_in_graph`` to annotate a function that\n+invokes an ``nn.Module``. This is because the outputs now depend on the\n+parameters of the ``nn.Module``. To get this to work, use\n+``torch.func.functional_call`` to extract the module state.\n+\n+Which API to use for fine grain tracing?\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+In some cases, you might need to exclude small parts of your code from the \n+torch.compile compilations. This section provides some of the answers and\n+you can find more information in :ref:`torchdynamo_fine_grain_tracing`.\n+\n+How do I graph break on a function?\n+___________________________________\n+\n+Graph break on a function is not enough to sufficiently express what you  want\n+PyTorch to do. You need to be more specific about your use case. Some of the\n+most common use cases you might want to consider:\n+\n+* If you want to disable compilation on this function frame and the\n+  recursively invoked frames \u2014\u00a0use ``_dynamo.disable``.\n+* If you want a particular operator, such as ``fbgemm`` to use the  eager mode \u2013\n+  use ``_dynamo.disallow_in_graph``.\n+\n+Some of the uncommon use cases include:\n+\n+* If you want to disable TorchDynamo on the function frame but enable it back\n+  on the recursively invoked frames \u2013 use ``_dynamo.disable(recursive=False)``.\n+\n+* If you want to prevent inlining of a function frame \u2013 use ``_dynamo.graph_break``\n+  at the beginning of the function you want to prevent inlining.\n+\n+What's the difference between ``_dynamo.disable`` and ``_dynamo.disallow_in_graph``\n+-----------------------------------------------------------------------------------\n+\n+Disallow-in-graph works at the level of operators, or more specifically,\n+the operators that you see in the TorchDynamo extracted graphs.\n+\n+Disable works at the function frame level and decides if TorchDynamo\n+should look into the function frame or not.\n+\n+What's the difference between ``_dynamo.disable`` and ``_dynamo_skip``\n+----------------------------------------------------------------------\n+\n+.. note::\n+   ``_dynamo_skip`` is deprecated.\n+\n+You most likely need ``_dynamo.disable``. But in an unlikely scenario, you\n+might need even finer control. Suppose you want to disable the tracing on just\n+the ``a_fn`` function, but want to continue the tracing back in ``aa_fn`` and\n+``ab_fn``. The image below demonstrates this use case:\n+\n+\n+.. figure:: _static/img/fine_grained_apis/call_stack_diagram.png\n+   :alt: diagram of torch.compile + disable(a_fn, recursive=False)\n+\n+In this case, you can use ``torch._dynamo.disable(recursive=False)``.\n+In previous versions, this functionality was provided by ``_dynamo.skip``.\n+This is now supported by the ``recursive`` flag inside ``torch._dynamo.disable``.\ndiff --git a/docs/source/torch.compiler_fine_grain_apis.rst b/docs/source/torch.compiler_fine_grain_apis.rst\nnew file mode 100644\nindex 00000000000000..107621818ce2fe\n--- /dev/null\n+++ b/docs/source/torch.compiler_fine_grain_apis.rst\n@@ -0,0 +1,100 @@\n+.. _torchdynamo_fine_grain_tracing:\n+\n+TorchDynamo APIs to control fine-grained tracing\n+================================================\n+\n+``torch.compile`` performs TorchDynamo tracing on the whole user model.\n+However, it is possible that a small part of the model code cannot be\n+handeled by ``torch.cmpile``. In this case, you might want to disable\n+the compiler on that particular portion, while running compilation on\n+the rest of the model. This section describe the existing APIs that\n+use to define parts of your code in which you want to skip compilation\n+and the relevant use cases.\n+\n+The API that you can use to define portions of the code on which you can\n+disable compilation are listed in the following table:\n+\n+.. csv-table:: TorchDynamo APIs to control fine-grained tracing\n+   :header: \"API\", \"Description\", \"When to use?\"\n+   :widths: auto\n+\n+   \"``torch._dynamo.disable``\", \"Disables Dynamo on the decorated function as well as recursively invoked functions.\", \"Excellent for unblocking a user, if a small portion of the model cannot be handeled with ``torch.compile``.\"\n+   \"``torch._dynamo.disallow_in_graph``\", \"Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.\\n\\nThis is suitable for the ops, while ``_dynamo.disable`` is suitable for decorating functions.\", \"This API is excellent for both debugging and unblocking if a custom op like ``torch.ops.fbgemm.*`` is causing issues with ``torch.compile``.\"\n+   \"``torch._dynamo.allow_in_graph``\", \"The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.\\n\\nNote that AOT Autograd will trace through it, so the ``allow_in_graph`` is only a Dynamo-level concept.\", \"This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or ``autograd.Function``. However, each usage of ``allow_in_graph`` **must be carefully screened** (no graph breaks, no closures).\"\n+   \"``torch._dynamo.graph_break``\", \"Adds a graph break. The code before and after the graph break goes through TorchDynamo.\", \"**Rarely useful for deployment** - If you think you need this, most probably you need either ``disable`` or ``disallow_in_graph``.\"\n+\n+``torch._dynamo.disable``\n+~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+``torch._dynamo.disable`` disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\n+\n+TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function ``fn`` calls functions ``a_fn`` and ``b_fn``. And ``a_fn`` calls ``aa_fn`` and ``ab_fn``. When you use the PyTorch eager mode rather than ``torch.compile``, these function frames run as is. With ``torch.compile``, TorchDynamo intercepts each of these function frames (indicated by the green color):\n+\n+.. figure:: ../_static/img/fine_grained_apis/api_diagram.png\n+   :alt: Callstack diagram of differnet apis.\n+\n+Let's imagine, that function ``a_fn`` is causing troubles with ``torch.compile``.\n+And this is a non-critical portion of the model. You can use ``_dynamo.disable``\n+on function ``a_fn``. As shown above, TorchDynamo will stop looking at frames\n+originating from the ``a_fn`` call (white color indicates original Python behavior).\n+\n+To skip compilation, you can decorate the offending function with\n+``@torch._dynamo.disable``.\n+\n+You can also use the non-decorator syntax if you don\u2019t want to change the source\n+code\n+However, we recommend that you avoid this style if possible. Here, you have to\n+take care that all users of the original function are now using the patched\n+version.\n+\n+``torch._dynamo.disallow_in_graph``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+``torch._dynamo.disallow_in_graph`` disallows an operator but not the function\n+to be present in the TorchDynamo extracted graph. Note that this is suitable\n+for operators and not general functions as in the case of ``_dynamo.disable``.\n+\n+Let's imagine you compile your model with PyTorch. TorchDynamo is able to\n+extract a graph, but then you see the downstream compiler failing. For example,\n+the meta kernel is missing, or some Autograd dispatch key is set incorrectly\n+for a particular operator. Then you can mark that operator as\n+``disallow_in_graph``, and TorchDynamo will cause a graph break and run that\n+operator by using the PyTorch eager mode.\n+\n+The catch is that you will have to find the corresponding Dynamo level operator,\n+and not the ATen level operator. See more in the Limitations section of the doc.\n+\n+.. warning:: \n+   ``torch._dynamo.disallow_in_graph`` is a global flag. If you are comparing\n+   different backend compilers, you might have to call ``allow_in_graph`` for\n+   the disallowed operator when switching to the other compiler.\n+\n+``torch._dynamo.allow_in_graph``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+``torch._dynamo.allow_in_graph`` is useful when the relevant function frame\n+has some known hard-to-support TorchDynamo feature, such as hooks and\n+``autograd.Function``, and you are confident that downstream PyTorch components\n+such as AOTAutograd can safely trace through the decorated function. When a\n+function is decorated with ``allow_in_graph``, TorchDynamo treats it as a\n+black-box and puts it as is in the generated graph.\n+\n+.. warning:: \n+   ``allow_in_graph`` skips TorchDynamo completely on the decorated function\n+   omitting all TorchDynamo safety checks, including graph breaks, handling\n+   closures, and others. Use `allow_in_graph` with caution. PyTorch downstream\n+   components, such as AOTAutograd rely on TorchDynamo to handle complex Python\n+   features, but ``allow_in_graph`` bypasses TorchDynamo. Using ``allow_in_graph``\n+   could lead to soundness and hard-to-debug issues.\n+\n+Limitations\n+~~~~~~~~~~~\n+\n+All the existing APIs are applied at the TorchDynamo level. Therefore, these\n+APIs have visibility to only what TorchDynamo sees. This can lead to confusing\n+scenarios.\n+\n+For example, ``_dynamo.disallow_in_graph`` will not work for ATen operators\n+because they are visible to AOT Autograd. For example,\n+``torch._dynamo.disallow_in_graph(torch.ops.aten.add)`` will not work in the\n+above example.\ndiff --git a/docs/source/torch.compiler_get_started.rst b/docs/source/torch.compiler_get_started.rst\nnew file mode 100644\nindex 00000000000000..c7d3b58a606300\n--- /dev/null\n+++ b/docs/source/torch.compiler_get_started.rst\n@@ -0,0 +1,151 @@\n+.. _torch.compiler_get_started:\n+\n+Getting Started\n+===============\n+\n+Before you read this section, make sure to read the :ref:`torch.compiler_overview`.\n+\n+Let\u2019s start by looking at a simple ``torch.compile`` example that demonstrates\n+how to use ``torch.compile`` for inference. This example demonstrates the\n+``torch.cos()`` and ``torch.sin()`` features which are examples of pointwise\n+operators as they operate element by element on a vector. This example might\n+not show significant performance gains but should help you form an intuitive\n+understanding of how you can use ``torch.compile`` in your own programs.\n+\n+.. note::\n+   To run this script, you need to have at least one GPU on your machine.\n+   If you do not have a GPU, you can remove the ``cuda()`` code from the\n+   code and it will run on CPU.\n+\n+.. code:: python\n+\n+   import torch\n+   def fn(x, y):\n+       a = torch.cos(x).cuda()\n+       b = torch.sin(y).cuda()\n+       return a + b\n+   new_fn = torch.compile(fn, backend=\"inductor\")\n+   input_tensor = torch.randn(10000).to(device=\"cuda:0\")\n+   a = new_fn(input_tensor, input_tensor)\n+\n+A more famous pointwise operator you might want to use would\n+be something like ``torch.relu()``. Pointwise ops in eager mode are\n+suboptimal because each one would need to read a tensor from\n+memory, make some changes, and then write back those changes. The single\n+most important optimization that inductor performs is fusion. In the\n+example above we can turn 2 reads and 2 writes into 1 read and 1 write which\n+is crucial especially for newer GPUs where the bottleneck is memory\n+bandwidth (how quickly you can send data to a GPU) rather than compute\n+(how quickly your GPU can crunch floating point operations).\n+\n+Another major optimization that inductor provides is automatic\n+support for CUDA graphs.\n+CUDA graphs help eliminate the overhead from launching individual\n+kernels from a Python program which is especially relevant for newer GPUs.\n+\n+TorchDynamo supports many different backends but TorchInductor specifically works\n+by generating `Triton <https://github.com/openai/triton>`__ kernels. Let's save\n+our example above into a file called ``example.py`` We can inspect the code\n+generated Triton kernels by running ``TORCH_COMPILE_DEBUG=1 python example.py``\n+As the script executes, you should see ``DEBUG`` messages printed to the\n+terminal. Closer to the end of the log, you should see a path to to a folder\n+that contains ``torchinductor_<your_username>``. In that folder, you can find\n+the ``output_code.py`` file that contains the generated kernel code similar to\n+the following:\n+\n+.. code-block:: python\n+\n+   @pointwise(size_hints=[16384], filename=__file__, meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n+   @triton.jit\n+   def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n+       xnumel = 10000\n+       xoffset = tl.program_id(0) * XBLOCK\n+       xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+       xmask = xindex < xnumel\n+       x0 = xindex\n+       tmp0 = tl.load(in_ptr0 + (x0), xmask)\n+       tmp1 = tl.cos(tmp0)\n+       tmp2 = tl.sin(tmp0)\n+       tmp3 = tmp1 + tmp2\n+       tl.store(out_ptr0 + (x0), tmp3, xmask)\n+\n+.. note:: The above is an extract and depending on your hardware, you will\n+   see different code generated.\n+\n+And you can verify that fusing the ``cos`` and ``sin`` did actually occur\n+because the ``cos`` and ``sin`` operations occur within a single Triton kernel\n+and the temporary variables are held in registers with very fast access.\n+\n+You can read up a lot more on Triton\u2019s performance\n+`here <https://openai.com/blog/triton/>`__ but the key is it\u2019s in Python\n+so you can easily understand it even if you have not written all that\n+many CUDA kernels.\n+\n+Next, let\u2019s try a real model like resnet50 from the PyTorch\n+hub.\n+\n+.. code-block:: python\n+\n+   import torch\n+   model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n+   opt_model = torch.compile(model, backend=\"inductor\")\n+   model(torch.randn(1,3,64,64))\n+\n+And that is not the only available backend, you can run in a REPL\n+``torch.compile.list_backends()`` to see all the available backends. Try out the\n+``cudagraphs`` or ``nvfuser`` next as inspiration.\n+\n+Using a pretrained model\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+PyTorch users frequently leverage pretrained models from\n+`transformers <https://github.com/huggingface/transformers>`__ or\n+`TIMM <https://github.com/rwightman/pytorch-image-models>`__ and one of\n+the design goals is TorchDynamo and TorchInductor is to work out of the box with\n+any model that people would like to author.\n+\n+Let's download a pretrained model directly from the HuggingFace hub and optimize\n+it:\n+\n+.. code-block:: python\n+\n+   import torch\n+   from transformers import BertTokenizer, BertModel\n+   # Copy pasted from here https://huggingface.co/bert-base-uncased\n+   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n+   model = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\n+   model = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\n+   text = \"Replace me by any text you'd like.\"\n+   encoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\n+   output = model(**encoded_input)\n+\n+If you remove the ``to(device=\"cuda:0\")`` from the model and\n+``encoded_input``, then Triton will generate C++ kernels that will be\n+optimized for running on your CPU. You can inspect both Triton or C++\n+kernels for BERT. They are more complex than the trigonometry\n+example we tried above but you can similarly skim through it and see if you\n+understand how PyTorch works.\n+\n+Similarly, let\u2019s try out a TIMM example:\n+\n+.. code-block:: python\n+\n+   import timm\n+   import torch._dynamo as dynamo\n+   import torch\n+   model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\n+   opt_model = torch.compile(model, backend=\"inductor\")\n+   opt_model(torch.randn(64,3,7,7))\n+\n+Our goal with TorchDynamo and TorchInductor is to build the highest coverage\n+ML compiler which should work with any model you throw at it.\n+\n+Next Steps\n+~~~~~~~~~~\n+\n+In this section, we have reviewed a few inference examples and developed a\n+basic understanding of how torch.compile works. Here is what you check out next:\n+\n+- `torch.compile tutorial on training <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_\n+- :ref:`torch.compiler_api`\n+- :ref:`torchdynamo_fine_grain_tracing`\ndiff --git a/docs/source/compile/guards-overview.rst b/docs/source/torch.compiler_guards_overview.rst\nsimilarity index 100%\nrename from docs/source/compile/guards-overview.rst\nrename to docs/source/torch.compiler_guards_overview.rst\ndiff --git a/docs/source/torch.compiler_inductor_profiling.rst b/docs/source/torch.compiler_inductor_profiling.rst\nnew file mode 100644\nindex 00000000000000..f7283db24f5979\n--- /dev/null\n+++ b/docs/source/torch.compiler_inductor_profiling.rst\n@@ -0,0 +1,177 @@\n+.. _torchinductor-gpu-profiling:\n+\n+TorchInductor GPU Profiling\n+===========================\n+\n+This section lists useful commands and workflows that can help\n+you dive into a model\u2019s performance in TorchInductor. When a model is not\n+running as fast as expected, you may want to check individual kernels of the\n+model. Usually, those kernels taking the majority of the\n+GPU time are the most interesting ones. After that, you\n+may also want to run individual kernels directly and inspect its perf.\n+PyTorch provides tools to cover everything mentioned above.\n+\n+Relevant Environment Variables\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+You can use the following environment variables in your analysis:\n+\n+-  ``TORCHINDUCTOR_UNIQUE_KERNEL_NAMES``\n+\n+   -  By default, TorchInductor names a Triton kernel as ``\u2018triton\\_\u2019``. When\n+      this environmental variable is enabled, inductor generates a more\n+      meaningful kernel name in the trace, for example,\n+      ``triton_poi_fused_cat_155`` which contains the kernel category\n+      (``poi`` for pointwise) and original ATen\n+      operator. This config is disabled by default to improve the chance of\n+      compilation cache hit.\n+\n+-  ``TORCHINDUCTOR_BENCHMARK_KERNEL``\n+\n+   -  Enabling this will make inductor codegen harness to benchmark\n+      individual triton kernels.\n+\n+-  ``TORCHINDUCTOR_MAX_AUTOTUNE``\n+\n+   -  Inductor autotuner will benchmark more ``triton.Configs`` and pick the\n+      one with the best performance results. This will increase compilation\n+      time with the hope to improve performance.\n+\n+Breakdown Model GPU Time\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Below are the steps to breakdown execution time of a model into\n+individual kernels. We take ``mixnet_l`` as an example.\n+\n+1. Run the benchmark script for the model:\n+\n+   .. code-block:: bash\n+\n+      TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1\n+      python -u benchmarks/dynamo/timm_models.py \u2013backend inductor \u2013amp\n+      \u2013performance \u2013dashboard \u2013only mixnet_l \u2013disable-cudagraphs \u2013training\n+\n+   .. note:: The tool relies on kernel name to decide its category. Enabling\n+      ``TORCHINDUCTOR_UNIQUE_KERNEL_NAMES`` is crucial for that.\n+\n+2. In the output log, look for lines:\n+\n+   .. code-block:: bash\n+\n+      **Compiled module path:\n+      /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n+\n+We have one line for each compiled module. If there are no extra graph\n+breaks, we would see 2 such lines in the log, one for the forward graph\n+and one for the backward graph.\n+\n+For our example command, we get the following compiled module for the\n+forward and backward graphs respectively:\n+\n+-  https://gist.github.com/shunting314/c2a4d8a28b00fcb5586d0e9d9bf77f9f\n+-  https://gist.github.com/shunting314/48efc83b12ec3ead950052e4a0220b10\n+\n+3. Now we can dive into the perf for each individual compiled module.\n+   Let\u2019s pick the one for the forward graph for illustration purposes.\n+   I\u2019ll name it ``fwd.py`` for convenience. Run it directly with the\n+   ``-p`` argument:\n+\n+   .. code-block:: bash\n+\n+      **> python fwd.py -p**\n+\n+See the full output log in this\n+`example gist <https://gist.github.com/shunting314/8243734a38b5733ea78479209c0ae893>`__.\n+\n+In the output, you can notice the following:\n+\n+* We write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.\n+\n+ **Chrome trace for the profile is written to\n+ /tmp/compiled_module_profile.json**\n+\n+ Loading the trace into Chrome (visit chrome://tracing in the chrome\n+ browser and load the file as the UI suggested) will show UI as follows:\n+\n+ .. image:: _static/img/inductor_profiling/trace.png\n+\n+ You can zoom in and out to check the profile.\n+\n+* We report the percent of GPU time regarding to the wall time by log line like:\n+\n+  **Percent of time when GPU is busy: 102.88%**\n+\n+  Sometimes you may see a value larger than 100%. The reason is because PyTorch\n+  uses the kernel execution time with profiling enabled while using wall time\n+  with profiling disabled. Profiling may distort the kernel execution time a\n+  bit. But overall it should not be a big deal.\n+\n+  If we run the model like ``densenet121`` with a small batch size, we would see\n+  low percent of time when GPU is busy:\n+\n+  ::\n+\n+     (Forward graph) Percent of time when GPU is busy: 32.69%\n+\n+  This means the model has a lot of CPU overhead. This is consistent with\n+  the fact that enabling cudagraphs improve densenet121\u2019s perf a lot.\n+\n+* We can break down the GPU time to different categories of kernels.\n+  In the ``mixnet_l`` example, we see\n+\n+  -  pointwise kernel takes 28.58%\n+  -  reduction kernel takes 13.85%\n+  -  persistent reduction kernel takes 3.89%\n+  -  the rest are cutlass/cudnn kernels for mm/conv which takes 56.57%\n+\n+  This information can be found in the summary line (last line)\n+  of the report for each kernel category.\n+\n+* We also call zoom into a certain category of kernels. For example,\n+  let\u2019s check reduction kernels:\n+\n+  .. image:: ../_static/img/inductor_profiling/kernel_breakdown.png\n+\n+  We can see an ordered table of execution time for each individual\n+  reduction kernel. We also see how many times a kernel is executed. This\n+  is helpful for a few reasons:\n+\n+  - If a kernel only takes a tiny amount of time, for example, 0.1%,\n+    improving it will at most bring 0.1% overall gain. It is not\n+    worth spending a lot of effort on it.\n+  - Ff a kernel takes 2% of time, improving it by 2x will bring in 1%\n+    overall gain which justifies the effort.\n+\n+Benchmark Individual Triton Kernel\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Let\u2019s say we want to take a closer look at\n+``triton_red_fused\\__native_batch_norm_legit_functional_16`` which is the\n+most expensive reduction kernel and takes 2.19% of overall wall time for\n+the forward graph.\n+\n+We can lookup the kernel name in the ``fwd.py``, and find comment like:\n+\n+**# kernel path:\n+/tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py**\n+\n+.. image:: ../_static/img/inductor_profiling/inductor_code.png\n+\n+I\u2019ll rename it k.py for convenience. Here is a paste for this\n+`file <https://gist.github.com/shunting314/96a0afef9dce53d6357bf1633094f358>`__.\n+\n+``k.py`` is a standalone Python module containing the kernel code and its\n+benchmark.\n+\n+Run ``k.py``directly will report its execution time and bandwidth:\n+\n+.. image:: ../_static/img/inductor_profiling/terminal_printout.png\n+\n+We can check if max-autotune helps this kernel, by running:\n+\n+.. code-block:: bash\n+\n+   **TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py**\n+\n+We may also temporarily add more reduction heuristics and run the script\n+again to check how that helps with the kernel.\ndiff --git a/docs/source/ir.rst b/docs/source/torch.compiler_ir.rst\nsimilarity index 100%\nrename from docs/source/ir.rst\nrename to docs/source/torch.compiler_ir.rst\ndiff --git a/docs/source/compile/nn-module.rst b/docs/source/torch.compiler_nn_module.rst\nsimilarity index 100%\nrename from docs/source/compile/nn-module.rst\nrename to docs/source/torch.compiler_nn_module.rst\ndiff --git a/docs/source/compile/performance-dashboard.rst b/docs/source/torch.compiler_performance_dashboard.rst\nsimilarity index 100%\nrename from docs/source/compile/performance-dashboard.rst\nrename to docs/source/torch.compiler_performance_dashboard.rst\ndiff --git a/docs/source/compile/profiling_torch_compile.rst b/docs/source/torch.compiler_profiling_torch_compile.rst\nsimilarity index 100%\nrename from docs/source/compile/profiling_torch_compile.rst\nrename to docs/source/torch.compiler_profiling_torch_compile.rst\ndiff --git a/docs/source/compile/transformations.rst b/docs/source/torch.compiler_transformations.rst\nsimilarity index 100%\nrename from docs/source/compile/transformations.rst\nrename to docs/source/torch.compiler_transformations.rst\ndiff --git a/docs/source/compile/troubleshooting.rst b/docs/source/torch.compiler_troubleshooting.rst\nsimilarity index 83%\nrename from docs/source/compile/troubleshooting.rst\nrename to docs/source/torch.compiler_troubleshooting.rst\nindex 486c3c791d8c39..628fa52ba10e17 100644\n--- a/docs/source/compile/troubleshooting.rst\n+++ b/docs/source/torch.compiler_troubleshooting.rst\n@@ -3,19 +3,8 @@ PyTorch 2.0 Troubleshooting\n \n **Author**: `Michael Lazos <https://github.com/mlazos>`_\n \n-`torch.compile` is still in active development, and many of the reasons for\n-graph breaks and excessive recompilation will be fixed with upcoming\n-support for `tracing dynamic tensor\n-shapes <https://docs.google.com/document/d/1QJB-GOnbv-9PygGlOMXwiO9K6vVNm8sNg_olixJ9koc/edit?usp=sharing>`__,\n-more careful choices for guards and better tuned heuristics.\n-\n-In the meantime, you may need to diagnose a particular issue and\n-determine if it is easy to work around with a change to your model, or\n-file an issue for support.\n-\n-Also, we are actively developing debug tools, profilers, and improving our\n-errors/warnings. Please give us feedback if you have an issue with this\n-infra, or an idea for an improvement. Below is a table of the available\n+We are actively developing debug tools, profilers, and improving our\n+error and warning messages. Below is a table of the available\n tools and their typical usage. For additional help see\n `Diagnosing Runtime Errors <#diagnosing-runtime-errors>`__.\n \n@@ -71,27 +60,26 @@ tools and their typical usage. For additional help see\n      - set the environment variable TORCH_COMPILE_DEBUG=1 or\n        ``torch._inductor.config.trace.enabled = True``\n \n-In addition to info and debug logging, you can use `torch._logging <https://pytorch.org/docs/main/logging.html>`__\n+In addition to info and debug logging,\n+you can use `torch._logging <https://pytorch.org/docs/main/logging.html>`__\n for more fine-grained logging.\n \n Diagnosing Runtime Errors\n ~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-Below is the TorchDynamo compiler stack.\n-\n At a high level, the TorchDynamo stack consists of a graph capture from\n-Python code (TorchDynamo) and a backend compiler. In this example, the\n-backend compiler consists of backward graph tracing (AOTAutograd) and\n+Python code (TorchDynamo) and a backend compiler. For example, a\n+backend compiler may consist of backward graph tracing (AOTAutograd) and\n graph lowering (TorchInductor)*. Errors can occur in any component of\n the stack and will provide full stack traces.\n \n-You may use info logging\n-(``torch._logging.set_logs(dynamo = logging.INFO)`` or ``TORCH_LOGS=\"dynamo\"``) and look for\n-``Step #: ...`` outputs in order to determine in which component the\n-error has occurred. Logs are made at the beginning and end of each step,\n-so the step that an error should correspond to is the most recent logged\n-step whose end has not yet been logged. The steps correspond to the\n-following parts of the stack (according to the image above):\n+To determine in which component an error occurred,\n+you may use info-level logging\n+``torch._logging.set_logs(dynamo = logging.INFO)`` or ``TORCH_LOGS=\"dynamo\"``\n+and look for``Step #: ...`` outputs. Logs are made at the beginning and end of\n+each step, so the step that an error should correspond to is the most recently\n+logged step whose end has not yet been logged. The steps correspond to the\n+following parts of the stack:\n \n ==== ================\n Step Component\n@@ -101,19 +89,14 @@ Step Component\n 3    TorchInductor\n ==== ================\n \n-The beginning and end of AOTAutograd is currently not logged, but we\n-plan to add it soon.\n-\n-If info logging is insufficient, then there are also some backend\n-options which can enable you to determine which component is causing the\n-error if you\u2019re unable to understand the error message that is\n-generated. These are the following:\n+If info logging is insufficient, you can use available backend\n+options. These options include:\n \n--  ``\"eager\"``: only runs torchdynamo forward graph capture and then\n+-  ``\"eager\"``: only runs TorchDynamo forward graph capture and then\n    runs the captured graph with PyTorch. This provides an indication as\n    to whether TorchDynamo is raising the error.\n \n--  ``\"aot_eager\"``: runs torchdynamo to capture a forward graph, and\n+-  ``\"aot_eager\"``: runs TorchDynamo to capture a forward graph, and\n    then AOTAutograd to trace the backward graph without any additional\n    backend compiler steps. PyTorch eager will then be used to run the\n    forward and backward graphs. This is useful to narrow down the issue\n@@ -147,7 +130,7 @@ Torchdynamo Errors\n ------------------\n \n If the error that is generated occurs with the ``\"eager\"`` backend, then\n-TorchDynamo is the most likely source of the error. Here is a sample code\n+TorchDynamo is most likely the source of the error. Here is a sample code\n which will generate an error.\n \n .. code-block:: py\n@@ -166,7 +149,7 @@ which will generate an error.\n \n    compiled_test_assertion_error()\n \n-Which will generate the following error:\n+The code above generates the following error:\n \n ::\n \n@@ -187,29 +170,30 @@ Which will generate the following error:\n As the message suggests you can set\n ``torch._dynamo.config.verbose=True`` to get a full stack trace to both\n the error in TorchDynamo and the user code. In addition to this flag,\n-you can also set the ``log_level`` of torchdynamo through\n-``torch._dynamo.config.log_level``. The available levels are the\n-following:\n+you can also set the ``log_level`` of TorchDynamo through\n+``torch._dynamo.config.log_level``. These levels include:\n \n - ``logging.DEBUG``: Print every instruction that is\n-  encountered in addition to all below log levels.\n+  encountered in addition to all the log levels listed below.\n - ``logging.INFO``:\n   Print each function that is compiled (original and modified bytecode)\n-  and the graph that is captured in addition to all below log levels.\n+  and the graph that is captured in addition to all the log levels listed below.\n - ``logging.WARNING`` (default): Print graph breaks in addition to all\n-  below log levels.\n+  the log levels listed below.\n - ``logging.ERROR``: Print errors only.\n \n-If a model is sufficiently large, the logs can become overwhelming. If\n+If a model is very large, the logs can become overwhelming. If\n an error occurs deep within a model's Python code, it can be useful to\n execute only the frame in which the error occurs to enable easier\n debugging. There are two tools available to enable this:\n \n-- Setting the environment variable ``TORCHDYNAMO_DEBUG_FUNCTION`` to the desired function name will only run torchdynamo on functions with that name.\n+- Setting the environment variable ``TORCHDYNAMO_DEBUG_FUNCTION``\n+to the desired function name will only run torchdynamo on functions with that\n+name.\n - Enabling the record/replay tool (set ``torch._dynamo.config.replay_record_enabled = True``) which dumps an execution record when an error is encountered. This record can then be replayed to run only the frame where an error occurred.\n \n-TorchInductor Errors\n---------------------\n+Diagnosing TorchInductor Errors\n+-------------------------------\n \n If the error does not occur with the ``\"eager\"`` backend, then the\n backend compiler is the source of the error (`example\n@@ -218,7 +202,7 @@ There are `different\n choices <https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#existing-backends>`__\n for backend compilers for TorchDynamo, with TorchInductor or nvfuser\n fitting the needs of most users. This section focuses on TorchInductor\n-as the motivating example, but some tools will be usable with other\n+as the motivating example, but some tools can also be used with other\n backend compilers.\n \n Below is the portion of the stack which we are focusing on:\n@@ -292,7 +276,7 @@ environment variable ``TORCHDYNAMO_REPRO_AFTER=\u201caot\u201d`` (or setting\n ``torch._dynamo.config.repro_after=\"aot\"`` directly) will generate a\n Python program which reduces the graph produced by AOTAutograd to the\n smallest subgraph which reproduces the error. (See below for an example\n-where we minify the graph produced by torchdynamo) Running the program\n+where we minify the graph produced by TorchDynamo) Running the program\n with this environment variable should show nearly `identical\n output <https://gist.github.com/mlazos/0458ab828aa403c779fe73c012aa5982>`__,\n with an additional line indicating where ``minifier_launcher.py`` has\n@@ -395,7 +379,7 @@ this program with ``TORCHDYNAMO_REPRO_AFTER=\u201cdynamo\u201d`` (or\n output <https://gist.github.com/mlazos/244e3d5b53667e44078e194762c0c92b>`__\\ and\n the following code in ``{torch._dynamo.config.base_dir}/repro.py``.\n \n-.. note:: The other option for TORCHDYNAMO_REPRO_AFTER are ``\"aot\"``, which\n+.. note:: The other option for TORCHDYNAMO_REPRO_AFTER is ``\"aot\"``, which\n    will run the minifier after the backward graph has been generated.\n \n .. code-block:: python\n@@ -444,7 +428,7 @@ Performance Profiling\n Accessing TorchDynamo Profiler\n ------------------------------\n \n-TorchDynamo has a builtin stats function for collecting and displaying\n+TorchDynamo has a built-in stats function for collecting and displaying\n the time spent in each compilation phase. These stats can be accessed by\n calling ``torch._dynamo.utils.compile_times()`` after executing\n Torch._Dynamo. By default, this returns a string representation of the\n@@ -458,7 +442,7 @@ spent in each compilation phase, output code, output graph visualization\n and IR dump. This is a debugging tool designed to make it easier to\n understand and troubleshoot the internals of TorchInductor.\n \n-Let's run an example with the following test program (repro.py):\n+Let's run an example with the following test program (``repro.py``):\n \n ::\n \n@@ -477,9 +461,12 @@ Let's run an example with the following test program (repro.py):\n   y = test_model(torch.ones(10, 10))\n \n Setting the environment variable ``TORCH_COMPILE_DEBUG=1`` will cause a\n-debug trace directory to be created, by default this directory will be in the current directory and named torch_compile_debug\n-(this can be overridden in the torchdynamo configuration field ``debug_dir_root`` and also the env var TORCH_COMPILE_DEBUG_DIR).\n-Inside this directory, each run will have a separate folder named with the timestamp and process id of the run:\n+debug trace directory to be created, by default this directory will be in the\n+current directory and named torch_compile_debug (this can be overridden in\n+the torchdynamo configuration field ``debug_dir_root`` and also the\n+``env var TORCH_COMPILE_DEBUG_DIR``). Inside this directory, each run will\n+have a separate folder named with the timestamp and process id of the run:\n+\n ::\n \n    $ env TORCH_COMPILE_DEBUG=1 python repro.py\n@@ -487,8 +474,9 @@ Inside this directory, each run will have a separate folder named with the times\n    $ ls\n    run_2023_03_01_08_20_52_143510-pid_180167\n \n-In the run folder there will be a torchdynamo directory which contains debug logs, and an torchinductor\n-folder which contains a subfolder for each compiled kernel with inductor debug artifacts.\n+In the run folder there will be a ``torchdynamo`` directory which contains\n+debug logs, and an ``torchinductor`` folder which contains a subfolder for each\n+compiled kernel with inductor debug artifacts.\n \n ::\n \n@@ -497,7 +485,9 @@ folder which contains a subfolder for each compiled kernel with inductor debug a\n    $ ls\n    torchinductor  torchdynamo\n \n-Moving further into the torchinductor directory, the \\*.log files are logs from the aot autograd phase of compilation, model__0_forward_1.0 contains the inductor debug artifacts.\n+Moving further into the ``torchinductor`` directory, the ``\\*.log`` files are\n+logs from the AOT Autograd phase of compilation, ``model__0_forward_1.0`` contains\n+the inductor debug artifacts.\n \n ::\n \n@@ -509,10 +499,11 @@ Moving further into the torchinductor directory, the \\*.log files are logs from\n    debug.log  fx_graph_readable.py  fx_graph_runnable.py  fx_graph_transformed.py  ir_post_fusion.txt  ir_pre_fusion.txt  output_code.py\n \n Here is a summary of the contents:\n- - fx_graph_readable.py and fx_graph_runnable.py are the readable and runnable versions of the fx_graph received by inductor.\n- - fx_graph_transformed.py is the fx graph after inductor has run all fx passes.\n- - ir\\*.txt is the inductor ir pre and post fusion.\n- - output_code.py is the compiled triton kernel for the subgraph.\n+ - ``fx_graph_readable.py`` and ``fx_graph_runnable.py`` are the readable and\n+ runnable versions of the ``fx_graph`` received by inductor.\n+ - ``fx_graph_transformed.py`` is the fx graph after inductor has run all fx passes.\n+ - ``ir\\*.txt`` is the inductor ir pre and post fusion.\n+ - ``output_code.py`` is the compiled triton kernel for the subgraph.\n \n Here are `example debug directory contents\n <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\n@@ -680,7 +671,7 @@ cost of recompilation outweighs any optimization benefits.\n \n    torch._dynamo.config.cache_size_limit = <your desired cache limit>\n \n-Torchdynamo plans to support many common cases of dynamic tensor shapes,\n+TorchDynamo plans to support many common cases of dynamic tensor shapes,\n such as varying batch size or sequence length. It does not plan to\n support rank-dynamism. In the meantime, setting a specific cache limit\n can be used in coordination with bucketing techniques to achieve an\n@@ -714,29 +705,3 @@ to detect bugs in our codegen or with a backend compiler.\n If you'd like to ensure that random number generation is the same across both torch\n and triton then you can enable ``torch._inductor.config.fallback_random = True``\n \n-File an Issue\n-~~~~~~~~~~~~~\n-\n-If you experience problems with TorchDynamo, `file a GitHub\n-issue <https://github.com/pytorch/torchdynamo/issues>`__.\n-\n-Before filing an issue, read over the `README <../README.md>`__,\n-`TROUBLESHOOTING <./TROUBLESHOOTING.md>`__, and search for similar\n-issues.\n-\n-When filing an issue, include the information about your\n-OS, Python< PyTorch, CUDA, and Triton versions info by running:\n-\n-.. code-block:: shell\n-\n-   python tools/verify_install.py\n-\n--  A minimal repro script if possible, which can be generated by running\n-   Minifier\n--  A description of the error\n--  The expected behavior\n--  A log (set ``torch._dynamo.config.log_file`` to a valid file name to\n-   dump the logs to a file and\n-   ``torch._logging.set_logs(dynamo = logging.DEBUG)`` and\n-   ``torch._dynamo.config.verbose = True``), or\n-   ``TORCH_LOGS=\"+dynamo\" TORCHDYNAMO_VERBOSE=1``\n\nFrom 9f9c928de166d8662020a2f2a4041726079404e1 Mon Sep 17 00:00:00 2001\nFrom: Svetlana Karslioglu <svekars@fb.com>\nDate: Mon, 17 Jul 2023 17:07:19 -0700\nSubject: [PATCH 2/3] Fix lint\n\n---\n docs/source/torch.compiler_faq.rst | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/docs/source/torch.compiler_faq.rst b/docs/source/torch.compiler_faq.rst\nindex 9190ec9292acf6..8e74fe42d22d4a 100644\n--- a/docs/source/torch.compiler_faq.rst\n+++ b/docs/source/torch.compiler_faq.rst\n@@ -450,8 +450,9 @@ PyTorch to do. You need to be more specific about your use case. Some of the\n most common use cases you might want to consider:\n \n * If you want to disable compilation on this function frame and the\n-  recursively invoked frames \u2014\u00a0use ``_dynamo.disable``.\n-* If you want a particular operator, such as ``fbgemm`` to use the  eager mode \u2013\n+  recursively invoked frames,\u00a0use ``_dynamo.disable``.\n+\n+* If you want a particular operator, such as ``fbgemm`` to use the  eager mode,\n   use ``_dynamo.disallow_in_graph``.\n \n Some of the uncommon use cases include:\n\nFrom c4a1bb3d246da78665abefd72c117da4e49c0e2b Mon Sep 17 00:00:00 2001\nFrom: Svetlana Karslioglu <svekars@fb.com>\nDate: Tue, 18 Jul 2023 11:42:15 -0700\nSubject: [PATCH 3/3] Address feedback\n\n---\n docs/source/torch.compiler.rst                |  9 +++---\n docs/source/torch.compiler_deepdive.rst       | 25 ++++++++++++++-\n docs/source/torch.compiler_faq.rst            | 25 ++++++++-------\n .../source/torch.compiler_fine_grain_apis.rst | 32 +++++++++++--------\n docs/source/torch.compiler_get_started.rst    |  3 --\n 5 files changed, 60 insertions(+), 34 deletions(-)\n\ndiff --git a/docs/source/torch.compiler.rst b/docs/source/torch.compiler.rst\nindex 02167ca5fd7a8e..9a963bfe54fac9 100644\n--- a/docs/source/torch.compiler.rst\n+++ b/docs/source/torch.compiler.rst\n@@ -30,16 +30,15 @@ written in PyThon and it marks the transition of PyTorch from C++ to PyThon.\n   acceleration of both forwards and backwards pass using TorchInductor.\n \n .. note:: In some cases, the terms ``torch.compile``, TorchDynamo, ``torch.compiler``\n-  might be used interchangeably in this documentation.\n+   might be used interchangeably in this documentation.\n \n As mentioned above, to run your workflows faster, ``torch.compile`` through\n TorchDynamo requires a backend that converts the captured graphs into fast\n machine code. Different backends can result in various optimization gains.\n The default backend is called TorchInductor, also known as *inductor*,\n-TorchDynamo has a growing list of backends developed by our partners,\n-which can be found in the `backends <https://github.com/pytorch/pytorch/tree/main/torch/_dynamo/backends>`_\n-folder or by running  ``torch.compile.list_backends()`` each of which with its\n-optional dependencies.\n+TorchDynamo has a list of supported backends developed by our partners,\n+which can be see by running ``torch.compile.list_backends()`` each of which\n+with its optional dependencies.\n \n Some of the most commonly used backends include:\n \ndiff --git a/docs/source/torch.compiler_deepdive.rst b/docs/source/torch.compiler_deepdive.rst\nindex 32eb7b7ef49a01..603804934607f1 100644\n--- a/docs/source/torch.compiler_deepdive.rst\n+++ b/docs/source/torch.compiler_deepdive.rst\n@@ -49,7 +49,30 @@ What is a guard?\n ----------------\n \n TorchDynamo operates just-in-time and specializes graphs based on\n-dynamic properties. For example, the first graph above has the following\n+dynamic properties. Below is a basic example of how to use TorchDynamo.\n+One can decorate a function or a method using ``torchdynamo.optimize`` to enable\n+TorchDynamo optimization:\n+\n+.. code-block:: python\n+\n+   from typing import List\n+   import torch\n+   from torch import _dynamo as torchdynamo\n+   def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n+       print(\"my_compiler() called with FX graph:\")\n+       gm.graph.print_tabular()\n+       return gm.forward  # return a python callable\n+\n+   @torchdynamo.optimize(my_compiler)\n+   def toy_example(a, b):\n+       x = a / (torch.abs(a) + 1)\n+       if b.sum() < 0:\n+           b = b * -1\n+       return x * b\n+   for _ in range(100):\n+       toy_example(torch.randn(10), torch.randn(10))\n+\n+For example, the first graph above has the following\n guards:\n \n ::\ndiff --git a/docs/source/torch.compiler_faq.rst b/docs/source/torch.compiler_faq.rst\nindex 8e74fe42d22d4a..ab221c7ca688e2 100644\n--- a/docs/source/torch.compiler_faq.rst\n+++ b/docs/source/torch.compiler_faq.rst\n@@ -449,22 +449,25 @@ Graph break on a function is not enough to sufficiently express what you  want\n PyTorch to do. You need to be more specific about your use case. Some of the\n most common use cases you might want to consider:\n \n+* If you want to disable compilation on this function frame and the recursively\n+  invoked frames, use ``torch._dynamo.disable\n+\n * If you want to disable compilation on this function frame and the\n-  recursively invoked frames,\u00a0use ``_dynamo.disable``.\n+  recursively invoked frames,\u00a0use ``torch._dynamo.disable``.\n \n * If you want a particular operator, such as ``fbgemm`` to use the  eager mode,\n-  use ``_dynamo.disallow_in_graph``.\n+  use ``torch._dynamo.disallow_in_graph``.\n \n Some of the uncommon use cases include:\n \n * If you want to disable TorchDynamo on the function frame but enable it back\n-  on the recursively invoked frames \u2013 use ``_dynamo.disable(recursive=False)``.\n+  on the recursively invoked frames \u2013 use ``torch._dynamo.disable(recursive=False)``.\n \n-* If you want to prevent inlining of a function frame \u2013 use ``_dynamo.graph_break``\n+* If you want to prevent inlining of a function frame \u2013 use ``torch._dynamo.graph_break``\n   at the beginning of the function you want to prevent inlining.\n \n-What's the difference between ``_dynamo.disable`` and ``_dynamo.disallow_in_graph``\n------------------------------------------------------------------------------------\n+What's the difference between ``torch._dynamo.disable`` and ``torch._dynamo.disallow_in_graph``\n+-----------------------------------------------------------------------------------------------\n \n Disallow-in-graph works at the level of operators, or more specifically,\n the operators that you see in the TorchDynamo extracted graphs.\n@@ -472,13 +475,13 @@ the operators that you see in the TorchDynamo extracted graphs.\n Disable works at the function frame level and decides if TorchDynamo\n should look into the function frame or not.\n \n-What's the difference between ``_dynamo.disable`` and ``_dynamo_skip``\n-----------------------------------------------------------------------\n+What's the difference between ``torch._dynamo.disable`` and ``torch._dynamo_skip``\n+----------------------------------------------------------------------------------\n \n .. note::\n-   ``_dynamo_skip`` is deprecated.\n+   ``torch._dynamo_skip`` is deprecated.\n \n-You most likely need ``_dynamo.disable``. But in an unlikely scenario, you\n+You most likely need ``torch._dynamo.disable``. But in an unlikely scenario, you\n might need even finer control. Suppose you want to disable the tracing on just\n the ``a_fn`` function, but want to continue the tracing back in ``aa_fn`` and\n ``ab_fn``. The image below demonstrates this use case:\n@@ -488,5 +491,5 @@ the ``a_fn`` function, but want to continue the tracing back in ``aa_fn`` and\n    :alt: diagram of torch.compile + disable(a_fn, recursive=False)\n \n In this case, you can use ``torch._dynamo.disable(recursive=False)``.\n-In previous versions, this functionality was provided by ``_dynamo.skip``.\n+In previous versions, this functionality was provided by ``torch._dynamo.skip``.\n This is now supported by the ``recursive`` flag inside ``torch._dynamo.disable``.\ndiff --git a/docs/source/torch.compiler_fine_grain_apis.rst b/docs/source/torch.compiler_fine_grain_apis.rst\nindex 107621818ce2fe..7806f371e280c9 100644\n--- a/docs/source/torch.compiler_fine_grain_apis.rst\n+++ b/docs/source/torch.compiler_fine_grain_apis.rst\n@@ -1,11 +1,15 @@\n .. _torchdynamo_fine_grain_tracing:\n \n-TorchDynamo APIs to control fine-grained tracing\n-================================================\n+TorchDynamo APIs for fine-grained tracing\n+=========================================\n+\n+.. note:: In this document ``torch.compiler.compile`` and\n+   ``torch.compile`` are used interchangeably. Both versions\n+   will work in your code.\n \n ``torch.compile`` performs TorchDynamo tracing on the whole user model.\n However, it is possible that a small part of the model code cannot be\n-handeled by ``torch.cmpile``. In this case, you might want to disable\n+handeled by ``torch.compiler``. In this case, you might want to disable\n the compiler on that particular portion, while running compilation on\n the rest of the model. This section describe the existing APIs that\n use to define parts of your code in which you want to skip compilation\n@@ -18,15 +22,15 @@ disable compilation are listed in the following table:\n    :header: \"API\", \"Description\", \"When to use?\"\n    :widths: auto\n \n-   \"``torch._dynamo.disable``\", \"Disables Dynamo on the decorated function as well as recursively invoked functions.\", \"Excellent for unblocking a user, if a small portion of the model cannot be handeled with ``torch.compile``.\"\n-   \"``torch._dynamo.disallow_in_graph``\", \"Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.\\n\\nThis is suitable for the ops, while ``_dynamo.disable`` is suitable for decorating functions.\", \"This API is excellent for both debugging and unblocking if a custom op like ``torch.ops.fbgemm.*`` is causing issues with ``torch.compile``.\"\n-   \"``torch._dynamo.allow_in_graph``\", \"The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.\\n\\nNote that AOT Autograd will trace through it, so the ``allow_in_graph`` is only a Dynamo-level concept.\", \"This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or ``autograd.Function``. However, each usage of ``allow_in_graph`` **must be carefully screened** (no graph breaks, no closures).\"\n+   \"``torch.compiler.disable``\", \"Disables Dynamo on the decorated function as well as recursively invoked functions.\", \"Excellent for unblocking a user, if a small portion of the model cannot be handeled with ``torch.compile``.\"\n+   \"``torch._dynamo.disallow_in_graph``\", \"Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.\\n\\nThis is suitable for the ops, while ``torch.compiler.disable`` is suitable for decorating functions.\", \"This API is excellent for both debugging and unblocking if a custom op like ``torch.ops.fbgemm.*`` is causing issues with the ``torch.compile`` function.\"\n+   \"``torch.compile.allow_in_graph``\", \"The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.\\n\\nNote that AOT Autograd will trace through it, so the ``allow_in_graph`` is only a Dynamo-level concept.\", \"This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or ``autograd.Function``. However, each usage of ``allow_in_graph`` **must be carefully screened** (no graph breaks, no closures).\"\n    \"``torch._dynamo.graph_break``\", \"Adds a graph break. The code before and after the graph break goes through TorchDynamo.\", \"**Rarely useful for deployment** - If you think you need this, most probably you need either ``disable`` or ``disallow_in_graph``.\"\n \n-``torch._dynamo.disable``\n-~~~~~~~~~~~~~~~~~~~~~~~~~\n+``torch.compiler.disable``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-``torch._dynamo.disable`` disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\n+``torch.compiler.disable`` disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\n \n TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function ``fn`` calls functions ``a_fn`` and ``b_fn``. And ``a_fn`` calls ``aa_fn`` and ``ab_fn``. When you use the PyTorch eager mode rather than ``torch.compile``, these function frames run as is. With ``torch.compile``, TorchDynamo intercepts each of these function frames (indicated by the green color):\n \n@@ -34,12 +38,12 @@ TorchDynamo intercepts the execution of each Python function frame. So, suppose\n    :alt: Callstack diagram of differnet apis.\n \n Let's imagine, that function ``a_fn`` is causing troubles with ``torch.compile``.\n-And this is a non-critical portion of the model. You can use ``_dynamo.disable``\n+And this is a non-critical portion of the model. You can use ``compiler.disable``\n on function ``a_fn``. As shown above, TorchDynamo will stop looking at frames\n originating from the ``a_fn`` call (white color indicates original Python behavior).\n \n To skip compilation, you can decorate the offending function with\n-``@torch._dynamo.disable``.\n+``@torch.compiler.disable``.\n \n You can also use the non-decorator syntax if you don\u2019t want to change the source\n code\n@@ -69,10 +73,10 @@ and not the ATen level operator. See more in the Limitations section of the doc.\n    different backend compilers, you might have to call ``allow_in_graph`` for\n    the disallowed operator when switching to the other compiler.\n \n-``torch._dynamo.allow_in_graph``\n+``torch.compiler.allow_in_graph``\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-``torch._dynamo.allow_in_graph`` is useful when the relevant function frame\n+``torch.compiler.allow_in_graph`` is useful when the relevant function frame\n has some known hard-to-support TorchDynamo feature, such as hooks and\n ``autograd.Function``, and you are confident that downstream PyTorch components\n such as AOTAutograd can safely trace through the decorated function. When a\n@@ -94,7 +98,7 @@ All the existing APIs are applied at the TorchDynamo level. Therefore, these\n APIs have visibility to only what TorchDynamo sees. This can lead to confusing\n scenarios.\n \n-For example, ``_dynamo.disallow_in_graph`` will not work for ATen operators\n+For example, ``torch._dynamo.disallow_in_graph`` will not work for ATen operators\n because they are visible to AOT Autograd. For example,\n ``torch._dynamo.disallow_in_graph(torch.ops.aten.add)`` will not work in the\n above example.\ndiff --git a/docs/source/torch.compiler_get_started.rst b/docs/source/torch.compiler_get_started.rst\nindex c7d3b58a606300..453a6fecf7a2b9 100644\n--- a/docs/source/torch.compiler_get_started.rst\n+++ b/docs/source/torch.compiler_get_started.rst\n@@ -137,9 +137,6 @@ Similarly, let\u2019s try out a TIMM example:\n    opt_model = torch.compile(model, backend=\"inductor\")\n    opt_model(torch.randn(64,3,7,7))\n \n-Our goal with TorchDynamo and TorchInductor is to build the highest coverage\n-ML compiler which should work with any model you throw at it.\n-\n Next Steps\n ~~~~~~~~~~\n \n"
  },
  {
    "number": 105375,
    "title": "[pt2][inductor] only use global cache on MAST",
    "body": "Summary:\nuntil we can further investigate the autotuning differences between MAST and non-MAST (devserver) environments, turn off the global cache for all non-MAST environments. this ensures we don't see unexpected regressions\n\nalso update scuba logging for cache lookup, and add scuba logging for autotuning results.\n\nTest Plan: sandcastle + CI\n\nDifferential Revision: D47516633\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @ngimel @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov",
    "merge_commit_sha": "0b50c48bd92d5259edae8bbbab3d0b99c4c7d15f",
    "url": "https://api.github.com/repos/pytorch/pytorch/pulls/105375",
    "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105375/commits",
    "diff_url": "https://github.com/pytorch/pytorch/pull/105375.diff",
    "patch_url": "https://github.com/pytorch/pytorch/pull/105375.patch",
    "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/105375/comments",
    "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/105375/comments",
    "labels": [
      "Merged",
      "fb-exported",
      "ciflow/inductor",
      "ciflow/trunk",
      "module: inductor"
    ],
    "_event_time": "2023-07-17T23:31:45.301354Z",
    "state": "closed",
    "patch": "From 022fcc9c71a521dea8c9d7e400e93cc4426025f5 Mon Sep 17 00:00:00 2001\nFrom: Nicolas Macchioni <nmacchioni@meta.com>\nDate: Mon, 17 Jul 2023 16:29:41 -0700\nSubject: [PATCH] [pt2][inductor] only use global cache on MAST\n\nSummary:\nuntil we can further investigate the autotuning differences between MAST and non-MAST (devserver) environments, turn off the global cache for all non-MAST environments. this ensures we don't see unexpected regressions\n\nalso update scuba logging for cache lookup, and add scuba logging for autotuning results.\n\nTest Plan: sandcastle + CI\n\nDifferential Revision: D47516633\n\nfbshipit-source-id: 1dcd42c360572f9d24db5eeaa83ba8cff642baa3\n---\n torch/_inductor/codecache.py  | 36 ++++++++++++++++++++++++-----------\n torch/_inductor/compile_fx.py |  2 +-\n 2 files changed, 26 insertions(+), 12 deletions(-)\n\ndiff --git a/torch/_inductor/codecache.py b/torch/_inductor/codecache.py\nindex fd1f4228c9d29a..ade4a01a440680 100644\n--- a/torch/_inductor/codecache.py\n+++ b/torch/_inductor/codecache.py\n@@ -44,12 +44,22 @@\n     from triton.fb import build_paths\n     from triton.fb.build import _run_build_command\n \n-    from torch._inductor.fb.logging import global_cache_log\n+    from torch._inductor.fb.utils import (\n+        log_global_cache_stats,\n+        log_global_cache_vals,\n+        use_global_cache,\n+    )\n else:\n \n-    def global_cache_log(*args, **kwargs):\n+    def log_global_cache_stats(*args, **kwargs):\n+        pass\n+\n+    def log_global_cache_vals(*args, **kwargs):\n         pass\n \n+    def use_global_cache():\n+        return False\n+\n \n LOCK_TIMEOUT = 600\n \n@@ -224,7 +234,8 @@ def lookup(\n                 b. `max_autotune_gemm=False`: don't benchmark the choice, return nothing.\n         \"\"\"\n \n-        gc_log = partial(global_cache_log, self.system, name, inputs)\n+        log_stats = partial(log_global_cache_stats, self.system, name, inputs)\n+        log_vals = partial(log_global_cache_vals, self.system, name, inputs)\n         timings = {}\n \n         def check_cache(cache, callback=None):\n@@ -235,20 +246,20 @@ def check_cache(cache, callback=None):\n                 if choice_hash in cache.get(name, {}).get(inputs, {}):\n                     # cache hit\n                     timings[choice] = cache[name][inputs][choice_hash]\n-                    if callback:\n-                        callback(choice_hash, cached=True)\n                 else:\n                     # cache miss\n                     hit = False\n-                    if callback:\n-                        callback(choice_hash, cached=False)\n+                    break\n+            if callback:\n+                callback(cached=hit)\n             return hit\n \n         if config.max_autotune or config.max_autotune_gemm:\n             local_cache = self.get_local_cache()\n             # check local cache first since it is data specific to the current machine\n-            if not check_cache(local_cache) and not check_cache(\n-                self.get_global_cache(), callback=gc_log\n+            if not check_cache(local_cache) and not (\n+                use_global_cache()\n+                and check_cache(self.get_global_cache(), callback=log_stats)\n             ):\n                 # re-benchmark everything to try to get consistent numbers from the same machine\n                 for choice in choices:\n@@ -258,9 +269,12 @@ def check_cache(cache, callback=None):\n                     local_cache[name][inputs][choice.hash_key()] = timings[choice]\n \n                 self.update_local_cache(local_cache)\n-        else:\n+\n+                if use_global_cache():\n+                    log_vals(timings)\n+        elif use_global_cache():\n             # only check global cache, not local one\n-            check_cache(self.get_global_cache(), callback=gc_log)\n+            check_cache(self.get_global_cache(), callback=log_stats)\n             # may have a partial cache hit, where not everything is benchmarked\n \n         return timings\ndiff --git a/torch/_inductor/compile_fx.py b/torch/_inductor/compile_fx.py\nindex 0f6d539d7710aa..3719da801b8d9f 100644\n--- a/torch/_inductor/compile_fx.py\n+++ b/torch/_inductor/compile_fx.py\n@@ -38,7 +38,7 @@\n from .virtualized import V\n \n if config.is_fbcode():\n-    from torch._inductor.fb.logging import time_and_log\n+    from torch._inductor.fb.utils import time_and_log\n else:\n     # no-op decorator\n     def time_and_log(attr: str):\n"
  }
]
